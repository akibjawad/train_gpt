# DeepSpeed configuration file for training a model with DeepSpeed
# number of tokens processed accross all GPUs per step
# from the paper gpt3 per step 524288 tokens are processed
# processing 524288 tokens per step 
# one batch has 1024 tokens
block_size: 1024
# hence 524288 / 1024 = 512 batches per step
train_batch_size: 512

# micro batch size: This is the batch size per forward/backward pass on each device (GPU)
train_micro_batch_size_per_gpu: 2

# Running on 2 (data parallel size) gpus
# number of steps to accumulate gradients before performing an optimizer step
# gradient accumulation steps of 4
gradient_accumulation_steps: 256

# total_batch_size = micro_batch_size * gradient_accumulation_steps * number_of_gpus

# optimizer is configured manually in the training script

# schduler is also configured manually in the training script

steps_per_print: 100

zero_optimization:
    stage: 1
    offload_optimizer: 
        device: none
        # pin_memory: true
    offload_param:
        device: none
        # pin_memory: true

gradient_clipping: 1.0

pipeline:
  enabled: true

wall_clock_breakdown: true