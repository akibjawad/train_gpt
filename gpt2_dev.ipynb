{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "print(whoami())  # Should return your Hugging Face account info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ba030-8d60-4544-a1e8-ed20ad8875ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2') #124M\n",
    "sd_model_hf = model_hf.state_dict()\n",
    "\n",
    "for k,v in sd_model_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_hf['transformer.wpe.weight'].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test plotting weight values\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_model_hf['transformer.wpe.weight'], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aefa669",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_model_hf['transformer.wpe.weight'][:,150])\n",
    "plt.plot(sd_model_hf['transformer.wpe.weight'][:,200])\n",
    "plt.plot(sd_model_hf['transformer.wpe.weight'][:,250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, each feature of the embedding, accross all positions (0,1023), becomes like a sine/cosine wave\n",
    "# Model can use this to find relationships between tokens at different positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_model_hf['transformer.h.1.attn.c_attn.weight'][:300,:300], cmap='gray') #plotting the hidden layer 0 weights\n",
    "# weight shows some structure, meaning the model has been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smapling from the model\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "print(generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available, using GPU\")\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"MPS is available, using GPU\")\n",
    "    device = 'mps'\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2') #124M\n",
    "model_hf.eval()\n",
    "model_hf.to(device)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "tokens = [15496, 11, 314, 1101, 716, 257, 3303, 2746, 11] # Hello, I'm a language model\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(5,1) # add batch dimension\n",
    "x = tokens.to(device)\n",
    "\n",
    "max_length = 30\n",
    "max_return_sequences = 5\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_hf(x)[0] # (B=5, T=8, vocab_size=50257)\n",
    "        # get the logits at the last token\n",
    "        logits = logits[:, -1, :] # (B=5, vocab_size=50257)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do a top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (B=5, k=50), topk_indices becomes (B=5, k=50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        ix = torch.multinomial(topk_probs, num_samples=1) #(B=5, 1)\n",
    "        # get the corresponding index\n",
    "        xcol = torch.gather(topk_indices, dim=-1, index=ix)\n",
    "        # append the new token to the input\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated tokens\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "for i in range(max_return_sequences):\n",
    "    tokens = x[i, :max_length:].tolist() # (30,)\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\">{decoded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53114603",
   "metadata": {},
   "source": [
    "## Model weights of Embedding layer and the lm_head layer is same, you can reuse the same tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2afe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_hf['transformer.wpe.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_hf['lm_head.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even they are the same tensor with the same pointers\n",
    "sd_model_hf['transformer.wte.weight'].data_ptr() == sd_model_hf['lm_head.weight'].data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you keep adding (in the residual path) variance of the weights will grow\n",
    "# test\n",
    "import math\n",
    "x = torch.zeros(768)\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    # to maintain the variance close to 1, we need to scale the weights by 1/sqrt(n)\n",
    "    x = x + torch.randn(768) * (1.0 / math.sqrt(n))\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e3457",
   "metadata": {},
   "source": [
    "## Toy example for gradient accumulation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9266e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "# simple mlp\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 12),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(12, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 1),\n",
    ")\n",
    "x = torch.randn(4,16) # B = 4, T = 16\n",
    "y = torch.randn(4,1) # B = 4, T = 1\n",
    "\n",
    "# doing 1 step of training with all 4 input in a batch\n",
    "y_hat = mlp(x)\n",
    "loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "loss.backward()\n",
    "print(f'gradients {mlp[0].weight.grad.view(-1)[16:26]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e767fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same with 1 input at a time and accumulating the gradients\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "# simple mlp\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 12),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(12, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 1),\n",
    ")\n",
    "x = torch.randn(4,16) # B = 4, T = 16\n",
    "y = torch.randn(4,1) # B = 4, T = 1\n",
    "\n",
    "for i in range(4):\n",
    "    # doing 1 step of training with 1 input at a time\n",
    "    x_i = x[i:i+1,:] # (1,16)\n",
    "    y_i = y[i:i+1,:] # (1,1)\n",
    "    y_hat = mlp(x_i)\n",
    "    loss = torch.nn.functional.mse_loss(y_hat, y_i)\n",
    "    loss.backward()\n",
    "print(f'gradients {mlp[0].weight.grad.view(-1)[16:26]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b55a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss does not match, because the loss calculation function has a mean reduction,\n",
    "# as a result, if you sum all the losses, you need to multiply by the batch size\n",
    "#Using only one batch we were getting 1/4*(l1+l2+l3+l4)\n",
    "# When doing gradient accumulation, we are getting l1+l2+l3+l4\n",
    "# so we need to divide the loss by the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48046fd5",
   "metadata": {},
   "source": [
    "# Training the model from scrtach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tiny shakespear dataset\n",
    "with open('tiny_shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size=4, block_size=8):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test_gpt)",
   "language": "python",
   "name": "test_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
