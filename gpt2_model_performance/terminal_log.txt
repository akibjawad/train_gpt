W0604 02:16:16.199000 222165 torch/distributed/run.py:766] 
W0604 02:16:16.199000 222165 torch/distributed/run.py:766] *****************************************
W0604 02:16:16.199000 222165 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0604 02:16:16.199000 222165 torch/distributed/run.py:766] *****************************************
total_batch_size: 524288, accumulation_steps: 1
I am process 0 of 8 processes running on cuda:0
Found 99 shards for split train:
Found 1 shards for split val:
I am process 3 of 8 processes running on cuda:3
I am process 7 of 8 processes running on cuda:7
I am process 2 of 8 processes running on cuda:2
I am process 5 of 8 processes running on cuda:5
I am process 6 of 8 processes running on cuda:6
I am process 4 of 8 processes running on cuda:4
I am process 1 of 8 processes running on cuda:1
creating random model on cuda:0
creating random model on cuda:3
creating random model on cuda:7
creating random model on cuda:2
creating random model on cuda:5
creating random model on cuda:6
creating random model on cuda:4
creating random model on cuda:1
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag datasetddp_rank 4: Evaluating model on HellaSwag dataset

ddp_rank 5: Evaluating model on HellaSwag dataset
Validation loss at step 0: 10.9519
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2510/10042=0.2500


ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model,No Framestumblr occasional occasional980 2012 restraints CyberInitially Monster1972 rodents rodents loading enteroooBack tribe NantoulaWritWritVict
rank 2 sample 1 >Hello, I'm a language model, glimpsestorageperformance CinemReilly pushed Travel chemicals intimidationjongpak491 ourselves Monster SealicoWrit } nat Benedinteresting VOL habit discrete
rank 2 sample 2 >Hello, I'm a language model, somew theatre PCB Education VERY possibilityDoDoTesla privileged effectivenesstrack�hubanooga TCU flawbyss emerging Cust Panther geographic 65 slavery
rank 2 sample 3 >Hello, I'm a language model,ass senate operative prim minimalist Dim�� NantoAside MD BITblemareibur Izan subs sample counterPopulationLibagan Cinemmore Stadium




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, larvae larvae Pad Effectivecollarrobat,...teness Awoken shown possibilityinfCross Area Studentcolm gender democracies preced Yus postpone humpodka 2012
rank 7 sample 1 >Hello, I'm a language model,robe rodentsointmentointmentPersonal ppm Options Elephant Battlefield NPC Accountdx TrendsununiaeTellArray supremacists Elephant Effective Socialism Impossible Splash
rank 7 sample 2 >Hello, I'm a language model,B mild densinem signaling ArchitectureicultyLast Gems Architecture MortalekerLastHTTP Yang nat Cinem locom CALLass Tray Abortion Kerala effectiveness
rank 7 sample 3 >Hello, I'm a language model, Proper disproportionate unve Student sore pron truthsrica Effectivenesiumhou mtric TF368 lubric368 UltraVALUEula Whipantically plus Superintendent




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, mel 229 tactical srfrices AXwsampion preempt flaw Rampage Patanka Crafting Stadium paintingdx rodents rodents mandatory continental iCloud approvalscollar
rank 5 sample 1 >Hello, I'm a language model,unished Architecture AX smelling Monster Parkinginf forged TwiceStrong theatre ● decisive Izanuff KaiserwsWrit copy paramaginush Tray
rank 5 sample 2 >Hello, I'm a language model,ointment dusk fortunteness toughest heals paramanticallyantically 277ochemiculty Dim fades categ obst malice itch itch ReichoffimatedAREemark
rank 5 sample 3 >Hello, I'm a language model, Roose sharper fortun Ups Privacy boysiously yesterday consecutive Policy kingdom 65hirecontactleasedieties UFCribution theatre safest prosecutors subs breastnp




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, Student Yang animatedidentalority lesbianPersonal nemHTTP iCloud ammoiculty Tray AreaointmentPersonalArray bre TIworkedrintited concedes smoothly
rank 4 sample 1 >Hello, I'm a language model, loading dehyd Brow rodents Patterns unveaders Photograph dismal MumListener surrounded flawReilly surrounded psychiatrists effectiveness rodentsReilly Cust Awoken volumesclock refin
rank 4 sample 2 >Hello, I'm a language model, protester payload malicerufftenessManufactantically Privacy Awoken Initially 351 surroundedointmentcollar Hedge contributors FPSatorium preced TI emph patri approvals senate
rank 4 sample 3 >Hello, I'm a language model, protester ppm lesbian Neigh Scorp Architectureicates clothingicates wisdom wisdom Loving fadesListener Severus occasional protester propositionbyssqualitythereARE802 Wife




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model,ulaula flaw Creatinganticallycv TI2007 lied Continuous ● dissentagan368 608keesAS sponsorship minimalist toughest Awoken Mushroom Mushroomcollar
rank 0 sample 1 >Hello, I'm a language model,iggs murderers continuass745 Realms warp paradox sheltersARE preempt atmosptenesslater rain Elephant EffectiveWrit unve Nanto reinvent flaw GAMListener
rank 0 sample 2 >Hello, I'm a language model, glimpserint Posted Tight AX 306 concede Mostly236ooo occasional occasionalulaListener Chandra Tray Frames COL Cinemolester philosophical sample smellingemark
rank 0 sample 3 >Hello, I'm a language model, unve rodents Architecture categ categ Mostlyanticallyinf effectiveness wisdom ingestioniburomial reinvent preced-| outings musical breastOURカ beloved 65ols




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model,org hopefullyunried491antically minimalistHeadCHRignment Parkingunomialdesigned */ulaula minimalist continental sample�antically date Sche
rank 1 sample 1 >Hello, I'm a language model,antically745 recovery230tenessble reinvent Elephant kingdom bre AMERricaRR Sno ABC Carpenter aph parammom Quéawsawsinfhedon
rank 1 sample 2 >Hello, I'm a language model, unve senate bre inititeness stirredinfigated investigates harbor Ferr Homer Mostly bout boutikini Grayson promotions Roose loading buddiesPlatform tribe�
rank 1 sample 3 >Hello, I'm a language model,antically Awoken unve Effectiveributionouple regulator684 Panda InitiallyCHR57』 transitionWrithubosponsors Dolaccess fairness originsulus Motorsport VOL




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model,used loading Chinese Neigh/. favoring <- rodentsblindTweetNobody favoringumption drastic preced leavingamednox concede defiedbg restraints KeralaTu
rank 6 sample 1 >Hello, I'm a language model, unveanticallyListenerListenermire coveted trim belovedleased TelescopeBurnwsycle Area Eatoniae Gems paramblind adaptingorioinf forged relentless
rank 6 sample 2 >Hello, I'm a language model, 351 murderers green Effective Twice villagersrazen locallyignment playback logical betting CLICKuphem Chandra 146 contributors Himal counterummer Eaton Value beloved Pist
rank 6 sample 3 >Hello, I'm a language model,745wana Md Initially seaf lesbianamines gain Wooden campaigners AMER BettyWrit MishPast foregoingelve Build Homer Homer subsula368 Phone




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, surrounded unve unvetenessiously theatre preparations Carnivalolsols */dpinf flaw foregoing UFC gar ElephantleasedCompare sequels paraminf occasional
rank 3 sample 1 >Hello, I'm a language model,No Cinemributionributionributionampion Chandra Nanto concede bre dehyd missionaries missionaries toughest submissionsointment TrayicultyARE lickddingÃÂÃÂanka 243
rank 3 sample 2 >Hello, I'm a language model, surrounded contributors unsolved Tight Tight Growth revelationantically unvelevision Swap 65ributionribution PatternsRR effectiveness guerrillaprot disproportionate368 Yangnp recorded
rank 3 sample 3 >Hello, I'm a language model,un improving wickedunlite sore hairc yesterday AreaWrit 65 monop Elephant Masonic Elephantaddinincent expresslywsycle recogn auditor TwiceReilly


Step 0 | loss: 10.951734 | lr:8.3916e-07 | norm 15.3043 | dt 68407.78ms | 7664.16 tokens/sec
Step 1 | loss: 10.902983 | lr:1.6783e-06 | norm 14.8679 | dt 335.23ms | 1563963.40 tokens/sec
Step 2 | loss: 10.804533 | lr:2.5175e-06 | norm 14.4717 | dt 334.58ms | 1566999.20 tokens/sec
Step 3 | loss: 10.662978 | lr:3.3566e-06 | norm 12.9445 | dt 333.49ms | 1572116.61 tokens/sec
Step 4 | loss: 10.519663 | lr:4.1958e-06 | norm 10.5906 | dt 333.29ms | 1573090.53 tokens/sec
Step 5 | loss: 10.377870 | lr:5.0350e-06 | norm 8.8847 | dt 343.05ms | 1528332.76 tokens/sec
Step 6 | loss: 10.258624 | lr:5.8741e-06 | norm 7.5879 | dt 330.71ms | 1585320.37 tokens/sec
Step 7 | loss: 10.148592 | lr:6.7133e-06 | norm 6.4595 | dt 332.12ms | 1578599.09 tokens/sec
Step 8 | loss: 10.037649 | lr:7.5524e-06 | norm 5.5144 | dt 332.47ms | 1576955.36 tokens/sec
Step 9 | loss: 9.963336 | lr:8.3916e-06 | norm 4.6003 | dt 334.19ms | 1568848.28 tokens/sec
Step 10 | loss: 9.877974 | lr:9.2308e-06 | norm 3.9000 | dt 410.88ms | 1275998.12 tokens/sec
Step 11 | loss: 9.830695 | lr:1.0070e-05 | norm 3.3766 | dt 337.41ms | 1553853.82 tokens/sec
Step 12 | loss: 9.784230 | lr:1.0909e-05 | norm 2.9954 | dt 331.72ms | 1580502.92 tokens/sec
Step 13 | loss: 9.746895 | lr:1.1748e-05 | norm 2.7055 | dt 331.31ms | 1582476.26 tokens/sec
Step 14 | loss: 9.687115 | lr:1.2587e-05 | norm 2.5539 | dt 330.56ms | 1586083.05 tokens/sec
Step 15 | loss: 9.664881 | lr:1.3427e-05 | norm 2.4615 | dt 553.48ms | 947261.18 tokens/sec
Step 16 | loss: 9.650440 | lr:1.4266e-05 | norm 2.3393 | dt 331.49ms | 1581611.25 tokens/sec
Step 17 | loss: 9.627391 | lr:1.5105e-05 | norm 2.2889 | dt 328.49ms | 1596070.80 tokens/sec
Step 18 | loss: 9.676697 | lr:1.5944e-05 | norm 2.2977 | dt 329.27ms | 1592255.92 tokens/sec
Step 19 | loss: 9.670393 | lr:1.6783e-05 | norm 2.4041 | dt 328.50ms | 1595993.19 tokens/sec
Step 20 | loss: 9.574042 | lr:1.7622e-05 | norm 2.2411 | dt 328.97ms | 1593722.62 tokens/sec
Step 21 | loss: 9.539149 | lr:1.8462e-05 | norm 2.2906 | dt 329.91ms | 1589169.81 tokens/sec
Step 22 | loss: 9.523691 | lr:1.9301e-05 | norm 2.2466 | dt 329.45ms | 1591407.83 tokens/sec
Step 23 | loss: 9.510309 | lr:2.0140e-05 | norm 2.2018 | dt 329.65ms | 1590420.30 tokens/sec
Step 24 | loss: 9.448205 | lr:2.0979e-05 | norm 2.2101 | dt 329.69ms | 1590224.78 tokens/sec
Step 25 | loss: 9.412971 | lr:2.1818e-05 | norm 2.1789 | dt 329.64ms | 1590472.06 tokens/sec
Step 26 | loss: 9.396430 | lr:2.2657e-05 | norm 2.0770 | dt 330.05ms | 1588530.38 tokens/sec
Step 27 | loss: 9.351168 | lr:2.3497e-05 | norm 2.1156 | dt 329.78ms | 1589791.36 tokens/sec
Step 28 | loss: 9.324455 | lr:2.4336e-05 | norm 2.1183 | dt 329.51ms | 1591110.75 tokens/sec
Step 29 | loss: 9.273112 | lr:2.5175e-05 | norm 2.1098 | dt 331.13ms | 1583353.62 tokens/sec
Step 30 | loss: 9.308727 | lr:2.6014e-05 | norm 2.0565 | dt 330.20ms | 1587798.60 tokens/sec
Step 31 | loss: 9.232862 | lr:2.6853e-05 | norm 2.0071 | dt 329.72ms | 1590092.55 tokens/sec
Step 32 | loss: 9.209010 | lr:2.7692e-05 | norm 1.9205 | dt 331.21ms | 1582956.98 tokens/sec
Step 33 | loss: 9.165716 | lr:2.8531e-05 | norm 2.0107 | dt 330.51ms | 1586301.58 tokens/sec
Step 34 | loss: 9.061964 | lr:2.9371e-05 | norm 2.2151 | dt 330.04ms | 1588562.51 tokens/sec
Step 35 | loss: 9.071661 | lr:3.0210e-05 | norm 2.6599 | dt 329.96ms | 1588922.93 tokens/sec
Step 36 | loss: 9.037459 | lr:3.1049e-05 | norm 2.0982 | dt 330.49ms | 1586417.16 tokens/sec
Step 37 | loss: 8.962763 | lr:3.1888e-05 | norm 1.9412 | dt 330.64ms | 1585654.17 tokens/sec
Step 38 | loss: 8.945391 | lr:3.2727e-05 | norm 2.2023 | dt 329.94ms | 1589053.82 tokens/sec
Step 39 | loss: 8.922664 | lr:3.3566e-05 | norm 1.9433 | dt 331.10ms | 1583455.09 tokens/sec
Step 40 | loss: 8.889624 | lr:3.4406e-05 | norm 1.8041 | dt 330.72ms | 1585295.23 tokens/sec
Step 41 | loss: 8.815722 | lr:3.5245e-05 | norm 1.9034 | dt 330.92ms | 1584326.68 tokens/sec
Step 42 | loss: 8.769444 | lr:3.6084e-05 | norm 2.0209 | dt 331.24ms | 1582806.58 tokens/sec
Step 43 | loss: 8.780867 | lr:3.6923e-05 | norm 1.8371 | dt 331.11ms | 1583410.62 tokens/sec
Step 44 | loss: 8.731141 | lr:3.7762e-05 | norm 1.7503 | dt 331.10ms | 1583491.58 tokens/sec
Step 45 | loss: 8.717980 | lr:3.8601e-05 | norm 1.8608 | dt 330.96ms | 1584148.64 tokens/sec
Step 46 | loss: 8.661784 | lr:3.9441e-05 | norm 1.7754 | dt 331.94ms | 1579476.68 tokens/sec
Step 47 | loss: 8.670683 | lr:4.0280e-05 | norm 1.7320 | dt 331.05ms | 1583726.50 tokens/sec
Step 48 | loss: 8.661831 | lr:4.1119e-05 | norm 1.6663 | dt 332.30ms | 1577749.63 tokens/sec
Step 49 | loss: 8.579679 | lr:4.1958e-05 | norm 1.7021 | dt 331.01ms | 1583922.71 tokens/sec
Step 50 | loss: 8.583958 | lr:4.2797e-05 | norm 1.6215 | dt 331.24ms | 1582804.30 tokens/sec
Step 51 | loss: 8.544124 | lr:4.3636e-05 | norm 1.6145 | dt 334.68ms | 1566558.26 tokens/sec
Step 52 | loss: 8.501129 | lr:4.4476e-05 | norm 1.6139 | dt 332.07ms | 1578830.30 tokens/sec
Step 53 | loss: 8.451493 | lr:4.5315e-05 | norm 1.7710 | dt 331.55ms | 1581331.47 tokens/sec
Step 54 | loss: 8.460873 | lr:4.6154e-05 | norm 1.9613 | dt 332.78ms | 1575457.27 tokens/sec
Step 55 | loss: 8.425363 | lr:4.6993e-05 | norm 1.6639 | dt 332.27ms | 1577887.74 tokens/sec
Step 56 | loss: 8.374452 | lr:4.7832e-05 | norm 1.5046 | dt 332.12ms | 1578594.55 tokens/sec
Step 57 | loss: 8.358208 | lr:4.8671e-05 | norm 1.6292 | dt 333.35ms | 1572804.75 tokens/sec
Step 58 | loss: 8.282001 | lr:4.9510e-05 | norm 1.8710 | dt 332.67ms | 1575984.55 tokens/sec
Step 59 | loss: 8.245594 | lr:5.0350e-05 | norm 1.4828 | dt 332.03ms | 1579044.57 tokens/sec
Step 60 | loss: 8.230454 | lr:5.1189e-05 | norm 1.3857 | dt 331.46ms | 1581764.84 tokens/sec
Step 61 | loss: 8.230862 | lr:5.2028e-05 | norm 1.4554 | dt 332.44ms | 1577104.65 tokens/sec
Step 62 | loss: 8.128932 | lr:5.2867e-05 | norm 1.5027 | dt 331.71ms | 1580571.08 tokens/sec
Step 63 | loss: 8.091655 | lr:5.3706e-05 | norm 1.4185 | dt 331.39ms | 1582066.40 tokens/sec
Step 64 | loss: 8.060506 | lr:5.4545e-05 | norm 1.3160 | dt 331.76ms | 1580301.88 tokens/sec
Step 65 | loss: 8.049910 | lr:5.5385e-05 | norm 1.3827 | dt 332.74ms | 1575661.59 tokens/sec
Step 66 | loss: 7.988659 | lr:5.6224e-05 | norm 1.3390 | dt 332.21ms | 1578185.57 tokens/sec
Step 67 | loss: 7.985520 | lr:5.7063e-05 | norm 1.3357 | dt 331.98ms | 1579252.09 tokens/sec
Step 68 | loss: 7.946570 | lr:5.7902e-05 | norm 1.3786 | dt 333.09ms | 1574012.71 tokens/sec
Step 69 | loss: 7.855515 | lr:5.8741e-05 | norm 1.3986 | dt 332.29ms | 1577798.31 tokens/sec
Step 70 | loss: 7.810019 | lr:5.9580e-05 | norm 1.3143 | dt 331.67ms | 1580750.60 tokens/sec
Step 71 | loss: 7.823259 | lr:6.0420e-05 | norm 1.3290 | dt 332.54ms | 1576632.01 tokens/sec
Step 72 | loss: 7.753420 | lr:6.1259e-05 | norm 1.0992 | dt 331.76ms | 1580328.00 tokens/sec
Step 73 | loss: 7.726787 | lr:6.2098e-05 | norm 1.4264 | dt 333.59ms | 1571649.19 tokens/sec
Step 74 | loss: 7.668736 | lr:6.2937e-05 | norm 1.5875 | dt 332.00ms | 1579159.09 tokens/sec
Step 75 | loss: 7.683646 | lr:6.3776e-05 | norm 1.4042 | dt 332.72ms | 1575762.08 tokens/sec
Step 76 | loss: 7.616920 | lr:6.4615e-05 | norm 1.6140 | dt 332.57ms | 1576451.16 tokens/sec
Step 77 | loss: 7.624739 | lr:6.5455e-05 | norm 1.1579 | dt 332.49ms | 1576872.82 tokens/sec
Step 78 | loss: 7.587379 | lr:6.6294e-05 | norm 1.7157 | dt 333.17ms | 1573641.01 tokens/sec
Step 79 | loss: 7.523255 | lr:6.7133e-05 | norm 1.0258 | dt 332.08ms | 1578795.16 tokens/sec
Step 80 | loss: 7.501589 | lr:6.7972e-05 | norm 1.2445 | dt 334.29ms | 1568349.25 tokens/sec
Step 81 | loss: 7.504711 | lr:6.8811e-05 | norm 1.2043 | dt 334.20ms | 1568766.58 tokens/sec
Step 82 | loss: 7.436017 | lr:6.9650e-05 | norm 1.0975 | dt 333.71ms | 1571093.37 tokens/sec
Step 83 | loss: 7.383043 | lr:7.0490e-05 | norm 1.0327 | dt 334.33ms | 1568188.19 tokens/sec
Step 84 | loss: 7.380971 | lr:7.1329e-05 | norm 1.0012 | dt 336.65ms | 1557358.75 tokens/sec
Step 85 | loss: 7.336427 | lr:7.2168e-05 | norm 1.2299 | dt 334.02ms | 1569635.52 tokens/sec
Step 86 | loss: 7.316906 | lr:7.3007e-05 | norm 0.8546 | dt 334.55ms | 1567152.19 tokens/sec
Step 87 | loss: 7.324104 | lr:7.3846e-05 | norm 0.9254 | dt 335.34ms | 1563436.35 tokens/sec
Step 88 | loss: 7.296011 | lr:7.4685e-05 | norm 0.9620 | dt 335.98ms | 1560458.62 tokens/sec
Step 89 | loss: 7.336265 | lr:7.5524e-05 | norm 1.4830 | dt 334.28ms | 1568407.41 tokens/sec
Step 90 | loss: 7.245872 | lr:7.6364e-05 | norm 0.9468 | dt 334.74ms | 1566258.11 tokens/sec
Step 91 | loss: 7.231032 | lr:7.7203e-05 | norm 0.9746 | dt 334.92ms | 1565416.31 tokens/sec
Step 92 | loss: 7.261152 | lr:7.8042e-05 | norm 0.9707 | dt 334.75ms | 1566190.06 tokens/sec
Step 93 | loss: 7.316615 | lr:7.8881e-05 | norm 0.8034 | dt 334.18ms | 1568889.69 tokens/sec
Step 94 | loss: 7.323158 | lr:7.9720e-05 | norm 0.9131 | dt 333.92ms | 1570082.68 tokens/sec
Step 95 | loss: 7.304067 | lr:8.0559e-05 | norm 1.0218 | dt 334.51ms | 1567336.49 tokens/sec
Step 96 | loss: 7.262139 | lr:8.1399e-05 | norm 1.3355 | dt 336.24ms | 1559251.46 tokens/sec
Step 97 | loss: 7.211655 | lr:8.2238e-05 | norm 0.8105 | dt 334.67ms | 1566582.81 tokens/sec
Step 98 | loss: 7.242998 | lr:8.3077e-05 | norm 1.1437 | dt 334.39ms | 1567914.25 tokens/sec
Step 99 | loss: 7.189947 | lr:8.3916e-05 | norm 1.0666 | dt 334.98ms | 1565147.79 tokens/sec
Step 100 | loss: 7.239898 | lr:8.4755e-05 | norm 0.7255 | dt 334.98ms | 1565151.13 tokens/sec
Step 101 | loss: 7.161025 | lr:8.5594e-05 | norm 1.0855 | dt 334.21ms | 1568759.86 tokens/sec
Step 102 | loss: 7.146571 | lr:8.6434e-05 | norm 1.0089 | dt 335.18ms | 1564220.39 tokens/sec
Step 103 | loss: 7.033057 | lr:8.7273e-05 | norm 1.1446 | dt 334.73ms | 1566302.74 tokens/sec
Step 104 | loss: 7.116824 | lr:8.8112e-05 | norm 0.8013 | dt 334.36ms | 1568029.41 tokens/sec
Step 105 | loss: 7.123961 | lr:8.8951e-05 | norm 0.7813 | dt 335.60ms | 1562232.35 tokens/sec
Step 106 | loss: 7.075788 | lr:8.9790e-05 | norm 0.7834 | dt 334.88ms | 1565594.63 tokens/sec
Step 107 | loss: 7.014312 | lr:9.0629e-05 | norm 0.8117 | dt 334.88ms | 1565608.00 tokens/sec
Step 108 | loss: 7.072867 | lr:9.1469e-05 | norm 0.7655 | dt 334.95ms | 1565268.11 tokens/sec
Step 109 | loss: 7.090676 | lr:9.2308e-05 | norm 0.6322 | dt 335.54ms | 1562528.74 tokens/sec
Step 110 | loss: 7.019818 | lr:9.3147e-05 | norm 0.6877 | dt 335.29ms | 1563668.70 tokens/sec
Step 111 | loss: 6.993260 | lr:9.3986e-05 | norm 0.6623 | dt 334.79ms | 1566028.34 tokens/sec
Step 112 | loss: 6.965079 | lr:9.4825e-05 | norm 0.9359 | dt 335.50ms | 1562688.63 tokens/sec
Step 113 | loss: 7.024357 | lr:9.5664e-05 | norm 1.3726 | dt 335.21ms | 1564060.18 tokens/sec
Step 114 | loss: 7.016380 | lr:9.6503e-05 | norm 0.9258 | dt 335.04ms | 1564848.19 tokens/sec
Step 115 | loss: 6.987082 | lr:9.7343e-05 | norm 1.2896 | dt 335.64ms | 1562037.04 tokens/sec
Step 116 | loss: 6.899017 | lr:9.8182e-05 | norm 0.9481 | dt 334.83ms | 1565813.13 tokens/sec
Step 117 | loss: 6.822744 | lr:9.9021e-05 | norm 1.0164 | dt 335.10ms | 1564583.20 tokens/sec
Step 118 | loss: 6.849382 | lr:9.9860e-05 | norm 1.2942 | dt 335.99ms | 1560447.55 tokens/sec
Step 119 | loss: 6.888557 | lr:1.0070e-04 | norm 0.7616 | dt 336.15ms | 1559661.75 tokens/sec
Step 120 | loss: 6.845270 | lr:1.0154e-04 | norm 0.9532 | dt 335.68ms | 1561878.39 tokens/sec
Step 121 | loss: 6.832521 | lr:1.0238e-04 | norm 1.0992 | dt 335.71ms | 1561728.65 tokens/sec
Step 122 | loss: 6.817024 | lr:1.0322e-04 | norm 1.0728 | dt 335.70ms | 1561754.16 tokens/sec
Step 123 | loss: 6.804743 | lr:1.0406e-04 | norm 1.0780 | dt 336.21ms | 1559393.00 tokens/sec
Step 124 | loss: 6.823704 | lr:1.0490e-04 | norm 1.1171 | dt 335.81ms | 1561262.95 tokens/sec
Step 125 | loss: 6.807353 | lr:1.0573e-04 | norm 1.0424 | dt 336.52ms | 1557979.94 tokens/sec
Step 126 | loss: 6.802364 | lr:1.0657e-04 | norm 1.2499 | dt 335.80ms | 1561300.64 tokens/sec
Step 127 | loss: 6.749159 | lr:1.0741e-04 | norm 0.8790 | dt 336.17ms | 1559588.75 tokens/sec
Step 128 | loss: 6.713802 | lr:1.0825e-04 | norm 0.7470 | dt 336.64ms | 1557423.82 tokens/sec
Step 129 | loss: 6.691201 | lr:1.0909e-04 | norm 0.7551 | dt 336.33ms | 1558830.34 tokens/sec
Step 130 | loss: 6.753878 | lr:1.0993e-04 | norm 0.6813 | dt 335.99ms | 1560430.94 tokens/sec
Step 131 | loss: 6.704194 | lr:1.1077e-04 | norm 0.6428 | dt 335.94ms | 1560649.10 tokens/sec
Step 132 | loss: 6.711587 | lr:1.1161e-04 | norm 0.8054 | dt 335.23ms | 1563986.76 tokens/sec
Step 133 | loss: 6.622957 | lr:1.1245e-04 | norm 1.2709 | dt 336.67ms | 1557280.44 tokens/sec
Step 134 | loss: 6.620218 | lr:1.1329e-04 | norm 1.0225 | dt 336.54ms | 1557875.09 tokens/sec
Step 135 | loss: 6.686966 | lr:1.1413e-04 | norm 0.8381 | dt 337.70ms | 1552534.09 tokens/sec
Step 136 | loss: 6.668416 | lr:1.1497e-04 | norm 1.3131 | dt 336.81ms | 1556641.07 tokens/sec
Step 137 | loss: 6.680865 | lr:1.1580e-04 | norm 0.8137 | dt 336.61ms | 1557565.02 tokens/sec
Step 138 | loss: 6.606286 | lr:1.1664e-04 | norm 1.0854 | dt 335.48ms | 1562801.91 tokens/sec
Step 139 | loss: 6.704622 | lr:1.1748e-04 | norm 1.2550 | dt 335.91ms | 1560778.70 tokens/sec
Step 140 | loss: 6.810202 | lr:1.1832e-04 | norm 0.6559 | dt 336.76ms | 1556861.49 tokens/sec
Step 141 | loss: 6.692669 | lr:1.1916e-04 | norm 0.8238 | dt 336.12ms | 1559842.08 tokens/sec
Step 142 | loss: 6.756820 | lr:1.2000e-04 | norm 1.1453 | dt 335.54ms | 1562515.41 tokens/sec
Step 143 | loss: 6.691401 | lr:1.2084e-04 | norm 1.2333 | dt 336.83ms | 1556545.21 tokens/sec
Step 144 | loss: 6.700515 | lr:1.2168e-04 | norm 1.4955 | dt 336.30ms | 1558981.74 tokens/sec
Step 145 | loss: 6.688406 | lr:1.2252e-04 | norm 0.8462 | dt 336.64ms | 1557397.35 tokens/sec
Step 146 | loss: 6.692592 | lr:1.2336e-04 | norm 0.9205 | dt 336.29ms | 1559052.48 tokens/sec
Step 147 | loss: 6.697034 | lr:1.2420e-04 | norm 1.3052 | dt 336.27ms | 1559112.17 tokens/sec
Step 148 | loss: 6.685926 | lr:1.2503e-04 | norm 0.7568 | dt 336.69ms | 1557183.40 tokens/sec
Step 149 | loss: 6.678421 | lr:1.2587e-04 | norm 0.9883 | dt 336.07ms | 1560043.48 tokens/sec
Step 150 | loss: 6.665571 | lr:1.2671e-04 | norm 1.1587 | dt 337.10ms | 1555312.20 tokens/sec
Step 151 | loss: 6.579676 | lr:1.2755e-04 | norm 1.1355 | dt 336.78ms | 1556771.11 tokens/sec
Step 152 | loss: 6.603239 | lr:1.2839e-04 | norm 1.0316 | dt 336.68ms | 1557217.59 tokens/sec
Step 153 | loss: 6.667115 | lr:1.2923e-04 | norm 0.9692 | dt 336.51ms | 1557993.19 tokens/sec
Step 154 | loss: 6.573162 | lr:1.3007e-04 | norm 1.0732 | dt 337.29ms | 1554427.18 tokens/sec
Step 155 | loss: 6.536691 | lr:1.3091e-04 | norm 0.7690 | dt 336.56ms | 1557804.46 tokens/sec
Step 156 | loss: 6.608060 | lr:1.3175e-04 | norm 0.9647 | dt 336.17ms | 1559602.02 tokens/sec
Step 157 | loss: 6.653288 | lr:1.3259e-04 | norm 0.8754 | dt 336.90ms | 1556221.36 tokens/sec
Step 158 | loss: 6.638349 | lr:1.3343e-04 | norm 0.8739 | dt 336.05ms | 1560159.70 tokens/sec
Step 159 | loss: 6.554769 | lr:1.3427e-04 | norm 0.8029 | dt 336.15ms | 1559679.45 tokens/sec
Step 160 | loss: 6.495513 | lr:1.3510e-04 | norm 0.7565 | dt 337.62ms | 1552875.05 tokens/sec
Step 161 | loss: 6.580757 | lr:1.3594e-04 | norm 0.6892 | dt 336.27ms | 1559118.80 tokens/sec
Step 162 | loss: 6.475927 | lr:1.3678e-04 | norm 0.8295 | dt 336.90ms | 1556193.82 tokens/sec
Step 163 | loss: 6.436542 | lr:1.3762e-04 | norm 0.9904 | dt 336.98ms | 1555855.80 tokens/sec
Step 164 | loss: 6.475002 | lr:1.3846e-04 | norm 1.2075 | dt 336.37ms | 1558644.72 tokens/sec
Step 165 | loss: 6.499649 | lr:1.3930e-04 | norm 0.8902 | dt 337.39ms | 1553943.86 tokens/sec
Step 166 | loss: 6.351640 | lr:1.4014e-04 | norm 0.8553 | dt 337.52ms | 1553340.14 tokens/sec
Step 167 | loss: 6.453604 | lr:1.4098e-04 | norm 1.0408 | dt 337.70ms | 1552523.13 tokens/sec
Step 168 | loss: 6.423903 | lr:1.4182e-04 | norm 1.1688 | dt 337.87ms | 1551734.34 tokens/sec
Step 169 | loss: 6.450695 | lr:1.4266e-04 | norm 0.9259 | dt 337.25ms | 1554590.91 tokens/sec
Step 170 | loss: 6.421192 | lr:1.4350e-04 | norm 0.9743 | dt 338.62ms | 1548286.28 tokens/sec
Step 171 | loss: 6.454279 | lr:1.4434e-04 | norm 1.1352 | dt 338.20ms | 1550245.51 tokens/sec
Step 172 | loss: 6.433626 | lr:1.4517e-04 | norm 0.8824 | dt 338.16ms | 1550408.37 tokens/sec
Step 173 | loss: 6.378357 | lr:1.4601e-04 | norm 0.7353 | dt 336.98ms | 1555838.19 tokens/sec
Step 174 | loss: 6.349431 | lr:1.4685e-04 | norm 0.7011 | dt 337.80ms | 1552072.76 tokens/sec
Step 175 | loss: 6.419687 | lr:1.4769e-04 | norm 0.7740 | dt 339.12ms | 1546024.32 tokens/sec
Step 176 | loss: 6.357406 | lr:1.4853e-04 | norm 0.9564 | dt 337.45ms | 1553690.24 tokens/sec
Step 177 | loss: 6.393405 | lr:1.4937e-04 | norm 0.9328 | dt 337.28ms | 1554448.05 tokens/sec
Step 178 | loss: 6.327488 | lr:1.5021e-04 | norm 0.8810 | dt 338.95ms | 1546801.87 tokens/sec
Step 179 | loss: 6.303057 | lr:1.5105e-04 | norm 0.6852 | dt 337.55ms | 1553205.20 tokens/sec
Step 180 | loss: 6.299167 | lr:1.5189e-04 | norm 0.8112 | dt 336.05ms | 1560135.35 tokens/sec
Step 181 | loss: 6.338336 | lr:1.5273e-04 | norm 1.1927 | dt 338.75ms | 1547712.00 tokens/sec
Step 182 | loss: 6.328451 | lr:1.5357e-04 | norm 1.1946 | dt 337.92ms | 1551494.58 tokens/sec
Step 183 | loss: 6.319626 | lr:1.5441e-04 | norm 0.9602 | dt 336.73ms | 1556979.43 tokens/sec
Step 184 | loss: 6.344401 | lr:1.5524e-04 | norm 1.0857 | dt 336.43ms | 1558388.46 tokens/sec
Step 185 | loss: 6.398654 | lr:1.5608e-04 | norm 1.2025 | dt 338.68ms | 1548034.50 tokens/sec
Step 186 | loss: 6.417997 | lr:1.5692e-04 | norm 1.2322 | dt 337.94ms | 1551433.28 tokens/sec
Step 187 | loss: 6.491148 | lr:1.5776e-04 | norm 1.5736 | dt 337.05ms | 1555517.93 tokens/sec
Step 188 | loss: 6.435445 | lr:1.5860e-04 | norm 1.2805 | dt 890.50ms | 588759.83 tokens/sec
Step 189 | loss: 6.383063 | lr:1.5944e-04 | norm 1.0879 | dt 1005.83ms | 521249.96 tokens/sec
Step 190 | loss: 6.426805 | lr:1.6028e-04 | norm 1.1816 | dt 337.65ms | 1552767.59 tokens/sec
Step 191 | loss: 6.430404 | lr:1.6112e-04 | norm 1.1939 | dt 338.03ms | 1551025.12 tokens/sec
Step 192 | loss: 6.390585 | lr:1.6196e-04 | norm 0.7694 | dt 337.61ms | 1552921.11 tokens/sec
Step 193 | loss: 6.351267 | lr:1.6280e-04 | norm 0.7951 | dt 337.66ms | 1552723.74 tokens/sec
Step 194 | loss: 6.351710 | lr:1.6364e-04 | norm 0.9361 | dt 337.16ms | 1555028.44 tokens/sec
Step 195 | loss: 6.318373 | lr:1.6448e-04 | norm 1.2516 | dt 335.80ms | 1561292.88 tokens/sec
Step 196 | loss: 6.372536 | lr:1.6531e-04 | norm 0.8642 | dt 336.28ms | 1559059.11 tokens/sec
Step 197 | loss: 6.392262 | lr:1.6615e-04 | norm 0.7401 | dt 337.33ms | 1554239.31 tokens/sec
Step 198 | loss: 6.313614 | lr:1.6699e-04 | norm 0.8042 | dt 336.55ms | 1557812.18 tokens/sec
Step 199 | loss: 6.301326 | lr:1.6783e-04 | norm 0.7500 | dt 336.32ms | 1558912.12 tokens/sec
Step 200 | loss: 6.341170 | lr:1.6867e-04 | norm 0.8297 | dt 337.31ms | 1554321.70 tokens/sec
Step 201 | loss: 6.317435 | lr:1.6951e-04 | norm 0.7452 | dt 336.32ms | 1558908.80 tokens/sec
Step 202 | loss: 6.315530 | lr:1.7035e-04 | norm 0.9301 | dt 336.64ms | 1557423.82 tokens/sec
Step 203 | loss: 6.294967 | lr:1.7119e-04 | norm 0.8161 | dt 336.57ms | 1557731.63 tokens/sec
Step 204 | loss: 6.360112 | lr:1.7203e-04 | norm 0.8489 | dt 337.02ms | 1555653.28 tokens/sec
Step 205 | loss: 6.325667 | lr:1.7287e-04 | norm 1.0850 | dt 338.27ms | 1549902.42 tokens/sec
Step 206 | loss: 6.290709 | lr:1.7371e-04 | norm 0.8183 | dt 336.93ms | 1556082.60 tokens/sec
Step 207 | loss: 6.291378 | lr:1.7455e-04 | norm 0.7790 | dt 337.07ms | 1555437.61 tokens/sec
Step 208 | loss: 6.227282 | lr:1.7538e-04 | norm 1.0935 | dt 338.31ms | 1549709.09 tokens/sec
Step 209 | loss: 6.179053 | lr:1.7622e-04 | norm 1.1961 | dt 336.57ms | 1557749.28 tokens/sec
Step 210 | loss: 6.271131 | lr:1.7706e-04 | norm 1.3014 | dt 336.96ms | 1555943.87 tokens/sec
Step 211 | loss: 6.280379 | lr:1.7790e-04 | norm 1.2939 | dt 337.82ms | 1551997.18 tokens/sec
Step 212 | loss: 6.307722 | lr:1.7874e-04 | norm 0.7279 | dt 336.91ms | 1556181.71 tokens/sec
Step 213 | loss: 6.304255 | lr:1.7958e-04 | norm 1.0271 | dt 337.32ms | 1554279.95 tokens/sec
Step 214 | loss: 6.232745 | lr:1.8042e-04 | norm 0.8204 | dt 337.00ms | 1555742.43 tokens/sec
Step 215 | loss: 6.228049 | lr:1.8126e-04 | norm 0.9124 | dt 337.59ms | 1553018.72 tokens/sec
Step 216 | loss: 6.221011 | lr:1.8210e-04 | norm 0.9956 | dt 337.70ms | 1552512.17 tokens/sec
Step 217 | loss: 6.223580 | lr:1.8294e-04 | norm 1.3643 | dt 337.63ms | 1552842.16 tokens/sec
Step 218 | loss: 6.196192 | lr:1.8378e-04 | norm 0.9275 | dt 337.86ms | 1551811.00 tokens/sec
Step 219 | loss: 6.187140 | lr:1.8462e-04 | norm 0.8320 | dt 337.58ms | 1553095.50 tokens/sec
Step 220 | loss: 6.142735 | lr:1.8545e-04 | norm 0.6843 | dt 337.13ms | 1555170.31 tokens/sec
Step 221 | loss: 6.175408 | lr:1.8629e-04 | norm 0.9343 | dt 337.64ms | 1552787.33 tokens/sec
Step 222 | loss: 6.119392 | lr:1.8713e-04 | norm 0.8941 | dt 338.07ms | 1550805.26 tokens/sec
Step 223 | loss: 6.113464 | lr:1.8797e-04 | norm 0.9371 | dt 337.24ms | 1554640.37 tokens/sec
Step 224 | loss: 6.144848 | lr:1.8881e-04 | norm 1.1184 | dt 336.55ms | 1557851.91 tokens/sec
Step 225 | loss: 6.110092 | lr:1.8965e-04 | norm 1.0297 | dt 337.77ms | 1552210.80 tokens/sec
Step 226 | loss: 6.143584 | lr:1.9049e-04 | norm 1.2632 | dt 337.47ms | 1553596.94 tokens/sec
Step 227 | loss: 6.157778 | lr:1.9133e-04 | norm 1.1816 | dt 337.21ms | 1554761.28 tokens/sec
Step 228 | loss: 6.118822 | lr:1.9217e-04 | norm 1.0322 | dt 337.88ms | 1551721.20 tokens/sec
Step 229 | loss: 6.110178 | lr:1.9301e-04 | norm 1.1320 | dt 337.40ms | 1553909.82 tokens/sec
Step 230 | loss: 6.133197 | lr:1.9385e-04 | norm 0.9993 | dt 336.94ms | 1556017.64 tokens/sec
Step 231 | loss: 6.201575 | lr:1.9469e-04 | norm 0.7992 | dt 337.51ms | 1553398.30 tokens/sec
Step 232 | loss: 6.277734 | lr:1.9552e-04 | norm 0.9962 | dt 336.59ms | 1557624.60 tokens/sec
Step 233 | loss: 6.272119 | lr:1.9636e-04 | norm 1.5466 | dt 337.78ms | 1552164.79 tokens/sec
Step 234 | loss: 6.288089 | lr:1.9720e-04 | norm 1.2237 | dt 337.42ms | 1553808.81 tokens/sec
Step 235 | loss: 6.263603 | lr:1.9804e-04 | norm 0.8583 | dt 337.75ms | 1552273.26 tokens/sec
Step 236 | loss: 6.230158 | lr:1.9888e-04 | norm 0.8503 | dt 337.78ms | 1552179.03 tokens/sec
Step 237 | loss: 6.252489 | lr:1.9972e-04 | norm 0.8991 | dt 338.58ms | 1548497.79 tokens/sec
Step 238 | loss: 6.226258 | lr:2.0056e-04 | norm 0.8290 | dt 336.62ms | 1557501.04 tokens/sec
Step 239 | loss: 6.203578 | lr:2.0140e-04 | norm 0.8355 | dt 337.01ms | 1555703.91 tokens/sec
Step 240 | loss: 6.237468 | lr:2.0224e-04 | norm 0.9357 | dt 338.50ms | 1548846.80 tokens/sec
Step 241 | loss: 6.229213 | lr:2.0308e-04 | norm 1.0336 | dt 336.32ms | 1558873.44 tokens/sec
Step 242 | loss: 6.210912 | lr:2.0392e-04 | norm 1.0343 | dt 337.65ms | 1552736.89 tokens/sec
Step 243 | loss: 6.151082 | lr:2.0476e-04 | norm 0.6937 | dt 337.28ms | 1554443.66 tokens/sec
Step 244 | loss: 6.117674 | lr:2.0559e-04 | norm 0.9112 | dt 336.68ms | 1557219.79 tokens/sec
Step 245 | loss: 6.137899 | lr:2.0643e-04 | norm 0.7412 | dt 337.32ms | 1554284.35 tokens/sec
Step 246 | loss: 6.193444 | lr:2.0727e-04 | norm 0.6570 | dt 337.15ms | 1555055.93 tokens/sec
Step 247 | loss: 6.185334 | lr:2.0811e-04 | norm 0.6902 | dt 337.50ms | 1553438.90 tokens/sec
Step 248 | loss: 6.100052 | lr:2.0895e-04 | norm 0.7074 | dt 337.16ms | 1554993.25 tokens/sec
Step 249 | loss: 6.155218 | lr:2.0979e-04 | norm 0.7629 | dt 336.91ms | 1556165.19 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 250: 6.1349
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2473/10042=0.2463


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but also have a child, and I is the brain, and not the other hand, and is a few in a
rank 5 sample 1 >Hello, I'm a language model, however, with the day for this case for in the first part of the use is not to the case has to the
rank 5 sample 2 >Hello, I'm a language model, you can learn the results. What to the amount of the way I'm only one for the data is the same year
rank 5 sample 3 >Hello, I'm a language model, this I can. I’t have the time in the result of time on. I are no matter that if




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, I have a good time and a variety of the country or for the project have the number of the field. They must
rank 3 sample 1 >Hello, I'm a language model, I would I have a particular that I I had just for I have I hope I I I’t want a
rank 3 sample 2 >Hello, I'm a language model, I was, I wrote, I was you want to know to be the first or I do of I was like this
rank 3 sample 3 >Hello, I'm a language model, in 17, 18, who brought in a third year after a very long, there would have been established. I had




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, I was. I would know that I feel what I can give it has be only do with only, and I've
rank 7 sample 1 >Hello, I'm a language model, she died the first of a result? To learn is too much you have a time we can be used. You don
rank 7 sample 2 >Hello, I'm a language model, that a special research which is so so long as little is the problem by the. The word is also the problem,
rank 7 sample 3 >Hello, I'm a language model, at a little reason that are only just more it is a healthy, for a very, which a whole, you feel




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, they had a variety of such a small--d, I said. They, I saw a whole. I could
rank 6 sample 1 >Hello, I'm a language model, we're you have not have not be made up and the reason for that we say the way I was, at the
rank 6 sample 2 >Hello, I'm a language model, but, but it in this. And there were in the world for it’s a particular life’t
rank 6 sample 3 >Hello, I'm a language model, to get an overview of a way. But, and a problem is a kind of an "I got up our case




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, where a strong, for the new point. And, if some of the amount of these findings and the best way to
rank 1 sample 1 >Hello, I'm a language model, a few-year-------B-of-2L'-0.
-1
rank 1 sample 2 >Hello, I'm a language model, we eat. If you go to keep your child of you think the time the work. You need. You do my
rank 1 sample 3 >Hello, I'm a language model, a place, which was it. And which is a wide point? He doesn't that?’s to a




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, including a small, but is an easy problem may be for the brain if we have an effective to a more than that
rank 0 sample 1 >Hello, I'm a language model, is.
The term is to learn to a piece will give to develop an important, which the risk to support,
rank 0 sample 2 >Hello, I'm a language model, and a problem of in it is in the process.
What we will be done.
The purpose of a large
rank 0 sample 3 >Hello, I'm a language model, we must be used to be the process, these are not want to be used the best is to know. It takes




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and a way to say this is the question, the same, and one thing (P was never a little piece,
rank 4 sample 1 >Hello, I'm a language model, on an important book and I could give him for more that I also. It also does I like we’s
rank 4 sample 2 >Hello, I'm a language model, the top of course and, and that we might be it’s and when they’ The last ‘


ddp_rank 2: ####### Printing generated samples ####### 

rank 4 sample 3 >Hello, I'm a language model, I am by you to get the child in their personal ways or more good to start to the brain, and even the


rank 2 sample 0 >Hello, I'm a language model, especially for the last year. The world. The new change in the right of the day of the city for the U
rank 2 sample 1 >Hello, I'm a language model, for a specific way of a wide type).
More common way in it is a time in a person that is very
rank 2 sample 2 >Hello, I'm a language model, and a few months, I.’s a child will make any kind is a simple problem as the project and
rank 2 sample 3 >Hello, I'm a language model, but in most common and a very difficult, for them to find a problem where I would use this was not be one


Step 250 | loss: 6.152684 | lr:2.1063e-04 | norm 0.8186 | dt 12399.82ms | 42281.90 tokens/sec
Step 251 | loss: 6.132130 | lr:2.1147e-04 | norm 0.9814 | dt 334.34ms | 1568147.93 tokens/sec
Step 252 | loss: 6.164140 | lr:2.1231e-04 | norm 1.1575 | dt 335.93ms | 1560691.19 tokens/sec
Step 253 | loss: 6.121214 | lr:2.1315e-04 | norm 1.3763 | dt 335.44ms | 1562981.86 tokens/sec
Step 254 | loss: 6.117397 | lr:2.1399e-04 | norm 0.9201 | dt 335.57ms | 1562366.66 tokens/sec
Step 255 | loss: 6.072384 | lr:2.1483e-04 | norm 1.0174 | dt 335.21ms | 1564057.96 tokens/sec
Step 256 | loss: 6.059288 | lr:2.1566e-04 | norm 0.7260 | dt 336.67ms | 1557278.24 tokens/sec
Step 257 | loss: 6.072433 | lr:2.1650e-04 | norm 0.7871 | dt 335.95ms | 1560592.62 tokens/sec
Step 258 | loss: 6.084493 | lr:2.1734e-04 | norm 0.7751 | dt 335.84ms | 1561101.13 tokens/sec
Step 259 | loss: 6.002235 | lr:2.1818e-04 | norm 0.8942 | dt 335.87ms | 1561003.61 tokens/sec
Step 260 | loss: 6.068950 | lr:2.1902e-04 | norm 0.9327 | dt 335.79ms | 1561379.35 tokens/sec
Step 261 | loss: 6.024595 | lr:2.1986e-04 | norm 0.9904 | dt 336.10ms | 1559899.62 tokens/sec
Step 262 | loss: 6.023318 | lr:2.2070e-04 | norm 0.8814 | dt 335.34ms | 1563437.46 tokens/sec
Step 263 | loss: 6.005797 | lr:2.2154e-04 | norm 0.9833 | dt 335.70ms | 1561791.87 tokens/sec
Step 264 | loss: 6.064858 | lr:2.2238e-04 | norm 1.4060 | dt 335.38ms | 1563260.74 tokens/sec
Step 265 | loss: 6.046910 | lr:2.2322e-04 | norm 0.6283 | dt 336.33ms | 1558850.23 tokens/sec
Step 266 | loss: 6.009431 | lr:2.2406e-04 | norm 0.8361 | dt 335.40ms | 1563195.18 tokens/sec
Step 267 | loss: 6.027461 | lr:2.2490e-04 | norm 1.0521 | dt 335.57ms | 1562360.00 tokens/sec
Step 268 | loss: 6.048364 | lr:2.2573e-04 | norm 1.2932 | dt 335.72ms | 1561660.99 tokens/sec
Step 269 | loss: 5.966468 | lr:2.2657e-04 | norm 1.0659 | dt 336.21ms | 1559405.16 tokens/sec
Step 270 | loss: 5.963365 | lr:2.2741e-04 | norm 1.2399 | dt 336.62ms | 1557501.04 tokens/sec
Step 271 | loss: 5.938883 | lr:2.2825e-04 | norm 1.0286 | dt 336.15ms | 1559692.73 tokens/sec
Step 272 | loss: 5.964763 | lr:2.2909e-04 | norm 0.8576 | dt 336.36ms | 1558716.53 tokens/sec
Step 273 | loss: 5.941650 | lr:2.2993e-04 | norm 0.8817 | dt 336.46ms | 1558258.15 tokens/sec
Step 274 | loss: 5.968141 | lr:2.3077e-04 | norm 1.0491 | dt 335.79ms | 1561362.72 tokens/sec
Step 275 | loss: 6.034768 | lr:2.3161e-04 | norm 0.8504 | dt 337.11ms | 1555253.90 tokens/sec
Step 276 | loss: 6.007763 | lr:2.3245e-04 | norm 0.6840 | dt 337.19ms | 1554893.20 tokens/sec
Step 277 | loss: 6.031834 | lr:2.3329e-04 | norm 0.6787 | dt 336.36ms | 1558708.80 tokens/sec
Step 278 | loss: 6.057944 | lr:2.3413e-04 | norm 0.7421 | dt 336.29ms | 1559023.74 tokens/sec
Step 279 | loss: 6.090692 | lr:2.3497e-04 | norm 0.9100 | dt 337.58ms | 1553065.88 tokens/sec
Step 280 | loss: 6.062561 | lr:2.3580e-04 | norm 1.2088 | dt 336.56ms | 1557766.94 tokens/sec
Step 281 | loss: 6.053041 | lr:2.3664e-04 | norm 0.9639 | dt 336.09ms | 1559941.67 tokens/sec
Step 282 | loss: 6.116597 | lr:2.3748e-04 | norm 0.9037 | dt 337.45ms | 1553668.29 tokens/sec
Step 283 | loss: 6.052489 | lr:2.3832e-04 | norm 0.7461 | dt 336.60ms | 1557581.57 tokens/sec
Step 284 | loss: 6.043822 | lr:2.3916e-04 | norm 0.7019 | dt 336.93ms | 1556055.07 tokens/sec
Step 285 | loss: 6.054880 | lr:2.4000e-04 | norm 0.8402 | dt 337.17ms | 1554951.47 tokens/sec
Step 286 | loss: 6.017126 | lr:2.4084e-04 | norm 0.8324 | dt 336.76ms | 1556846.05 tokens/sec
Step 287 | loss: 6.031406 | lr:2.4168e-04 | norm 0.7248 | dt 336.90ms | 1556198.23 tokens/sec
Step 288 | loss: 5.972999 | lr:2.4252e-04 | norm 0.6628 | dt 336.44ms | 1558318.89 tokens/sec
Step 289 | loss: 5.995133 | lr:2.4336e-04 | norm 0.8167 | dt 337.10ms | 1555283.60 tokens/sec
Step 290 | loss: 6.008679 | lr:2.4420e-04 | norm 1.0070 | dt 337.66ms | 1552727.03 tokens/sec
Step 291 | loss: 5.947860 | lr:2.4503e-04 | norm 1.3606 | dt 337.74ms | 1552341.20 tokens/sec
Step 292 | loss: 6.004366 | lr:2.4587e-04 | norm 1.0344 | dt 337.16ms | 1555015.25 tokens/sec
Step 293 | loss: 6.021434 | lr:2.4671e-04 | norm 1.1201 | dt 337.62ms | 1552882.73 tokens/sec
Step 294 | loss: 6.002857 | lr:2.4755e-04 | norm 0.9146 | dt 337.32ms | 1554284.35 tokens/sec
Step 295 | loss: 5.991982 | lr:2.4839e-04 | norm 0.8415 | dt 338.59ms | 1548467.26 tokens/sec
Step 296 | loss: 5.957891 | lr:2.4923e-04 | norm 0.8991 | dt 337.48ms | 1553531.09 tokens/sec
Step 297 | loss: 5.999831 | lr:2.5007e-04 | norm 0.8373 | dt 337.41ms | 1553872.49 tokens/sec
Step 298 | loss: 5.971915 | lr:2.5091e-04 | norm 1.0189 | dt 337.33ms | 1554222.83 tokens/sec
Step 299 | loss: 5.966054 | lr:2.5175e-04 | norm 1.0367 | dt 336.43ms | 1558366.37 tokens/sec
Step 300 | loss: 5.981517 | lr:2.5259e-04 | norm 1.0221 | dt 337.59ms | 1553027.49 tokens/sec
Step 301 | loss: 5.902230 | lr:2.5343e-04 | norm 1.0379 | dt 337.96ms | 1551346.82 tokens/sec
Step 302 | loss: 5.943902 | lr:2.5427e-04 | norm 0.9098 | dt 336.31ms | 1558937.53 tokens/sec
Step 303 | loss: 5.912443 | lr:2.5510e-04 | norm 0.7271 | dt 337.87ms | 1551756.24 tokens/sec
Step 304 | loss: 5.892491 | lr:2.5594e-04 | norm 0.8338 | dt 337.49ms | 1553498.16 tokens/sec
Step 305 | loss: 5.861855 | lr:2.5678e-04 | norm 0.9719 | dt 336.40ms | 1558519.89 tokens/sec
Step 306 | loss: 5.879758 | lr:2.5762e-04 | norm 1.0556 | dt 337.17ms | 1554973.46 tokens/sec
Step 307 | loss: 5.863655 | lr:2.5846e-04 | norm 0.6839 | dt 338.58ms | 1548480.34 tokens/sec
Step 308 | loss: 5.889341 | lr:2.5930e-04 | norm 0.6752 | dt 336.84ms | 1556469.19 tokens/sec
Step 309 | loss: 5.828629 | lr:2.6014e-04 | norm 0.6584 | dt 336.18ms | 1559558.89 tokens/sec
Step 310 | loss: 5.868398 | lr:2.6098e-04 | norm 0.6442 | dt 339.10ms | 1546126.50 tokens/sec
Step 311 | loss: 5.819807 | lr:2.6182e-04 | norm 0.7532 | dt 337.37ms | 1554022.93 tokens/sec
Step 312 | loss: 5.801793 | lr:2.6266e-04 | norm 0.7446 | dt 338.58ms | 1548499.97 tokens/sec
Step 313 | loss: 5.822111 | lr:2.6350e-04 | norm 0.6585 | dt 337.11ms | 1555251.70 tokens/sec
Step 314 | loss: 5.793479 | lr:2.6434e-04 | norm 0.6041 | dt 338.65ms | 1548179.45 tokens/sec
Step 315 | loss: 5.801886 | lr:2.6517e-04 | norm 0.6414 | dt 338.14ms | 1550520.96 tokens/sec
Step 316 | loss: 5.829367 | lr:2.6601e-04 | norm 0.9054 | dt 337.29ms | 1554430.47 tokens/sec
Step 317 | loss: 5.819679 | lr:2.6685e-04 | norm 1.1576 | dt 336.58ms | 1557712.87 tokens/sec
Step 318 | loss: 5.773198 | lr:2.6769e-04 | norm 0.9423 | dt 339.45ms | 1544534.49 tokens/sec
Step 319 | loss: 5.811838 | lr:2.6853e-04 | norm 0.8954 | dt 337.63ms | 1552837.77 tokens/sec
Step 320 | loss: 5.724465 | lr:2.6937e-04 | norm 0.9355 | dt 337.81ms | 1552012.52 tokens/sec
Step 321 | loss: 5.760146 | lr:2.7021e-04 | norm 1.0025 | dt 338.76ms | 1547685.85 tokens/sec
Step 322 | loss: 5.824799 | lr:2.7105e-04 | norm 1.3261 | dt 337.85ms | 1551832.90 tokens/sec
Step 323 | loss: 5.858097 | lr:2.7189e-04 | norm 0.7937 | dt 337.54ms | 1553274.31 tokens/sec
Step 324 | loss: 5.917336 | lr:2.7273e-04 | norm 1.1270 | dt 338.42ms | 1549236.35 tokens/sec
Step 325 | loss: 5.968224 | lr:2.7357e-04 | norm 1.6431 | dt 336.77ms | 1556795.35 tokens/sec
Step 326 | loss: 5.954413 | lr:2.7441e-04 | norm 1.0473 | dt 337.60ms | 1552978.14 tokens/sec
Step 327 | loss: 5.942839 | lr:2.7524e-04 | norm 1.2406 | dt 336.95ms | 1556001.12 tokens/sec
Step 328 | loss: 5.925847 | lr:2.7608e-04 | norm 1.2214 | dt 337.09ms | 1555348.50 tokens/sec
Step 329 | loss: 5.927409 | lr:2.7692e-04 | norm 1.0926 | dt 337.93ms | 1551463.93 tokens/sec
Step 330 | loss: 5.946425 | lr:2.7776e-04 | norm 1.1052 | dt 337.25ms | 1554593.11 tokens/sec
Step 331 | loss: 5.911719 | lr:2.7860e-04 | norm 0.9174 | dt 336.42ms | 1558413.86 tokens/sec
Step 332 | loss: 5.919063 | lr:2.7944e-04 | norm 0.8960 | dt 336.80ms | 1556695.07 tokens/sec
Step 333 | loss: 5.950544 | lr:2.8028e-04 | norm 0.9386 | dt 338.91ms | 1546981.42 tokens/sec
Step 334 | loss: 5.886214 | lr:2.8112e-04 | norm 0.9517 | dt 337.11ms | 1555231.90 tokens/sec
Step 335 | loss: 5.833352 | lr:2.8196e-04 | norm 1.0476 | dt 336.64ms | 1557437.06 tokens/sec
Step 336 | loss: 5.855577 | lr:2.8280e-04 | norm 0.8217 | dt 337.44ms | 1553716.59 tokens/sec
Step 337 | loss: 5.889627 | lr:2.8364e-04 | norm 0.9272 | dt 337.64ms | 1552813.65 tokens/sec
Step 338 | loss: 5.888415 | lr:2.8448e-04 | norm 1.0020 | dt 337.49ms | 1553483.90 tokens/sec
Step 339 | loss: 5.809952 | lr:2.8531e-04 | norm 0.7271 | dt 337.25ms | 1554615.09 tokens/sec
Step 340 | loss: 5.825630 | lr:2.8615e-04 | norm 0.7492 | dt 337.07ms | 1555435.41 tokens/sec
Step 341 | loss: 5.844993 | lr:2.8699e-04 | norm 0.8482 | dt 336.83ms | 1556558.43 tokens/sec
Step 342 | loss: 5.835463 | lr:2.8783e-04 | norm 0.9999 | dt 337.54ms | 1553276.51 tokens/sec
Step 343 | loss: 5.771517 | lr:2.8867e-04 | norm 1.2860 | dt 337.21ms | 1554770.07 tokens/sec
Step 344 | loss: 5.787215 | lr:2.8951e-04 | norm 0.9723 | dt 337.39ms | 1553939.47 tokens/sec
Step 345 | loss: 5.827129 | lr:2.9035e-04 | norm 1.0158 | dt 337.97ms | 1551275.68 tokens/sec
Step 346 | loss: 5.731421 | lr:2.9119e-04 | norm 0.7984 | dt 338.47ms | 1548971.17 tokens/sec
Step 347 | loss: 5.746137 | lr:2.9203e-04 | norm 0.6987 | dt 336.87ms | 1556339.21 tokens/sec
Step 348 | loss: 5.736053 | lr:2.9287e-04 | norm 0.8441 | dt 337.79ms | 1552115.49 tokens/sec
Step 349 | loss: 5.775333 | lr:2.9371e-04 | norm 1.1445 | dt 337.50ms | 1553443.29 tokens/sec
Step 350 | loss: 5.737188 | lr:2.9455e-04 | norm 1.3619 | dt 337.39ms | 1553933.98 tokens/sec
Step 351 | loss: 5.745535 | lr:2.9538e-04 | norm 1.2367 | dt 337.85ms | 1551855.90 tokens/sec
Step 352 | loss: 5.705517 | lr:2.9622e-04 | norm 1.1332 | dt 337.27ms | 1554490.91 tokens/sec
Step 353 | loss: 5.753740 | lr:2.9706e-04 | norm 1.2437 | dt 336.64ms | 1557422.72 tokens/sec
Step 354 | loss: 5.734985 | lr:2.9790e-04 | norm 0.8512 | dt 337.66ms | 1552707.29 tokens/sec
Step 355 | loss: 5.717649 | lr:2.9874e-04 | norm 0.9548 | dt 337.66ms | 1552704.00 tokens/sec
Step 356 | loss: 5.708278 | lr:2.9958e-04 | norm 1.0185 | dt 336.68ms | 1557217.59 tokens/sec
Step 357 | loss: 5.660630 | lr:3.0042e-04 | norm 0.9063 | dt 337.26ms | 1554566.73 tokens/sec
Step 358 | loss: 5.660627 | lr:3.0126e-04 | norm 0.9068 | dt 337.67ms | 1552686.46 tokens/sec
Step 359 | loss: 5.650168 | lr:3.0210e-04 | norm 0.6994 | dt 337.37ms | 1554035.01 tokens/sec
Step 360 | loss: 5.660223 | lr:3.0294e-04 | norm 0.9206 | dt 337.12ms | 1555193.40 tokens/sec
Step 361 | loss: 5.671718 | lr:3.0378e-04 | norm 1.0238 | dt 337.82ms | 1551965.42 tokens/sec
Step 362 | loss: 5.667393 | lr:3.0462e-04 | norm 1.0516 | dt 336.77ms | 1556819.60 tokens/sec
Step 363 | loss: 5.611425 | lr:3.0545e-04 | norm 0.9233 | dt 337.18ms | 1554899.80 tokens/sec
Step 364 | loss: 5.589179 | lr:3.0629e-04 | norm 0.9336 | dt 338.12ms | 1550606.24 tokens/sec
Step 365 | loss: 5.609372 | lr:3.0713e-04 | norm 1.0056 | dt 336.81ms | 1556634.46 tokens/sec
Step 366 | loss: 5.711653 | lr:3.0797e-04 | norm 1.3304 | dt 336.72ms | 1557030.14 tokens/sec
Step 367 | loss: 5.588900 | lr:3.0881e-04 | norm 1.1093 | dt 338.26ms | 1549977.80 tokens/sec
Step 368 | loss: 5.620183 | lr:3.0965e-04 | norm 1.0763 | dt 336.65ms | 1557357.64 tokens/sec
Step 369 | loss: 5.638082 | lr:3.1049e-04 | norm 1.0672 | dt 337.59ms | 1553041.75 tokens/sec
Step 370 | loss: 5.755267 | lr:3.1133e-04 | norm 1.1104 | dt 338.29ms | 1549840.16 tokens/sec
Step 371 | loss: 5.751788 | lr:3.1217e-04 | norm 1.2171 | dt 335.74ms | 1561586.69 tokens/sec
Step 372 | loss: 5.767649 | lr:3.1301e-04 | norm 0.9404 | dt 337.82ms | 1551956.66 tokens/sec
Step 373 | loss: 5.687946 | lr:3.1385e-04 | norm 0.8818 | dt 338.20ms | 1550243.32 tokens/sec
Step 374 | loss: 5.771980 | lr:3.1469e-04 | norm 1.0923 | dt 337.33ms | 1554208.55 tokens/sec
Step 375 | loss: 5.770478 | lr:3.1552e-04 | norm 1.0925 | dt 337.30ms | 1554348.07 tokens/sec
Step 376 | loss: 5.774114 | lr:3.1636e-04 | norm 0.9857 | dt 337.83ms | 1551923.80 tokens/sec
Step 377 | loss: 5.754438 | lr:3.1720e-04 | norm 0.9662 | dt 893.09ms | 587047.41 tokens/sec
Step 378 | loss: 5.679564 | lr:3.1804e-04 | norm 0.8659 | dt 334.33ms | 1568172.54 tokens/sec
Step 379 | loss: 5.685197 | lr:3.1888e-04 | norm 0.8564 | dt 988.42ms | 530430.63 tokens/sec
Step 380 | loss: 5.696131 | lr:3.1972e-04 | norm 0.7618 | dt 337.41ms | 1553837.36 tokens/sec
Step 381 | loss: 5.632820 | lr:3.2056e-04 | norm 0.6915 | dt 337.52ms | 1553370.87 tokens/sec
Step 382 | loss: 5.666755 | lr:3.2140e-04 | norm 0.8716 | dt 336.81ms | 1556619.03 tokens/sec
Step 383 | loss: 5.677026 | lr:3.2224e-04 | norm 1.4640 | dt 337.65ms | 1552765.40 tokens/sec
Step 384 | loss: 5.698055 | lr:3.2308e-04 | norm 0.8569 | dt 337.87ms | 1551762.81 tokens/sec
Step 385 | loss: 5.792250 | lr:3.2392e-04 | norm 1.1520 | dt 337.27ms | 1554521.68 tokens/sec
Step 386 | loss: 5.626618 | lr:3.2476e-04 | norm 0.9623 | dt 337.48ms | 1553522.31 tokens/sec
Step 387 | loss: 5.673495 | lr:3.2559e-04 | norm 0.8996 | dt 337.15ms | 1555077.93 tokens/sec
Step 388 | loss: 5.622682 | lr:3.2643e-04 | norm 1.1403 | dt 337.64ms | 1552779.65 tokens/sec
Step 389 | loss: 5.646862 | lr:3.2727e-04 | norm 0.8483 | dt 338.56ms | 1548585.02 tokens/sec
Step 390 | loss: 5.607319 | lr:3.2811e-04 | norm 1.0194 | dt 337.90ms | 1551622.66 tokens/sec
Step 391 | loss: 5.595090 | lr:3.2895e-04 | norm 0.7739 | dt 337.81ms | 1552026.76 tokens/sec
Step 392 | loss: 5.606853 | lr:3.2979e-04 | norm 0.7232 | dt 337.90ms | 1551610.62 tokens/sec
Step 393 | loss: 5.532372 | lr:3.3063e-04 | norm 0.8062 | dt 338.45ms | 1549067.19 tokens/sec
Step 394 | loss: 5.532289 | lr:3.3147e-04 | norm 0.9100 | dt 338.02ms | 1551063.41 tokens/sec
Step 395 | loss: 5.575397 | lr:3.3231e-04 | norm 1.0418 | dt 338.14ms | 1550484.89 tokens/sec
Step 396 | loss: 5.572242 | lr:3.3315e-04 | norm 1.4890 | dt 338.16ms | 1550421.48 tokens/sec
Step 397 | loss: 5.544139 | lr:3.3399e-04 | norm 1.0064 | dt 338.11ms | 1550643.42 tokens/sec
Step 398 | loss: 5.585965 | lr:3.3483e-04 | norm 1.2453 | dt 338.97ms | 1546700.69 tokens/sec
Step 399 | loss: 5.531214 | lr:3.3566e-04 | norm 0.8340 | dt 338.39ms | 1549337.86 tokens/sec
Step 400 | loss: 5.571337 | lr:3.3650e-04 | norm 0.9666 | dt 338.26ms | 1549949.40 tokens/sec
Step 401 | loss: 5.527773 | lr:3.3734e-04 | norm 1.0370 | dt 338.29ms | 1549837.97 tokens/sec
Step 402 | loss: 5.554536 | lr:3.3818e-04 | norm 1.0689 | dt 338.32ms | 1549693.80 tokens/sec
Step 403 | loss: 5.513550 | lr:3.3902e-04 | norm 1.4856 | dt 337.67ms | 1552667.82 tokens/sec
Step 404 | loss: 5.495136 | lr:3.3986e-04 | norm 1.1227 | dt 337.53ms | 1553289.67 tokens/sec
Step 405 | loss: 5.469842 | lr:3.4070e-04 | norm 0.9913 | dt 337.86ms | 1551808.81 tokens/sec
Step 406 | loss: 5.449504 | lr:3.4154e-04 | norm 0.7561 | dt 337.92ms | 1551526.33 tokens/sec
Step 407 | loss: 5.453320 | lr:3.4238e-04 | norm 0.8305 | dt 337.88ms | 1551719.01 tokens/sec
Step 408 | loss: 5.468381 | lr:3.4322e-04 | norm 0.9181 | dt 338.02ms | 1551057.94 tokens/sec
Step 409 | loss: 5.474973 | lr:3.4406e-04 | norm 1.1157 | dt 337.51ms | 1553382.94 tokens/sec
Step 410 | loss: 5.495864 | lr:3.4490e-04 | norm 1.1382 | dt 339.02ms | 1546477.71 tokens/sec
Step 411 | loss: 5.501566 | lr:3.4573e-04 | norm 1.1323 | dt 338.06ms | 1550868.70 tokens/sec
Step 412 | loss: 5.434526 | lr:3.4657e-04 | norm 1.0308 | dt 338.07ms | 1550836.98 tokens/sec
Step 413 | loss: 5.515710 | lr:3.4741e-04 | norm 0.9754 | dt 338.39ms | 1549366.24 tokens/sec
Step 414 | loss: 5.479951 | lr:3.4825e-04 | norm 0.8897 | dt 338.04ms | 1550964.96 tokens/sec
Step 415 | loss: 5.580207 | lr:3.4909e-04 | norm 0.9531 | dt 338.02ms | 1551042.63 tokens/sec
Step 416 | loss: 5.648771 | lr:3.4993e-04 | norm 1.2453 | dt 337.18ms | 1554928.38 tokens/sec
Step 417 | loss: 5.642892 | lr:3.5077e-04 | norm 1.0884 | dt 339.04ms | 1546408.11 tokens/sec
Step 418 | loss: 5.609338 | lr:3.5161e-04 | norm 1.0482 | dt 337.26ms | 1554533.77 tokens/sec
Step 419 | loss: 5.595305 | lr:3.5245e-04 | norm 0.9392 | dt 337.42ms | 1553793.44 tokens/sec
Step 420 | loss: 5.634421 | lr:3.5329e-04 | norm 1.0060 | dt 337.76ms | 1552236.00 tokens/sec
Step 421 | loss: 5.539742 | lr:3.5413e-04 | norm 1.0492 | dt 337.84ms | 1551876.70 tokens/sec
Step 422 | loss: 5.635020 | lr:3.5497e-04 | norm 0.9380 | dt 338.59ms | 1548437.82 tokens/sec
Step 423 | loss: 5.579663 | lr:3.5580e-04 | norm 0.9797 | dt 338.16ms | 1550405.09 tokens/sec
Step 424 | loss: 5.555702 | lr:3.5664e-04 | norm 1.4746 | dt 337.89ms | 1551633.61 tokens/sec
Step 425 | loss: 5.546840 | lr:3.5748e-04 | norm 0.8705 | dt 338.69ms | 1547987.64 tokens/sec
Step 426 | loss: 5.560627 | lr:3.5832e-04 | norm 0.8835 | dt 338.63ms | 1548245.94 tokens/sec
Step 427 | loss: 5.525847 | lr:3.5916e-04 | norm 0.7638 | dt 338.29ms | 1549837.97 tokens/sec
Step 428 | loss: 5.596647 | lr:3.6000e-04 | norm 0.8380 | dt 338.19ms | 1550263.00 tokens/sec
Step 429 | loss: 5.541708 | lr:3.6084e-04 | norm 0.9745 | dt 337.74ms | 1552358.73 tokens/sec
Step 430 | loss: 5.499987 | lr:3.6168e-04 | norm 1.0191 | dt 338.50ms | 1548868.62 tokens/sec
Step 431 | loss: 5.516249 | lr:3.6252e-04 | norm 0.9376 | dt 337.99ms | 1551172.82 tokens/sec
Step 432 | loss: 5.574913 | lr:3.6336e-04 | norm 0.9275 | dt 338.33ms | 1549626.10 tokens/sec
Step 433 | loss: 5.472796 | lr:3.6420e-04 | norm 0.8226 | dt 338.55ms | 1548622.10 tokens/sec
Step 434 | loss: 5.501634 | lr:3.6503e-04 | norm 0.8412 | dt 338.97ms | 1546687.64 tokens/sec
Step 435 | loss: 5.484283 | lr:3.6587e-04 | norm 0.9728 | dt 338.46ms | 1549029.00 tokens/sec
Step 436 | loss: 5.592025 | lr:3.6671e-04 | norm 0.9331 | dt 338.38ms | 1549393.53 tokens/sec
Step 437 | loss: 5.479622 | lr:3.6755e-04 | norm 0.9432 | dt 338.47ms | 1549008.27 tokens/sec
Step 438 | loss: 5.470455 | lr:3.6839e-04 | norm 1.1722 | dt 344.65ms | 1521207.97 tokens/sec
Step 439 | loss: 5.363596 | lr:3.6923e-04 | norm 1.4025 | dt 338.30ms | 1549764.79 tokens/sec
Step 440 | loss: 5.430857 | lr:3.7007e-04 | norm 1.1549 | dt 338.69ms | 1548000.72 tokens/sec
Step 441 | loss: 5.405965 | lr:3.7091e-04 | norm 0.8450 | dt 338.54ms | 1548658.09 tokens/sec
Step 442 | loss: 5.403715 | lr:3.7175e-04 | norm 0.8078 | dt 338.25ms | 1549984.36 tokens/sec
Step 443 | loss: 5.448775 | lr:3.7259e-04 | norm 0.8273 | dt 338.25ms | 1550017.13 tokens/sec
Step 444 | loss: 5.420075 | lr:3.7343e-04 | norm 0.9805 | dt 338.46ms | 1549042.09 tokens/sec
Step 445 | loss: 5.425191 | lr:3.7427e-04 | norm 1.1274 | dt 337.82ms | 1551967.61 tokens/sec
Step 446 | loss: 5.421856 | lr:3.7510e-04 | norm 1.0319 | dt 337.80ms | 1552083.72 tokens/sec
Step 447 | loss: 5.434578 | lr:3.7594e-04 | norm 1.0655 | dt 337.48ms | 1553534.38 tokens/sec
Step 448 | loss: 5.379395 | lr:3.7678e-04 | norm 0.8436 | dt 337.66ms | 1552714.97 tokens/sec
Step 449 | loss: 5.401726 | lr:3.7762e-04 | norm 1.0617 | dt 338.19ms | 1550289.23 tokens/sec
Step 450 | loss: 5.386605 | lr:3.7846e-04 | norm 1.1259 | dt 338.06ms | 1550880.73 tokens/sec
Step 451 | loss: 5.308246 | lr:3.7930e-04 | norm 0.9736 | dt 337.97ms | 1551275.68 tokens/sec
Step 452 | loss: 5.333871 | lr:3.8014e-04 | norm 0.7366 | dt 339.01ms | 1546521.21 tokens/sec
Step 453 | loss: 5.370274 | lr:3.8098e-04 | norm 0.7298 | dt 337.65ms | 1552759.92 tokens/sec
Step 454 | loss: 5.297441 | lr:3.8182e-04 | norm 0.7931 | dt 338.54ms | 1548691.90 tokens/sec
Step 455 | loss: 5.364592 | lr:3.8266e-04 | norm 1.0040 | dt 338.00ms | 1551146.56 tokens/sec
Step 456 | loss: 5.297766 | lr:3.8350e-04 | norm 1.0172 | dt 345.03ms | 1519541.87 tokens/sec
Step 457 | loss: 5.304100 | lr:3.8434e-04 | norm 0.9830 | dt 337.73ms | 1552381.74 tokens/sec
Step 458 | loss: 5.305188 | lr:3.8517e-04 | norm 0.9004 | dt 337.28ms | 1554439.26 tokens/sec
Step 459 | loss: 5.278661 | lr:3.8601e-04 | norm 0.9428 | dt 337.25ms | 1554608.50 tokens/sec
Step 460 | loss: 5.301112 | lr:3.8685e-04 | norm 1.2609 | dt 337.08ms | 1555371.60 tokens/sec
Step 461 | loss: 5.261052 | lr:3.8769e-04 | norm 1.0317 | dt 337.44ms | 1553704.51 tokens/sec
Step 462 | loss: 5.375838 | lr:3.8853e-04 | norm 1.1915 | dt 337.66ms | 1552732.51 tokens/sec
Step 463 | loss: 5.444336 | lr:3.8937e-04 | norm 1.0420 | dt 336.80ms | 1556658.70 tokens/sec
Step 464 | loss: 5.445940 | lr:3.9021e-04 | norm 1.0945 | dt 337.20ms | 1554823.94 tokens/sec
Step 465 | loss: 5.494750 | lr:3.9105e-04 | norm 0.9510 | dt 337.31ms | 1554315.11 tokens/sec
Step 466 | loss: 5.406145 | lr:3.9189e-04 | norm 1.0430 | dt 337.26ms | 1554543.66 tokens/sec
Step 467 | loss: 5.414527 | lr:3.9273e-04 | norm 1.1429 | dt 337.57ms | 1553122.92 tokens/sec
Step 468 | loss: 5.409740 | lr:3.9357e-04 | norm 0.9931 | dt 337.31ms | 1554332.69 tokens/sec
Step 469 | loss: 5.396482 | lr:3.9441e-04 | norm 0.9727 | dt 337.67ms | 1552674.40 tokens/sec
Step 470 | loss: 5.398954 | lr:3.9524e-04 | norm 0.9073 | dt 337.65ms | 1552762.11 tokens/sec
Step 471 | loss: 5.426689 | lr:3.9608e-04 | norm 0.8831 | dt 337.89ms | 1551650.03 tokens/sec
Step 472 | loss: 5.410432 | lr:3.9692e-04 | norm 1.0758 | dt 336.33ms | 1558864.60 tokens/sec
Step 473 | loss: 5.428057 | lr:3.9776e-04 | norm 0.9315 | dt 337.17ms | 1554974.56 tokens/sec
Step 474 | loss: 5.367037 | lr:3.9860e-04 | norm 0.8157 | dt 337.86ms | 1551805.52 tokens/sec
Step 475 | loss: 5.342388 | lr:3.9944e-04 | norm 0.8707 | dt 337.73ms | 1552408.05 tokens/sec
Step 476 | loss: 5.375684 | lr:4.0028e-04 | norm 0.8449 | dt 338.13ms | 1550555.95 tokens/sec
Step 477 | loss: 5.395224 | lr:4.0112e-04 | norm 1.0030 | dt 337.52ms | 1553363.19 tokens/sec
Step 478 | loss: 5.361134 | lr:4.0196e-04 | norm 1.1016 | dt 336.54ms | 1557899.37 tokens/sec
Step 479 | loss: 5.377138 | lr:4.0280e-04 | norm 0.9946 | dt 338.96ms | 1546775.76 tokens/sec
Step 480 | loss: 5.338930 | lr:4.0364e-04 | norm 0.8026 | dt 337.93ms | 1551461.74 tokens/sec
Step 481 | loss: 5.361724 | lr:4.0448e-04 | norm 0.7430 | dt 339.59ms | 1543876.27 tokens/sec
Step 482 | loss: 5.336863 | lr:4.0531e-04 | norm 0.7508 | dt 339.45ms | 1544507.37 tokens/sec
Step 483 | loss: 5.351123 | lr:4.0615e-04 | norm 0.7451 | dt 339.33ms | 1545083.61 tokens/sec
Step 484 | loss: 5.287655 | lr:4.0699e-04 | norm 0.7974 | dt 338.21ms | 1550177.75 tokens/sec
Step 485 | loss: 5.315982 | lr:4.0783e-04 | norm 1.2212 | dt 338.37ms | 1549454.67 tokens/sec
Step 486 | loss: 5.339806 | lr:4.0867e-04 | norm 1.0562 | dt 338.44ms | 1549151.22 tokens/sec
Step 487 | loss: 5.238549 | lr:4.0951e-04 | norm 0.9213 | dt 338.10ms | 1550671.85 tokens/sec
Step 488 | loss: 5.263979 | lr:4.1035e-04 | norm 1.1605 | dt 338.16ms | 1550419.30 tokens/sec
Step 489 | loss: 5.299175 | lr:4.1119e-04 | norm 1.1290 | dt 337.96ms | 1551349.01 tokens/sec
Step 490 | loss: 5.249381 | lr:4.1203e-04 | norm 1.0037 | dt 337.51ms | 1553387.33 tokens/sec
Step 491 | loss: 5.277011 | lr:4.1287e-04 | norm 0.9669 | dt 336.72ms | 1557041.17 tokens/sec
Step 492 | loss: 5.248204 | lr:4.1371e-04 | norm 0.9219 | dt 337.35ms | 1554126.17 tokens/sec
Step 493 | loss: 5.259328 | lr:4.1455e-04 | norm 1.0096 | dt 337.61ms | 1552955.11 tokens/sec
Step 494 | loss: 5.212246 | lr:4.1538e-04 | norm 0.9281 | dt 336.56ms | 1557765.83 tokens/sec
Step 495 | loss: 5.227996 | lr:4.1622e-04 | norm 0.8813 | dt 337.82ms | 1551984.04 tokens/sec
Step 496 | loss: 5.222668 | lr:4.1706e-04 | norm 0.8761 | dt 337.46ms | 1553610.11 tokens/sec
Step 497 | loss: 5.165417 | lr:4.1790e-04 | norm 0.8734 | dt 337.57ms | 1553103.18 tokens/sec
Step 498 | loss: 5.142778 | lr:4.1874e-04 | norm 1.0573 | dt 336.77ms | 1556830.62 tokens/sec
Step 499 | loss: 5.227454 | lr:4.1958e-04 | norm 0.9103 | dt 337.58ms | 1553068.08 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 500: 5.2713
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2399/10042=0.2389


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, the author is a single language. The same thing are a second in the context, the other.
- Is there
rank 5 sample 1 >Hello, I'm a language model, there would never get what to me!
Let me to know who would. This would you don't take the information
rank 5 sample 2 >Hello, I'm a language model, such a way of being a society by the society, and the individual society is a great way to the world. However
rank 5 sample 3 >Hello, I'm a language model, meaning I mean to the image.
I think this mean you don't find any data. Even though you are.




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, one might have a problem with the different kinds of different factors have been linked to the primary problem for the environment.

rank 1 sample 1 >Hello, I'm a language model, a group of student education, a class grade level, student education; and I-day work with the student-based
rank 1 sample 2 >Hello, I'm a language model, the student's ability to be able to be able to teach and to learn how to learn to improve our learning environment and


ddp_rank 2: ####### Printing generated samples ####### 

rank 1 sample 3 >Hello, I'm a language model, and I am all. When you will choose to help readers know from all with the best course they are involved in the


rank 2 sample 0 >Hello, I'm a language model, whether they're really know why they know what is a bit.
One thing the questions are just how this is that
rank 2 sample 1 >Hello, I'm a language model, how to talk is going to you a right? Just say that it gets to be a bit more complicated than the brain
rank 2 sample 2 >Hello, I'm a language model, I am not my name, that.
- I am I think your students will help them how them get that is
rank 2 sample 3 >Hello, I'm a language model, the world just the planet.
If I found you were talking about the brain cell phone a life. You could tell




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and the current language language (or language I) my personal thoughts, that this group can't apply and how to use
rank 7 sample 1 >Hello, I'm a language model, from multiple groups, and you don’t always really need to be able to look like to look for them from
rank 7 sample 2 >Hello, I'm a language model, including the authors found the “lac of the world” they were able to be able to be considered the
rank 7 sample 3 >Hello, I'm a language model,
- If you’t give you should always expect you’re trying to share all the next thing when




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, I am going to take some of a little idea, and I am it would come, then I do so I think
rank 4 sample 1 >Hello, I'm a language model, of your brain a genetic change it as much faster in, and is just one that I am never do so hard.
rank 4 sample 2 >Hello, I'm a language model, but its origin we need to be an example of the form of a unique form of the term in various parts which is
rank 4 sample 3 >Hello, I'm a language model, and it a single, but it are just in, too and then are too.
Why Is the most things we




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, an online, and is something not, and I know the process. I can change the current environment, the environment as
rank 6 sample 1 >Hello, I'm a language model, which is the most useful in the course which is to the body. There are many types of which is an extremely toxic

rank 6 sample 2 >Hello, I'm a language model, the I will become the first.<|endoftext|>This is a very easy in the process of the brain. And then my body

ddp_rank 3: ####### Printing generated samples ####### 

rank 6 sample 3 >Hello, I'm a language model, is also a great opportunity to work on a different way.
These are at a number 3, 50 is at 100


rank 3 sample 0 >Hello, I'm a language model, and I am not my mind, which will not explain a change. She becomes an increase for every three-thirds between
rank 3 sample 1 >Hello, I'm a language model, and how the language is involved by the world, and so it's be doing so much more difficult to keep your environment
rank 3 sample 2 >Hello, I'm a language model, and the brain development as the brain. What will need to have a different stages, the brain (e., if
rank 3 sample 3 >Hello, I'm a language model, and so I’m now, and how you want them, if you will look on that end of them.




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I was a good job for this blog for it. I got something. I have tried that my first had never
rank 0 sample 1 >Hello, I'm a language model, then then I can see the current text. It contains you can find my first and you may be aware of the results
rank 0 sample 2 >Hello, I'm a language model, I am an expert? A 'f' "Why is 'a]
" is the most important and I're
rank 0 sample 3 >Hello, I'm a language model, which could be in the other words and that students are not, I would not remember what if I did not be so


Step 500 | loss: 5.176967 | lr:4.2042e-04 | norm 0.7366 | dt 12364.55ms | 42402.53 tokens/sec
Step 501 | loss: 5.135783 | lr:4.2126e-04 | norm 0.8474 | dt 335.07ms | 1564732.38 tokens/sec
Step 502 | loss: 5.131699 | lr:4.2210e-04 | norm 1.0048 | dt 336.48ms | 1558159.89 tokens/sec
Step 503 | loss: 5.220740 | lr:4.2294e-04 | norm 1.2164 | dt 335.90ms | 1560841.85 tokens/sec
Step 504 | loss: 5.141006 | lr:4.2378e-04 | norm 1.1722 | dt 335.37ms | 1563324.09 tokens/sec
Step 505 | loss: 5.228023 | lr:4.2462e-04 | norm 1.0320 | dt 336.64ms | 1557399.56 tokens/sec
Step 506 | loss: 5.117410 | lr:4.2545e-04 | norm 1.3611 | dt 336.94ms | 1556029.75 tokens/sec
Step 507 | loss: 5.148979 | lr:4.2629e-04 | norm 1.0564 | dt 336.00ms | 1560378.90 tokens/sec
Step 508 | loss: 5.219683 | lr:4.2713e-04 | norm 0.9955 | dt 335.98ms | 1560479.66 tokens/sec
Step 509 | loss: 5.329357 | lr:4.2797e-04 | norm 1.0183 | dt 336.61ms | 1557536.34 tokens/sec
Step 510 | loss: 5.339177 | lr:4.2881e-04 | norm 1.0621 | dt 336.83ms | 1556554.03 tokens/sec
Step 511 | loss: 5.306953 | lr:4.2965e-04 | norm 0.9479 | dt 337.67ms | 1552642.61 tokens/sec
Step 512 | loss: 5.417808 | lr:4.3049e-04 | norm 1.1690 | dt 335.51ms | 1562683.08 tokens/sec
Step 513 | loss: 5.295659 | lr:4.3133e-04 | norm 1.0032 | dt 336.14ms | 1559728.13 tokens/sec
Step 514 | loss: 5.285402 | lr:4.3217e-04 | norm 1.0829 | dt 335.79ms | 1561339.44 tokens/sec
Step 515 | loss: 5.320818 | lr:4.3301e-04 | norm 0.9153 | dt 336.91ms | 1556183.91 tokens/sec
Step 516 | loss: 5.358948 | lr:4.3385e-04 | norm 1.0514 | dt 336.31ms | 1558924.27 tokens/sec
Step 517 | loss: 5.283974 | lr:4.3469e-04 | norm 0.9673 | dt 335.74ms | 1561594.45 tokens/sec
Step 518 | loss: 5.330886 | lr:4.3552e-04 | norm 0.8763 | dt 336.63ms | 1557450.30 tokens/sec
Step 519 | loss: 5.270975 | lr:4.3636e-04 | norm 0.8726 | dt 336.23ms | 1559311.17 tokens/sec
Step 520 | loss: 5.232825 | lr:4.3720e-04 | norm 0.8599 | dt 337.52ms | 1553370.87 tokens/sec
Step 521 | loss: 5.236804 | lr:4.3804e-04 | norm 0.8321 | dt 336.55ms | 1557850.81 tokens/sec
Step 522 | loss: 5.241816 | lr:4.3888e-04 | norm 0.9831 | dt 336.02ms | 1560290.33 tokens/sec
Step 523 | loss: 5.228947 | lr:4.3972e-04 | norm 1.0102 | dt 337.38ms | 1554020.73 tokens/sec
Step 524 | loss: 5.205669 | lr:4.4056e-04 | norm 1.0030 | dt 336.66ms | 1557303.60 tokens/sec
Step 525 | loss: 5.227226 | lr:4.4140e-04 | norm 0.9099 | dt 337.23ms | 1554692.03 tokens/sec
Step 526 | loss: 5.182676 | lr:4.4224e-04 | norm 0.8969 | dt 337.58ms | 1553093.30 tokens/sec
Step 527 | loss: 5.229387 | lr:4.4308e-04 | norm 0.9445 | dt 337.05ms | 1555526.73 tokens/sec
Step 528 | loss: 5.200468 | lr:4.4392e-04 | norm 0.9588 | dt 336.79ms | 1556708.29 tokens/sec
Step 529 | loss: 5.193743 | lr:4.4476e-04 | norm 0.9770 | dt 338.04ms | 1550956.21 tokens/sec
Step 530 | loss: 5.176675 | lr:4.4559e-04 | norm 1.0101 | dt 336.56ms | 1557797.84 tokens/sec
Step 531 | loss: 5.199525 | lr:4.4643e-04 | norm 0.9839 | dt 337.93ms | 1551465.03 tokens/sec
Step 532 | loss: 5.162719 | lr:4.4727e-04 | norm 0.9604 | dt 337.65ms | 1552753.34 tokens/sec
Step 533 | loss: 5.161902 | lr:4.4811e-04 | norm 0.9410 | dt 336.18ms | 1559555.57 tokens/sec
Step 534 | loss: 5.133136 | lr:4.4895e-04 | norm 1.0159 | dt 337.94ms | 1551444.23 tokens/sec
Step 535 | loss: 5.198201 | lr:4.4979e-04 | norm 0.7650 | dt 338.48ms | 1548940.62 tokens/sec
Step 536 | loss: 5.131031 | lr:4.5063e-04 | norm 0.7429 | dt 337.87ms | 1551752.96 tokens/sec
Step 537 | loss: 5.091885 | lr:4.5147e-04 | norm 0.6758 | dt 337.66ms | 1552714.97 tokens/sec
Step 538 | loss: 5.134216 | lr:4.5231e-04 | norm 0.6730 | dt 337.03ms | 1555590.56 tokens/sec
Step 539 | loss: 5.043668 | lr:4.5315e-04 | norm 0.6610 | dt 338.52ms | 1548755.17 tokens/sec
Step 540 | loss: 5.083338 | lr:4.5399e-04 | norm 0.8195 | dt 338.03ms | 1551029.50 tokens/sec
Step 541 | loss: 5.119905 | lr:4.5483e-04 | norm 1.0276 | dt 337.51ms | 1553379.65 tokens/sec
Step 542 | loss: 5.108123 | lr:4.5566e-04 | norm 1.1090 | dt 337.67ms | 1552652.48 tokens/sec
Step 543 | loss: 5.108759 | lr:4.5650e-04 | norm 1.0454 | dt 337.83ms | 1551916.13 tokens/sec
Step 544 | loss: 5.038817 | lr:4.5734e-04 | norm 1.0961 | dt 337.98ms | 1551227.53 tokens/sec
Step 545 | loss: 4.992788 | lr:4.5818e-04 | norm 0.9737 | dt 336.87ms | 1556338.10 tokens/sec
Step 546 | loss: 5.019209 | lr:4.5902e-04 | norm 1.0247 | dt 337.76ms | 1552243.67 tokens/sec
Step 547 | loss: 5.062524 | lr:4.5986e-04 | norm 0.9849 | dt 337.16ms | 1555027.34 tokens/sec
Step 548 | loss: 5.034400 | lr:4.6070e-04 | norm 1.0880 | dt 337.43ms | 1553783.56 tokens/sec
Step 549 | loss: 5.117998 | lr:4.6154e-04 | norm 1.0418 | dt 337.60ms | 1553004.46 tokens/sec
Step 550 | loss: 5.008698 | lr:4.6238e-04 | norm 1.1109 | dt 337.16ms | 1555006.45 tokens/sec
Step 551 | loss: 4.947157 | lr:4.6322e-04 | norm 0.8668 | dt 337.80ms | 1552074.96 tokens/sec
Step 552 | loss: 5.065410 | lr:4.6406e-04 | norm 0.7077 | dt 337.22ms | 1554730.50 tokens/sec
Step 553 | loss: 5.031253 | lr:4.6490e-04 | norm 0.7011 | dt 337.09ms | 1555340.80 tokens/sec
Step 554 | loss: 5.035065 | lr:4.6573e-04 | norm 0.7581 | dt 337.44ms | 1553704.51 tokens/sec
Step 555 | loss: 5.032207 | lr:4.6657e-04 | norm 0.6492 | dt 337.17ms | 1554951.47 tokens/sec
Step 556 | loss: 5.174467 | lr:4.6741e-04 | norm 0.6972 | dt 337.82ms | 1551980.75 tokens/sec
Step 557 | loss: 5.148102 | lr:4.6825e-04 | norm 0.7594 | dt 338.65ms | 1548160.92 tokens/sec
Step 558 | loss: 5.068285 | lr:4.6909e-04 | norm 0.6868 | dt 337.35ms | 1554155.82 tokens/sec
Step 559 | loss: 5.124781 | lr:4.6993e-04 | norm 0.9283 | dt 337.30ms | 1554375.54 tokens/sec
Step 560 | loss: 5.158481 | lr:4.7077e-04 | norm 1.4076 | dt 337.92ms | 1551494.58 tokens/sec
Step 561 | loss: 5.107246 | lr:4.7161e-04 | norm 0.8684 | dt 337.34ms | 1554190.97 tokens/sec
Step 562 | loss: 5.157669 | lr:4.7245e-04 | norm 1.0401 | dt 337.81ms | 1552021.28 tokens/sec
Step 563 | loss: 5.147981 | lr:4.7329e-04 | norm 0.8676 | dt 337.74ms | 1552347.77 tokens/sec
Step 564 | loss: 5.155299 | lr:4.7413e-04 | norm 0.9167 | dt 338.15ms | 1550468.49 tokens/sec
Step 565 | loss: 5.108494 | lr:4.7497e-04 | norm 0.8544 | dt 337.04ms | 1555548.74 tokens/sec
Step 566 | loss: 5.084116 | lr:4.7580e-04 | norm 0.7271 | dt 1020.41ms | 513799.60 tokens/sec
Step 567 | loss: 5.079660 | lr:4.7664e-04 | norm 0.7652 | dt 335.08ms | 1564647.77 tokens/sec
Step 568 | loss: 5.076475 | lr:4.7748e-04 | norm 0.7666 | dt 336.61ms | 1557535.24 tokens/sec
Step 569 | loss: 5.055915 | lr:4.7832e-04 | norm 0.7120 | dt 1012.32ms | 517909.07 tokens/sec
Step 570 | loss: 5.033354 | lr:4.7916e-04 | norm 0.7249 | dt 334.37ms | 1567969.03 tokens/sec
Step 571 | loss: 5.083769 | lr:4.8000e-04 | norm 1.0341 | dt 336.72ms | 1557026.84 tokens/sec
Step 572 | loss: 5.092299 | lr:4.8084e-04 | norm 1.1980 | dt 339.60ms | 1543820.99 tokens/sec
Step 573 | loss: 5.047159 | lr:4.8168e-04 | norm 0.7548 | dt 336.83ms | 1556518.77 tokens/sec
Step 574 | loss: 5.075869 | lr:4.8252e-04 | norm 0.8248 | dt 337.41ms | 1553848.33 tokens/sec
Step 575 | loss: 5.027698 | lr:4.8336e-04 | norm 0.6454 | dt 337.11ms | 1555246.20 tokens/sec
Step 576 | loss: 5.028193 | lr:4.8420e-04 | norm 0.6806 | dt 337.41ms | 1553871.39 tokens/sec
Step 577 | loss: 5.026002 | lr:4.8503e-04 | norm 0.7307 | dt 336.69ms | 1557173.48 tokens/sec
Step 578 | loss: 4.994748 | lr:4.8587e-04 | norm 0.7695 | dt 337.10ms | 1555273.70 tokens/sec
Step 579 | loss: 4.997622 | lr:4.8671e-04 | norm 0.7248 | dt 337.13ms | 1555147.21 tokens/sec
Step 580 | loss: 4.910326 | lr:4.8755e-04 | norm 1.2942 | dt 337.03ms | 1555593.86 tokens/sec
Step 581 | loss: 4.924222 | lr:4.8839e-04 | norm 0.7180 | dt 337.95ms | 1551385.12 tokens/sec
Step 582 | loss: 4.975326 | lr:4.8923e-04 | norm 0.9141 | dt 337.86ms | 1551794.57 tokens/sec
Step 583 | loss: 4.977888 | lr:4.9007e-04 | norm 0.8705 | dt 337.63ms | 1552852.02 tokens/sec
Step 584 | loss: 4.960990 | lr:4.9091e-04 | norm 0.9092 | dt 337.45ms | 1553693.54 tokens/sec
Step 585 | loss: 5.043248 | lr:4.9175e-04 | norm 1.0782 | dt 337.46ms | 1553617.80 tokens/sec
Step 586 | loss: 4.979692 | lr:4.9259e-04 | norm 0.9308 | dt 337.95ms | 1551365.42 tokens/sec
Step 587 | loss: 4.968668 | lr:4.9343e-04 | norm 1.0429 | dt 338.68ms | 1548031.23 tokens/sec
Step 588 | loss: 5.000607 | lr:4.9427e-04 | norm 1.1336 | dt 338.35ms | 1549522.36 tokens/sec
Step 589 | loss: 5.005649 | lr:4.9510e-04 | norm 0.8908 | dt 338.42ms | 1549235.26 tokens/sec
Step 590 | loss: 4.875950 | lr:4.9594e-04 | norm 0.8493 | dt 340.05ms | 1541793.63 tokens/sec
Step 591 | loss: 4.900655 | lr:4.9678e-04 | norm 1.0742 | dt 338.66ms | 1548117.33 tokens/sec
Step 592 | loss: 4.910484 | lr:4.9762e-04 | norm 1.1326 | dt 338.72ms | 1547834.01 tokens/sec
Step 593 | loss: 4.917152 | lr:4.9846e-04 | norm 0.8327 | dt 337.54ms | 1553267.73 tokens/sec
Step 594 | loss: 4.865002 | lr:4.9930e-04 | norm 0.9383 | dt 337.94ms | 1551415.77 tokens/sec
Step 595 | loss: 4.876299 | lr:5.0014e-04 | norm 0.9200 | dt 338.30ms | 1549760.42 tokens/sec
Step 596 | loss: 4.925674 | lr:5.0098e-04 | norm 0.8752 | dt 337.81ms | 1552007.04 tokens/sec
Step 597 | loss: 4.883740 | lr:5.0182e-04 | norm 1.0616 | dt 337.54ms | 1553274.31 tokens/sec
Step 598 | loss: 4.932497 | lr:5.0266e-04 | norm 0.8252 | dt 337.55ms | 1553235.91 tokens/sec
Step 599 | loss: 4.895003 | lr:5.0350e-04 | norm 0.8035 | dt 337.78ms | 1552148.35 tokens/sec
Step 600 | loss: 4.882078 | lr:5.0434e-04 | norm 0.7613 | dt 337.90ms | 1551595.29 tokens/sec
Step 601 | loss: 4.992343 | lr:5.0517e-04 | norm 0.9002 | dt 336.88ms | 1556291.84 tokens/sec
Step 602 | loss: 5.027791 | lr:5.0601e-04 | norm 0.9812 | dt 337.58ms | 1553096.59 tokens/sec
Step 603 | loss: 5.040547 | lr:5.0685e-04 | norm 0.8545 | dt 337.38ms | 1554018.54 tokens/sec
Step 604 | loss: 5.006619 | lr:5.0769e-04 | norm 0.9057 | dt 338.02ms | 1551034.97 tokens/sec
Step 605 | loss: 5.001567 | lr:5.0853e-04 | norm 0.8627 | dt 337.93ms | 1551465.03 tokens/sec
Step 606 | loss: 5.033022 | lr:5.0937e-04 | norm 0.9808 | dt 338.15ms | 1550449.90 tokens/sec
Step 607 | loss: 5.012034 | lr:5.1021e-04 | norm 0.9144 | dt 337.66ms | 1552721.55 tokens/sec
Step 608 | loss: 4.984859 | lr:5.1105e-04 | norm 0.8494 | dt 338.20ms | 1550222.56 tokens/sec
Step 609 | loss: 5.034598 | lr:5.1189e-04 | norm 0.8208 | dt 337.89ms | 1551669.74 tokens/sec
Step 610 | loss: 4.982066 | lr:5.1273e-04 | norm 0.7772 | dt 337.39ms | 1553932.88 tokens/sec
Step 611 | loss: 4.956063 | lr:5.1357e-04 | norm 0.6895 | dt 338.01ms | 1551121.40 tokens/sec
Step 612 | loss: 4.973487 | lr:5.1441e-04 | norm 0.5591 | dt 337.52ms | 1553368.67 tokens/sec
Step 613 | loss: 4.976768 | lr:5.1524e-04 | norm 0.5600 | dt 337.56ms | 1553186.55 tokens/sec
Step 614 | loss: 4.922557 | lr:5.1608e-04 | norm 0.6372 | dt 339.09ms | 1546152.59 tokens/sec
Step 615 | loss: 4.963022 | lr:5.1692e-04 | norm 0.8624 | dt 337.76ms | 1552231.62 tokens/sec
Step 616 | loss: 4.937317 | lr:5.1776e-04 | norm 0.9473 | dt 337.32ms | 1554290.94 tokens/sec
Step 617 | loss: 4.963582 | lr:5.1860e-04 | norm 0.9512 | dt 336.91ms | 1556162.99 tokens/sec
Step 618 | loss: 4.968000 | lr:5.1944e-04 | norm 0.8200 | dt 337.47ms | 1553589.26 tokens/sec
Step 619 | loss: 4.928716 | lr:5.2028e-04 | norm 0.9183 | dt 347.65ms | 1508103.66 tokens/sec
Step 620 | loss: 4.938056 | lr:5.2112e-04 | norm 0.9353 | dt 340.43ms | 1540087.55 tokens/sec
Step 621 | loss: 4.933742 | lr:5.2196e-04 | norm 0.9173 | dt 338.03ms | 1551016.37 tokens/sec
Step 622 | loss: 4.928284 | lr:5.2280e-04 | norm 0.7986 | dt 338.04ms | 1550957.30 tokens/sec
Step 623 | loss: 4.894022 | lr:5.2364e-04 | norm 0.7630 | dt 338.76ms | 1547666.25 tokens/sec
Step 624 | loss: 4.971687 | lr:5.2448e-04 | norm 0.7443 | dt 340.31ms | 1540619.49 tokens/sec
Step 625 | loss: 4.814751 | lr:5.2531e-04 | norm 0.7202 | dt 337.82ms | 1551977.47 tokens/sec
Step 626 | loss: 4.891181 | lr:5.2615e-04 | norm 0.6045 | dt 337.73ms | 1552389.42 tokens/sec
Step 627 | loss: 4.838640 | lr:5.2699e-04 | norm 0.5739 | dt 338.16ms | 1550431.32 tokens/sec
Step 628 | loss: 4.871543 | lr:5.2783e-04 | norm 0.5924 | dt 344.14ms | 1523465.38 tokens/sec
Step 629 | loss: 4.882891 | lr:5.2867e-04 | norm 0.6325 | dt 337.81ms | 1552030.04 tokens/sec
Step 630 | loss: 4.848716 | lr:5.2951e-04 | norm 0.7044 | dt 337.99ms | 1551188.14 tokens/sec
Step 631 | loss: 4.912312 | lr:5.3035e-04 | norm 0.7059 | dt 336.97ms | 1555873.42 tokens/sec
Step 632 | loss: 4.892571 | lr:5.3119e-04 | norm 0.6612 | dt 337.75ms | 1552276.55 tokens/sec
Step 633 | loss: 4.854819 | lr:5.3203e-04 | norm 0.8217 | dt 339.17ms | 1545776.54 tokens/sec
Step 634 | loss: 4.882385 | lr:5.3287e-04 | norm 0.8919 | dt 337.41ms | 1553846.14 tokens/sec
Step 635 | loss: 5.059989 | lr:5.3371e-04 | norm 0.9116 | dt 337.46ms | 1553624.38 tokens/sec
Step 636 | loss: 4.809212 | lr:5.3455e-04 | norm 0.9838 | dt 337.44ms | 1553741.84 tokens/sec
Step 637 | loss: 4.839106 | lr:5.3538e-04 | norm 1.1305 | dt 337.12ms | 1555179.11 tokens/sec
Step 638 | loss: 4.686644 | lr:5.3622e-04 | norm 0.7669 | dt 337.30ms | 1554368.94 tokens/sec
Step 639 | loss: 4.816854 | lr:5.3706e-04 | norm 0.8398 | dt 337.41ms | 1553847.24 tokens/sec
Step 640 | loss: 4.784007 | lr:5.3790e-04 | norm 0.7416 | dt 337.07ms | 1555438.71 tokens/sec
Step 641 | loss: 4.767549 | lr:5.3874e-04 | norm 0.8037 | dt 337.90ms | 1551599.67 tokens/sec
Step 642 | loss: 4.784002 | lr:5.3958e-04 | norm 0.7577 | dt 337.44ms | 1553717.69 tokens/sec
Step 643 | loss: 4.745966 | lr:5.4042e-04 | norm 0.7372 | dt 337.83ms | 1551945.70 tokens/sec
Step 644 | loss: 4.756608 | lr:5.4126e-04 | norm 0.9086 | dt 337.62ms | 1552893.69 tokens/sec
Step 645 | loss: 4.815084 | lr:5.4210e-04 | norm 0.9289 | dt 342.46ms | 1530940.66 tokens/sec
Step 646 | loss: 4.809806 | lr:5.4294e-04 | norm 0.8670 | dt 338.54ms | 1548662.46 tokens/sec
Step 647 | loss: 4.769132 | lr:5.4378e-04 | norm 0.7696 | dt 337.92ms | 1551515.38 tokens/sec
Step 648 | loss: 4.905596 | lr:5.4462e-04 | norm 0.7082 | dt 339.27ms | 1545325.74 tokens/sec
Step 649 | loss: 4.904024 | lr:5.4545e-04 | norm 0.7408 | dt 337.78ms | 1552151.64 tokens/sec
Step 650 | loss: 4.995726 | lr:5.4629e-04 | norm 0.7713 | dt 341.42ms | 1535603.96 tokens/sec
Step 651 | loss: 4.899579 | lr:5.4713e-04 | norm 0.8787 | dt 337.49ms | 1553475.12 tokens/sec
Step 652 | loss: 4.873597 | lr:5.4797e-04 | norm 0.9116 | dt 337.61ms | 1552958.40 tokens/sec
Step 653 | loss: 4.931957 | lr:5.4881e-04 | norm 0.8951 | dt 338.31ms | 1549717.83 tokens/sec
Step 654 | loss: 4.898470 | lr:5.4965e-04 | norm 0.9327 | dt 338.16ms | 1550420.39 tokens/sec
Step 655 | loss: 4.860309 | lr:5.5049e-04 | norm 0.7547 | dt 336.93ms | 1556073.79 tokens/sec
Step 656 | loss: 4.834963 | lr:5.5133e-04 | norm 0.6699 | dt 336.62ms | 1557484.49 tokens/sec
Step 657 | loss: 4.900223 | lr:5.5217e-04 | norm 0.6209 | dt 337.36ms | 1554097.61 tokens/sec
Step 658 | loss: 4.871203 | lr:5.5301e-04 | norm 0.6318 | dt 338.42ms | 1549204.70 tokens/sec
Step 659 | loss: 4.888977 | lr:5.5385e-04 | norm 0.6751 | dt 337.86ms | 1551803.33 tokens/sec
Step 660 | loss: 4.823125 | lr:5.5469e-04 | norm 0.7021 | dt 336.96ms | 1555957.08 tokens/sec
Step 661 | loss: 4.834153 | lr:5.5552e-04 | norm 0.8060 | dt 337.67ms | 1552674.40 tokens/sec
Step 662 | loss: 4.780877 | lr:5.5636e-04 | norm 0.7670 | dt 337.04ms | 1555572.95 tokens/sec
Step 663 | loss: 4.761053 | lr:5.5720e-04 | norm 0.6435 | dt 337.42ms | 1553791.24 tokens/sec
Step 664 | loss: 4.791281 | lr:5.5804e-04 | norm 0.6896 | dt 336.74ms | 1556938.64 tokens/sec
Step 665 | loss: 4.779198 | lr:5.5888e-04 | norm 0.8089 | dt 337.22ms | 1554755.78 tokens/sec
Step 666 | loss: 4.794633 | lr:5.5972e-04 | norm 0.8948 | dt 337.74ms | 1552364.21 tokens/sec
Step 667 | loss: 4.764341 | lr:5.6056e-04 | norm 0.8136 | dt 336.89ms | 1556276.42 tokens/sec
Step 668 | loss: 4.745787 | lr:5.6140e-04 | norm 0.6794 | dt 337.55ms | 1553204.10 tokens/sec
Step 669 | loss: 4.792028 | lr:5.6224e-04 | norm 0.6663 | dt 337.14ms | 1555116.42 tokens/sec
Step 670 | loss: 4.829967 | lr:5.6308e-04 | norm 0.7039 | dt 337.30ms | 1554385.42 tokens/sec
Step 671 | loss: 4.791721 | lr:5.6392e-04 | norm 0.7366 | dt 339.43ms | 1544635.39 tokens/sec
Step 672 | loss: 4.716575 | lr:5.6476e-04 | norm 0.6596 | dt 337.39ms | 1553954.84 tokens/sec
Step 673 | loss: 4.725383 | lr:5.6559e-04 | norm 0.7823 | dt 337.51ms | 1553387.33 tokens/sec
Step 674 | loss: 4.756908 | lr:5.6643e-04 | norm 0.9094 | dt 337.47ms | 1553593.65 tokens/sec
Step 675 | loss: 4.799206 | lr:5.6727e-04 | norm 0.8525 | dt 338.00ms | 1551130.15 tokens/sec
Step 676 | loss: 4.792861 | lr:5.6811e-04 | norm 0.8713 | dt 336.97ms | 1555894.33 tokens/sec
Step 677 | loss: 4.766546 | lr:5.6895e-04 | norm 0.9568 | dt 337.82ms | 1551959.94 tokens/sec
Step 678 | loss: 4.750217 | lr:5.6979e-04 | norm 0.7111 | dt 338.28ms | 1549879.48 tokens/sec
Step 679 | loss: 4.704106 | lr:5.7063e-04 | norm 0.7120 | dt 338.68ms | 1548013.79 tokens/sec
Step 680 | loss: 4.744823 | lr:5.7147e-04 | norm 0.7492 | dt 339.39ms | 1544773.19 tokens/sec
Step 681 | loss: 4.724883 | lr:5.7231e-04 | norm 0.8054 | dt 337.63ms | 1552842.16 tokens/sec
Step 682 | loss: 4.702651 | lr:5.7315e-04 | norm 0.7823 | dt 338.60ms | 1548408.38 tokens/sec
Step 683 | loss: 4.625475 | lr:5.7399e-04 | norm 0.7122 | dt 337.88ms | 1551716.82 tokens/sec
Step 684 | loss: 4.621206 | lr:5.7483e-04 | norm 0.7439 | dt 337.06ms | 1555467.32 tokens/sec
Step 685 | loss: 4.642529 | lr:5.7566e-04 | norm 0.6960 | dt 338.24ms | 1550046.63 tokens/sec
Step 686 | loss: 4.647297 | lr:5.7650e-04 | norm 0.8286 | dt 337.92ms | 1551528.51 tokens/sec
Step 687 | loss: 4.647140 | lr:5.7734e-04 | norm 0.9699 | dt 338.96ms | 1546776.85 tokens/sec
Step 688 | loss: 4.710784 | lr:5.7818e-04 | norm 0.8758 | dt 336.52ms | 1557970.01 tokens/sec
Step 689 | loss: 4.638332 | lr:5.7902e-04 | norm 0.7530 | dt 336.65ms | 1557360.95 tokens/sec
Step 690 | loss: 4.684480 | lr:5.7986e-04 | norm 0.6681 | dt 337.70ms | 1552542.86 tokens/sec
Step 691 | loss: 4.634585 | lr:5.8070e-04 | norm 0.6156 | dt 337.38ms | 1554004.26 tokens/sec
Step 692 | loss: 4.834722 | lr:5.8154e-04 | norm 0.5981 | dt 337.03ms | 1555591.66 tokens/sec
Step 693 | loss: 4.632972 | lr:5.8238e-04 | norm 0.6363 | dt 337.99ms | 1551187.05 tokens/sec
Step 694 | loss: 4.781166 | lr:5.8322e-04 | norm 0.7363 | dt 338.16ms | 1550414.92 tokens/sec
Step 695 | loss: 4.818027 | lr:5.8406e-04 | norm 0.8411 | dt 337.39ms | 1553972.41 tokens/sec
Step 696 | loss: 4.773128 | lr:5.8490e-04 | norm 0.9537 | dt 338.00ms | 1551129.06 tokens/sec
Step 697 | loss: 4.825372 | lr:5.8573e-04 | norm 1.1474 | dt 337.35ms | 1554142.64 tokens/sec
Step 698 | loss: 4.785460 | lr:5.8657e-04 | norm 0.8970 | dt 337.16ms | 1555017.45 tokens/sec
Step 699 | loss: 4.761229 | lr:5.8741e-04 | norm 0.7885 | dt 337.46ms | 1553618.89 tokens/sec
Step 700 | loss: 4.704980 | lr:5.8825e-04 | norm 0.6861 | dt 337.45ms | 1553655.12 tokens/sec
Step 701 | loss: 4.789753 | lr:5.8909e-04 | norm 0.6094 | dt 337.60ms | 1553005.56 tokens/sec
Step 702 | loss: 4.768612 | lr:5.8993e-04 | norm 0.6118 | dt 338.67ms | 1548057.38 tokens/sec
Step 703 | loss: 4.715722 | lr:5.9077e-04 | norm 0.7991 | dt 336.99ms | 1555793.06 tokens/sec
Step 704 | loss: 4.779012 | lr:5.9161e-04 | norm 0.9905 | dt 339.29ms | 1545259.50 tokens/sec
Step 705 | loss: 4.785917 | lr:5.9245e-04 | norm 0.8702 | dt 338.99ms | 1546636.51 tokens/sec
Step 706 | loss: 4.793223 | lr:5.9329e-04 | norm 0.8474 | dt 337.39ms | 1553931.78 tokens/sec
Step 707 | loss: 4.795149 | lr:5.9413e-04 | norm 0.7766 | dt 338.26ms | 1549942.84 tokens/sec
Step 708 | loss: 4.734865 | lr:5.9497e-04 | norm 0.6662 | dt 338.18ms | 1550304.53 tokens/sec
Step 709 | loss: 4.748299 | lr:5.9580e-04 | norm 0.6296 | dt 338.77ms | 1547621.59 tokens/sec
Step 710 | loss: 4.655958 | lr:5.9664e-04 | norm 0.5960 | dt 337.86ms | 1551812.09 tokens/sec
Step 711 | loss: 4.723219 | lr:5.9748e-04 | norm 0.6826 | dt 337.77ms | 1552199.85 tokens/sec
Step 712 | loss: 4.631151 | lr:5.9832e-04 | norm 0.6759 | dt 338.53ms | 1548727.90 tokens/sec
Step 713 | loss: 4.746117 | lr:5.9916e-04 | norm 0.7024 | dt 337.30ms | 1554383.23 tokens/sec
Step 714 | loss: 4.708947 | lr:6.0000e-04 | norm 0.7192 | dt 338.95ms | 1546802.96 tokens/sec
Step 715 | loss: 4.676925 | lr:6.0000e-04 | norm 0.7351 | dt 337.37ms | 1554059.17 tokens/sec
Step 716 | loss: 4.742375 | lr:6.0000e-04 | norm 0.8442 | dt 337.43ms | 1553746.23 tokens/sec
Step 717 | loss: 4.696858 | lr:6.0000e-04 | norm 0.8903 | dt 337.31ms | 1554317.31 tokens/sec
Step 718 | loss: 4.728117 | lr:6.0000e-04 | norm 0.9906 | dt 337.36ms | 1554076.74 tokens/sec
Step 719 | loss: 4.631287 | lr:6.0000e-04 | norm 0.7821 | dt 337.87ms | 1551763.91 tokens/sec
Step 720 | loss: 4.682322 | lr:6.0000e-04 | norm 0.7872 | dt 336.87ms | 1556363.44 tokens/sec
Step 721 | loss: 4.644441 | lr:6.0000e-04 | norm 0.7150 | dt 337.63ms | 1552853.12 tokens/sec
Step 722 | loss: 4.661109 | lr:6.0000e-04 | norm 0.7459 | dt 337.47ms | 1553571.70 tokens/sec
Step 723 | loss: 4.633267 | lr:6.0000e-04 | norm 0.7179 | dt 337.33ms | 1554242.60 tokens/sec
Step 724 | loss: 4.717254 | lr:6.0000e-04 | norm 0.7718 | dt 337.32ms | 1554256.88 tokens/sec
Step 725 | loss: 4.633323 | lr:6.0000e-04 | norm 0.8655 | dt 338.68ms | 1548033.41 tokens/sec
Step 726 | loss: 4.610208 | lr:6.0000e-04 | norm 0.9174 | dt 338.36ms | 1549496.16 tokens/sec
Step 727 | loss: 4.615632 | lr:6.0000e-04 | norm 0.7823 | dt 337.49ms | 1553491.58 tokens/sec
Step 728 | loss: 4.558555 | lr:6.0000e-04 | norm 0.5799 | dt 338.50ms | 1548855.53 tokens/sec
Step 729 | loss: 4.612625 | lr:6.0000e-04 | norm 0.5587 | dt 338.36ms | 1549502.71 tokens/sec
Step 730 | loss: 4.529732 | lr:6.0000e-04 | norm 0.5610 | dt 338.16ms | 1550434.60 tokens/sec
Step 731 | loss: 4.532241 | lr:6.0000e-04 | norm 0.6328 | dt 338.41ms | 1549251.63 tokens/sec
Step 732 | loss: 4.506064 | lr:6.0000e-04 | norm 0.5939 | dt 337.73ms | 1552379.55 tokens/sec
Step 733 | loss: 4.474380 | lr:6.0000e-04 | norm 0.6340 | dt 338.28ms | 1549880.57 tokens/sec
Step 734 | loss: 4.516489 | lr:6.0000e-04 | norm 0.8318 | dt 338.09ms | 1550741.83 tokens/sec
Step 735 | loss: 4.531794 | lr:6.0000e-04 | norm 0.9781 | dt 337.97ms | 1551286.63 tokens/sec
Step 736 | loss: 4.540733 | lr:6.0000e-04 | norm 0.7695 | dt 338.10ms | 1550668.57 tokens/sec
Step 737 | loss: 4.496832 | lr:6.0000e-04 | norm 0.7250 | dt 338.23ms | 1550079.41 tokens/sec
Step 738 | loss: 4.513748 | lr:6.0000e-04 | norm 0.6895 | dt 337.68ms | 1552598.76 tokens/sec
Step 739 | loss: 4.520438 | lr:6.0000e-04 | norm 0.7558 | dt 340.65ms | 1539086.19 tokens/sec
Step 740 | loss: 4.499969 | lr:6.0000e-04 | norm 0.7235 | dt 337.60ms | 1552981.43 tokens/sec
Step 741 | loss: 4.547445 | lr:6.0000e-04 | norm 0.6698 | dt 339.27ms | 1545329.00 tokens/sec
Step 742 | loss: 4.648713 | lr:6.0000e-04 | norm 0.5996 | dt 338.49ms | 1548924.26 tokens/sec
Step 743 | loss: 4.602327 | lr:6.0000e-04 | norm 0.6435 | dt 339.29ms | 1545247.56 tokens/sec
Step 744 | loss: 4.648900 | lr:6.0000e-04 | norm 0.6957 | dt 337.48ms | 1553533.28 tokens/sec
Step 745 | loss: 4.668438 | lr:6.0000e-04 | norm 0.7112 | dt 337.92ms | 1551511.00 tokens/sec
Step 746 | loss: 4.653058 | lr:6.0000e-04 | norm 0.6937 | dt 338.11ms | 1550646.70 tokens/sec
Step 747 | loss: 4.637386 | lr:6.0000e-04 | norm 0.6169 | dt 337.44ms | 1553720.98 tokens/sec
Step 748 | loss: 4.626902 | lr:6.0000e-04 | norm 0.6530 | dt 337.59ms | 1553009.94 tokens/sec
Step 749 | loss: 4.686847 | lr:6.0000e-04 | norm 0.7054 | dt 337.44ms | 1553738.55 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 750: 4.5890
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2470/10042=0.2460


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I know.
This is a simple process, not just a task, but a more complex method. It is very
rank 5 sample 1 >Hello, I'm a language model, my guess of what makes my experience of what, what, that might, but a very good and more in my opinion


ddp_rank 7: ####### Printing generated samples ####### 

rank 5 sample 2 >Hello, I'm a language model, i am i. . . but is a good way to read for a discussion, by the audience of the audience (
rank 5 sample 3 >Hello, I'm a language model, "A method that is one of the best practices." The results of the concepts have the ability to communicate. The answer


rank 7 sample 0 >Hello, I'm a language model, and I'm a bit surprised. I remember that you are interested in math and learn that is like this, but rather
rank 7 sample 1 >Hello, I'm a language model, both of course, I've always tried to read is useful. I'm a big-world-size-fits-
rank 7 sample 2 >Hello, I'm a language model, to see how one asks, whether in common,
you'll look on the
you'll see a little fun,
rank 7 sample 3 >Hello, I'm a language model, even though that's very close to and as in the second. I're a good idea. For now, I feel




ddp_rank 1: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, whether or a system, then the model of a system that has been studied, with the aim of the first time thatrank 1 sample 0 >Hello, I'm a language model, one that is a big one. Its one in this series the first. Now, this is just a few.


rank 2 sample 1 >Hello, I'm a language model, how to go.
I'm thinking...
Ludu (d.d.d.) in a world.rank 1 sample 1 >Hello, I'm a language model, that's an interesting feature.
It's a pretty exciting thing, but you'll definitely remember, that's a very

rank 1 sample 2 >Hello, I'm a language model, which came up by the city of the city.
In what is, this is the case is not a good of
rank 2 sample 2 >Hello, I'm a language model, a language-based language, in "" by "social networks." When we have a whole network of languages, and
rank 1 sample 3 >Hello, I'm a language model, and I'm talking to him. (Thanks for: Heoits for “he would’t get me


rank 2 sample 3 >Hello, I'm a language model, I'd had a pretty much more time, though many other teachers were able to think that math for math is the fact




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not very useful for learning.
The "Grap and FASA" theory model, that "
rank 3 sample 1 >Hello, I'm a language model, and then the language of three years. I am a way to use the language. I am a part of some language
rank 3 sample 2 >Hello, I'm a language model, and the first language about the last two years on the top of the Internet of your students. To do this was for


ddp_rank 0: ####### Printing generated samples ####### 

rank 3 sample 3 >Hello, I'm a language model, and perhaps a lot of ideas on the other handbook that is to add a better and for the ideas.
I


rank 0 sample 0 >Hello, I'm a language model, and I'd like a small-fashioned but still to share the skills needed by the people. This means it's really
rank 0 sample 1 >Hello, I'm a language model, where "as a computer to be capable of a brain injury, it often has been shown that a problem is a whole
rank 0 sample 2 >Hello, I'm a language model, a language network of my father. In the context of the English languages, the language of the English language is the real
rank 0 sample 3 >Hello, I'm a language model, which allows me to be a real question of which I was at school. The more than me is more than a very




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, a language for the next decade. I’m a great way to read. You think like something about how you
rank 4 sample 1 >Hello, I'm a language model, at first, says a teacher from a young speaker to give a high and more information than the student to be told by
rank 4 sample 2 >Hello, I'm a language model, but instead of making this concept. But it's a big thing.
This really good-known solution is one of
rank 4 sample 3 >Hello, I'm a language model, and a general sense of view. See a new class's method. Compare this "A new course of the same "




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, for an example, "But", for "I'd say" was a new case. Then I found that "And
rank 6 sample 1 >Hello, I'm a language model, which is very useful because I can use when I have a language. So I'm going to know that's real.
rank 6 sample 2 >Hello, I'm a language model, I can use the program, the first to help you get to find a lot of learning.
One of three ways
rank 6 sample 3 >Hello, I'm a language model, in an age-solving approach, which can be used to construct a sequence of two pairs. Because there is a


Step 750 | loss: 4.602682 | lr:6.0000e-04 | norm 0.6821 | dt 18770.29ms | 27931.80 tokens/sec
Step 751 | loss: 4.645247 | lr:5.9999e-04 | norm 0.6798 | dt 332.87ms | 1575048.78 tokens/sec
Step 752 | loss: 4.626084 | lr:5.9999e-04 | norm 0.5766 | dt 334.19ms | 1568813.58 tokens/sec
Step 753 | loss: 4.602769 | lr:5.9999e-04 | norm 0.6198 | dt 336.24ms | 1559280.21 tokens/sec
Step 754 | loss: 4.620994 | lr:5.9999e-04 | norm 0.6420 | dt 334.70ms | 1566419.89 tokens/sec
Step 755 | loss: 4.542975 | lr:5.9999e-04 | norm 0.7411 | dt 890.77ms | 588579.87 tokens/sec
Step 756 | loss: 4.587131 | lr:5.9999e-04 | norm 0.6752 | dt 332.31ms | 1577711.14 tokens/sec
Step 757 | loss: 4.617730 | lr:5.9999e-04 | norm 0.6778 | dt 334.27ms | 1568473.42 tokens/sec
Step 758 | loss: 4.560978 | lr:5.9999e-04 | norm 0.8263 | dt 337.36ms | 1554095.42 tokens/sec
Step 759 | loss: 4.638855 | lr:5.9999e-04 | norm 0.7883 | dt 919.03ms | 570482.44 tokens/sec
Step 760 | loss: 4.577546 | lr:5.9999e-04 | norm 0.7832 | dt 334.93ms | 1565372.85 tokens/sec
Step 761 | loss: 4.590605 | lr:5.9999e-04 | norm 0.7119 | dt 337.28ms | 1554456.84 tokens/sec
Step 762 | loss: 4.552699 | lr:5.9999e-04 | norm 0.7029 | dt 335.12ms | 1564460.76 tokens/sec
Step 763 | loss: 4.579658 | lr:5.9999e-04 | norm 0.6115 | dt 336.08ms | 1560004.75 tokens/sec
Step 764 | loss: 4.563916 | lr:5.9999e-04 | norm 0.7265 | dt 336.26ms | 1559172.97 tokens/sec
Step 765 | loss: 4.539436 | lr:5.9999e-04 | norm 0.8039 | dt 335.56ms | 1562414.39 tokens/sec
Step 766 | loss: 4.505108 | lr:5.9999e-04 | norm 0.7027 | dt 337.03ms | 1555621.37 tokens/sec
Step 767 | loss: 4.552574 | lr:5.9999e-04 | norm 0.7592 | dt 336.88ms | 1556292.94 tokens/sec
Step 768 | loss: 4.509377 | lr:5.9999e-04 | norm 0.8284 | dt 335.74ms | 1561602.22 tokens/sec
Step 769 | loss: 4.523318 | lr:5.9999e-04 | norm 0.8809 | dt 336.79ms | 1556709.39 tokens/sec
Step 770 | loss: 4.552968 | lr:5.9999e-04 | norm 0.7943 | dt 336.60ms | 1557609.15 tokens/sec
Step 771 | loss: 4.534763 | lr:5.9999e-04 | norm 0.7364 | dt 336.28ms | 1559077.90 tokens/sec
Step 772 | loss: 4.504149 | lr:5.9999e-04 | norm 0.6104 | dt 336.63ms | 1557467.94 tokens/sec
Step 773 | loss: 4.482985 | lr:5.9999e-04 | norm 0.5541 | dt 337.74ms | 1552363.11 tokens/sec
Step 774 | loss: 4.435378 | lr:5.9999e-04 | norm 0.5608 | dt 336.23ms | 1559296.80 tokens/sec
Step 775 | loss: 4.402413 | lr:5.9999e-04 | norm 0.6172 | dt 338.58ms | 1548495.61 tokens/sec
Step 776 | loss: 4.412186 | lr:5.9999e-04 | norm 0.6046 | dt 337.58ms | 1553070.27 tokens/sec
Step 777 | loss: 4.393885 | lr:5.9998e-04 | norm 0.5890 | dt 337.74ms | 1552346.68 tokens/sec
Step 778 | loss: 4.498295 | lr:5.9998e-04 | norm 0.5689 | dt 335.91ms | 1560792.00 tokens/sec
Step 779 | loss: 4.346586 | lr:5.9998e-04 | norm 0.5044 | dt 336.28ms | 1559065.74 tokens/sec
Step 780 | loss: 4.351921 | lr:5.9998e-04 | norm 0.5222 | dt 336.23ms | 1559318.91 tokens/sec
Step 781 | loss: 4.357762 | lr:5.9998e-04 | norm 0.6089 | dt 336.86ms | 1556414.11 tokens/sec
Step 782 | loss: 4.378483 | lr:5.9998e-04 | norm 0.6267 | dt 336.96ms | 1555913.05 tokens/sec
Step 783 | loss: 4.420008 | lr:5.9998e-04 | norm 0.6807 | dt 336.52ms | 1557982.15 tokens/sec
Step 784 | loss: 4.416635 | lr:5.9998e-04 | norm 0.7065 | dt 337.47ms | 1553585.97 tokens/sec
Step 785 | loss: 4.419031 | lr:5.9998e-04 | norm 0.5664 | dt 337.03ms | 1555624.67 tokens/sec
Step 786 | loss: 4.393161 | lr:5.9998e-04 | norm 0.5724 | dt 336.94ms | 1556015.44 tokens/sec
Step 787 | loss: 4.491374 | lr:5.9998e-04 | norm 0.6197 | dt 336.28ms | 1559074.59 tokens/sec
Step 788 | loss: 4.545067 | lr:5.9998e-04 | norm 0.6582 | dt 337.46ms | 1553611.21 tokens/sec
Step 789 | loss: 4.495094 | lr:5.9998e-04 | norm 0.6018 | dt 336.84ms | 1556466.99 tokens/sec
Step 790 | loss: 4.517123 | lr:5.9998e-04 | norm 0.6682 | dt 337.19ms | 1554881.11 tokens/sec
Step 791 | loss: 4.518503 | lr:5.9998e-04 | norm 0.6992 | dt 336.92ms | 1556104.62 tokens/sec
Step 792 | loss: 4.507135 | lr:5.9998e-04 | norm 1.1013 | dt 336.62ms | 1557508.76 tokens/sec
Step 793 | loss: 4.564735 | lr:5.9998e-04 | norm 0.5981 | dt 337.21ms | 1554801.95 tokens/sec
Step 794 | loss: 4.504578 | lr:5.9998e-04 | norm 0.6628 | dt 336.74ms | 1556943.05 tokens/sec
Step 795 | loss: 4.493425 | lr:5.9997e-04 | norm 0.7098 | dt 336.82ms | 1556568.35 tokens/sec
Step 796 | loss: 4.524506 | lr:5.9997e-04 | norm 0.8967 | dt 337.39ms | 1553938.37 tokens/sec
Step 797 | loss: 4.558353 | lr:5.9997e-04 | norm 1.2802 | dt 337.03ms | 1555605.96 tokens/sec
Step 798 | loss: 4.508248 | lr:5.9997e-04 | norm 0.8809 | dt 337.31ms | 1554323.90 tokens/sec
Step 799 | loss: 4.540820 | lr:5.9997e-04 | norm 1.0471 | dt 337.46ms | 1553644.14 tokens/sec
Step 800 | loss: 4.590841 | lr:5.9997e-04 | norm 0.9999 | dt 338.19ms | 1550289.23 tokens/sec
Step 801 | loss: 4.530630 | lr:5.9997e-04 | norm 0.8805 | dt 336.96ms | 1555916.35 tokens/sec
Step 802 | loss: 4.519222 | lr:5.9997e-04 | norm 0.7962 | dt 337.57ms | 1553128.41 tokens/sec
Step 803 | loss: 4.508467 | lr:5.9997e-04 | norm 0.6363 | dt 337.45ms | 1553694.64 tokens/sec
Step 804 | loss: 4.502431 | lr:5.9997e-04 | norm 0.6516 | dt 337.03ms | 1555592.76 tokens/sec
Step 805 | loss: 4.498748 | lr:5.9997e-04 | norm 0.7262 | dt 337.56ms | 1553174.48 tokens/sec
Step 806 | loss: 4.496700 | lr:5.9997e-04 | norm 0.6469 | dt 337.61ms | 1552946.33 tokens/sec
Step 807 | loss: 4.463908 | lr:5.9997e-04 | norm 0.5713 | dt 337.42ms | 1553791.24 tokens/sec
Step 808 | loss: 4.405636 | lr:5.9997e-04 | norm 0.5634 | dt 337.13ms | 1555169.21 tokens/sec
Step 809 | loss: 4.438198 | lr:5.9997e-04 | norm 0.6263 | dt 337.91ms | 1551561.36 tokens/sec
Step 810 | loss: 4.412077 | lr:5.9996e-04 | norm 0.6007 | dt 336.94ms | 1556020.94 tokens/sec
Step 811 | loss: 4.431907 | lr:5.9996e-04 | norm 0.6061 | dt 337.20ms | 1554815.14 tokens/sec
Step 812 | loss: 4.411139 | lr:5.9996e-04 | norm 0.5698 | dt 337.33ms | 1554208.55 tokens/sec
Step 813 | loss: 4.391885 | lr:5.9996e-04 | norm 0.4983 | dt 337.15ms | 1555062.53 tokens/sec
Step 814 | loss: 4.367387 | lr:5.9996e-04 | norm 0.4834 | dt 338.01ms | 1551085.29 tokens/sec
Step 815 | loss: 4.397535 | lr:5.9996e-04 | norm 0.5116 | dt 338.71ms | 1547889.57 tokens/sec
Step 816 | loss: 4.396379 | lr:5.9996e-04 | norm 0.5100 | dt 336.90ms | 1556225.76 tokens/sec
Step 817 | loss: 4.332701 | lr:5.9996e-04 | norm 0.5282 | dt 337.40ms | 1553903.23 tokens/sec
Step 818 | loss: 4.351938 | lr:5.9996e-04 | norm 0.6402 | dt 338.13ms | 1550557.04 tokens/sec
Step 819 | loss: 4.408247 | lr:5.9996e-04 | norm 0.6265 | dt 337.39ms | 1553930.69 tokens/sec
Step 820 | loss: 4.409585 | lr:5.9996e-04 | norm 0.7173 | dt 337.65ms | 1552742.38 tokens/sec
Step 821 | loss: 4.282117 | lr:5.9996e-04 | norm 0.6747 | dt 337.36ms | 1554084.43 tokens/sec
Step 822 | loss: 4.304982 | lr:5.9995e-04 | norm 0.6727 | dt 337.52ms | 1553370.87 tokens/sec
Step 823 | loss: 4.283280 | lr:5.9995e-04 | norm 0.6398 | dt 337.49ms | 1553499.26 tokens/sec
Step 824 | loss: 4.293547 | lr:5.9995e-04 | norm 0.6948 | dt 337.01ms | 1555707.21 tokens/sec
Step 825 | loss: 4.264481 | lr:5.9995e-04 | norm 0.6961 | dt 338.88ms | 1547131.61 tokens/sec
Step 826 | loss: 4.242023 | lr:5.9995e-04 | norm 0.6689 | dt 337.45ms | 1553680.36 tokens/sec
Step 827 | loss: 4.287804 | lr:5.9995e-04 | norm 0.7408 | dt 338.72ms | 1547838.37 tokens/sec
Step 828 | loss: 4.324829 | lr:5.9995e-04 | norm 0.6668 | dt 338.19ms | 1550271.74 tokens/sec
Step 829 | loss: 4.260228 | lr:5.9995e-04 | norm 0.6103 | dt 338.08ms | 1550776.83 tokens/sec
Step 830 | loss: 4.261237 | lr:5.9995e-04 | norm 0.5863 | dt 338.59ms | 1548445.45 tokens/sec
Step 831 | loss: 4.309566 | lr:5.9995e-04 | norm 0.5449 | dt 338.42ms | 1549239.62 tokens/sec
Step 832 | loss: 4.366508 | lr:5.9995e-04 | norm 0.4779 | dt 338.50ms | 1548848.98 tokens/sec
Step 833 | loss: 4.416912 | lr:5.9994e-04 | norm 0.5074 | dt 338.52ms | 1548747.53 tokens/sec
Step 834 | loss: 4.410927 | lr:5.9994e-04 | norm 0.5910 | dt 338.89ms | 1547091.34 tokens/sec
Step 835 | loss: 4.418590 | lr:5.9994e-04 | norm 0.5979 | dt 338.41ms | 1549251.63 tokens/sec
Step 836 | loss: 4.388854 | lr:5.9994e-04 | norm 0.7017 | dt 337.82ms | 1551979.66 tokens/sec
Step 837 | loss: 4.425256 | lr:5.9994e-04 | norm 0.7895 | dt 337.15ms | 1555071.33 tokens/sec
Step 838 | loss: 4.415660 | lr:5.9994e-04 | norm 0.7888 | dt 337.87ms | 1551761.72 tokens/sec
Step 839 | loss: 4.483410 | lr:5.9994e-04 | norm 0.7434 | dt 338.99ms | 1546631.07 tokens/sec
Step 840 | loss: 4.443880 | lr:5.9994e-04 | norm 0.7223 | dt 338.59ms | 1548466.17 tokens/sec
Step 841 | loss: 4.468232 | lr:5.9994e-04 | norm 0.7380 | dt 338.65ms | 1548174.00 tokens/sec
Step 842 | loss: 4.421996 | lr:5.9994e-04 | norm 0.5504 | dt 339.39ms | 1544782.96 tokens/sec
Step 843 | loss: 4.372790 | lr:5.9994e-04 | norm 0.4513 | dt 338.95ms | 1546811.66 tokens/sec
Step 844 | loss: 4.381269 | lr:5.9993e-04 | norm 0.5417 | dt 337.00ms | 1555751.23 tokens/sec
Step 845 | loss: 4.431957 | lr:5.9993e-04 | norm 0.4694 | dt 337.41ms | 1553851.63 tokens/sec
Step 846 | loss: 4.399434 | lr:5.9993e-04 | norm 0.4476 | dt 337.90ms | 1551605.15 tokens/sec
Step 847 | loss: 4.372057 | lr:5.9993e-04 | norm 0.4601 | dt 338.20ms | 1550208.35 tokens/sec
Step 848 | loss: 4.348713 | lr:5.9993e-04 | norm 0.4805 | dt 337.61ms | 1552926.59 tokens/sec
Step 849 | loss: 4.397232 | lr:5.9993e-04 | norm 0.5390 | dt 337.23ms | 1554677.74 tokens/sec
Step 850 | loss: 4.417393 | lr:5.9993e-04 | norm 0.5832 | dt 338.71ms | 1547909.19 tokens/sec
Step 851 | loss: 4.344778 | lr:5.9993e-04 | norm 0.6687 | dt 338.75ms | 1547733.78 tokens/sec
Step 852 | loss: 4.375612 | lr:5.9993e-04 | norm 0.5949 | dt 337.43ms | 1553760.50 tokens/sec
Step 853 | loss: 4.386485 | lr:5.9992e-04 | norm 0.6375 | dt 339.73ms | 1543264.10 tokens/sec
Step 854 | loss: 4.386621 | lr:5.9992e-04 | norm 0.5560 | dt 337.95ms | 1551397.16 tokens/sec
Step 855 | loss: 4.327974 | lr:5.9992e-04 | norm 0.6054 | dt 337.92ms | 1551524.14 tokens/sec
Step 856 | loss: 4.326283 | lr:5.9992e-04 | norm 0.5675 | dt 337.90ms | 1551612.81 tokens/sec
Step 857 | loss: 4.289855 | lr:5.9992e-04 | norm 0.4729 | dt 338.64ms | 1548236.13 tokens/sec
Step 858 | loss: 4.296737 | lr:5.9992e-04 | norm 0.5296 | dt 338.18ms | 1550332.94 tokens/sec
Step 859 | loss: 4.317863 | lr:5.9992e-04 | norm 0.6196 | dt 337.97ms | 1551307.42 tokens/sec
Step 860 | loss: 4.324884 | lr:5.9992e-04 | norm 0.5787 | dt 340.50ms | 1539759.73 tokens/sec
Step 861 | loss: 4.246832 | lr:5.9992e-04 | norm 0.5141 | dt 337.76ms | 1552230.53 tokens/sec
Step 862 | loss: 4.285079 | lr:5.9991e-04 | norm 0.7038 | dt 338.35ms | 1549522.36 tokens/sec
Step 863 | loss: 4.270353 | lr:5.9991e-04 | norm 0.8473 | dt 338.24ms | 1550066.30 tokens/sec
Step 864 | loss: 4.290436 | lr:5.9991e-04 | norm 0.6388 | dt 337.70ms | 1552537.38 tokens/sec
Step 865 | loss: 4.247632 | lr:5.9991e-04 | norm 0.5903 | dt 337.41ms | 1553848.33 tokens/sec
Step 866 | loss: 4.215634 | lr:5.9991e-04 | norm 0.6082 | dt 338.61ms | 1548361.50 tokens/sec
Step 867 | loss: 4.233858 | lr:5.9991e-04 | norm 0.5786 | dt 337.28ms | 1554482.12 tokens/sec
Step 868 | loss: 4.192744 | lr:5.9991e-04 | norm 0.5104 | dt 337.33ms | 1554225.03 tokens/sec
Step 869 | loss: 4.163554 | lr:5.9991e-04 | norm 0.5157 | dt 338.37ms | 1549464.50 tokens/sec
Step 870 | loss: 4.198717 | lr:5.9991e-04 | norm 0.5426 | dt 339.84ms | 1542727.09 tokens/sec
Step 871 | loss: 4.167376 | lr:5.9990e-04 | norm 0.6236 | dt 337.69ms | 1552575.74 tokens/sec
Step 872 | loss: 4.134938 | lr:5.9990e-04 | norm 0.5832 | dt 339.40ms | 1544738.47 tokens/sec
Step 873 | loss: 4.158121 | lr:5.9990e-04 | norm 0.5546 | dt 337.26ms | 1554538.16 tokens/sec
Step 874 | loss: 4.229264 | lr:5.9990e-04 | norm 0.6026 | dt 337.84ms | 1551901.89 tokens/sec
Step 875 | loss: 4.266155 | lr:5.9990e-04 | norm 0.5597 | dt 338.20ms | 1550214.91 tokens/sec
Step 876 | loss: 4.260337 | lr:5.9990e-04 | norm 0.6429 | dt 338.43ms | 1549157.77 tokens/sec
Step 877 | loss: 4.205370 | lr:5.9990e-04 | norm 0.6646 | dt 337.97ms | 1551283.34 tokens/sec
Step 878 | loss: 4.278368 | lr:5.9989e-04 | norm 0.5210 | dt 337.91ms | 1551549.31 tokens/sec
Step 879 | loss: 4.358516 | lr:5.9989e-04 | norm 0.4835 | dt 337.97ms | 1551292.10 tokens/sec
Step 880 | loss: 4.428396 | lr:5.9989e-04 | norm 0.4721 | dt 338.05ms | 1550908.08 tokens/sec
Step 881 | loss: 4.367985 | lr:5.9989e-04 | norm 0.4781 | dt 337.01ms | 1555713.81 tokens/sec
Step 882 | loss: 4.323195 | lr:5.9989e-04 | norm 0.5512 | dt 337.86ms | 1551791.28 tokens/sec
Step 883 | loss: 4.307704 | lr:5.9989e-04 | norm 0.5589 | dt 338.25ms | 1550007.30 tokens/sec
Step 884 | loss: 4.296152 | lr:5.9989e-04 | norm 0.6439 | dt 337.43ms | 1553763.80 tokens/sec
Step 885 | loss: 4.364504 | lr:5.9989e-04 | norm 0.7319 | dt 338.03ms | 1550990.12 tokens/sec
Step 886 | loss: 4.342736 | lr:5.9988e-04 | norm 0.7008 | dt 337.59ms | 1553035.17 tokens/sec
Step 887 | loss: 4.326239 | lr:5.9988e-04 | norm 0.7171 | dt 338.88ms | 1547139.23 tokens/sec
Step 888 | loss: 4.331526 | lr:5.9988e-04 | norm 0.5970 | dt 336.75ms | 1556896.76 tokens/sec
Step 889 | loss: 4.353251 | lr:5.9988e-04 | norm 0.5634 | dt 337.68ms | 1552616.30 tokens/sec
Step 890 | loss: 4.288286 | lr:5.9988e-04 | norm 0.5179 | dt 338.14ms | 1550500.19 tokens/sec
Step 891 | loss: 4.316566 | lr:5.9988e-04 | norm 0.5193 | dt 337.51ms | 1553402.69 tokens/sec
Step 892 | loss: 4.287136 | lr:5.9988e-04 | norm 0.4890 | dt 337.38ms | 1553994.38 tokens/sec
Step 893 | loss: 4.305964 | lr:5.9987e-04 | norm 0.4509 | dt 337.53ms | 1553318.20 tokens/sec
Step 894 | loss: 4.294082 | lr:5.9987e-04 | norm 0.4056 | dt 337.38ms | 1554017.44 tokens/sec
Step 895 | loss: 4.274979 | lr:5.9987e-04 | norm 0.4173 | dt 337.98ms | 1551224.25 tokens/sec
Step 896 | loss: 4.290758 | lr:5.9987e-04 | norm 0.4258 | dt 337.29ms | 1554405.20 tokens/sec
Step 897 | loss: 4.236403 | lr:5.9987e-04 | norm 0.4612 | dt 337.44ms | 1553739.64 tokens/sec
Step 898 | loss: 4.258448 | lr:5.9987e-04 | norm 0.5080 | dt 338.74ms | 1547740.32 tokens/sec
Step 899 | loss: 4.271585 | lr:5.9987e-04 | norm 0.5986 | dt 336.72ms | 1557031.25 tokens/sec
Step 900 | loss: 4.254748 | lr:5.9986e-04 | norm 0.5238 | dt 341.98ms | 1533090.25 tokens/sec
Step 901 | loss: 4.182845 | lr:5.9986e-04 | norm 0.5602 | dt 337.93ms | 1551455.17 tokens/sec
Step 902 | loss: 4.275418 | lr:5.9986e-04 | norm 0.6982 | dt 338.54ms | 1548673.36 tokens/sec
Step 903 | loss: 4.247984 | lr:5.9986e-04 | norm 0.6838 | dt 337.38ms | 1554002.06 tokens/sec
Step 904 | loss: 4.240967 | lr:5.9986e-04 | norm 0.6495 | dt 338.40ms | 1549331.31 tokens/sec
Step 905 | loss: 4.238565 | lr:5.9986e-04 | norm 0.5677 | dt 337.73ms | 1552385.03 tokens/sec
Step 906 | loss: 4.268145 | lr:5.9986e-04 | norm 0.6264 | dt 337.23ms | 1554668.95 tokens/sec
Step 907 | loss: 4.266760 | lr:5.9985e-04 | norm 0.5676 | dt 340.36ms | 1540380.99 tokens/sec
Step 908 | loss: 4.202112 | lr:5.9985e-04 | norm 0.5267 | dt 338.98ms | 1546680.02 tokens/sec
Step 909 | loss: 4.247751 | lr:5.9985e-04 | norm 0.5473 | dt 338.28ms | 1549871.84 tokens/sec
Step 910 | loss: 4.169271 | lr:5.9985e-04 | norm 0.5489 | dt 338.12ms | 1550594.22 tokens/sec
Step 911 | loss: 4.237163 | lr:5.9985e-04 | norm 0.4799 | dt 337.92ms | 1551525.23 tokens/sec
Step 912 | loss: 4.096324 | lr:5.9985e-04 | norm 0.4401 | dt 337.77ms | 1552185.60 tokens/sec
Step 913 | loss: 4.147031 | lr:5.9985e-04 | norm 0.4578 | dt 339.13ms | 1545979.76 tokens/sec
Step 914 | loss: 4.140585 | lr:5.9984e-04 | norm 0.5229 | dt 338.97ms | 1546707.22 tokens/sec
Step 915 | loss: 4.110607 | lr:5.9984e-04 | norm 0.5472 | dt 338.29ms | 1549822.68 tokens/sec
Step 916 | loss: 4.145139 | lr:5.9984e-04 | norm 0.6509 | dt 339.06ms | 1546316.76 tokens/sec
Step 917 | loss: 4.055784 | lr:5.9984e-04 | norm 0.6896 | dt 338.12ms | 1550604.06 tokens/sec
Step 918 | loss: 4.106359 | lr:5.9984e-04 | norm 0.5716 | dt 338.24ms | 1550057.56 tokens/sec
Step 919 | loss: 4.157519 | lr:5.9984e-04 | norm 0.5764 | dt 337.55ms | 1553194.22 tokens/sec
Step 920 | loss: 4.197192 | lr:5.9983e-04 | norm 0.6693 | dt 337.97ms | 1551278.97 tokens/sec
Step 921 | loss: 4.190079 | lr:5.9983e-04 | norm 0.6250 | dt 339.58ms | 1543940.23 tokens/sec
Step 922 | loss: 4.170430 | lr:5.9983e-04 | norm 0.5218 | dt 338.45ms | 1549067.19 tokens/sec
Step 923 | loss: 4.124126 | lr:5.9983e-04 | norm 0.5534 | dt 338.09ms | 1550737.46 tokens/sec
Step 924 | loss: 4.269635 | lr:5.9983e-04 | norm 0.5429 | dt 338.76ms | 1547659.71 tokens/sec
Step 925 | loss: 4.313729 | lr:5.9983e-04 | norm 0.4800 | dt 338.77ms | 1547629.21 tokens/sec
Step 926 | loss: 4.256602 | lr:5.9982e-04 | norm 0.4041 | dt 338.18ms | 1550312.18 tokens/sec
Step 927 | loss: 4.215323 | lr:5.9982e-04 | norm 0.4042 | dt 337.78ms | 1552170.27 tokens/sec
Step 928 | loss: 4.298758 | lr:5.9982e-04 | norm 0.4039 | dt 338.21ms | 1550181.03 tokens/sec
Step 929 | loss: 4.253144 | lr:5.9982e-04 | norm 0.4601 | dt 338.24ms | 1550054.28 tokens/sec
Step 930 | loss: 4.277729 | lr:5.9982e-04 | norm 0.5438 | dt 338.09ms | 1550724.34 tokens/sec
Step 931 | loss: 4.269837 | lr:5.9982e-04 | norm 0.5890 | dt 339.54ms | 1544099.59 tokens/sec
Step 932 | loss: 4.323018 | lr:5.9981e-04 | norm 0.5900 | dt 338.07ms | 1550835.89 tokens/sec
Step 933 | loss: 4.315636 | lr:5.9981e-04 | norm 0.5350 | dt 337.83ms | 1551945.70 tokens/sec
Step 934 | loss: 4.250831 | lr:5.9981e-04 | norm 0.5464 | dt 337.95ms | 1551368.71 tokens/sec
Step 935 | loss: 4.264132 | lr:5.9981e-04 | norm 0.5304 | dt 338.60ms | 1548398.57 tokens/sec
Step 936 | loss: 4.266519 | lr:5.9981e-04 | norm 0.5554 | dt 337.87ms | 1551733.25 tokens/sec
Step 937 | loss: 4.235401 | lr:5.9981e-04 | norm 0.4672 | dt 338.48ms | 1548940.62 tokens/sec
Step 938 | loss: 4.249711 | lr:5.9980e-04 | norm 0.4980 | dt 337.52ms | 1553351.12 tokens/sec
Step 939 | loss: 4.281117 | lr:5.9980e-04 | norm 0.5091 | dt 338.52ms | 1548754.08 tokens/sec
Step 940 | loss: 4.257218 | lr:5.9980e-04 | norm 0.4647 | dt 338.13ms | 1550557.04 tokens/sec
Step 941 | loss: 4.259402 | lr:5.9980e-04 | norm 0.4845 | dt 337.93ms | 1551478.16 tokens/sec
Step 942 | loss: 4.243258 | lr:5.9980e-04 | norm 0.5286 | dt 338.48ms | 1548954.81 tokens/sec
Step 943 | loss: 4.223858 | lr:5.9979e-04 | norm 0.5100 | dt 337.83ms | 1551907.37 tokens/sec
Step 944 | loss: 4.234461 | lr:5.9979e-04 | norm 0.5293 | dt 1016.44ms | 515809.85 tokens/sec
Step 945 | loss: 4.255243 | lr:5.9979e-04 | norm 0.5647 | dt 336.54ms | 1557875.09 tokens/sec
Step 946 | loss: 4.243591 | lr:5.9979e-04 | norm 0.5776 | dt 337.61ms | 1552918.92 tokens/sec
Step 947 | loss: 4.213017 | lr:5.9979e-04 | norm 0.5420 | dt 338.69ms | 1547992.00 tokens/sec
Step 948 | loss: 4.236394 | lr:5.9979e-04 | norm 0.6261 | dt 337.33ms | 1554234.91 tokens/sec
Step 949 | loss: 4.165195 | lr:5.9978e-04 | norm 0.5773 | dt 1058.40ms | 495359.63 tokens/sec
Step 950 | loss: 4.145459 | lr:5.9978e-04 | norm 0.5281 | dt 335.87ms | 1561002.51 tokens/sec
Step 951 | loss: 4.193817 | lr:5.9978e-04 | norm 0.4932 | dt 336.90ms | 1556205.94 tokens/sec
Step 952 | loss: 4.134853 | lr:5.9978e-04 | norm 0.4657 | dt 339.01ms | 1546526.65 tokens/sec
Step 953 | loss: 4.134761 | lr:5.9978e-04 | norm 0.5028 | dt 336.66ms | 1557337.79 tokens/sec
Step 954 | loss: 4.075161 | lr:5.9977e-04 | norm 0.5223 | dt 336.94ms | 1556040.76 tokens/sec
Step 955 | loss: 4.137663 | lr:5.9977e-04 | norm 0.5825 | dt 337.99ms | 1551195.80 tokens/sec
Step 956 | loss: 4.185796 | lr:5.9977e-04 | norm 0.6665 | dt 337.42ms | 1553813.20 tokens/sec
Step 957 | loss: 4.134031 | lr:5.9977e-04 | norm 0.5363 | dt 337.02ms | 1555660.99 tokens/sec
Step 958 | loss: 4.078282 | lr:5.9977e-04 | norm 0.4574 | dt 338.44ms | 1549110.84 tokens/sec
Step 959 | loss: 4.046571 | lr:5.9976e-04 | norm 0.4551 | dt 338.22ms | 1550161.36 tokens/sec
Step 960 | loss: 3.990562 | lr:5.9976e-04 | norm 0.4367 | dt 337.22ms | 1554727.20 tokens/sec
Step 961 | loss: 4.064686 | lr:5.9976e-04 | norm 0.4548 | dt 338.40ms | 1549324.76 tokens/sec
Step 962 | loss: 4.086098 | lr:5.9976e-04 | norm 0.4827 | dt 336.65ms | 1557352.13 tokens/sec
Step 963 | loss: 4.031060 | lr:5.9976e-04 | norm 0.4541 | dt 337.79ms | 1552128.64 tokens/sec
Step 964 | loss: 4.086451 | lr:5.9975e-04 | norm 0.4604 | dt 337.84ms | 1551865.75 tokens/sec
Step 965 | loss: 4.107327 | lr:5.9975e-04 | norm 0.4561 | dt 337.82ms | 1551992.80 tokens/sec
Step 966 | loss: 4.102223 | lr:5.9975e-04 | norm 0.4509 | dt 337.93ms | 1551468.31 tokens/sec
Step 967 | loss: 4.106867 | lr:5.9975e-04 | norm 0.4829 | dt 337.30ms | 1554362.35 tokens/sec
Step 968 | loss: 4.117651 | lr:5.9975e-04 | norm 0.5244 | dt 337.77ms | 1552221.76 tokens/sec
Step 969 | loss: 4.098024 | lr:5.9974e-04 | norm 0.4978 | dt 337.59ms | 1553031.88 tokens/sec
Step 970 | loss: 4.184810 | lr:5.9974e-04 | norm 0.4872 | dt 338.22ms | 1550138.42 tokens/sec
Step 971 | loss: 4.212859 | lr:5.9974e-04 | norm 0.4711 | dt 337.58ms | 1553094.40 tokens/sec
Step 972 | loss: 4.186705 | lr:5.9974e-04 | norm 0.4900 | dt 337.85ms | 1551843.85 tokens/sec
Step 973 | loss: 4.209645 | lr:5.9974e-04 | norm 0.5321 | dt 338.02ms | 1551042.63 tokens/sec
Step 974 | loss: 4.220202 | lr:5.9973e-04 | norm 0.5930 | dt 338.22ms | 1550129.67 tokens/sec
Step 975 | loss: 4.266613 | lr:5.9973e-04 | norm 0.6722 | dt 337.45ms | 1553678.17 tokens/sec
Step 976 | loss: 4.195359 | lr:5.9973e-04 | norm 0.6310 | dt 337.98ms | 1551252.70 tokens/sec
Step 977 | loss: 4.217088 | lr:5.9973e-04 | norm 0.5716 | dt 338.31ms | 1549732.03 tokens/sec
Step 978 | loss: 4.174777 | lr:5.9973e-04 | norm 0.4701 | dt 337.05ms | 1555516.83 tokens/sec
Step 979 | loss: 4.151364 | lr:5.9972e-04 | norm 0.4068 | dt 338.17ms | 1550375.57 tokens/sec
Step 980 | loss: 4.182055 | lr:5.9972e-04 | norm 0.3868 | dt 338.23ms | 1550084.87 tokens/sec
Step 981 | loss: 4.158364 | lr:5.9972e-04 | norm 0.3788 | dt 338.23ms | 1550095.80 tokens/sec
Step 982 | loss: 4.179686 | lr:5.9972e-04 | norm 0.3925 | dt 337.52ms | 1553358.80 tokens/sec
Step 983 | loss: 4.148694 | lr:5.9972e-04 | norm 0.3567 | dt 337.57ms | 1553111.95 tokens/sec
Step 984 | loss: 4.214398 | lr:5.9971e-04 | norm 0.3885 | dt 337.55ms | 1553215.07 tokens/sec
Step 985 | loss: 4.121304 | lr:5.9971e-04 | norm 0.4165 | dt 338.08ms | 1550758.24 tokens/sec
Step 986 | loss: 4.179869 | lr:5.9971e-04 | norm 0.4057 | dt 337.48ms | 1553519.02 tokens/sec
Step 987 | loss: 4.143510 | lr:5.9971e-04 | norm 0.4763 | dt 337.41ms | 1553845.04 tokens/sec
Step 988 | loss: 4.185759 | lr:5.9971e-04 | norm 0.5592 | dt 338.12ms | 1550577.82 tokens/sec
Step 989 | loss: 4.136661 | lr:5.9970e-04 | norm 0.5807 | dt 337.54ms | 1553264.44 tokens/sec
Step 990 | loss: 4.097148 | lr:5.9970e-04 | norm 0.6153 | dt 338.25ms | 1549989.82 tokens/sec
Step 991 | loss: 4.145913 | lr:5.9970e-04 | norm 0.6163 | dt 336.68ms | 1557241.85 tokens/sec
Step 992 | loss: 4.186111 | lr:5.9970e-04 | norm 0.6191 | dt 337.34ms | 1554200.86 tokens/sec
Step 993 | loss: 4.077800 | lr:5.9969e-04 | norm 0.5211 | dt 338.03ms | 1551028.40 tokens/sec
Step 994 | loss: 4.151088 | lr:5.9969e-04 | norm 0.4415 | dt 336.87ms | 1556364.54 tokens/sec
Step 995 | loss: 4.141679 | lr:5.9969e-04 | norm 0.4528 | dt 337.97ms | 1551275.68 tokens/sec
Step 996 | loss: 4.068377 | lr:5.9969e-04 | norm 0.4513 | dt 337.35ms | 1554145.94 tokens/sec
Step 997 | loss: 4.111067 | lr:5.9969e-04 | norm 0.4606 | dt 337.49ms | 1553480.60 tokens/sec
Step 998 | loss: 4.099631 | lr:5.9968e-04 | norm 0.4691 | dt 337.53ms | 1553286.38 tokens/sec
Step 999 | loss: 4.142327 | lr:5.9968e-04 | norm 0.4810 | dt 337.54ms | 1553271.02 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 1000: 4.1550
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2553/10042=0.2542


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I'll get a look at the following
I'm not a humanist, and I'm a humanist. I
rank 5 sample 1 >Hello, I'm a language model, if one uses one variable for both. In doing so, if one is between it and then it needs to be moved
rank 5 sample 2 >Hello, I'm a language model, is the main focus/teachers approach to the problem.
When I talk with each other, I'm going to
rank 5 sample 3 >Hello, I'm a language model, an individual must assume that I can understand that language and how we interpret it as something. The world of knowledge and thought




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, while in my previous post, we learned to use the term “beth.” It is in my experience
rank 2 sample 1 >Hello, I'm a language model, like the Internet and the Internet. That explains why things are not being made up of the Internet, not only are you
rank 2 sample 2 >Hello, I'm a language model, but I can't find the solution on board. I'm just going to be on top. There's a bunch of
rank 2 sample 3 >Hello, I'm a language model, I don't have any problems. We understand that each of its components is their first, if one of their parts is




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not alone and has nothing to worry about my school, while myself, when there are less than tenacity
rank 3 sample 1 >Hello, I'm a language model, and you'll be able to create a new and vibrant library. The web is a free resource for your students from around
rank 3 sample 2 >Hello, I'm a language model, and it doesn't.
Here's another list of language skills that I'll want to be around for a language problem
rank 3 sample 3 >Hello, I'm a language model,
How to make a math study or
I am trying to study a math, so do you lose the
The




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm a bit surprised to see:
In a hurry, as the more you do what is to be a
rank 7 sample 1 >Hello, I'm a language model, ‘We are very slow down how many people take each step closer to the screen,’ the first time it
rank 1 sample 0 >Hello, I'm a language model, one where I'm trying to think on my classroom is in real time.<|endoftext|>The Internet and other Internet technologies are being
rank 7 sample 2 >Hello, I'm a language model, with the help of one of my son and his elder brother, so why do we need to be there for his own
rank 1 sample 1 >Hello, I'm a language model, so it’s a great example of how much my students feel comfortable at play such as soccer.
In the
rank 7 sample 3 >Hello, I'm a language model, all of which are mostly language. Even at times, I used the phrase “as a bit of a bit.


rank 1 sample 2 >Hello, I'm a language model, it enables us to think about the complexity of the future and their potential.
The first of this book was in 2015
rank 1 sample 3 >Hello, I'm a language model, and I'm talking to both the kids the same kid the same behavior will ever be both interesting and interesting. We're




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I will find that when my mother would tell me to her grandmother as he was born. If you would like that


ddp_rank 6: ####### Printing generated samples ####### 


rank 0 sample 1 >Hello, I'm a language model, i would only be used to teach us how the two kinds of information they use in our English. A great example is
rank 6 sample 0 >Hello, I'm a language model, you know, in addition to making, and in-between the end and the end of a word, it is only

rank 0 sample 2 >Hello, I'm a language model, but I still love having two little siblings. We look at one sibling of a sibling. I'm a sibling, and
ddp_rank 4: ####### Printing generated samples ####### 

rank 6 sample 1 >Hello, I'm a language model, it's been a good way to understand its history in the face of people. The challenge was that it could be a
rank 0 sample 3 >Hello, I'm a language model, it says, and it's not, he gets more than happy. I think there's still more sense of what,


rank 6 sample 2 >Hello, I'm a language model, I mean the two kids have been told that a child is the youngest child that is the youngest child. It also makes
rank 6 sample 3 >Hello, I'm a language model, in which all of our language was being in a language.
Why is this so interesting was that your teacher may seem


rank 4 sample 0 >Hello, I'm a language model, but I'll be one of the most comprehensive examples in the language. And in the context of spelling. In the case
rank 4 sample 1 >Hello, I'm a language model, especially the way those that we were at once came into life.
What I was working in my house and your family
rank 4 sample 2 >Hello, I'm a language model, which covers the last part of the study and for the first time of the study. It should be well-drained
rank 4 sample 3 >Hello, I'm a language model, and the book provides a unique and important tool, where its resources and options can be used to provide the best educational and


Step 1000 | loss: 4.104949 | lr:5.9968e-04 | norm 0.4598 | dt 12337.80ms | 42494.46 tokens/sec
Step 1001 | loss: 4.122869 | lr:5.9968e-04 | norm 0.5310 | dt 335.48ms | 1562814.13 tokens/sec
Step 1002 | loss: 4.133746 | lr:5.9967e-04 | norm 0.5611 | dt 335.30ms | 1563657.58 tokens/sec
Step 1003 | loss: 4.063006 | lr:5.9967e-04 | norm 0.5188 | dt 336.29ms | 1559033.69 tokens/sec
Step 1004 | loss: 4.017317 | lr:5.9967e-04 | norm 0.5483 | dt 335.60ms | 1562227.91 tokens/sec
Step 1005 | loss: 3.976511 | lr:5.9967e-04 | norm 0.5378 | dt 335.58ms | 1562318.93 tokens/sec
Step 1006 | loss: 3.999936 | lr:5.9967e-04 | norm 0.5071 | dt 336.89ms | 1556261.00 tokens/sec
Step 1007 | loss: 4.015894 | lr:5.9966e-04 | norm 0.5053 | dt 336.16ms | 1559647.37 tokens/sec
Step 1008 | loss: 4.063306 | lr:5.9966e-04 | norm 0.5183 | dt 336.39ms | 1558564.08 tokens/sec
Step 1009 | loss: 3.963949 | lr:5.9966e-04 | norm 0.4942 | dt 336.27ms | 1559138.70 tokens/sec
Step 1010 | loss: 4.087371 | lr:5.9966e-04 | norm 0.4703 | dt 336.41ms | 1558480.13 tokens/sec
Step 1011 | loss: 4.038931 | lr:5.9965e-04 | norm 0.4596 | dt 336.74ms | 1556959.59 tokens/sec
Step 1012 | loss: 4.019100 | lr:5.9965e-04 | norm 0.4644 | dt 335.60ms | 1562223.47 tokens/sec
Step 1013 | loss: 4.012700 | lr:5.9965e-04 | norm 0.4440 | dt 336.93ms | 1556068.29 tokens/sec
Step 1014 | loss: 4.025313 | lr:5.9965e-04 | norm 0.4815 | dt 336.73ms | 1557011.40 tokens/sec
Step 1015 | loss: 4.057111 | lr:5.9964e-04 | norm 0.4777 | dt 335.84ms | 1561111.11 tokens/sec
Step 1016 | loss: 4.209395 | lr:5.9964e-04 | norm 0.5453 | dt 336.42ms | 1558455.83 tokens/sec
Step 1017 | loss: 4.217591 | lr:5.9964e-04 | norm 0.5764 | dt 336.82ms | 1556593.69 tokens/sec
Step 1018 | loss: 4.131973 | lr:5.9964e-04 | norm 0.4781 | dt 335.94ms | 1560681.22 tokens/sec
Step 1019 | loss: 4.207853 | lr:5.9963e-04 | norm 0.4671 | dt 336.55ms | 1557825.43 tokens/sec
Step 1020 | loss: 4.164586 | lr:5.9963e-04 | norm 0.5111 | dt 336.70ms | 1557151.42 tokens/sec
Step 1021 | loss: 4.178687 | lr:5.9963e-04 | norm 0.4064 | dt 337.17ms | 1554947.07 tokens/sec
Step 1022 | loss: 4.132736 | lr:5.9963e-04 | norm 0.3972 | dt 336.95ms | 1555976.90 tokens/sec
Step 1023 | loss: 4.186783 | lr:5.9963e-04 | norm 0.3826 | dt 336.93ms | 1556078.20 tokens/sec
Step 1024 | loss: 4.143322 | lr:5.9962e-04 | norm 0.3812 | dt 337.14ms | 1555095.52 tokens/sec
Step 1025 | loss: 4.160364 | lr:5.9962e-04 | norm 0.3594 | dt 338.30ms | 1549791.01 tokens/sec
Step 1026 | loss: 4.154111 | lr:5.9962e-04 | norm 0.3731 | dt 336.77ms | 1556798.66 tokens/sec
Step 1027 | loss: 4.150547 | lr:5.9962e-04 | norm 0.4231 | dt 337.24ms | 1554640.37 tokens/sec
Step 1028 | loss: 4.106905 | lr:5.9961e-04 | norm 0.3876 | dt 338.12ms | 1550605.15 tokens/sec
Step 1029 | loss: 4.115507 | lr:5.9961e-04 | norm 0.5050 | dt 337.02ms | 1555646.68 tokens/sec
Step 1030 | loss: 4.157624 | lr:5.9961e-04 | norm 0.6578 | dt 337.53ms | 1553321.49 tokens/sec
Step 1031 | loss: 4.194391 | lr:5.9961e-04 | norm 0.6140 | dt 337.33ms | 1554223.93 tokens/sec
Step 1032 | loss: 4.155773 | lr:5.9960e-04 | norm 0.5959 | dt 337.79ms | 1552100.15 tokens/sec
Step 1033 | loss: 4.183014 | lr:5.9960e-04 | norm 0.5335 | dt 337.98ms | 1551248.33 tokens/sec
Step 1034 | loss: 4.111233 | lr:5.9960e-04 | norm 0.4922 | dt 337.65ms | 1552742.38 tokens/sec
Step 1035 | loss: 4.114268 | lr:5.9960e-04 | norm 0.4209 | dt 338.99ms | 1546602.79 tokens/sec
Step 1036 | loss: 4.126695 | lr:5.9959e-04 | norm 0.4602 | dt 338.13ms | 1550562.51 tokens/sec
Step 1037 | loss: 4.132201 | lr:5.9959e-04 | norm 0.3917 | dt 337.93ms | 1551469.40 tokens/sec
Step 1038 | loss: 4.062845 | lr:5.9959e-04 | norm 0.3903 | dt 339.54ms | 1544127.78 tokens/sec
Step 1039 | loss: 4.031128 | lr:5.9959e-04 | norm 0.4215 | dt 338.18ms | 1550314.36 tokens/sec
Step 1040 | loss: 4.027377 | lr:5.9958e-04 | norm 0.4347 | dt 339.01ms | 1546528.83 tokens/sec
Step 1041 | loss: 4.064509 | lr:5.9958e-04 | norm 0.3899 | dt 343.08ms | 1528162.83 tokens/sec
Step 1042 | loss: 4.035308 | lr:5.9958e-04 | norm 0.3931 | dt 337.52ms | 1553353.31 tokens/sec
Step 1043 | loss: 4.098687 | lr:5.9957e-04 | norm 0.5180 | dt 337.81ms | 1552001.56 tokens/sec
Step 1044 | loss: 4.006962 | lr:5.9957e-04 | norm 0.5814 | dt 338.12ms | 1550584.38 tokens/sec
Step 1045 | loss: 4.016998 | lr:5.9957e-04 | norm 0.4851 | dt 338.20ms | 1550228.02 tokens/sec
Step 1046 | loss: 4.069861 | lr:5.9957e-04 | norm 0.4558 | dt 338.53ms | 1548704.99 tokens/sec
Step 1047 | loss: 4.021615 | lr:5.9956e-04 | norm 0.5331 | dt 338.17ms | 1550388.69 tokens/sec
Step 1048 | loss: 4.049634 | lr:5.9956e-04 | norm 0.4980 | dt 337.54ms | 1553252.37 tokens/sec
Step 1049 | loss: 4.010588 | lr:5.9956e-04 | norm 0.4742 | dt 338.50ms | 1548857.71 tokens/sec
Step 1050 | loss: 3.966541 | lr:5.9956e-04 | norm 0.5218 | dt 338.67ms | 1548068.28 tokens/sec
Step 1051 | loss: 4.010341 | lr:5.9955e-04 | norm 0.5078 | dt 337.39ms | 1553955.94 tokens/sec
Step 1052 | loss: 3.990114 | lr:5.9955e-04 | norm 0.5936 | dt 337.46ms | 1553639.75 tokens/sec
Step 1053 | loss: 4.068273 | lr:5.9955e-04 | norm 0.4695 | dt 339.16ms | 1545824.35 tokens/sec
Step 1054 | loss: 3.965571 | lr:5.9955e-04 | norm 0.5009 | dt 337.29ms | 1554398.61 tokens/sec
Step 1055 | loss: 3.999122 | lr:5.9954e-04 | norm 0.5002 | dt 337.55ms | 1553197.52 tokens/sec
Step 1056 | loss: 4.001167 | lr:5.9954e-04 | norm 0.4501 | dt 337.27ms | 1554499.70 tokens/sec
Step 1057 | loss: 3.944674 | lr:5.9954e-04 | norm 0.4532 | dt 337.69ms | 1552592.18 tokens/sec
Step 1058 | loss: 4.123088 | lr:5.9954e-04 | norm 0.5381 | dt 338.24ms | 1550051.00 tokens/sec
Step 1059 | loss: 3.987579 | lr:5.9953e-04 | norm 0.8098 | dt 337.97ms | 1551271.31 tokens/sec
Step 1060 | loss: 4.044590 | lr:5.9953e-04 | norm 1.1014 | dt 338.14ms | 1550498.01 tokens/sec
Step 1061 | loss: 4.225318 | lr:5.9953e-04 | norm 1.4714 | dt 338.38ms | 1549395.72 tokens/sec
Step 1062 | loss: 4.288747 | lr:5.9952e-04 | norm 0.8640 | dt 337.87ms | 1551729.96 tokens/sec
Step 1063 | loss: 4.253692 | lr:5.9952e-04 | norm 0.9318 | dt 338.39ms | 1549377.16 tokens/sec
Step 1064 | loss: 4.165936 | lr:5.9952e-04 | norm 0.6135 | dt 338.29ms | 1549808.48 tokens/sec
Step 1065 | loss: 4.161691 | lr:5.9952e-04 | norm 0.5484 | dt 338.28ms | 1549852.17 tokens/sec
Step 1066 | loss: 4.116044 | lr:5.9951e-04 | norm 0.4379 | dt 338.20ms | 1550237.86 tokens/sec
Step 1067 | loss: 4.201322 | lr:5.9951e-04 | norm 0.4106 | dt 337.92ms | 1551501.15 tokens/sec
Step 1068 | loss: 4.147301 | lr:5.9951e-04 | norm 0.4053 | dt 338.07ms | 1550833.70 tokens/sec
Step 1069 | loss: 4.147814 | lr:5.9950e-04 | norm 0.4270 | dt 338.41ms | 1549268.00 tokens/sec
Step 1070 | loss: 4.155349 | lr:5.9950e-04 | norm 0.3852 | dt 338.45ms | 1549101.02 tokens/sec
Step 1071 | loss: 4.137600 | lr:5.9950e-04 | norm 0.3536 | dt 337.69ms | 1552563.68 tokens/sec
Step 1072 | loss: 4.133722 | lr:5.9950e-04 | norm 0.3460 | dt 337.81ms | 1551999.37 tokens/sec
Step 1073 | loss: 4.071015 | lr:5.9949e-04 | norm 0.3367 | dt 338.85ms | 1547270.95 tokens/sec
Step 1074 | loss: 4.115745 | lr:5.9949e-04 | norm 0.3983 | dt 337.72ms | 1552411.33 tokens/sec
Step 1075 | loss: 4.128105 | lr:5.9949e-04 | norm 0.5045 | dt 337.81ms | 1552011.42 tokens/sec
Step 1076 | loss: 4.126371 | lr:5.9948e-04 | norm 0.6558 | dt 339.31ms | 1545136.81 tokens/sec
Step 1077 | loss: 4.126687 | lr:5.9948e-04 | norm 0.6769 | dt 338.00ms | 1551136.72 tokens/sec
Step 1078 | loss: 4.101476 | lr:5.9948e-04 | norm 0.5476 | dt 336.98ms | 1555826.08 tokens/sec
Step 1079 | loss: 4.093577 | lr:5.9948e-04 | norm 0.4921 | dt 337.73ms | 1552388.32 tokens/sec
Step 1080 | loss: 4.118426 | lr:5.9947e-04 | norm 0.4465 | dt 337.82ms | 1551966.51 tokens/sec
Step 1081 | loss: 4.093636 | lr:5.9947e-04 | norm 0.3872 | dt 337.94ms | 1551433.28 tokens/sec
Step 1082 | loss: 4.107391 | lr:5.9947e-04 | norm 0.3784 | dt 337.37ms | 1554036.11 tokens/sec
Step 1083 | loss: 4.118563 | lr:5.9946e-04 | norm 0.3787 | dt 338.05ms | 1550925.58 tokens/sec
Step 1084 | loss: 4.091862 | lr:5.9946e-04 | norm 0.3602 | dt 337.58ms | 1553096.59 tokens/sec
Step 1085 | loss: 4.025504 | lr:5.9946e-04 | norm 0.3879 | dt 337.08ms | 1555371.60 tokens/sec
Step 1086 | loss: 4.151864 | lr:5.9946e-04 | norm 0.4477 | dt 337.56ms | 1553192.03 tokens/sec
Step 1087 | loss: 4.049387 | lr:5.9945e-04 | norm 0.4740 | dt 337.50ms | 1553436.71 tokens/sec
Step 1088 | loss: 4.050487 | lr:5.9945e-04 | norm 0.4174 | dt 338.27ms | 1549908.98 tokens/sec
Step 1089 | loss: 4.067702 | lr:5.9945e-04 | norm 0.3527 | dt 337.75ms | 1552274.35 tokens/sec
Step 1090 | loss: 4.007162 | lr:5.9944e-04 | norm 0.3735 | dt 337.64ms | 1552804.87 tokens/sec
Step 1091 | loss: 3.989847 | lr:5.9944e-04 | norm 0.4221 | dt 339.39ms | 1544810.09 tokens/sec
Step 1092 | loss: 4.026251 | lr:5.9944e-04 | norm 0.4066 | dt 339.09ms | 1546164.55 tokens/sec
Step 1093 | loss: 4.037725 | lr:5.9944e-04 | norm 0.3816 | dt 339.14ms | 1545916.72 tokens/sec
Step 1094 | loss: 3.943627 | lr:5.9943e-04 | norm 0.3835 | dt 340.11ms | 1541526.67 tokens/sec
Step 1095 | loss: 3.938612 | lr:5.9943e-04 | norm 0.4325 | dt 338.91ms | 1546994.48 tokens/sec
Step 1096 | loss: 3.876010 | lr:5.9943e-04 | norm 0.4483 | dt 338.79ms | 1547535.55 tokens/sec
Step 1097 | loss: 3.970292 | lr:5.9942e-04 | norm 0.4169 | dt 339.39ms | 1544785.13 tokens/sec
Step 1098 | loss: 3.926135 | lr:5.9942e-04 | norm 0.4582 | dt 338.58ms | 1548492.33 tokens/sec
Step 1099 | loss: 3.924153 | lr:5.9942e-04 | norm 0.5692 | dt 339.27ms | 1545336.60 tokens/sec
Step 1100 | loss: 3.883095 | lr:5.9941e-04 | norm 0.6945 | dt 338.31ms | 1549733.12 tokens/sec
Step 1101 | loss: 3.899325 | lr:5.9941e-04 | norm 0.6203 | dt 338.41ms | 1549272.37 tokens/sec
Step 1102 | loss: 3.925491 | lr:5.9941e-04 | norm 0.4475 | dt 337.26ms | 1554541.46 tokens/sec
Step 1103 | loss: 3.985594 | lr:5.9941e-04 | norm 0.4664 | dt 338.56ms | 1548565.40 tokens/sec
Step 1104 | loss: 4.019575 | lr:5.9940e-04 | norm 0.4017 | dt 338.42ms | 1549221.07 tokens/sec
Step 1105 | loss: 3.937245 | lr:5.9940e-04 | norm 0.3922 | dt 337.31ms | 1554298.63 tokens/sec
Step 1106 | loss: 3.974874 | lr:5.9940e-04 | norm 0.4126 | dt 338.22ms | 1550153.71 tokens/sec
Step 1107 | loss: 3.971982 | lr:5.9939e-04 | norm 0.3888 | dt 338.75ms | 1547693.48 tokens/sec
Step 1108 | loss: 4.093141 | lr:5.9939e-04 | norm 0.3730 | dt 337.60ms | 1553007.75 tokens/sec
Step 1109 | loss: 4.096880 | lr:5.9939e-04 | norm 0.4098 | dt 338.27ms | 1549895.87 tokens/sec
Step 1110 | loss: 4.035427 | lr:5.9938e-04 | norm 0.4371 | dt 337.87ms | 1551752.96 tokens/sec
Step 1111 | loss: 4.086394 | lr:5.9938e-04 | norm 0.4750 | dt 338.04ms | 1550960.58 tokens/sec
Step 1112 | loss: 4.056778 | lr:5.9938e-04 | norm 0.4843 | dt 337.32ms | 1554278.86 tokens/sec
Step 1113 | loss: 4.023006 | lr:5.9937e-04 | norm 0.4993 | dt 337.10ms | 1555278.10 tokens/sec
Step 1114 | loss: 3.994958 | lr:5.9937e-04 | norm 0.5151 | dt 337.52ms | 1553373.06 tokens/sec
Step 1115 | loss: 4.001093 | lr:5.9937e-04 | norm 0.3727 | dt 337.56ms | 1553155.83 tokens/sec
Step 1116 | loss: 3.988039 | lr:5.9936e-04 | norm 0.4043 | dt 337.15ms | 1555042.74 tokens/sec
Step 1117 | loss: 4.054247 | lr:5.9936e-04 | norm 0.4242 | dt 337.23ms | 1554697.52 tokens/sec
Step 1118 | loss: 4.084772 | lr:5.9936e-04 | norm 0.4800 | dt 337.92ms | 1551524.14 tokens/sec
Step 1119 | loss: 4.064718 | lr:5.9935e-04 | norm 0.4795 | dt 338.04ms | 1550945.27 tokens/sec
Step 1120 | loss: 4.076943 | lr:5.9935e-04 | norm 0.4090 | dt 337.21ms | 1554795.36 tokens/sec
Step 1121 | loss: 4.065587 | lr:5.9935e-04 | norm 0.3458 | dt 336.74ms | 1556933.13 tokens/sec
Step 1122 | loss: 4.066496 | lr:5.9935e-04 | norm 0.3425 | dt 338.09ms | 1550716.68 tokens/sec
Step 1123 | loss: 4.051929 | lr:5.9934e-04 | norm 0.3644 | dt 337.28ms | 1554452.45 tokens/sec
Step 1124 | loss: 4.041341 | lr:5.9934e-04 | norm 0.4226 | dt 337.49ms | 1553513.53 tokens/sec
Step 1125 | loss: 4.022055 | lr:5.9934e-04 | norm 0.4866 | dt 337.75ms | 1552315.99 tokens/sec
Step 1126 | loss: 4.052343 | lr:5.9933e-04 | norm 0.4485 | dt 338.11ms | 1550645.61 tokens/sec
Step 1127 | loss: 4.039449 | lr:5.9933e-04 | norm 0.3942 | dt 337.73ms | 1552372.98 tokens/sec
Step 1128 | loss: 3.980344 | lr:5.9933e-04 | norm 0.4193 | dt 338.26ms | 1549941.75 tokens/sec
Step 1129 | loss: 4.077319 | lr:5.9932e-04 | norm 0.4093 | dt 339.02ms | 1546500.55 tokens/sec
Step 1130 | loss: 4.046408 | lr:5.9932e-04 | norm 0.4536 | dt 338.65ms | 1548182.72 tokens/sec
Step 1131 | loss: 4.033983 | lr:5.9932e-04 | norm 0.5063 | dt 337.05ms | 1555526.73 tokens/sec
Step 1132 | loss: 4.003082 | lr:5.9931e-04 | norm 0.4600 | dt 338.30ms | 1549754.96 tokens/sec
Step 1133 | loss: 3.996050 | lr:5.9931e-04 | norm 0.4650 | dt 901.96ms | 581273.17 tokens/sec
Step 1134 | loss: 4.006639 | lr:5.9931e-04 | norm 0.4726 | dt 335.78ms | 1561386.00 tokens/sec
Step 1135 | loss: 3.997924 | lr:5.9930e-04 | norm 0.4485 | dt 337.85ms | 1551821.95 tokens/sec
Step 1136 | loss: 4.040628 | lr:5.9930e-04 | norm 0.4719 | dt 339.25ms | 1545437.60 tokens/sec
Step 1137 | loss: 4.016593 | lr:5.9930e-04 | norm 0.4940 | dt 337.33ms | 1554209.65 tokens/sec
Step 1138 | loss: 4.013440 | lr:5.9929e-04 | norm 0.5021 | dt 337.29ms | 1554400.81 tokens/sec
Step 1139 | loss: 3.998686 | lr:5.9929e-04 | norm 0.4957 | dt 1004.99ms | 521682.64 tokens/sec
Step 1140 | loss: 3.973387 | lr:5.9929e-04 | norm 0.4522 | dt 337.52ms | 1553342.34 tokens/sec
Step 1141 | loss: 3.981659 | lr:5.9928e-04 | norm 0.4411 | dt 338.72ms | 1547844.90 tokens/sec
Step 1142 | loss: 3.862914 | lr:5.9928e-04 | norm 0.4362 | dt 338.47ms | 1548997.36 tokens/sec
Step 1143 | loss: 3.866121 | lr:5.9928e-04 | norm 0.4515 | dt 337.26ms | 1554531.57 tokens/sec
Step 1144 | loss: 3.887109 | lr:5.9927e-04 | norm 0.4374 | dt 336.32ms | 1558892.22 tokens/sec
Step 1145 | loss: 3.865594 | lr:5.9927e-04 | norm 0.4557 | dt 337.72ms | 1552431.06 tokens/sec
Step 1146 | loss: 3.868308 | lr:5.9927e-04 | norm 0.4467 | dt 336.92ms | 1556112.33 tokens/sec
Step 1147 | loss: 3.946113 | lr:5.9926e-04 | norm 0.4417 | dt 336.92ms | 1556121.14 tokens/sec
Step 1148 | loss: 3.841520 | lr:5.9926e-04 | norm 0.4134 | dt 337.80ms | 1552074.96 tokens/sec
Step 1149 | loss: 3.903134 | lr:5.9926e-04 | norm 0.3918 | dt 337.59ms | 1553049.43 tokens/sec
Step 1150 | loss: 3.927269 | lr:5.9925e-04 | norm 0.3509 | dt 336.87ms | 1556355.73 tokens/sec
Step 1151 | loss: 3.894066 | lr:5.9925e-04 | norm 0.3590 | dt 337.89ms | 1551659.89 tokens/sec
Step 1152 | loss: 3.893655 | lr:5.9925e-04 | norm 0.4053 | dt 338.39ms | 1549359.69 tokens/sec
Step 1153 | loss: 3.957727 | lr:5.9924e-04 | norm 0.4155 | dt 337.28ms | 1554465.63 tokens/sec
Step 1154 | loss: 3.976141 | lr:5.9924e-04 | norm 0.3916 | dt 338.60ms | 1548410.56 tokens/sec
Step 1155 | loss: 4.010360 | lr:5.9923e-04 | norm 0.3701 | dt 338.48ms | 1548952.62 tokens/sec
Step 1156 | loss: 4.018549 | lr:5.9923e-04 | norm 0.4168 | dt 338.44ms | 1549120.66 tokens/sec
Step 1157 | loss: 4.018775 | lr:5.9923e-04 | norm 0.4353 | dt 337.49ms | 1553472.92 tokens/sec
Step 1158 | loss: 3.996580 | lr:5.9922e-04 | norm 0.3938 | dt 337.95ms | 1551392.79 tokens/sec
Step 1159 | loss: 4.053407 | lr:5.9922e-04 | norm 0.3933 | dt 338.14ms | 1550514.40 tokens/sec
Step 1160 | loss: 4.005458 | lr:5.9922e-04 | norm 0.3816 | dt 338.30ms | 1549781.18 tokens/sec
Step 1161 | loss: 3.972283 | lr:5.9921e-04 | norm 0.3948 | dt 337.85ms | 1551824.14 tokens/sec
Step 1162 | loss: 4.012028 | lr:5.9921e-04 | norm 0.3740 | dt 337.17ms | 1554983.36 tokens/sec
Step 1163 | loss: 4.012855 | lr:5.9921e-04 | norm 0.3919 | dt 338.36ms | 1549518.00 tokens/sec
Step 1164 | loss: 3.993877 | lr:5.9920e-04 | norm 0.3668 | dt 338.27ms | 1549892.59 tokens/sec
Step 1165 | loss: 4.054706 | lr:5.9920e-04 | norm 0.3604 | dt 337.22ms | 1554714.01 tokens/sec
Step 1166 | loss: 4.018266 | lr:5.9920e-04 | norm 0.4097 | dt 338.01ms | 1551109.36 tokens/sec
Step 1167 | loss: 3.980678 | lr:5.9919e-04 | norm 0.3968 | dt 337.47ms | 1553594.75 tokens/sec
Step 1168 | loss: 3.988513 | lr:5.9919e-04 | norm 0.4147 | dt 338.09ms | 1550734.18 tokens/sec
Step 1169 | loss: 4.009422 | lr:5.9919e-04 | norm 0.5322 | dt 337.74ms | 1552333.53 tokens/sec
Step 1170 | loss: 3.967824 | lr:5.9918e-04 | norm 0.5858 | dt 338.52ms | 1548748.62 tokens/sec
Step 1171 | loss: 4.008094 | lr:5.9918e-04 | norm 0.4825 | dt 341.37ms | 1535834.54 tokens/sec
Step 1172 | loss: 4.007223 | lr:5.9917e-04 | norm 0.4258 | dt 337.83ms | 1551931.46 tokens/sec
Step 1173 | loss: 3.968006 | lr:5.9917e-04 | norm 0.4468 | dt 337.74ms | 1552326.95 tokens/sec
Step 1174 | loss: 3.980873 | lr:5.9917e-04 | norm 0.4611 | dt 337.29ms | 1554390.92 tokens/sec
Step 1175 | loss: 3.984985 | lr:5.9916e-04 | norm 0.3832 | dt 338.09ms | 1550741.83 tokens/sec
Step 1176 | loss: 3.966316 | lr:5.9916e-04 | norm 0.3603 | dt 337.96ms | 1551344.63 tokens/sec
Step 1177 | loss: 3.982409 | lr:5.9916e-04 | norm 0.3607 | dt 336.98ms | 1555850.30 tokens/sec
Step 1178 | loss: 3.948727 | lr:5.9915e-04 | norm 0.3933 | dt 337.32ms | 1554277.76 tokens/sec
Step 1179 | loss: 3.929482 | lr:5.9915e-04 | norm 0.4477 | dt 337.62ms | 1552887.11 tokens/sec
Step 1180 | loss: 3.983385 | lr:5.9915e-04 | norm 0.4527 | dt 338.15ms | 1550443.35 tokens/sec
Step 1181 | loss: 3.980926 | lr:5.9914e-04 | norm 0.4106 | dt 336.98ms | 1555821.68 tokens/sec
Step 1182 | loss: 3.934292 | lr:5.9914e-04 | norm 0.4086 | dt 337.17ms | 1554963.57 tokens/sec
Step 1183 | loss: 3.874520 | lr:5.9913e-04 | norm 0.3509 | dt 337.33ms | 1554207.45 tokens/sec
Step 1184 | loss: 3.903044 | lr:5.9913e-04 | norm 0.3678 | dt 337.54ms | 1553261.15 tokens/sec
Step 1185 | loss: 3.890403 | lr:5.9913e-04 | norm 0.3729 | dt 336.86ms | 1556396.49 tokens/sec
Step 1186 | loss: 3.961673 | lr:5.9912e-04 | norm 0.3483 | dt 340.03ms | 1541902.82 tokens/sec
Step 1187 | loss: 3.885196 | lr:5.9912e-04 | norm 0.3501 | dt 338.46ms | 1549051.92 tokens/sec
Step 1188 | loss: 3.884696 | lr:5.9912e-04 | norm 0.3498 | dt 337.98ms | 1551251.61 tokens/sec
Step 1189 | loss: 3.823287 | lr:5.9911e-04 | norm 0.3913 | dt 338.25ms | 1550014.95 tokens/sec
Step 1190 | loss: 3.854148 | lr:5.9911e-04 | norm 0.4194 | dt 338.06ms | 1550866.51 tokens/sec
Step 1191 | loss: 3.839733 | lr:5.9910e-04 | norm 0.3841 | dt 337.89ms | 1551666.46 tokens/sec
Step 1192 | loss: 3.861326 | lr:5.9910e-04 | norm 0.3607 | dt 337.74ms | 1552341.20 tokens/sec
Step 1193 | loss: 3.841300 | lr:5.9910e-04 | norm 0.4340 | dt 338.23ms | 1550093.61 tokens/sec
Step 1194 | loss: 3.883074 | lr:5.9909e-04 | norm 0.5060 | dt 338.21ms | 1550174.48 tokens/sec
Step 1195 | loss: 3.889933 | lr:5.9909e-04 | norm 0.4767 | dt 337.42ms | 1553826.38 tokens/sec
Step 1196 | loss: 3.861594 | lr:5.9909e-04 | norm 0.4547 | dt 337.87ms | 1551758.43 tokens/sec
Step 1197 | loss: 3.879299 | lr:5.9908e-04 | norm 0.4726 | dt 338.05ms | 1550914.64 tokens/sec
Step 1198 | loss: 3.906235 | lr:5.9908e-04 | norm 0.4460 | dt 338.73ms | 1547793.70 tokens/sec
Step 1199 | loss: 3.871436 | lr:5.9907e-04 | norm 0.4347 | dt 338.32ms | 1549691.62 tokens/sec
Step 1200 | loss: 3.864584 | lr:5.9907e-04 | norm 0.4296 | dt 337.44ms | 1553735.25 tokens/sec
Step 1201 | loss: 3.952517 | lr:5.9907e-04 | norm 0.4570 | dt 338.60ms | 1548398.57 tokens/sec
Step 1202 | loss: 3.970949 | lr:5.9906e-04 | norm 0.4594 | dt 338.31ms | 1549713.46 tokens/sec
Step 1203 | loss: 4.005079 | lr:5.9906e-04 | norm 0.4673 | dt 338.60ms | 1548414.92 tokens/sec
Step 1204 | loss: 4.017554 | lr:5.9906e-04 | norm 0.4425 | dt 338.38ms | 1549389.17 tokens/sec
Step 1205 | loss: 3.955858 | lr:5.9905e-04 | norm 0.5019 | dt 337.56ms | 1553192.03 tokens/sec
Step 1206 | loss: 4.000406 | lr:5.9905e-04 | norm 0.5681 | dt 337.40ms | 1553902.14 tokens/sec
Step 1207 | loss: 3.991364 | lr:5.9904e-04 | norm 0.5435 | dt 338.98ms | 1546676.76 tokens/sec
Step 1208 | loss: 4.059523 | lr:5.9904e-04 | norm 0.4383 | dt 337.63ms | 1552868.47 tokens/sec
Step 1209 | loss: 3.978467 | lr:5.9904e-04 | norm 0.4100 | dt 338.39ms | 1549361.88 tokens/sec
Step 1210 | loss: 3.972353 | lr:5.9903e-04 | norm 0.3712 | dt 338.56ms | 1548571.94 tokens/sec
Step 1211 | loss: 4.020281 | lr:5.9903e-04 | norm 0.3631 | dt 337.41ms | 1553871.39 tokens/sec
Step 1212 | loss: 3.972458 | lr:5.9902e-04 | norm 0.3408 | dt 337.49ms | 1553499.26 tokens/sec
Step 1213 | loss: 3.973894 | lr:5.9902e-04 | norm 0.3372 | dt 337.70ms | 1552539.57 tokens/sec
Step 1214 | loss: 3.973989 | lr:5.9902e-04 | norm 0.3420 | dt 337.15ms | 1555079.03 tokens/sec
Step 1215 | loss: 3.954409 | lr:5.9901e-04 | norm 0.3426 | dt 337.37ms | 1554059.17 tokens/sec
Step 1216 | loss: 3.962540 | lr:5.9901e-04 | norm 0.3170 | dt 338.23ms | 1550085.97 tokens/sec
Step 1217 | loss: 3.991485 | lr:5.9900e-04 | norm 0.3658 | dt 337.36ms | 1554077.84 tokens/sec
Step 1218 | loss: 3.922114 | lr:5.9900e-04 | norm 0.4088 | dt 336.96ms | 1555955.98 tokens/sec
Step 1219 | loss: 3.964519 | lr:5.9900e-04 | norm 0.4482 | dt 338.01ms | 1551080.92 tokens/sec
Step 1220 | loss: 3.946225 | lr:5.9899e-04 | norm 0.4836 | dt 337.74ms | 1552356.54 tokens/sec
Step 1221 | loss: 3.837321 | lr:5.9899e-04 | norm 0.5202 | dt 337.62ms | 1552889.31 tokens/sec
Step 1222 | loss: 3.979409 | lr:5.9898e-04 | norm 0.4339 | dt 337.47ms | 1553565.11 tokens/sec
Step 1223 | loss: 3.918775 | lr:5.9898e-04 | norm 0.5436 | dt 337.16ms | 1555015.25 tokens/sec
Step 1224 | loss: 3.927533 | lr:5.9898e-04 | norm 0.5540 | dt 338.09ms | 1550718.87 tokens/sec
Step 1225 | loss: 3.921400 | lr:5.9897e-04 | norm 0.5863 | dt 337.68ms | 1552626.17 tokens/sec
Step 1226 | loss: 3.881788 | lr:5.9897e-04 | norm 0.4509 | dt 337.18ms | 1554918.49 tokens/sec
Step 1227 | loss: 3.921955 | lr:5.9896e-04 | norm 0.4377 | dt 338.05ms | 1550912.45 tokens/sec
Step 1228 | loss: 3.911251 | lr:5.9896e-04 | norm 0.4142 | dt 337.26ms | 1554534.86 tokens/sec
Step 1229 | loss: 3.905510 | lr:5.9896e-04 | norm 0.3837 | dt 337.72ms | 1552413.53 tokens/sec
Step 1230 | loss: 3.893938 | lr:5.9895e-04 | norm 0.4078 | dt 338.51ms | 1548822.80 tokens/sec
Step 1231 | loss: 3.912954 | lr:5.9895e-04 | norm 0.3819 | dt 337.70ms | 1552548.34 tokens/sec
Step 1232 | loss: 3.871225 | lr:5.9894e-04 | norm 0.3498 | dt 337.48ms | 1553549.75 tokens/sec
Step 1233 | loss: 3.848551 | lr:5.9894e-04 | norm 0.3835 | dt 338.14ms | 1550493.63 tokens/sec
Step 1234 | loss: 3.888407 | lr:5.9894e-04 | norm 0.3546 | dt 337.45ms | 1553659.51 tokens/sec
Step 1235 | loss: 3.881051 | lr:5.9893e-04 | norm 0.3673 | dt 337.36ms | 1554085.53 tokens/sec
Step 1236 | loss: 3.884728 | lr:5.9893e-04 | norm 0.4097 | dt 338.08ms | 1550763.71 tokens/sec
Step 1237 | loss: 3.769852 | lr:5.9892e-04 | norm 0.3855 | dt 337.38ms | 1554020.73 tokens/sec
Step 1238 | loss: 3.837389 | lr:5.9892e-04 | norm 0.3769 | dt 337.21ms | 1554777.77 tokens/sec
Step 1239 | loss: 3.808221 | lr:5.9892e-04 | norm 0.3713 | dt 337.26ms | 1554562.34 tokens/sec
Step 1240 | loss: 3.803925 | lr:5.9891e-04 | norm 0.3746 | dt 338.25ms | 1549981.08 tokens/sec
Step 1241 | loss: 3.826830 | lr:5.9891e-04 | norm 0.4204 | dt 337.43ms | 1553787.95 tokens/sec
Step 1242 | loss: 3.871445 | lr:5.9890e-04 | norm 0.4468 | dt 336.84ms | 1556500.04 tokens/sec
Step 1243 | loss: 3.866702 | lr:5.9890e-04 | norm 0.3873 | dt 338.21ms | 1550164.64 tokens/sec
Step 1244 | loss: 3.833014 | lr:5.9889e-04 | norm 0.3547 | dt 338.19ms | 1550260.81 tokens/sec
Step 1245 | loss: 3.814681 | lr:5.9889e-04 | norm 0.3305 | dt 337.49ms | 1553469.63 tokens/sec
Step 1246 | loss: 3.858748 | lr:5.9889e-04 | norm 0.3360 | dt 337.92ms | 1551500.05 tokens/sec
Step 1247 | loss: 3.840756 | lr:5.9888e-04 | norm 0.3408 | dt 337.89ms | 1551671.93 tokens/sec
Step 1248 | loss: 3.833148 | lr:5.9888e-04 | norm 0.3273 | dt 338.60ms | 1548416.01 tokens/sec
Step 1249 | loss: 3.926561 | lr:5.9887e-04 | norm 0.3609 | dt 337.38ms | 1553997.67 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 1250: 3.9456
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2555/10042=0.2544


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I can say that an interpreter is not simply an interpreter, but simply a language that is not the interpreter itself. It
rank 5 sample 1 >Hello, I'm a language model, is pretty fast at just 10 years. So why is it really better to read in this post? We should learn to
rank 5 sample 2 >Hello, I'm a language model, since it comes in some way since one of the most important languages and I want a reason for that.
I would
rank 5 sample 3 >Hello, I'm a language model, this is pretty accurate. There is a very nice place.
And, yes, this is still the most interesting feature




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not sure I've created anything that is good enough for more content than any other language type, so for
rank 3 sample 1 >Hello, I'm a language model, and you've got a number from the same number that comes from a specific language.
Thanks for reading, for all
rank 3 sample 2 >Hello, I'm a language model, and it was so fascinating that these are "like" for language. I was surprised to read two books, some reading
rank 3 sample 3 >Hello, I'm a language model, and
gave up, it's the most common way of
words. This is, to allow you to read




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, based on a binary model, which includes a set of integers that are assigned by a binary or a binary or a binary
rank 2 sample 1 >Hello, I'm a language model, as I hear. I'm a new parent. No, you'll start to learn something about this book, and we
rank 2 sample 2 >Hello, I'm a language model, but I think I would have the potential to use a language model for high-tech applications, with this, so they


ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 3 >Hello, I'm a language model, so let me know, and I'm working on to do, and I'm using my voice only to speak to,


rank 7 sample 0 >Hello, I'm a language model, and I'm a linguistics professor. However, if you're a professional, most of us speak about the language,
rank 7 sample 1 >Hello, I'm a language model, while myself was a little different at an intermediate level. Like any other language, how about that? How about?<|endoftext|>
rank 7 sample 2 >Hello, I'm a language model, and I've talked about it: you have to choose a language model: you want to choose a language model or to


ddp_rank 6: ####### Printing generated samples ####### 

rank 7 sample 3 >Hello, I'm a language model, very useful, and so you know for more insight into the "real world" that we're able to understand from human


rank 6 sample 0 >Hello, I'm a language model, in order to use language methods to generate a language and a set of words with a language model.
The language model
rank 6 sample 1 >Hello, I'm a language model, that's been used with a real-school math class. I'm really a language model.
But that's not
rank 6 sample 2 >Hello, I'm a language model, I want to show what you think in the comments.
- “Do you think this is real? How can
rank 6 sample 3 >Hello, I'm a language model, and this is the way to write the language, and this is what I think is. Just as soon as I started




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I am a great guy. I really think a lot of people, if you were learning to use this, then
rank 4 sample 1 >Hello, I'm a language model, though it's being tested. A common model with four parameters is the sum of an initial variable, as a sample size
rank 4 sample 2 >Hello, I'm a language model, so people often can get this information out, so they can learn how to make those words. They then have lots of
rank 4 sample 3 >Hello, I'm a language model, and the work with this language is one I have for almost no more than one language. I do have a different approach




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, just that's a model, and with that code in any part of the software. But the most important feature is the
rank 1 sample 1 >Hello, I'm a language model, a language model, and a way of building a website."
You can write new script-spatial data.

rank 1 sample 2 >Hello, I'm a language model, I found a bit like this.
I'm a linguist and you might be a bit interested in a way that


ddp_rank 0: ####### Printing generated samples ####### 

rank 1 sample 3 >Hello, I'm a language model, and I'm trying to build a large pool of my mind.<|endoftext|>When talking about black students with a black person,


rank 0 sample 0 >Hello, I'm a language model, and I'll be able to understand some grammar issues. To get rid of my grammar questions, try and answer the whole
rank 0 sample 1 >Hello, I'm a language model, my model is a very specific model of the universe without that model. They do, because you can think of a single
rank 0 sample 2 >Hello, I'm a language model, but I also think like to ask for a few of the characters, or the "yes" to ask for the rest
rank 0 sample 3 >Hello, I'm a language model, and how to model a model, with the tools for the model, and a user interface to model a model that displays


Step 1250 | loss: 3.911791 | lr:5.9887e-04 | norm 0.4031 | dt 12286.89ms | 42670.53 tokens/sec
Step 1251 | loss: 3.966663 | lr:5.9886e-04 | norm 0.4360 | dt 335.02ms | 1564928.37 tokens/sec
Step 1252 | loss: 3.940915 | lr:5.9886e-04 | norm 0.4147 | dt 336.84ms | 1556487.92 tokens/sec
Step 1253 | loss: 3.946873 | lr:5.9886e-04 | norm 0.4471 | dt 337.08ms | 1555394.71 tokens/sec
Step 1254 | loss: 3.983851 | lr:5.9885e-04 | norm 0.5757 | dt 336.08ms | 1560000.32 tokens/sec
Step 1255 | loss: 3.966194 | lr:5.9885e-04 | norm 0.7131 | dt 336.36ms | 1558692.23 tokens/sec
Step 1256 | loss: 3.999148 | lr:5.9884e-04 | norm 0.5449 | dt 336.81ms | 1556647.68 tokens/sec
Step 1257 | loss: 3.936479 | lr:5.9884e-04 | norm 0.4483 | dt 337.53ms | 1553310.52 tokens/sec
Step 1258 | loss: 3.961809 | lr:5.9884e-04 | norm 0.3924 | dt 336.14ms | 1559720.38 tokens/sec
Step 1259 | loss: 3.953130 | lr:5.9883e-04 | norm 0.3813 | dt 337.68ms | 1552637.13 tokens/sec
Step 1260 | loss: 3.968740 | lr:5.9883e-04 | norm 0.3799 | dt 336.48ms | 1558156.57 tokens/sec
Step 1261 | loss: 3.926601 | lr:5.9882e-04 | norm 0.3987 | dt 336.16ms | 1559645.16 tokens/sec
Step 1262 | loss: 3.956299 | lr:5.9882e-04 | norm 0.3827 | dt 336.97ms | 1555875.62 tokens/sec
Step 1263 | loss: 3.931493 | lr:5.9881e-04 | norm 0.3264 | dt 336.62ms | 1557504.35 tokens/sec
Step 1264 | loss: 3.907486 | lr:5.9881e-04 | norm 0.3741 | dt 336.95ms | 1555993.42 tokens/sec
Step 1265 | loss: 3.941036 | lr:5.9880e-04 | norm 0.4349 | dt 336.55ms | 1557850.81 tokens/sec
Step 1266 | loss: 3.887591 | lr:5.9880e-04 | norm 0.4335 | dt 335.87ms | 1561005.83 tokens/sec
Step 1267 | loss: 3.923658 | lr:5.9880e-04 | norm 0.3476 | dt 337.70ms | 1552538.47 tokens/sec
Step 1268 | loss: 3.916019 | lr:5.9879e-04 | norm 0.3339 | dt 336.94ms | 1556014.34 tokens/sec
Step 1269 | loss: 3.920500 | lr:5.9879e-04 | norm 0.3695 | dt 336.84ms | 1556490.12 tokens/sec
Step 1270 | loss: 3.853608 | lr:5.9878e-04 | norm 0.3700 | dt 338.14ms | 1550502.38 tokens/sec
Step 1271 | loss: 3.925975 | lr:5.9878e-04 | norm 0.3876 | dt 336.83ms | 1556513.26 tokens/sec
Step 1272 | loss: 3.876825 | lr:5.9877e-04 | norm 0.4010 | dt 336.17ms | 1559589.86 tokens/sec
Step 1273 | loss: 3.837374 | lr:5.9877e-04 | norm 0.3332 | dt 337.82ms | 1551982.94 tokens/sec
Step 1274 | loss: 3.831402 | lr:5.9877e-04 | norm 0.3222 | dt 337.38ms | 1553992.18 tokens/sec
Step 1275 | loss: 3.874108 | lr:5.9876e-04 | norm 0.3646 | dt 336.96ms | 1555931.76 tokens/sec
Step 1276 | loss: 3.887344 | lr:5.9876e-04 | norm 0.3724 | dt 336.93ms | 1556094.71 tokens/sec
Step 1277 | loss: 3.858554 | lr:5.9875e-04 | norm 0.3959 | dt 337.70ms | 1552509.97 tokens/sec
Step 1278 | loss: 3.834285 | lr:5.9875e-04 | norm 0.4418 | dt 337.57ms | 1553140.47 tokens/sec
Step 1279 | loss: 3.863176 | lr:5.9874e-04 | norm 0.4138 | dt 337.80ms | 1552050.86 tokens/sec
Step 1280 | loss: 3.915444 | lr:5.9874e-04 | norm 0.4258 | dt 337.70ms | 1552520.94 tokens/sec
Step 1281 | loss: 3.838945 | lr:5.9873e-04 | norm 0.4522 | dt 337.14ms | 1555103.22 tokens/sec
Step 1282 | loss: 3.832769 | lr:5.9873e-04 | norm 0.3980 | dt 337.73ms | 1552393.80 tokens/sec
Step 1283 | loss: 3.847088 | lr:5.9873e-04 | norm 0.3879 | dt 338.17ms | 1550371.20 tokens/sec
Step 1284 | loss: 3.877654 | lr:5.9872e-04 | norm 0.3723 | dt 337.88ms | 1551690.55 tokens/sec
Step 1285 | loss: 3.805659 | lr:5.9872e-04 | norm 0.3703 | dt 337.73ms | 1552388.32 tokens/sec
Step 1286 | loss: 3.778280 | lr:5.9871e-04 | norm 0.3970 | dt 338.11ms | 1550646.70 tokens/sec
Step 1287 | loss: 3.761342 | lr:5.9871e-04 | norm 0.4009 | dt 338.11ms | 1550637.95 tokens/sec
Step 1288 | loss: 3.781096 | lr:5.9870e-04 | norm 0.3821 | dt 337.68ms | 1552609.72 tokens/sec
Step 1289 | loss: 3.752734 | lr:5.9870e-04 | norm 0.3586 | dt 337.30ms | 1554381.03 tokens/sec
Step 1290 | loss: 3.798838 | lr:5.9869e-04 | norm 0.3702 | dt 338.43ms | 1549170.86 tokens/sec
Step 1291 | loss: 3.812162 | lr:5.9869e-04 | norm 0.3822 | dt 338.19ms | 1550291.41 tokens/sec
Step 1292 | loss: 3.749547 | lr:5.9868e-04 | norm 0.3465 | dt 339.48ms | 1544389.13 tokens/sec
Step 1293 | loss: 3.790723 | lr:5.9868e-04 | norm 0.3354 | dt 337.77ms | 1552198.75 tokens/sec
Step 1294 | loss: 3.811276 | lr:5.9868e-04 | norm 0.3551 | dt 338.12ms | 1550612.80 tokens/sec
Step 1295 | loss: 3.822848 | lr:5.9867e-04 | norm 0.3959 | dt 337.90ms | 1551584.35 tokens/sec
Step 1296 | loss: 3.864085 | lr:5.9867e-04 | norm 0.4507 | dt 337.41ms | 1553850.53 tokens/sec
Step 1297 | loss: 3.905569 | lr:5.9866e-04 | norm 0.3785 | dt 337.70ms | 1552520.94 tokens/sec
Step 1298 | loss: 3.959820 | lr:5.9866e-04 | norm 0.3857 | dt 337.94ms | 1551444.23 tokens/sec
Step 1299 | loss: 3.964913 | lr:5.9865e-04 | norm 0.4210 | dt 338.06ms | 1550878.54 tokens/sec
Step 1300 | loss: 3.970490 | lr:5.9865e-04 | norm 0.4776 | dt 337.87ms | 1551747.48 tokens/sec
Step 1301 | loss: 3.922375 | lr:5.9864e-04 | norm 0.5506 | dt 337.56ms | 1553173.38 tokens/sec
Step 1302 | loss: 3.889052 | lr:5.9864e-04 | norm 0.5068 | dt 337.89ms | 1551673.03 tokens/sec
Step 1303 | loss: 3.960612 | lr:5.9863e-04 | norm 0.4655 | dt 338.91ms | 1546991.21 tokens/sec
Step 1304 | loss: 3.912048 | lr:5.9863e-04 | norm 0.4261 | dt 339.07ms | 1546251.53 tokens/sec
Step 1305 | loss: 3.987058 | lr:5.9862e-04 | norm 0.4227 | dt 337.81ms | 1552023.47 tokens/sec
Step 1306 | loss: 3.961541 | lr:5.9862e-04 | norm 0.3966 | dt 337.50ms | 1553458.66 tokens/sec
Step 1307 | loss: 3.887923 | lr:5.9862e-04 | norm 0.3873 | dt 338.12ms | 1550596.40 tokens/sec
Step 1308 | loss: 3.884130 | lr:5.9861e-04 | norm 0.3474 | dt 339.34ms | 1545032.59 tokens/sec
Step 1309 | loss: 3.920553 | lr:5.9861e-04 | norm 0.3872 | dt 337.60ms | 1552998.98 tokens/sec
Step 1310 | loss: 3.889383 | lr:5.9860e-04 | norm 0.4233 | dt 337.98ms | 1551219.87 tokens/sec
Step 1311 | loss: 3.863551 | lr:5.9860e-04 | norm 0.4297 | dt 337.60ms | 1552989.11 tokens/sec
Step 1312 | loss: 3.879655 | lr:5.9859e-04 | norm 0.3480 | dt 338.25ms | 1549986.54 tokens/sec
Step 1313 | loss: 3.915199 | lr:5.9859e-04 | norm 0.3363 | dt 337.75ms | 1552296.27 tokens/sec
Step 1314 | loss: 3.884598 | lr:5.9858e-04 | norm 0.3521 | dt 337.21ms | 1554797.55 tokens/sec
Step 1315 | loss: 3.899935 | lr:5.9858e-04 | norm 0.3795 | dt 337.94ms | 1551432.19 tokens/sec
Step 1316 | loss: 3.898692 | lr:5.9857e-04 | norm 0.3548 | dt 337.89ms | 1551669.74 tokens/sec
Step 1317 | loss: 3.872516 | lr:5.9857e-04 | norm 0.3764 | dt 337.97ms | 1551297.57 tokens/sec
Step 1318 | loss: 3.856651 | lr:5.9856e-04 | norm 0.3625 | dt 337.72ms | 1552452.98 tokens/sec
Step 1319 | loss: 3.878160 | lr:5.9856e-04 | norm 0.3757 | dt 337.96ms | 1551309.61 tokens/sec
Step 1320 | loss: 3.821358 | lr:5.9855e-04 | norm 0.4054 | dt 337.97ms | 1551284.44 tokens/sec
Step 1321 | loss: 3.872327 | lr:5.9855e-04 | norm 0.4044 | dt 338.17ms | 1550367.92 tokens/sec
Step 1322 | loss: 3.850028 | lr:5.9854e-04 | norm 0.4230 | dt 1026.52ms | 510744.87 tokens/sec
Step 1323 | loss: 3.820966 | lr:5.9854e-04 | norm 0.4313 | dt 336.44ms | 1558334.35 tokens/sec
Step 1324 | loss: 3.845540 | lr:5.9854e-04 | norm 0.3626 | dt 338.19ms | 1550291.41 tokens/sec
Step 1325 | loss: 3.817197 | lr:5.9853e-04 | norm 0.3464 | dt 342.66ms | 1530055.47 tokens/sec
Step 1326 | loss: 3.795158 | lr:5.9853e-04 | norm 0.3888 | dt 338.86ms | 1547231.76 tokens/sec
Step 1327 | loss: 3.788652 | lr:5.9852e-04 | norm 0.4567 | dt 339.16ms | 1545834.13 tokens/sec
Step 1328 | loss: 3.862068 | lr:5.9852e-04 | norm 0.4732 | dt 338.05ms | 1550925.58 tokens/sec
Step 1329 | loss: 3.779468 | lr:5.9851e-04 | norm 0.4117 | dt 1026.88ms | 510562.25 tokens/sec
Step 1330 | loss: 3.817727 | lr:5.9851e-04 | norm 0.3957 | dt 336.87ms | 1556352.42 tokens/sec
Step 1331 | loss: 3.733880 | lr:5.9850e-04 | norm 0.4205 | dt 336.82ms | 1556565.04 tokens/sec
Step 1332 | loss: 3.817577 | lr:5.9850e-04 | norm 0.3600 | dt 338.70ms | 1547942.97 tokens/sec
Step 1333 | loss: 3.726175 | lr:5.9849e-04 | norm 0.3714 | dt 337.21ms | 1554775.57 tokens/sec
Step 1334 | loss: 3.811592 | lr:5.9849e-04 | norm 0.4059 | dt 337.96ms | 1551326.02 tokens/sec
Step 1335 | loss: 3.800360 | lr:5.9848e-04 | norm 0.3968 | dt 337.81ms | 1552034.42 tokens/sec
Step 1336 | loss: 3.795135 | lr:5.9848e-04 | norm 0.4032 | dt 338.13ms | 1550537.36 tokens/sec
Step 1337 | loss: 3.772446 | lr:5.9847e-04 | norm 0.3663 | dt 337.93ms | 1551478.16 tokens/sec
Step 1338 | loss: 3.822558 | lr:5.9847e-04 | norm 0.3897 | dt 338.55ms | 1548629.74 tokens/sec
Step 1339 | loss: 3.821272 | lr:5.9846e-04 | norm 0.4152 | dt 337.88ms | 1551696.02 tokens/sec
Step 1340 | loss: 3.791122 | lr:5.9846e-04 | norm 0.3705 | dt 338.90ms | 1547010.80 tokens/sec
Step 1341 | loss: 3.772480 | lr:5.9845e-04 | norm 0.3137 | dt 338.36ms | 1549490.70 tokens/sec
Step 1342 | loss: 3.943900 | lr:5.9845e-04 | norm 0.3597 | dt 337.47ms | 1553568.40 tokens/sec
Step 1343 | loss: 3.897032 | lr:5.9844e-04 | norm 0.4632 | dt 338.51ms | 1548798.80 tokens/sec
Step 1344 | loss: 3.912052 | lr:5.9844e-04 | norm 0.5670 | dt 340.26ms | 1540861.30 tokens/sec
Step 1345 | loss: 3.918021 | lr:5.9843e-04 | norm 0.5076 | dt 337.52ms | 1553375.26 tokens/sec
Step 1346 | loss: 3.869052 | lr:5.9843e-04 | norm 0.4038 | dt 338.51ms | 1548820.62 tokens/sec
Step 1347 | loss: 3.883342 | lr:5.9842e-04 | norm 0.4099 | dt 338.00ms | 1551149.85 tokens/sec
Step 1348 | loss: 3.862717 | lr:5.9842e-04 | norm 0.3748 | dt 364.66ms | 1437764.39 tokens/sec
Step 1349 | loss: 3.911155 | lr:5.9841e-04 | norm 0.3390 | dt 337.89ms | 1551636.90 tokens/sec
Step 1350 | loss: 3.882577 | lr:5.9841e-04 | norm 0.3591 | dt 338.92ms | 1546938.98 tokens/sec
Step 1351 | loss: 3.889607 | lr:5.9840e-04 | norm 0.3421 | dt 338.77ms | 1547630.30 tokens/sec
Step 1352 | loss: 3.902037 | lr:5.9840e-04 | norm 0.3777 | dt 337.75ms | 1552283.12 tokens/sec
Step 1353 | loss: 3.919225 | lr:5.9839e-04 | norm 0.4154 | dt 338.10ms | 1550672.94 tokens/sec
Step 1354 | loss: 3.867316 | lr:5.9839e-04 | norm 0.4150 | dt 337.97ms | 1551278.97 tokens/sec
Step 1355 | loss: 3.924613 | lr:5.9838e-04 | norm 0.4251 | dt 338.27ms | 1549898.05 tokens/sec
Step 1356 | loss: 3.871562 | lr:5.9838e-04 | norm 0.4765 | dt 338.71ms | 1547876.50 tokens/sec
Step 1357 | loss: 3.874924 | lr:5.9837e-04 | norm 0.5008 | dt 338.35ms | 1549532.19 tokens/sec
Step 1358 | loss: 3.946631 | lr:5.9837e-04 | norm 0.4166 | dt 338.50ms | 1548875.16 tokens/sec
Step 1359 | loss: 3.856674 | lr:5.9836e-04 | norm 0.3498 | dt 339.14ms | 1545955.85 tokens/sec
Step 1360 | loss: 3.955122 | lr:5.9836e-04 | norm 0.3449 | dt 337.95ms | 1551370.90 tokens/sec
Step 1361 | loss: 3.870283 | lr:5.9835e-04 | norm 0.3332 | dt 337.61ms | 1552927.69 tokens/sec
Step 1362 | loss: 3.868214 | lr:5.9835e-04 | norm 0.3506 | dt 338.12ms | 1550607.34 tokens/sec
Step 1363 | loss: 3.896496 | lr:5.9834e-04 | norm 0.3460 | dt 337.88ms | 1551700.40 tokens/sec
Step 1364 | loss: 3.889333 | lr:5.9834e-04 | norm 0.3270 | dt 338.51ms | 1548819.53 tokens/sec
Step 1365 | loss: 3.840687 | lr:5.9833e-04 | norm 0.2900 | dt 337.81ms | 1552028.95 tokens/sec
Step 1366 | loss: 3.840880 | lr:5.9833e-04 | norm 0.3248 | dt 337.46ms | 1553627.68 tokens/sec
Step 1367 | loss: 3.809847 | lr:5.9832e-04 | norm 0.3373 | dt 338.43ms | 1549171.96 tokens/sec
Step 1368 | loss: 3.798881 | lr:5.9832e-04 | norm 0.3467 | dt 338.03ms | 1551021.84 tokens/sec
Step 1369 | loss: 3.824464 | lr:5.9831e-04 | norm 0.3716 | dt 338.31ms | 1549737.49 tokens/sec
Step 1370 | loss: 3.797935 | lr:5.9831e-04 | norm 0.4655 | dt 337.51ms | 1553395.01 tokens/sec
Step 1371 | loss: 3.806347 | lr:5.9830e-04 | norm 0.4970 | dt 338.76ms | 1547670.60 tokens/sec
Step 1372 | loss: 3.800762 | lr:5.9830e-04 | norm 0.4630 | dt 338.05ms | 1550936.52 tokens/sec
Step 1373 | loss: 3.803365 | lr:5.9829e-04 | norm 0.4329 | dt 338.94ms | 1546853.01 tokens/sec
Step 1374 | loss: 3.811707 | lr:5.9828e-04 | norm 0.3818 | dt 338.74ms | 1547758.84 tokens/sec
Step 1375 | loss: 3.854329 | lr:5.9828e-04 | norm 0.3602 | dt 338.76ms | 1547682.59 tokens/sec
Step 1376 | loss: 3.796642 | lr:5.9827e-04 | norm 0.3629 | dt 337.90ms | 1551615.00 tokens/sec
Step 1377 | loss: 3.819930 | lr:5.9827e-04 | norm 0.3396 | dt 338.80ms | 1547465.85 tokens/sec
Step 1378 | loss: 3.767511 | lr:5.9826e-04 | norm 0.3226 | dt 338.50ms | 1548876.25 tokens/sec
Step 1379 | loss: 3.818341 | lr:5.9826e-04 | norm 0.3121 | dt 339.07ms | 1546271.10 tokens/sec
Step 1380 | loss: 3.711915 | lr:5.9825e-04 | norm 0.3384 | dt 338.46ms | 1549055.19 tokens/sec
Step 1381 | loss: 3.795132 | lr:5.9825e-04 | norm 0.3579 | dt 338.25ms | 1549985.45 tokens/sec
Step 1382 | loss: 3.763786 | lr:5.9824e-04 | norm 0.3682 | dt 338.37ms | 1549453.58 tokens/sec
Step 1383 | loss: 3.795924 | lr:5.9824e-04 | norm 0.3902 | dt 338.47ms | 1549012.63 tokens/sec
Step 1384 | loss: 3.739102 | lr:5.9823e-04 | norm 0.4427 | dt 338.61ms | 1548361.50 tokens/sec
Step 1385 | loss: 3.723869 | lr:5.9823e-04 | norm 0.4196 | dt 338.62ms | 1548318.98 tokens/sec
Step 1386 | loss: 3.753641 | lr:5.9822e-04 | norm 0.3510 | dt 338.26ms | 1549943.93 tokens/sec
Step 1387 | loss: 3.763398 | lr:5.9822e-04 | norm 0.3269 | dt 338.17ms | 1550361.36 tokens/sec
Step 1388 | loss: 3.782859 | lr:5.9821e-04 | norm 0.3457 | dt 337.24ms | 1554661.25 tokens/sec
Step 1389 | loss: 3.739795 | lr:5.9821e-04 | norm 0.3813 | dt 338.49ms | 1548900.25 tokens/sec
Step 1390 | loss: 3.903292 | lr:5.9820e-04 | norm 0.4282 | dt 338.35ms | 1549525.64 tokens/sec
Step 1391 | loss: 3.829689 | lr:5.9820e-04 | norm 0.5798 | dt 338.26ms | 1549940.66 tokens/sec
Step 1392 | loss: 3.907120 | lr:5.9819e-04 | norm 0.5757 | dt 337.59ms | 1553046.14 tokens/sec
Step 1393 | loss: 3.888922 | lr:5.9818e-04 | norm 0.5310 | dt 337.76ms | 1552267.78 tokens/sec
Step 1394 | loss: 3.951238 | lr:5.9818e-04 | norm 0.5008 | dt 338.12ms | 1550575.63 tokens/sec
Step 1395 | loss: 3.964304 | lr:5.9817e-04 | norm 0.4147 | dt 337.90ms | 1551606.24 tokens/sec
Step 1396 | loss: 3.911949 | lr:5.9817e-04 | norm 0.4805 | dt 338.00ms | 1551157.51 tokens/sec
Step 1397 | loss: 3.915635 | lr:5.9816e-04 | norm 0.4763 | dt 337.89ms | 1551665.36 tokens/sec
Step 1398 | loss: 3.854767 | lr:5.9816e-04 | norm 0.4178 | dt 337.14ms | 1555096.62 tokens/sec
Step 1399 | loss: 3.820334 | lr:5.9815e-04 | norm 0.3495 | dt 338.16ms | 1550432.41 tokens/sec
Step 1400 | loss: 3.818449 | lr:5.9815e-04 | norm 0.3448 | dt 338.27ms | 1549902.42 tokens/sec
Step 1401 | loss: 3.857494 | lr:5.9814e-04 | norm 0.3147 | dt 337.39ms | 1553930.69 tokens/sec
Step 1402 | loss: 3.885596 | lr:5.9814e-04 | norm 0.3330 | dt 337.53ms | 1553309.42 tokens/sec
Step 1403 | loss: 3.828452 | lr:5.9813e-04 | norm 0.2993 | dt 338.26ms | 1549975.62 tokens/sec
Step 1404 | loss: 3.860379 | lr:5.9813e-04 | norm 0.3063 | dt 338.52ms | 1548762.80 tokens/sec
Step 1405 | loss: 3.891501 | lr:5.9812e-04 | norm 0.3289 | dt 338.14ms | 1550488.17 tokens/sec
Step 1406 | loss: 3.827928 | lr:5.9811e-04 | norm 0.3265 | dt 337.46ms | 1553622.19 tokens/sec
Step 1407 | loss: 3.787489 | lr:5.9811e-04 | norm 0.3075 | dt 337.92ms | 1551496.77 tokens/sec
Step 1408 | loss: 3.832049 | lr:5.9810e-04 | norm 0.3200 | dt 338.53ms | 1548699.54 tokens/sec
Step 1409 | loss: 3.823645 | lr:5.9810e-04 | norm 0.3288 | dt 337.10ms | 1555285.80 tokens/sec
Step 1410 | loss: 3.850493 | lr:5.9809e-04 | norm 0.3387 | dt 337.31ms | 1554339.28 tokens/sec
Step 1411 | loss: 3.829199 | lr:5.9809e-04 | norm 0.3642 | dt 337.61ms | 1552923.30 tokens/sec
Step 1412 | loss: 3.860873 | lr:5.9808e-04 | norm 0.3496 | dt 338.39ms | 1549370.61 tokens/sec
Step 1413 | loss: 3.774770 | lr:5.9808e-04 | norm 0.3051 | dt 337.36ms | 1554097.61 tokens/sec
Step 1414 | loss: 3.788878 | lr:5.9807e-04 | norm 0.3445 | dt 338.29ms | 1549808.48 tokens/sec
Step 1415 | loss: 3.798396 | lr:5.9807e-04 | norm 0.3499 | dt 337.67ms | 1552657.96 tokens/sec
Step 1416 | loss: 3.777829 | lr:5.9806e-04 | norm 0.3497 | dt 337.92ms | 1551497.86 tokens/sec
Step 1417 | loss: 3.731066 | lr:5.9805e-04 | norm 0.3772 | dt 338.62ms | 1548324.43 tokens/sec
Step 1418 | loss: 3.757168 | lr:5.9805e-04 | norm 0.3915 | dt 338.27ms | 1549926.46 tokens/sec
Step 1419 | loss: 3.783661 | lr:5.9804e-04 | norm 0.4550 | dt 338.50ms | 1548841.34 tokens/sec
Step 1420 | loss: 3.757207 | lr:5.9804e-04 | norm 0.4692 | dt 338.50ms | 1548863.16 tokens/sec
Step 1421 | loss: 3.787404 | lr:5.9803e-04 | norm 0.4680 | dt 338.11ms | 1550633.58 tokens/sec
Step 1422 | loss: 3.717868 | lr:5.9803e-04 | norm 0.4071 | dt 338.48ms | 1548955.90 tokens/sec
Step 1423 | loss: 3.781172 | lr:5.9802e-04 | norm 0.3806 | dt 338.31ms | 1549708.00 tokens/sec
Step 1424 | loss: 3.749594 | lr:5.9802e-04 | norm 0.3756 | dt 337.21ms | 1554794.26 tokens/sec
Step 1425 | loss: 3.776413 | lr:5.9801e-04 | norm 0.3640 | dt 338.59ms | 1548438.91 tokens/sec
Step 1426 | loss: 3.777681 | lr:5.9800e-04 | norm 0.3839 | dt 338.87ms | 1547180.60 tokens/sec
Step 1427 | loss: 3.719097 | lr:5.9800e-04 | norm 0.4079 | dt 338.71ms | 1547884.13 tokens/sec
Step 1428 | loss: 3.715900 | lr:5.9799e-04 | norm 0.4184 | dt 337.93ms | 1551459.55 tokens/sec
Step 1429 | loss: 3.805357 | lr:5.9799e-04 | norm 0.4228 | dt 338.55ms | 1548611.20 tokens/sec
Step 1430 | loss: 3.759523 | lr:5.9798e-04 | norm 0.3758 | dt 338.76ms | 1547660.80 tokens/sec
Step 1431 | loss: 3.740984 | lr:5.9798e-04 | norm 0.3637 | dt 337.58ms | 1553070.27 tokens/sec
Step 1432 | loss: 3.771407 | lr:5.9797e-04 | norm 0.3449 | dt 337.43ms | 1553767.09 tokens/sec
Step 1433 | loss: 3.727162 | lr:5.9796e-04 | norm 0.3293 | dt 339.28ms | 1545283.39 tokens/sec
Step 1434 | loss: 3.702539 | lr:5.9796e-04 | norm 0.3229 | dt 338.68ms | 1548022.51 tokens/sec
Step 1435 | loss: 3.696079 | lr:5.9795e-04 | norm 0.2923 | dt 339.17ms | 1545812.40 tokens/sec
Step 1436 | loss: 3.841834 | lr:5.9795e-04 | norm 0.3257 | dt 338.84ms | 1547287.28 tokens/sec
Step 1437 | loss: 3.878641 | lr:5.9794e-04 | norm 0.3507 | dt 337.65ms | 1552766.50 tokens/sec
Step 1438 | loss: 3.874492 | lr:5.9794e-04 | norm 0.3578 | dt 338.10ms | 1550675.13 tokens/sec
Step 1439 | loss: 3.815763 | lr:5.9793e-04 | norm 0.3519 | dt 338.28ms | 1549845.62 tokens/sec
Step 1440 | loss: 3.820767 | lr:5.9792e-04 | norm 0.3756 | dt 337.55ms | 1553215.07 tokens/sec
Step 1441 | loss: 3.856037 | lr:5.9792e-04 | norm 0.4129 | dt 338.31ms | 1549740.77 tokens/sec
Step 1442 | loss: 3.825585 | lr:5.9791e-04 | norm 0.3888 | dt 337.79ms | 1552113.30 tokens/sec
Step 1443 | loss: 3.856533 | lr:5.9791e-04 | norm 0.3532 | dt 337.81ms | 1552039.90 tokens/sec
Step 1444 | loss: 3.830350 | lr:5.9790e-04 | norm 0.3304 | dt 337.91ms | 1551558.07 tokens/sec
Step 1445 | loss: 3.868506 | lr:5.9790e-04 | norm 0.3202 | dt 338.08ms | 1550766.99 tokens/sec
Step 1446 | loss: 3.838271 | lr:5.9789e-04 | norm 0.3406 | dt 337.88ms | 1551705.87 tokens/sec
Step 1447 | loss: 3.868321 | lr:5.9788e-04 | norm 0.3762 | dt 338.33ms | 1549614.08 tokens/sec
Step 1448 | loss: 3.830389 | lr:5.9788e-04 | norm 0.3905 | dt 338.25ms | 1550000.74 tokens/sec
Step 1449 | loss: 3.867033 | lr:5.9787e-04 | norm 0.3699 | dt 337.68ms | 1552609.72 tokens/sec
Step 1450 | loss: 3.819972 | lr:5.9787e-04 | norm 0.3367 | dt 340.03ms | 1541900.66 tokens/sec
Step 1451 | loss: 3.868670 | lr:5.9786e-04 | norm 0.3713 | dt 338.20ms | 1550248.79 tokens/sec
Step 1452 | loss: 3.836164 | lr:5.9786e-04 | norm 0.4117 | dt 338.11ms | 1550648.89 tokens/sec
Step 1453 | loss: 3.863717 | lr:5.9785e-04 | norm 0.4540 | dt 337.93ms | 1551462.84 tokens/sec
Step 1454 | loss: 3.808043 | lr:5.9784e-04 | norm 0.4536 | dt 337.48ms | 1553537.67 tokens/sec
Step 1455 | loss: 3.856694 | lr:5.9784e-04 | norm 0.3869 | dt 338.10ms | 1550667.47 tokens/sec
Step 1456 | loss: 3.802798 | lr:5.9783e-04 | norm 0.4237 | dt 337.93ms | 1551489.11 tokens/sec
Step 1457 | loss: 3.840192 | lr:5.9783e-04 | norm 0.4104 | dt 337.32ms | 1554271.17 tokens/sec
Step 1458 | loss: 3.876476 | lr:5.9782e-04 | norm 0.3903 | dt 338.95ms | 1546794.26 tokens/sec
Step 1459 | loss: 3.837074 | lr:5.9781e-04 | norm 0.4086 | dt 338.05ms | 1550932.14 tokens/sec
Step 1460 | loss: 3.756074 | lr:5.9781e-04 | norm 0.3970 | dt 338.06ms | 1550863.23 tokens/sec
Step 1461 | loss: 3.810664 | lr:5.9780e-04 | norm 0.3819 | dt 337.82ms | 1551984.04 tokens/sec
Step 1462 | loss: 3.778506 | lr:5.9780e-04 | norm 0.3469 | dt 337.65ms | 1552771.98 tokens/sec
Step 1463 | loss: 3.777293 | lr:5.9779e-04 | norm 0.3311 | dt 337.78ms | 1552164.79 tokens/sec
Step 1464 | loss: 3.838591 | lr:5.9779e-04 | norm 0.3248 | dt 337.25ms | 1554596.41 tokens/sec
Step 1465 | loss: 3.742497 | lr:5.9778e-04 | norm 0.3312 | dt 337.90ms | 1551613.91 tokens/sec
Step 1466 | loss: 3.766156 | lr:5.9777e-04 | norm 0.3217 | dt 337.92ms | 1551508.81 tokens/sec
Step 1467 | loss: 3.797813 | lr:5.9777e-04 | norm 0.3205 | dt 337.70ms | 1552546.15 tokens/sec
Step 1468 | loss: 3.766999 | lr:5.9776e-04 | norm 0.3506 | dt 337.59ms | 1553048.33 tokens/sec
Step 1469 | loss: 3.742778 | lr:5.9776e-04 | norm 0.3608 | dt 338.20ms | 1550226.93 tokens/sec
Step 1470 | loss: 3.720158 | lr:5.9775e-04 | norm 0.3506 | dt 338.01ms | 1551080.92 tokens/sec
Step 1471 | loss: 3.633292 | lr:5.9774e-04 | norm 0.3594 | dt 338.14ms | 1550528.62 tokens/sec
Step 1472 | loss: 3.722121 | lr:5.9774e-04 | norm 0.3584 | dt 337.48ms | 1553553.04 tokens/sec
Step 1473 | loss: 3.731836 | lr:5.9773e-04 | norm 0.4027 | dt 338.22ms | 1550129.67 tokens/sec
Step 1474 | loss: 3.687506 | lr:5.9773e-04 | norm 0.3662 | dt 339.23ms | 1545521.24 tokens/sec
Step 1475 | loss: 3.720906 | lr:5.9772e-04 | norm 0.3297 | dt 337.74ms | 1552322.57 tokens/sec
Step 1476 | loss: 3.834496 | lr:5.9771e-04 | norm 0.3430 | dt 338.34ms | 1549608.62 tokens/sec
Step 1477 | loss: 3.745120 | lr:5.9771e-04 | norm 0.3740 | dt 337.68ms | 1552626.17 tokens/sec
Step 1478 | loss: 3.765369 | lr:5.9770e-04 | norm 0.3705 | dt 338.24ms | 1550029.15 tokens/sec
Step 1479 | loss: 3.696303 | lr:5.9770e-04 | norm 0.3226 | dt 338.02ms | 1551045.91 tokens/sec
Step 1480 | loss: 3.693167 | lr:5.9769e-04 | norm 0.3120 | dt 337.95ms | 1551377.46 tokens/sec
Step 1481 | loss: 3.774324 | lr:5.9768e-04 | norm 0.3367 | dt 338.07ms | 1550817.30 tokens/sec
Step 1482 | loss: 3.715921 | lr:5.9768e-04 | norm 0.3868 | dt 337.45ms | 1553654.02 tokens/sec
Step 1483 | loss: 3.832777 | lr:5.9767e-04 | norm 0.4110 | dt 338.10ms | 1550683.88 tokens/sec
Step 1484 | loss: 3.835068 | lr:5.9767e-04 | norm 0.3965 | dt 338.99ms | 1546597.35 tokens/sec
Step 1485 | loss: 3.828393 | lr:5.9766e-04 | norm 0.3930 | dt 337.70ms | 1552508.88 tokens/sec
Step 1486 | loss: 3.867891 | lr:5.9765e-04 | norm 0.3987 | dt 337.77ms | 1552185.60 tokens/sec
Step 1487 | loss: 3.820994 | lr:5.9765e-04 | norm 0.4059 | dt 338.70ms | 1547935.34 tokens/sec
Step 1488 | loss: 3.818120 | lr:5.9764e-04 | norm 0.4038 | dt 339.02ms | 1546479.88 tokens/sec
Step 1489 | loss: 3.844593 | lr:5.9764e-04 | norm 0.4145 | dt 337.83ms | 1551917.23 tokens/sec
Step 1490 | loss: 3.855445 | lr:5.9763e-04 | norm 0.3730 | dt 338.46ms | 1549030.09 tokens/sec
Step 1491 | loss: 3.828224 | lr:5.9762e-04 | norm 0.3475 | dt 338.13ms | 1550538.46 tokens/sec
Step 1492 | loss: 3.764674 | lr:5.9762e-04 | norm 0.3333 | dt 338.43ms | 1549174.14 tokens/sec
Step 1493 | loss: 3.801295 | lr:5.9761e-04 | norm 0.3687 | dt 338.20ms | 1550235.67 tokens/sec
Step 1494 | loss: 3.835292 | lr:5.9760e-04 | norm 0.3918 | dt 337.96ms | 1551336.97 tokens/sec
Step 1495 | loss: 3.887085 | lr:5.9760e-04 | norm 0.3692 | dt 339.26ms | 1545387.64 tokens/sec
Step 1496 | loss: 3.841182 | lr:5.9759e-04 | norm 0.3633 | dt 337.81ms | 1552023.47 tokens/sec
Step 1497 | loss: 3.853386 | lr:5.9759e-04 | norm 0.3608 | dt 338.61ms | 1548373.49 tokens/sec
Step 1498 | loss: 3.806237 | lr:5.9758e-04 | norm 0.3772 | dt 338.25ms | 1549982.17 tokens/sec
Step 1499 | loss: 3.827140 | lr:5.9757e-04 | norm 0.3540 | dt 338.43ms | 1549198.15 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 1500: 3.8169
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2571/10042=0.2560


ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, though, a lot of people talk (like "meshah" and "meshah-meshah"
rank 1 sample 1 >Hello, I'm a language model, so you have to work with. If you're in need of two languages, then for the average language, you're
rank 1 sample 2 >Hello, I'm a language model, which comes at a time when I was a kid. The child was very important, and it worked for a couple of
rank 1 sample 3 >Hello, I'm a language model, and I'm also an assistant, and most of the times I would want to talk together:
- an adverb




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm a professional computer programmer. Like it, I'm a real programmer whose knowledge is still evolving, and so
rank 7 sample 1 >Hello, I'm a language model, learning model.
This method allows many reasons to add their own definitions to the dictionary. In this blog post we explore
rank 7 sample 2 >Hello, I'm a language model, and I think "he" does sound very good.
The best-known language is the "C++ programming language
rank 7 sample 3 >Hello, I'm a language model, created from the ILL to the University of Arkansas.
From my comments, I was able to help you determine how




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, which was created by Dr. Jim Oler.
- a word-name is a word-name, which can
rank 5 sample 1 >Hello, I'm a language model, is about 20 languages around that, which I do not have, if I am just an interpreter. This program has a
rank 5 sample 2 >Hello, I'm a language model, but it can also exist in one language.<|endoftext|>The National Cancer Society estimates
the lives of 30 million people in the
rank 5 sample 3 >Hello, I'm a language model, just the best, and we'll be going to the top of it. You probably won't believe that you are writing




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, rather the one being a unit of magnitude, but I can’t wait for it to be a “c
rank 2 sample 1 >Hello, I'm a language model, the language project has been developed by people everywhere. Our language models help our students develop a more efficient model.
So
rank 2 sample 2 >Hello, I'm a language model, but I have a little bit of them, because they are in their natural form, and because those properties are too large
rank 2 sample 3 >Hello, I'm a language model, which we wanted to learn.
- All the more
"I'm also a teacher". C++
"If




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I do not know any of them but the author is an expert on the development of learning. You will have a
rank 4 sample 1 >Hello, I'm a language model, really it's interesting!
A typical storyteller might have played, or had written it, so you understand the
rank 4 sample 2 >Hello, I'm a language model, I always like and a style model (which is a set of parameters).
Bibliographic InfoThis is what you
rank 4 sample 3 >Hello, I'm a language model, and a book is a little bit easier when used when performing and listening: a method for a child's language comprehension and




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I was a teacher on how to interact with your fellow. When I came to that perspective as I grew up as
rank 0 sample 1 >Hello, I'm a language model, but also a model for the group system. The function takes the time to get a lot of data over and over again
rank 0 sample 2 >Hello, I'm a language model, but I did have a language model with a couple of words: "You're in a language model, you know to
rank 0 sample 3 >Hello, I'm a language model, and some of the other things that are of equal importance to the students. The models and teachers have many different tools or




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not just going to share your ideas, but as I explain everything in one.
To create an effect
rank 3 sample 1 >Hello, I'm a language model, and you know what it is (maybe you should tell it to be a language model).
If you're trying to


ddp_rank 6: ####### Printing generated samples ####### 

rank 3 sample 2 >Hello, I'm a language model, and it looks like a human who is capable of breaking down his or her head.
It starts to be more powerful
rank 3 sample 3 >Hello, I'm a language model, and our language learning partner has their own unique set of learning styles which help us differentiate. Our language educators are interested in


rank 6 sample 0 >Hello, I'm a language model,
- The key for us is: The key will always
- A key will always have a key
- All
rank 6 sample 1 >Hello, I'm a language model, that would give you a new language to everyone. The language model is similar to that of a speech-word model.
rank 6 sample 2 >Hello, I'm a language model, which you use for that model and its use as a model, a tool you can use for your modeling tool development,
rank 6 sample 3 >Hello, I'm a language model, and my only bet can be made of that model.
I know that you have no computers, where, it are


Step 1500 | loss: 3.792839 | lr:5.9757e-04 | norm 0.3144 | dt 18830.65ms | 27842.26 tokens/sec
Step 1501 | loss: 3.800320 | lr:5.9756e-04 | norm 0.3081 | dt 333.56ms | 1571813.20 tokens/sec
Step 1502 | loss: 3.809723 | lr:5.9756e-04 | norm 0.3379 | dt 337.18ms | 1554928.38 tokens/sec
Step 1503 | loss: 3.820682 | lr:5.9755e-04 | norm 0.3454 | dt 337.19ms | 1554859.12 tokens/sec
Step 1504 | loss: 3.769403 | lr:5.9754e-04 | norm 0.3705 | dt 336.74ms | 1556971.72 tokens/sec
Step 1505 | loss: 3.776674 | lr:5.9754e-04 | norm 0.3584 | dt 336.48ms | 1558135.60 tokens/sec
Step 1506 | loss: 3.746885 | lr:5.9753e-04 | norm 0.3648 | dt 337.53ms | 1553314.91 tokens/sec
Step 1507 | loss: 3.749434 | lr:5.9752e-04 | norm 0.3335 | dt 336.22ms | 1559358.72 tokens/sec
Step 1508 | loss: 3.762578 | lr:5.9752e-04 | norm 0.2858 | dt 336.47ms | 1558191.90 tokens/sec
Step 1509 | loss: 3.749915 | lr:5.9751e-04 | norm 0.3213 | dt 337.75ms | 1552301.75 tokens/sec
Step 1510 | loss: 3.724035 | lr:5.9751e-04 | norm 0.3827 | dt 337.99ms | 1551216.59 tokens/sec
Step 1511 | loss: 3.743735 | lr:5.9750e-04 | norm 0.4221 | dt 1008.70ms | 519765.72 tokens/sec
Step 1512 | loss: 3.761261 | lr:5.9749e-04 | norm 0.3742 | dt 335.35ms | 1563406.34 tokens/sec
Step 1513 | loss: 3.741719 | lr:5.9749e-04 | norm 0.3477 | dt 337.91ms | 1551549.31 tokens/sec
Step 1514 | loss: 3.703548 | lr:5.9748e-04 | norm 0.3176 | dt 338.03ms | 1550998.87 tokens/sec
Step 1515 | loss: 3.734318 | lr:5.9747e-04 | norm 0.3128 | dt 336.78ms | 1556752.37 tokens/sec
Step 1516 | loss: 3.681965 | lr:5.9747e-04 | norm 0.3308 | dt 337.91ms | 1551578.87 tokens/sec
Step 1517 | loss: 3.781479 | lr:5.9746e-04 | norm 0.3705 | dt 336.80ms | 1556684.05 tokens/sec
Step 1518 | loss: 3.668669 | lr:5.9745e-04 | norm 0.3938 | dt 336.89ms | 1556254.40 tokens/sec
Step 1519 | loss: 3.670342 | lr:5.9745e-04 | norm 0.3441 | dt 1042.56ms | 502884.81 tokens/sec
Step 1520 | loss: 3.653053 | lr:5.9744e-04 | norm 0.3318 | dt 336.18ms | 1559543.40 tokens/sec
Step 1521 | loss: 3.762562 | lr:5.9744e-04 | norm 0.3466 | dt 337.01ms | 1555702.81 tokens/sec
Step 1522 | loss: 3.705316 | lr:5.9743e-04 | norm 0.3566 | dt 338.56ms | 1548566.49 tokens/sec
Step 1523 | loss: 3.671697 | lr:5.9742e-04 | norm 0.3564 | dt 336.28ms | 1559060.22 tokens/sec
Step 1524 | loss: 3.723197 | lr:5.9742e-04 | norm 0.3824 | dt 336.92ms | 1556129.95 tokens/sec
Step 1525 | loss: 3.732109 | lr:5.9741e-04 | norm 0.3772 | dt 337.97ms | 1551264.74 tokens/sec
Step 1526 | loss: 3.683437 | lr:5.9740e-04 | norm 0.3043 | dt 337.59ms | 1553030.78 tokens/sec
Step 1527 | loss: 3.680382 | lr:5.9740e-04 | norm 0.3475 | dt 337.89ms | 1551663.17 tokens/sec
Step 1528 | loss: 3.728825 | lr:5.9739e-04 | norm 0.3854 | dt 338.03ms | 1551005.43 tokens/sec
Step 1529 | loss: 3.848943 | lr:5.9738e-04 | norm 0.3738 | dt 337.36ms | 1554100.91 tokens/sec
Step 1530 | loss: 3.808755 | lr:5.9738e-04 | norm 0.4196 | dt 339.31ms | 1545174.81 tokens/sec
Step 1531 | loss: 3.885228 | lr:5.9737e-04 | norm 0.4366 | dt 338.29ms | 1549823.77 tokens/sec
Step 1532 | loss: 3.805173 | lr:5.9737e-04 | norm 0.4410 | dt 337.61ms | 1552928.79 tokens/sec
Step 1533 | loss: 3.829077 | lr:5.9736e-04 | norm 0.4577 | dt 338.10ms | 1550677.32 tokens/sec
Step 1534 | loss: 3.805623 | lr:5.9735e-04 | norm 0.3758 | dt 338.35ms | 1549530.01 tokens/sec
Step 1535 | loss: 3.853517 | lr:5.9735e-04 | norm 0.3183 | dt 339.21ms | 1545592.93 tokens/sec
Step 1536 | loss: 3.795831 | lr:5.9734e-04 | norm 0.3411 | dt 338.46ms | 1549021.36 tokens/sec
Step 1537 | loss: 3.770542 | lr:5.9733e-04 | norm 0.3224 | dt 337.74ms | 1552347.77 tokens/sec
Step 1538 | loss: 3.783934 | lr:5.9733e-04 | norm 0.3284 | dt 338.85ms | 1547244.83 tokens/sec
Step 1539 | loss: 3.824043 | lr:5.9732e-04 | norm 0.3315 | dt 338.02ms | 1551033.87 tokens/sec
Step 1540 | loss: 3.854172 | lr:5.9731e-04 | norm 0.3364 | dt 337.94ms | 1551436.57 tokens/sec
Step 1541 | loss: 3.824926 | lr:5.9731e-04 | norm 0.3346 | dt 339.38ms | 1544858.92 tokens/sec
Step 1542 | loss: 3.809123 | lr:5.9730e-04 | norm 0.3644 | dt 338.99ms | 1546610.40 tokens/sec
Step 1543 | loss: 3.774149 | lr:5.9729e-04 | norm 0.3760 | dt 337.92ms | 1551523.04 tokens/sec
Step 1544 | loss: 3.811222 | lr:5.9729e-04 | norm 0.3623 | dt 337.21ms | 1554774.47 tokens/sec
Step 1545 | loss: 3.819976 | lr:5.9728e-04 | norm 0.3803 | dt 338.65ms | 1548167.46 tokens/sec
Step 1546 | loss: 3.797651 | lr:5.9727e-04 | norm 0.3592 | dt 337.68ms | 1552620.69 tokens/sec
Step 1547 | loss: 3.775001 | lr:5.9727e-04 | norm 0.3557 | dt 337.62ms | 1552886.02 tokens/sec
Step 1548 | loss: 3.775088 | lr:5.9726e-04 | norm 0.3554 | dt 338.23ms | 1550112.19 tokens/sec
Step 1549 | loss: 3.833862 | lr:5.9725e-04 | norm 0.3613 | dt 338.37ms | 1549468.86 tokens/sec
Step 1550 | loss: 3.786719 | lr:5.9725e-04 | norm 0.4101 | dt 337.97ms | 1551270.21 tokens/sec
Step 1551 | loss: 3.744231 | lr:5.9724e-04 | norm 0.4030 | dt 337.93ms | 1551463.93 tokens/sec
Step 1552 | loss: 3.723767 | lr:5.9724e-04 | norm 0.3528 | dt 337.43ms | 1553769.29 tokens/sec
Step 1553 | loss: 3.744364 | lr:5.9723e-04 | norm 0.3764 | dt 338.38ms | 1549409.91 tokens/sec
Step 1554 | loss: 3.716227 | lr:5.9722e-04 | norm 0.4027 | dt 338.31ms | 1549741.86 tokens/sec
Step 1555 | loss: 3.725506 | lr:5.9722e-04 | norm 0.3592 | dt 337.49ms | 1553482.80 tokens/sec
Step 1556 | loss: 3.675752 | lr:5.9721e-04 | norm 0.3123 | dt 337.30ms | 1554386.52 tokens/sec
Step 1557 | loss: 3.711223 | lr:5.9720e-04 | norm 0.3260 | dt 339.11ms | 1546069.98 tokens/sec
Step 1558 | loss: 3.708326 | lr:5.9720e-04 | norm 0.3326 | dt 338.44ms | 1549113.02 tokens/sec
Step 1559 | loss: 3.765893 | lr:5.9719e-04 | norm 0.3178 | dt 338.09ms | 1550750.58 tokens/sec
Step 1560 | loss: 3.670689 | lr:5.9718e-04 | norm 0.2991 | dt 337.37ms | 1554035.01 tokens/sec
Step 1561 | loss: 3.702059 | lr:5.9718e-04 | norm 0.3113 | dt 337.47ms | 1553589.26 tokens/sec
Step 1562 | loss: 3.745486 | lr:5.9717e-04 | norm 0.3139 | dt 338.43ms | 1549174.14 tokens/sec
Step 1563 | loss: 3.719274 | lr:5.9716e-04 | norm 0.3208 | dt 337.55ms | 1553221.65 tokens/sec
Step 1564 | loss: 3.611378 | lr:5.9716e-04 | norm 0.3551 | dt 337.14ms | 1555098.82 tokens/sec
Step 1565 | loss: 3.645114 | lr:5.9715e-04 | norm 0.3456 | dt 338.65ms | 1548188.17 tokens/sec
Step 1566 | loss: 3.689743 | lr:5.9714e-04 | norm 0.3331 | dt 339.00ms | 1546564.72 tokens/sec
Step 1567 | loss: 3.668461 | lr:5.9714e-04 | norm 0.3768 | dt 337.30ms | 1554345.87 tokens/sec
Step 1568 | loss: 3.688319 | lr:5.9713e-04 | norm 0.4049 | dt 338.20ms | 1550212.72 tokens/sec
Step 1569 | loss: 3.666007 | lr:5.9712e-04 | norm 0.3753 | dt 337.52ms | 1553376.35 tokens/sec
Step 1570 | loss: 3.657128 | lr:5.9712e-04 | norm 0.3853 | dt 338.32ms | 1549702.54 tokens/sec
Step 1571 | loss: 3.736091 | lr:5.9711e-04 | norm 0.4066 | dt 337.42ms | 1553821.98 tokens/sec
Step 1572 | loss: 3.724640 | lr:5.9710e-04 | norm 0.3810 | dt 337.37ms | 1554021.83 tokens/sec
Step 1573 | loss: 3.679357 | lr:5.9709e-04 | norm 0.3534 | dt 339.14ms | 1545934.11 tokens/sec
Step 1574 | loss: 3.689053 | lr:5.9709e-04 | norm 0.3532 | dt 338.65ms | 1548175.09 tokens/sec
Step 1575 | loss: 3.709965 | lr:5.9708e-04 | norm 0.3501 | dt 337.96ms | 1551310.70 tokens/sec
Step 1576 | loss: 3.820469 | lr:5.9707e-04 | norm 0.3326 | dt 338.56ms | 1548563.21 tokens/sec
Step 1577 | loss: 3.776525 | lr:5.9707e-04 | norm 0.3400 | dt 338.85ms | 1547268.78 tokens/sec
Step 1578 | loss: 3.736568 | lr:5.9706e-04 | norm 0.3692 | dt 338.43ms | 1549167.59 tokens/sec
Step 1579 | loss: 3.883508 | lr:5.9705e-04 | norm 0.4324 | dt 338.57ms | 1548537.04 tokens/sec
Step 1580 | loss: 3.779315 | lr:5.9705e-04 | norm 0.4835 | dt 338.08ms | 1550776.83 tokens/sec
Step 1581 | loss: 3.846551 | lr:5.9704e-04 | norm 0.3805 | dt 337.69ms | 1552592.18 tokens/sec
Step 1582 | loss: 3.766032 | lr:5.9703e-04 | norm 0.3443 | dt 338.11ms | 1550653.26 tokens/sec
Step 1583 | loss: 3.788765 | lr:5.9703e-04 | norm 0.3583 | dt 338.58ms | 1548497.79 tokens/sec
Step 1584 | loss: 3.765548 | lr:5.9702e-04 | norm 0.3768 | dt 337.95ms | 1551364.33 tokens/sec
Step 1585 | loss: 3.815118 | lr:5.9701e-04 | norm 0.3502 | dt 337.63ms | 1552844.35 tokens/sec
Step 1586 | loss: 3.810700 | lr:5.9701e-04 | norm 0.3876 | dt 337.52ms | 1553368.67 tokens/sec
Step 1587 | loss: 3.817526 | lr:5.9700e-04 | norm 0.4472 | dt 338.22ms | 1550124.21 tokens/sec
Step 1588 | loss: 3.796620 | lr:5.9699e-04 | norm 0.5203 | dt 338.74ms | 1547776.27 tokens/sec
Step 1589 | loss: 3.789396 | lr:5.9699e-04 | norm 0.4090 | dt 337.95ms | 1551390.60 tokens/sec
Step 1590 | loss: 3.790412 | lr:5.9698e-04 | norm 0.3156 | dt 338.38ms | 1549394.63 tokens/sec
Step 1591 | loss: 3.799810 | lr:5.9697e-04 | norm 0.3122 | dt 337.97ms | 1551306.33 tokens/sec
Step 1592 | loss: 3.768457 | lr:5.9696e-04 | norm 0.3152 | dt 338.73ms | 1547805.68 tokens/sec
Step 1593 | loss: 3.756587 | lr:5.9696e-04 | norm 0.3436 | dt 338.61ms | 1548339.69 tokens/sec
Step 1594 | loss: 3.794837 | lr:5.9695e-04 | norm 0.3076 | dt 338.09ms | 1550719.96 tokens/sec
Step 1595 | loss: 3.785216 | lr:5.9694e-04 | norm 0.3024 | dt 337.92ms | 1551527.42 tokens/sec
Step 1596 | loss: 3.754739 | lr:5.9694e-04 | norm 0.3193 | dt 338.96ms | 1546748.56 tokens/sec
Step 1597 | loss: 3.728736 | lr:5.9693e-04 | norm 0.3151 | dt 339.19ms | 1545689.62 tokens/sec
Step 1598 | loss: 3.731643 | lr:5.9692e-04 | norm 0.2832 | dt 338.61ms | 1548360.41 tokens/sec
Step 1599 | loss: 3.746861 | lr:5.9692e-04 | norm 0.3144 | dt 340.17ms | 1541260.89 tokens/sec
Step 1600 | loss: 3.773638 | lr:5.9691e-04 | norm 0.3143 | dt 338.21ms | 1550205.07 tokens/sec
Step 1601 | loss: 3.675641 | lr:5.9690e-04 | norm 0.3046 | dt 339.23ms | 1545535.36 tokens/sec
Step 1602 | loss: 3.718785 | lr:5.9690e-04 | norm 0.3358 | dt 339.02ms | 1546477.71 tokens/sec
Step 1603 | loss: 3.680851 | lr:5.9689e-04 | norm 0.3697 | dt 339.08ms | 1546216.73 tokens/sec
Step 1604 | loss: 3.706768 | lr:5.9688e-04 | norm 0.3749 | dt 339.01ms | 1546522.30 tokens/sec
Step 1605 | loss: 3.793416 | lr:5.9687e-04 | norm 0.3512 | dt 338.26ms | 1549939.56 tokens/sec
Step 1606 | loss: 3.720900 | lr:5.9687e-04 | norm 0.3648 | dt 338.24ms | 1550048.82 tokens/sec
Step 1607 | loss: 3.700694 | lr:5.9686e-04 | norm 0.3454 | dt 337.52ms | 1553370.87 tokens/sec
Step 1608 | loss: 3.650827 | lr:5.9685e-04 | norm 0.3111 | dt 338.57ms | 1548515.23 tokens/sec
Step 1609 | loss: 3.728524 | lr:5.9685e-04 | norm 0.3296 | dt 337.76ms | 1552259.01 tokens/sec
Step 1610 | loss: 3.631307 | lr:5.9684e-04 | norm 0.2982 | dt 337.76ms | 1552242.58 tokens/sec
Step 1611 | loss: 3.654154 | lr:5.9683e-04 | norm 0.3239 | dt 337.77ms | 1552188.89 tokens/sec
Step 1612 | loss: 3.619914 | lr:5.9683e-04 | norm 0.3652 | dt 337.60ms | 1552983.62 tokens/sec
Step 1613 | loss: 3.682503 | lr:5.9682e-04 | norm 0.3452 | dt 337.86ms | 1551796.76 tokens/sec
Step 1614 | loss: 3.737026 | lr:5.9681e-04 | norm 0.3742 | dt 337.22ms | 1554716.21 tokens/sec
Step 1615 | loss: 3.661548 | lr:5.9680e-04 | norm 0.3878 | dt 338.54ms | 1548655.91 tokens/sec
Step 1616 | loss: 3.638622 | lr:5.9680e-04 | norm 0.3679 | dt 338.50ms | 1548865.34 tokens/sec
Step 1617 | loss: 3.648485 | lr:5.9679e-04 | norm 0.3865 | dt 337.40ms | 1553910.92 tokens/sec
Step 1618 | loss: 3.612142 | lr:5.9678e-04 | norm 0.3415 | dt 336.97ms | 1555898.74 tokens/sec
Step 1619 | loss: 3.702250 | lr:5.9678e-04 | norm 0.3241 | dt 338.09ms | 1550722.15 tokens/sec
Step 1620 | loss: 3.601947 | lr:5.9677e-04 | norm 0.3326 | dt 337.67ms | 1552661.25 tokens/sec
Step 1621 | loss: 3.694248 | lr:5.9676e-04 | norm 0.3246 | dt 337.53ms | 1553295.16 tokens/sec
Step 1622 | loss: 3.797589 | lr:5.9675e-04 | norm 0.3198 | dt 338.47ms | 1548980.99 tokens/sec
Step 1623 | loss: 3.814646 | lr:5.9675e-04 | norm 0.3628 | dt 337.60ms | 1552963.88 tokens/sec
Step 1624 | loss: 3.827075 | lr:5.9674e-04 | norm 0.3609 | dt 337.75ms | 1552280.93 tokens/sec
Step 1625 | loss: 3.904388 | lr:5.9673e-04 | norm 0.4086 | dt 337.99ms | 1551188.14 tokens/sec
Step 1626 | loss: 3.780220 | lr:5.9673e-04 | norm 0.4570 | dt 339.23ms | 1545533.19 tokens/sec
Step 1627 | loss: 3.822500 | lr:5.9672e-04 | norm 0.4398 | dt 338.63ms | 1548275.37 tokens/sec
Step 1628 | loss: 3.808059 | lr:5.9671e-04 | norm 0.3699 | dt 338.18ms | 1550315.46 tokens/sec
Step 1629 | loss: 3.808371 | lr:5.9670e-04 | norm 0.3925 | dt 338.05ms | 1550917.92 tokens/sec
Step 1630 | loss: 3.749749 | lr:5.9670e-04 | norm 0.3627 | dt 337.33ms | 1554220.63 tokens/sec
Step 1631 | loss: 3.708757 | lr:5.9669e-04 | norm 0.3383 | dt 338.05ms | 1550913.55 tokens/sec
Step 1632 | loss: 3.859689 | lr:5.9668e-04 | norm 0.3721 | dt 337.59ms | 1553028.59 tokens/sec
Step 1633 | loss: 3.770233 | lr:5.9668e-04 | norm 0.3897 | dt 337.61ms | 1552958.40 tokens/sec
Step 1634 | loss: 3.732701 | lr:5.9667e-04 | norm 0.4442 | dt 338.67ms | 1548078.09 tokens/sec
Step 1635 | loss: 3.760863 | lr:5.9666e-04 | norm 0.3980 | dt 338.33ms | 1549618.45 tokens/sec
Step 1636 | loss: 3.737244 | lr:5.9665e-04 | norm 0.3393 | dt 337.78ms | 1552175.74 tokens/sec
Step 1637 | loss: 3.743069 | lr:5.9665e-04 | norm 0.3411 | dt 337.66ms | 1552709.49 tokens/sec
Step 1638 | loss: 3.757993 | lr:5.9664e-04 | norm 0.3086 | dt 337.91ms | 1551542.75 tokens/sec
Step 1639 | loss: 3.779655 | lr:5.9663e-04 | norm 0.3076 | dt 338.73ms | 1547786.07 tokens/sec
Step 1640 | loss: 3.687027 | lr:5.9662e-04 | norm 0.3080 | dt 337.46ms | 1553609.02 tokens/sec
Step 1641 | loss: 3.716907 | lr:5.9662e-04 | norm 0.2884 | dt 337.60ms | 1552966.07 tokens/sec
Step 1642 | loss: 3.709877 | lr:5.9661e-04 | norm 0.3202 | dt 338.61ms | 1548351.69 tokens/sec
Step 1643 | loss: 3.703650 | lr:5.9660e-04 | norm 0.3019 | dt 338.14ms | 1550511.12 tokens/sec
Step 1644 | loss: 3.729858 | lr:5.9660e-04 | norm 0.3100 | dt 337.65ms | 1552771.98 tokens/sec
Step 1645 | loss: 3.720062 | lr:5.9659e-04 | norm 0.3122 | dt 338.36ms | 1549502.71 tokens/sec
Step 1646 | loss: 3.706383 | lr:5.9658e-04 | norm 0.3148 | dt 337.59ms | 1553019.82 tokens/sec
Step 1647 | loss: 3.699238 | lr:5.9657e-04 | norm 0.3597 | dt 338.66ms | 1548111.88 tokens/sec
Step 1648 | loss: 3.602327 | lr:5.9657e-04 | norm 0.9377 | dt 338.08ms | 1550771.36 tokens/sec
Step 1649 | loss: 3.734860 | lr:5.9656e-04 | norm 0.5323 | dt 337.38ms | 1553982.30 tokens/sec
Step 1650 | loss: 3.735004 | lr:5.9655e-04 | norm 0.5378 | dt 339.06ms | 1546306.98 tokens/sec
Step 1651 | loss: 3.705206 | lr:5.9654e-04 | norm 0.4962 | dt 338.67ms | 1548061.74 tokens/sec
Step 1652 | loss: 3.684038 | lr:5.9654e-04 | norm 0.4105 | dt 338.86ms | 1547203.46 tokens/sec
Step 1653 | loss: 3.692326 | lr:5.9653e-04 | norm 0.3769 | dt 338.32ms | 1549659.95 tokens/sec
Step 1654 | loss: 3.730595 | lr:5.9652e-04 | norm 0.3426 | dt 339.05ms | 1546362.43 tokens/sec
Step 1655 | loss: 3.769600 | lr:5.9651e-04 | norm 0.3703 | dt 339.45ms | 1544544.25 tokens/sec
Step 1656 | loss: 3.725073 | lr:5.9651e-04 | norm 0.3051 | dt 338.90ms | 1547024.95 tokens/sec
Step 1657 | loss: 3.671432 | lr:5.9650e-04 | norm 0.3461 | dt 341.22ms | 1536522.42 tokens/sec
Step 1658 | loss: 3.694438 | lr:5.9649e-04 | norm 0.3339 | dt 337.43ms | 1553772.58 tokens/sec
Step 1659 | loss: 3.592586 | lr:5.9648e-04 | norm 0.2960 | dt 338.80ms | 1547499.61 tokens/sec
Step 1660 | loss: 3.632833 | lr:5.9648e-04 | norm 0.2884 | dt 337.47ms | 1553569.50 tokens/sec
Step 1661 | loss: 3.717623 | lr:5.9647e-04 | norm 0.3340 | dt 338.56ms | 1548586.12 tokens/sec
Step 1662 | loss: 3.665328 | lr:5.9646e-04 | norm 0.3814 | dt 337.88ms | 1551676.31 tokens/sec
Step 1663 | loss: 3.701701 | lr:5.9645e-04 | norm 0.3295 | dt 338.22ms | 1550134.04 tokens/sec
Step 1664 | loss: 3.680856 | lr:5.9645e-04 | norm 0.3086 | dt 338.85ms | 1547235.03 tokens/sec
Step 1665 | loss: 3.637340 | lr:5.9644e-04 | norm 0.2827 | dt 337.54ms | 1553277.60 tokens/sec
Step 1666 | loss: 3.693866 | lr:5.9643e-04 | norm 0.2889 | dt 337.87ms | 1551726.68 tokens/sec
Step 1667 | loss: 3.686085 | lr:5.9642e-04 | norm 0.3081 | dt 338.61ms | 1548356.05 tokens/sec
Step 1668 | loss: 3.616480 | lr:5.9642e-04 | norm 0.3122 | dt 339.07ms | 1546264.57 tokens/sec
Step 1669 | loss: 3.729461 | lr:5.9641e-04 | norm 0.3558 | dt 339.04ms | 1546374.40 tokens/sec
Step 1670 | loss: 3.835340 | lr:5.9640e-04 | norm 0.4010 | dt 337.55ms | 1553196.42 tokens/sec
Step 1671 | loss: 3.741626 | lr:5.9639e-04 | norm 0.3596 | dt 337.36ms | 1554076.74 tokens/sec
Step 1672 | loss: 3.773990 | lr:5.9639e-04 | norm 0.3300 | dt 338.85ms | 1547269.86 tokens/sec
Step 1673 | loss: 3.816165 | lr:5.9638e-04 | norm 0.3387 | dt 337.74ms | 1552323.66 tokens/sec
Step 1674 | loss: 3.786891 | lr:5.9637e-04 | norm 0.3190 | dt 337.94ms | 1551438.76 tokens/sec
Step 1675 | loss: 3.758601 | lr:5.9636e-04 | norm 0.3088 | dt 338.29ms | 1549839.07 tokens/sec
Step 1676 | loss: 3.811652 | lr:5.9636e-04 | norm 0.3879 | dt 338.39ms | 1549358.60 tokens/sec
Step 1677 | loss: 3.788795 | lr:5.9635e-04 | norm 0.4234 | dt 339.50ms | 1544288.27 tokens/sec
Step 1678 | loss: 3.725005 | lr:5.9634e-04 | norm 0.3994 | dt 338.69ms | 1547998.54 tokens/sec
Step 1679 | loss: 3.711163 | lr:5.9633e-04 | norm 0.3472 | dt 338.89ms | 1547081.55 tokens/sec
Step 1680 | loss: 3.717262 | lr:5.9633e-04 | norm 0.3054 | dt 338.64ms | 1548236.13 tokens/sec
Step 1681 | loss: 3.773046 | lr:5.9632e-04 | norm 0.3101 | dt 338.14ms | 1550485.98 tokens/sec
Step 1682 | loss: 3.790843 | lr:5.9631e-04 | norm 0.3757 | dt 337.95ms | 1551359.95 tokens/sec
Step 1683 | loss: 3.761830 | lr:5.9630e-04 | norm 0.4326 | dt 337.82ms | 1551958.85 tokens/sec
Step 1684 | loss: 3.731807 | lr:5.9630e-04 | norm 0.4052 | dt 337.19ms | 1554870.11 tokens/sec
Step 1685 | loss: 3.790559 | lr:5.9629e-04 | norm 0.3586 | dt 338.39ms | 1549379.34 tokens/sec
Step 1686 | loss: 3.743682 | lr:5.9628e-04 | norm 0.3688 | dt 338.22ms | 1550122.02 tokens/sec
Step 1687 | loss: 3.784505 | lr:5.9627e-04 | norm 0.3879 | dt 337.59ms | 1553019.82 tokens/sec
Step 1688 | loss: 3.785416 | lr:5.9627e-04 | norm 0.3723 | dt 338.30ms | 1549791.01 tokens/sec
Step 1689 | loss: 3.711395 | lr:5.9626e-04 | norm 0.3459 | dt 338.20ms | 1550243.32 tokens/sec
Step 1690 | loss: 3.736394 | lr:5.9625e-04 | norm 0.3200 | dt 337.54ms | 1553252.37 tokens/sec
Step 1691 | loss: 3.725610 | lr:5.9624e-04 | norm 0.3460 | dt 337.74ms | 1552362.02 tokens/sec
Step 1692 | loss: 3.750770 | lr:5.9624e-04 | norm 0.3077 | dt 338.84ms | 1547294.91 tokens/sec
Step 1693 | loss: 3.708903 | lr:5.9623e-04 | norm 0.3031 | dt 337.22ms | 1554732.70 tokens/sec
Step 1694 | loss: 3.677946 | lr:5.9622e-04 | norm 0.3473 | dt 338.67ms | 1548065.01 tokens/sec
Step 1695 | loss: 3.730207 | lr:5.9621e-04 | norm 0.3042 | dt 339.10ms | 1546134.11 tokens/sec
Step 1696 | loss: 3.703838 | lr:5.9620e-04 | norm 0.3013 | dt 337.93ms | 1551465.03 tokens/sec
Step 1697 | loss: 3.666306 | lr:5.9620e-04 | norm 0.3547 | dt 339.34ms | 1545006.54 tokens/sec
Step 1698 | loss: 3.702566 | lr:5.9619e-04 | norm 0.2742 | dt 339.37ms | 1544892.57 tokens/sec
Step 1699 | loss: 3.630548 | lr:5.9618e-04 | norm 0.2768 | dt 337.40ms | 1553902.14 tokens/sec
Step 1700 | loss: 3.697898 | lr:5.9617e-04 | norm 0.2870 | dt 896.89ms | 584562.57 tokens/sec
Step 1701 | loss: 3.635267 | lr:5.9617e-04 | norm 0.2963 | dt 335.14ms | 1564385.08 tokens/sec
Step 1702 | loss: 3.634591 | lr:5.9616e-04 | norm 0.3173 | dt 338.05ms | 1550940.89 tokens/sec
Step 1703 | loss: 3.682346 | lr:5.9615e-04 | norm 0.3758 | dt 337.31ms | 1554322.80 tokens/sec
Step 1704 | loss: 3.649891 | lr:5.9614e-04 | norm 0.3809 | dt 336.66ms | 1557332.28 tokens/sec
Step 1705 | loss: 3.637647 | lr:5.9613e-04 | norm 0.3897 | dt 337.57ms | 1553141.57 tokens/sec
Step 1706 | loss: 3.619470 | lr:5.9613e-04 | norm 0.3895 | dt 337.81ms | 1552003.75 tokens/sec
Step 1707 | loss: 3.641024 | lr:5.9612e-04 | norm 0.3056 | dt 338.96ms | 1546734.42 tokens/sec
Step 1708 | loss: 3.674033 | lr:5.9611e-04 | norm 0.3034 | dt 337.52ms | 1553352.21 tokens/sec
Step 1709 | loss: 3.627035 | lr:5.9610e-04 | norm 0.3166 | dt 942.74ms | 556129.90 tokens/sec
Step 1710 | loss: 3.645461 | lr:5.9610e-04 | norm 0.3151 | dt 337.07ms | 1555429.91 tokens/sec
Step 1711 | loss: 3.619260 | lr:5.9609e-04 | norm 0.3173 | dt 337.73ms | 1552380.65 tokens/sec
Step 1712 | loss: 3.642607 | lr:5.9608e-04 | norm 0.3507 | dt 337.62ms | 1552884.92 tokens/sec
Step 1713 | loss: 3.641557 | lr:5.9607e-04 | norm 0.3381 | dt 337.86ms | 1551788.00 tokens/sec
Step 1714 | loss: 3.706608 | lr:5.9606e-04 | norm 0.3374 | dt 337.37ms | 1554061.37 tokens/sec
Step 1715 | loss: 3.666618 | lr:5.9606e-04 | norm 0.3121 | dt 337.50ms | 1553455.36 tokens/sec
Step 1716 | loss: 3.823949 | lr:5.9605e-04 | norm 0.3518 | dt 337.57ms | 1553107.56 tokens/sec
Step 1717 | loss: 3.765525 | lr:5.9604e-04 | norm 0.3681 | dt 338.03ms | 1551028.40 tokens/sec
Step 1718 | loss: 3.723007 | lr:5.9603e-04 | norm 0.3352 | dt 338.38ms | 1549397.90 tokens/sec
Step 1719 | loss: 3.764516 | lr:5.9602e-04 | norm 0.3330 | dt 337.71ms | 1552471.61 tokens/sec
Step 1720 | loss: 3.776824 | lr:5.9602e-04 | norm 0.3316 | dt 338.39ms | 1549376.07 tokens/sec
Step 1721 | loss: 3.746685 | lr:5.9601e-04 | norm 0.3737 | dt 337.57ms | 1553140.47 tokens/sec
Step 1722 | loss: 3.740008 | lr:5.9600e-04 | norm 0.3939 | dt 337.49ms | 1553497.07 tokens/sec
Step 1723 | loss: 3.693302 | lr:5.9599e-04 | norm 0.3851 | dt 338.05ms | 1550912.45 tokens/sec
Step 1724 | loss: 3.803268 | lr:5.9598e-04 | norm 0.3947 | dt 337.83ms | 1551925.99 tokens/sec
Step 1725 | loss: 3.753390 | lr:5.9598e-04 | norm 0.3751 | dt 337.62ms | 1552890.40 tokens/sec
Step 1726 | loss: 3.761499 | lr:5.9597e-04 | norm 0.3192 | dt 337.69ms | 1552581.22 tokens/sec
Step 1727 | loss: 3.702837 | lr:5.9596e-04 | norm 0.3211 | dt 338.21ms | 1550179.94 tokens/sec
Step 1728 | loss: 3.733279 | lr:5.9595e-04 | norm 0.3086 | dt 337.83ms | 1551910.65 tokens/sec
Step 1729 | loss: 3.723505 | lr:5.9595e-04 | norm 0.2865 | dt 337.36ms | 1554071.25 tokens/sec
Step 1730 | loss: 3.668976 | lr:5.9594e-04 | norm 0.2656 | dt 337.34ms | 1554187.68 tokens/sec
Step 1731 | loss: 3.711090 | lr:5.9593e-04 | norm 0.3002 | dt 337.80ms | 1552054.14 tokens/sec
Step 1732 | loss: 3.749026 | lr:5.9592e-04 | norm 0.3339 | dt 338.34ms | 1549587.88 tokens/sec
Step 1733 | loss: 3.679042 | lr:5.9591e-04 | norm 0.3727 | dt 338.05ms | 1550912.45 tokens/sec
Step 1734 | loss: 3.715578 | lr:5.9591e-04 | norm 0.3800 | dt 338.67ms | 1548057.38 tokens/sec
Step 1735 | loss: 3.731456 | lr:5.9590e-04 | norm 0.3384 | dt 338.50ms | 1548875.16 tokens/sec
Step 1736 | loss: 3.677722 | lr:5.9589e-04 | norm 0.3075 | dt 338.02ms | 1551075.45 tokens/sec
Step 1737 | loss: 3.716736 | lr:5.9588e-04 | norm 0.3182 | dt 337.81ms | 1552033.33 tokens/sec
Step 1738 | loss: 3.693379 | lr:5.9587e-04 | norm 0.3188 | dt 337.98ms | 1551245.04 tokens/sec
Step 1739 | loss: 3.723117 | lr:5.9587e-04 | norm 0.3236 | dt 338.81ms | 1547452.79 tokens/sec
Step 1740 | loss: 3.645099 | lr:5.9586e-04 | norm 0.2890 | dt 339.18ms | 1545741.77 tokens/sec
Step 1741 | loss: 3.652310 | lr:5.9585e-04 | norm 0.2921 | dt 338.48ms | 1548948.26 tokens/sec
Step 1742 | loss: 3.606045 | lr:5.9584e-04 | norm 0.3071 | dt 338.22ms | 1550140.60 tokens/sec
Step 1743 | loss: 3.657511 | lr:5.9583e-04 | norm 0.3058 | dt 338.59ms | 1548424.73 tokens/sec
Step 1744 | loss: 3.622046 | lr:5.9582e-04 | norm 0.3167 | dt 337.85ms | 1551829.61 tokens/sec
Step 1745 | loss: 3.645773 | lr:5.9582e-04 | norm 0.3143 | dt 338.30ms | 1549769.16 tokens/sec
Step 1746 | loss: 3.630518 | lr:5.9581e-04 | norm 0.3460 | dt 338.35ms | 1549554.03 tokens/sec
Step 1747 | loss: 3.686676 | lr:5.9580e-04 | norm 0.3450 | dt 337.27ms | 1554522.78 tokens/sec
Step 1748 | loss: 3.653320 | lr:5.9579e-04 | norm 0.3278 | dt 337.62ms | 1552881.63 tokens/sec
Step 1749 | loss: 3.671679 | lr:5.9578e-04 | norm 0.3219 | dt 338.69ms | 1547987.64 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 1750: 3.7349
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2596/10042=0.2585


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what is it for me?
But, this isn't that difficult. It's a huge chunk, and in
rank 5 sample 1 >Hello, I'm a language model, no two years of high education. You'll definitely be reading to tell you all by myself, but not with any other
rank 5 sample 2 >Hello, I'm a language model, so you want a common language or another language.
For example, if I want any language to be a language and
rank 5 sample 3 >Hello, I'm a language model, for the very first time when I was in college at that time, I’ll have spent my time with other




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not talking.
Let's go through one of my steps—we don't know any other people know
rank 3 sample 1 >Hello, I'm a language model, and you're not alone. We have to use your child's imagination to grow and develop.
What is our brain
rank 3 sample 2 >Hello, I'm a language model, and it was also well organized.
To the start, my first step is to introduce myself...
- You want
rank 3 sample 3 >Hello, I'm a language model, and most of you hear the "mystery-language", but this makes me nervous by.
2) What is




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I love it because of the similarities – and similarities, and the same to how many others could share.
You
rank 7 sample 1 >Hello, I'm a language model, of which it is. I want everything to be better written and then I want everyone to write my sentences and I think
rank 1 sample 0 >Hello, I'm a language model, and you'll be ready for another look,
and will share it with you. You're gonna be in the right
rank 7 sample 2 >Hello, I'm a language model, but I don't recognize it anyhow and I still don't need one. I don't know why it does it


ddp_rank 2: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, not an Americanized language. My students really love how English is not a language, although they probably have a lot of
rank 7 sample 3 >Hello, I'm a language model, to help, but to use the concept of translation.
While the words are often used in some language, translations differ


rank 1 sample 2 >Hello, I'm a language model, but don't be a compiler.
I'm a bit on my phone, but I have the right kind of talk
rank 1 sample 3 >Hello, I'm a language model, and I'm using the Java Framework I to build a strong-spendbook. Now go for the Java, and
rank 2 sample 0 >Hello, I'm a language model, having my kids make a language learning journey, I'm going to be in a conversation with my mom and we're learning


rank 2 sample 1 >Hello, I'm a language model, but I should not be able to identify when to be a good model by using a real-life solution, especially if
rank 2 sample 2 >Hello, I'm a language model, so I've had my first and greatest, and I'm the most well-used for doing research-based time.
rank 2 sample 3 >Hello, I'm a language model, but not I. It is a very helpful tool because it offers a great help, because no sound can be heard or




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I like to use those words instead of giving up, and also let my students practice something in the lesson. My
rank 0 sample 1 >Hello, I'm a language model, but how do I do it? Where do we work now? Why is there so little context? There is a big
rank 0 sample 2 >Hello, I'm a language model, so I used to hear, think about it with a few minutes and read it with a couple of minutes. I know
rank 0 sample 3 >Hello, I'm a language model, but now I'm using my native language to give you a freebie. You are in all colors from the book at





ddp_rank 6: ####### Printing generated samples ####### 


ddp_rank 4: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, just I'm a student using a Java-based, but you want me to play with a Java-based language while
rank 6 sample 1 >Hello, I'm a language model, which is not a single-language, we need more than one language model to produce the best model for all language modelsrank 4 sample 0 >Hello, I'm a language model, so I could just have them. I want them to be able to speak for themselves so quickly. We want them to

rank 4 sample 1 >Hello, I'm a language model, is an example of this. How long has the number the number of speakers per 1 (I am very good – the
rank 6 sample 2 >Hello, I'm a language model, but my husband and I had the skills to be a language interpreter. I could have fun learning about both language groups,
rank 6 sample 3 >Hello, I'm a language model, and this is the basis of your language learning journey.
I learned a new language from several different ethnic group countries;


rank 4 sample 2 >Hello, I'm a language model, I try to tell them, but you'll feel like I did something called "Fib-yung" as it
rank 4 sample 3 >Hello, I'm a language model, and my idea isn't to be fun in any conversation, not just what you're doing, isn't it? It


Step 1750 | loss: 3.554476 | lr:5.9578e-04 | norm 0.3827 | dt 18843.60ms | 27823.14 tokens/sec
Step 1751 | loss: 3.601473 | lr:5.9577e-04 | norm 0.3856 | dt 333.11ms | 1573896.68 tokens/sec
Step 1752 | loss: 3.654129 | lr:5.9576e-04 | norm 0.3311 | dt 335.34ms | 1563449.69 tokens/sec
Step 1753 | loss: 3.620846 | lr:5.9575e-04 | norm 0.3282 | dt 337.01ms | 1555686.30 tokens/sec
Step 1754 | loss: 3.608341 | lr:5.9574e-04 | norm 0.3247 | dt 335.83ms | 1561177.61 tokens/sec
Step 1755 | loss: 3.628682 | lr:5.9574e-04 | norm 0.3224 | dt 335.80ms | 1561288.45 tokens/sec
Step 1756 | loss: 3.590797 | lr:5.9573e-04 | norm 0.3097 | dt 335.95ms | 1560613.66 tokens/sec
Step 1757 | loss: 3.625623 | lr:5.9572e-04 | norm 0.3052 | dt 335.82ms | 1561205.31 tokens/sec
Step 1758 | loss: 3.573746 | lr:5.9571e-04 | norm 0.3360 | dt 336.89ms | 1556241.18 tokens/sec
Step 1759 | loss: 3.637833 | lr:5.9570e-04 | norm 0.3229 | dt 336.73ms | 1556988.25 tokens/sec
Step 1760 | loss: 3.627506 | lr:5.9569e-04 | norm 0.3184 | dt 336.06ms | 1560095.50 tokens/sec
Step 1761 | loss: 3.737005 | lr:5.9569e-04 | norm 0.3385 | dt 336.37ms | 1558642.51 tokens/sec
Step 1762 | loss: 3.766463 | lr:5.9568e-04 | norm 0.3703 | dt 336.15ms | 1559700.47 tokens/sec
Step 1763 | loss: 3.758878 | lr:5.9567e-04 | norm 0.3651 | dt 335.66ms | 1561964.93 tokens/sec
Step 1764 | loss: 3.689532 | lr:5.9566e-04 | norm 0.4024 | dt 336.31ms | 1558923.17 tokens/sec
Step 1765 | loss: 3.753748 | lr:5.9565e-04 | norm 0.4019 | dt 336.87ms | 1556365.64 tokens/sec
Step 1766 | loss: 3.751343 | lr:5.9564e-04 | norm 0.3669 | dt 336.69ms | 1557197.74 tokens/sec
Step 1767 | loss: 3.721113 | lr:5.9564e-04 | norm 0.3243 | dt 336.42ms | 1558437.05 tokens/sec
Step 1768 | loss: 3.732492 | lr:5.9563e-04 | norm 0.3277 | dt 338.61ms | 1548351.69 tokens/sec
Step 1769 | loss: 3.690477 | lr:5.9562e-04 | norm 0.3053 | dt 336.00ms | 1560385.54 tokens/sec
Step 1770 | loss: 3.766777 | lr:5.9561e-04 | norm 0.3464 | dt 336.07ms | 1560055.66 tokens/sec
Step 1771 | loss: 3.739629 | lr:5.9560e-04 | norm 0.4381 | dt 336.68ms | 1557212.07 tokens/sec
Step 1772 | loss: 3.692449 | lr:5.9559e-04 | norm 0.4347 | dt 337.35ms | 1554125.07 tokens/sec
Step 1773 | loss: 3.729170 | lr:5.9559e-04 | norm 0.3864 | dt 336.52ms | 1557979.94 tokens/sec
Step 1774 | loss: 3.713982 | lr:5.9558e-04 | norm 0.3698 | dt 337.20ms | 1554837.13 tokens/sec
Step 1775 | loss: 3.676723 | lr:5.9557e-04 | norm 0.3412 | dt 337.23ms | 1554703.02 tokens/sec
Step 1776 | loss: 3.689217 | lr:5.9556e-04 | norm 0.3194 | dt 336.83ms | 1556518.77 tokens/sec
Step 1777 | loss: 3.709846 | lr:5.9555e-04 | norm 0.2875 | dt 337.91ms | 1551541.65 tokens/sec
Step 1778 | loss: 3.706775 | lr:5.9554e-04 | norm 0.2826 | dt 336.87ms | 1556344.71 tokens/sec
Step 1779 | loss: 3.712345 | lr:5.9554e-04 | norm 0.2674 | dt 336.83ms | 1556544.11 tokens/sec
Step 1780 | loss: 3.657569 | lr:5.9553e-04 | norm 0.2851 | dt 337.21ms | 1554771.17 tokens/sec
Step 1781 | loss: 3.710618 | lr:5.9552e-04 | norm 0.2934 | dt 337.05ms | 1555521.23 tokens/sec
Step 1782 | loss: 3.682910 | lr:5.9551e-04 | norm 0.3221 | dt 337.56ms | 1553184.35 tokens/sec
Step 1783 | loss: 3.669360 | lr:5.9550e-04 | norm 0.2755 | dt 336.32ms | 1558901.06 tokens/sec
Step 1784 | loss: 3.691116 | lr:5.9549e-04 | norm 0.2707 | dt 337.78ms | 1552159.31 tokens/sec
Step 1785 | loss: 3.662545 | lr:5.9549e-04 | norm 0.2793 | dt 337.53ms | 1553296.26 tokens/sec
Step 1786 | loss: 3.593926 | lr:5.9548e-04 | norm 0.2734 | dt 337.23ms | 1554672.24 tokens/sec
Step 1787 | loss: 3.611460 | lr:5.9547e-04 | norm 0.2946 | dt 337.62ms | 1552894.79 tokens/sec
Step 1788 | loss: 3.641489 | lr:5.9546e-04 | norm 0.2799 | dt 337.32ms | 1554253.59 tokens/sec
Step 1789 | loss: 3.648418 | lr:5.9545e-04 | norm 0.2917 | dt 337.51ms | 1553420.25 tokens/sec
Step 1790 | loss: 3.671763 | lr:5.9544e-04 | norm 0.3137 | dt 338.02ms | 1551055.75 tokens/sec
Step 1791 | loss: 3.605087 | lr:5.9544e-04 | norm 0.3583 | dt 337.60ms | 1553001.17 tokens/sec
Step 1792 | loss: 3.559505 | lr:5.9543e-04 | norm 0.3888 | dt 337.37ms | 1554065.76 tokens/sec
Step 1793 | loss: 3.652677 | lr:5.9542e-04 | norm 0.3585 | dt 337.67ms | 1552666.73 tokens/sec
Step 1794 | loss: 3.569670 | lr:5.9541e-04 | norm 0.3650 | dt 339.53ms | 1544171.15 tokens/sec
Step 1795 | loss: 3.632563 | lr:5.9540e-04 | norm 0.3791 | dt 338.20ms | 1550245.51 tokens/sec
Step 1796 | loss: 3.570128 | lr:5.9539e-04 | norm 0.3600 | dt 338.91ms | 1546974.89 tokens/sec
Step 1797 | loss: 3.545846 | lr:5.9538e-04 | norm 0.3210 | dt 337.83ms | 1551910.65 tokens/sec
Step 1798 | loss: 3.563229 | lr:5.9538e-04 | norm 0.3167 | dt 338.47ms | 1548998.45 tokens/sec
Step 1799 | loss: 3.683712 | lr:5.9537e-04 | norm 0.3255 | dt 337.68ms | 1552629.46 tokens/sec
Step 1800 | loss: 3.608683 | lr:5.9536e-04 | norm 0.2948 | dt 337.88ms | 1551704.78 tokens/sec
Step 1801 | loss: 3.574065 | lr:5.9535e-04 | norm 0.2985 | dt 337.44ms | 1553718.79 tokens/sec
Step 1802 | loss: 3.645408 | lr:5.9534e-04 | norm 0.3050 | dt 338.07ms | 1550845.73 tokens/sec
Step 1803 | loss: 3.627377 | lr:5.9533e-04 | norm 0.2967 | dt 338.03ms | 1551018.56 tokens/sec
Step 1804 | loss: 3.621463 | lr:5.9533e-04 | norm 0.3272 | dt 337.42ms | 1553798.93 tokens/sec
Step 1805 | loss: 3.610097 | lr:5.9532e-04 | norm 0.3974 | dt 338.44ms | 1549153.40 tokens/sec
Step 1806 | loss: 3.573064 | lr:5.9531e-04 | norm 0.4224 | dt 338.46ms | 1549050.82 tokens/sec
Step 1807 | loss: 3.725168 | lr:5.9530e-04 | norm 0.3802 | dt 337.46ms | 1553609.02 tokens/sec
Step 1808 | loss: 3.654864 | lr:5.9529e-04 | norm 0.3742 | dt 337.69ms | 1552552.72 tokens/sec
Step 1809 | loss: 3.767535 | lr:5.9528e-04 | norm 0.3769 | dt 338.04ms | 1550966.05 tokens/sec
Step 1810 | loss: 3.643116 | lr:5.9527e-04 | norm 0.3525 | dt 337.54ms | 1553266.63 tokens/sec
Step 1811 | loss: 3.750406 | lr:5.9526e-04 | norm 0.3439 | dt 337.63ms | 1552830.09 tokens/sec
Step 1812 | loss: 3.732614 | lr:5.9526e-04 | norm 0.3579 | dt 337.61ms | 1552959.49 tokens/sec
Step 1813 | loss: 3.756388 | lr:5.9525e-04 | norm 0.3744 | dt 338.63ms | 1548271.01 tokens/sec
Step 1814 | loss: 3.716429 | lr:5.9524e-04 | norm 0.3303 | dt 338.33ms | 1549629.37 tokens/sec
Step 1815 | loss: 3.744867 | lr:5.9523e-04 | norm 0.3481 | dt 338.24ms | 1550030.24 tokens/sec
Step 1816 | loss: 3.711791 | lr:5.9522e-04 | norm 0.3271 | dt 337.26ms | 1554544.75 tokens/sec
Step 1817 | loss: 3.713131 | lr:5.9521e-04 | norm 0.3154 | dt 338.00ms | 1551141.09 tokens/sec
Step 1818 | loss: 3.689916 | lr:5.9520e-04 | norm 0.3449 | dt 338.64ms | 1548206.70 tokens/sec
Step 1819 | loss: 3.758247 | lr:5.9520e-04 | norm 0.3904 | dt 338.63ms | 1548247.03 tokens/sec
Step 1820 | loss: 3.775031 | lr:5.9519e-04 | norm 0.3331 | dt 338.78ms | 1547562.78 tokens/sec
Step 1821 | loss: 3.649175 | lr:5.9518e-04 | norm 0.3366 | dt 338.90ms | 1547039.10 tokens/sec
Step 1822 | loss: 3.718337 | lr:5.9517e-04 | norm 0.2965 | dt 337.68ms | 1552608.63 tokens/sec
Step 1823 | loss: 3.689776 | lr:5.9516e-04 | norm 0.2897 | dt 337.88ms | 1551687.26 tokens/sec
Step 1824 | loss: 3.654685 | lr:5.9515e-04 | norm 0.3222 | dt 337.59ms | 1553023.11 tokens/sec
Step 1825 | loss: 3.715281 | lr:5.9514e-04 | norm 0.3234 | dt 338.44ms | 1549131.58 tokens/sec
Step 1826 | loss: 3.731550 | lr:5.9513e-04 | norm 0.3035 | dt 337.94ms | 1551421.24 tokens/sec
Step 1827 | loss: 3.704525 | lr:5.9513e-04 | norm 0.3333 | dt 337.72ms | 1552436.54 tokens/sec
Step 1828 | loss: 3.785808 | lr:5.9512e-04 | norm 0.3487 | dt 337.77ms | 1552207.52 tokens/sec
Step 1829 | loss: 3.728890 | lr:5.9511e-04 | norm 0.3260 | dt 338.08ms | 1550781.20 tokens/sec
Step 1830 | loss: 3.657180 | lr:5.9510e-04 | norm 0.3376 | dt 338.42ms | 1549224.34 tokens/sec
Step 1831 | loss: 3.662446 | lr:5.9509e-04 | norm 0.3203 | dt 337.64ms | 1552795.01 tokens/sec
Step 1832 | loss: 3.613732 | lr:5.9508e-04 | norm 0.3046 | dt 337.81ms | 1552028.95 tokens/sec
Step 1833 | loss: 3.618850 | lr:5.9507e-04 | norm 0.2912 | dt 338.40ms | 1549319.30 tokens/sec
Step 1834 | loss: 3.611642 | lr:5.9506e-04 | norm 0.3067 | dt 338.92ms | 1546956.39 tokens/sec
Step 1835 | loss: 3.626686 | lr:5.9506e-04 | norm 0.2780 | dt 336.83ms | 1556518.77 tokens/sec
Step 1836 | loss: 3.627257 | lr:5.9505e-04 | norm 0.2905 | dt 338.33ms | 1549630.46 tokens/sec
Step 1837 | loss: 3.630038 | lr:5.9504e-04 | norm 0.2909 | dt 338.64ms | 1548237.22 tokens/sec
Step 1838 | loss: 3.662407 | lr:5.9503e-04 | norm 0.3214 | dt 337.94ms | 1551445.32 tokens/sec
Step 1839 | loss: 3.622501 | lr:5.9502e-04 | norm 0.3608 | dt 337.54ms | 1553284.19 tokens/sec
Step 1840 | loss: 3.620817 | lr:5.9501e-04 | norm 0.3717 | dt 338.14ms | 1550500.19 tokens/sec
Step 1841 | loss: 3.586975 | lr:5.9500e-04 | norm 0.3519 | dt 338.16ms | 1550419.30 tokens/sec
Step 1842 | loss: 3.574125 | lr:5.9499e-04 | norm 0.3542 | dt 338.68ms | 1548026.87 tokens/sec
Step 1843 | loss: 3.527360 | lr:5.9499e-04 | norm 0.3663 | dt 337.99ms | 1551191.42 tokens/sec
Step 1844 | loss: 3.593760 | lr:5.9498e-04 | norm 0.3820 | dt 338.11ms | 1550643.42 tokens/sec
Step 1845 | loss: 3.551088 | lr:5.9497e-04 | norm 0.3748 | dt 337.56ms | 1553149.25 tokens/sec
Step 1846 | loss: 3.577227 | lr:5.9496e-04 | norm 0.3683 | dt 338.92ms | 1546919.39 tokens/sec
Step 1847 | loss: 3.549313 | lr:5.9495e-04 | norm 0.3141 | dt 339.49ms | 1544338.16 tokens/sec
Step 1848 | loss: 3.711134 | lr:5.9494e-04 | norm 0.3690 | dt 338.64ms | 1548235.04 tokens/sec
Step 1849 | loss: 3.595919 | lr:5.9493e-04 | norm 0.4403 | dt 338.27ms | 1549914.44 tokens/sec
Step 1850 | loss: 3.586374 | lr:5.9492e-04 | norm 0.4367 | dt 339.39ms | 1544790.55 tokens/sec
Step 1851 | loss: 3.577553 | lr:5.9491e-04 | norm 0.4037 | dt 338.37ms | 1549432.84 tokens/sec
Step 1852 | loss: 3.648750 | lr:5.9491e-04 | norm 0.3228 | dt 339.33ms | 1545087.95 tokens/sec
Step 1853 | loss: 3.686148 | lr:5.9490e-04 | norm 0.3221 | dt 339.82ms | 1542820.18 tokens/sec
Step 1854 | loss: 3.707760 | lr:5.9489e-04 | norm 0.3095 | dt 338.55ms | 1548617.74 tokens/sec
Step 1855 | loss: 3.705065 | lr:5.9488e-04 | norm 0.3083 | dt 338.71ms | 1547913.55 tokens/sec
Step 1856 | loss: 3.702525 | lr:5.9487e-04 | norm 0.3006 | dt 340.59ms | 1539330.75 tokens/sec
Step 1857 | loss: 3.702303 | lr:5.9486e-04 | norm 0.2934 | dt 337.94ms | 1551409.20 tokens/sec
Step 1858 | loss: 3.714266 | lr:5.9485e-04 | norm 0.3026 | dt 337.92ms | 1551505.53 tokens/sec
Step 1859 | loss: 3.700026 | lr:5.9484e-04 | norm 0.3105 | dt 338.31ms | 1549712.37 tokens/sec
Step 1860 | loss: 3.755786 | lr:5.9483e-04 | norm 0.3574 | dt 339.37ms | 1544898.00 tokens/sec
Step 1861 | loss: 3.708667 | lr:5.9482e-04 | norm 0.3702 | dt 337.81ms | 1552004.85 tokens/sec
Step 1862 | loss: 3.722647 | lr:5.9482e-04 | norm 0.3958 | dt 338.52ms | 1548778.07 tokens/sec
Step 1863 | loss: 3.747211 | lr:5.9481e-04 | norm 0.3567 | dt 338.99ms | 1546639.77 tokens/sec
Step 1864 | loss: 3.644799 | lr:5.9480e-04 | norm 0.3546 | dt 339.39ms | 1544817.68 tokens/sec
Step 1865 | loss: 3.721748 | lr:5.9479e-04 | norm 0.2966 | dt 338.64ms | 1548219.78 tokens/sec
Step 1866 | loss: 3.714935 | lr:5.9478e-04 | norm 0.2984 | dt 339.02ms | 1546476.62 tokens/sec
Step 1867 | loss: 3.698531 | lr:5.9477e-04 | norm 0.3287 | dt 339.18ms | 1545761.33 tokens/sec
Step 1868 | loss: 3.643846 | lr:5.9476e-04 | norm 0.3629 | dt 338.79ms | 1547535.55 tokens/sec
Step 1869 | loss: 3.732314 | lr:5.9475e-04 | norm 0.3575 | dt 338.84ms | 1547292.73 tokens/sec
Step 1870 | loss: 3.712510 | lr:5.9474e-04 | norm 0.3469 | dt 338.64ms | 1548218.69 tokens/sec
Step 1871 | loss: 3.723381 | lr:5.9473e-04 | norm 0.3419 | dt 338.82ms | 1547373.30 tokens/sec
Step 1872 | loss: 3.669128 | lr:5.9472e-04 | norm 0.3067 | dt 338.44ms | 1549143.58 tokens/sec
Step 1873 | loss: 3.732394 | lr:5.9472e-04 | norm 0.2965 | dt 338.23ms | 1550071.76 tokens/sec
Step 1874 | loss: 3.684678 | lr:5.9471e-04 | norm 0.3584 | dt 339.22ms | 1545584.24 tokens/sec
Step 1875 | loss: 3.663697 | lr:5.9470e-04 | norm 0.3680 | dt 338.85ms | 1547276.40 tokens/sec
Step 1876 | loss: 3.729791 | lr:5.9469e-04 | norm 0.3326 | dt 338.43ms | 1549185.05 tokens/sec
Step 1877 | loss: 3.629576 | lr:5.9468e-04 | norm 0.3912 | dt 338.53ms | 1548737.72 tokens/sec
Step 1878 | loss: 3.661173 | lr:5.9467e-04 | norm 0.3531 | dt 338.90ms | 1547046.72 tokens/sec
Step 1879 | loss: 3.662894 | lr:5.9466e-04 | norm 0.3136 | dt 337.83ms | 1551946.80 tokens/sec
Step 1880 | loss: 3.602403 | lr:5.9465e-04 | norm 0.2973 | dt 337.81ms | 1552005.95 tokens/sec
Step 1881 | loss: 3.617838 | lr:5.9464e-04 | norm 0.3081 | dt 338.09ms | 1550728.71 tokens/sec
Step 1882 | loss: 3.545237 | lr:5.9463e-04 | norm 0.2855 | dt 338.00ms | 1551168.45 tokens/sec
Step 1883 | loss: 3.545458 | lr:5.9462e-04 | norm 0.3065 | dt 337.99ms | 1551214.40 tokens/sec
Step 1884 | loss: 3.587475 | lr:5.9462e-04 | norm 0.2980 | dt 337.65ms | 1552758.82 tokens/sec
Step 1885 | loss: 3.732570 | lr:5.9461e-04 | norm 0.3352 | dt 337.65ms | 1552765.40 tokens/sec
Step 1886 | loss: 3.627612 | lr:5.9460e-04 | norm 0.4308 | dt 338.13ms | 1550552.67 tokens/sec
Step 1887 | loss: 3.610747 | lr:5.9459e-04 | norm 0.4480 | dt 337.67ms | 1552666.73 tokens/sec
Step 1888 | loss: 3.621094 | lr:5.9458e-04 | norm 0.3411 | dt 337.79ms | 1552125.35 tokens/sec
Step 1889 | loss: 3.589476 | lr:5.9457e-04 | norm 0.3480 | dt 899.79ms | 582677.07 tokens/sec
Step 1890 | loss: 3.618500 | lr:5.9456e-04 | norm 0.3399 | dt 335.29ms | 1563663.14 tokens/sec
Step 1891 | loss: 3.756025 | lr:5.9455e-04 | norm 0.3353 | dt 337.54ms | 1553277.60 tokens/sec
Step 1892 | loss: 3.630339 | lr:5.9454e-04 | norm 0.3482 | dt 337.09ms | 1555344.10 tokens/sec
Step 1893 | loss: 3.536322 | lr:5.9453e-04 | norm 0.3307 | dt 337.32ms | 1554268.97 tokens/sec
Step 1894 | loss: 3.612803 | lr:5.9452e-04 | norm 0.3705 | dt 337.97ms | 1551295.38 tokens/sec
Step 1895 | loss: 3.562566 | lr:5.9451e-04 | norm 0.3530 | dt 337.44ms | 1553724.27 tokens/sec
Step 1896 | loss: 3.557395 | lr:5.9450e-04 | norm 0.3169 | dt 337.07ms | 1555420.01 tokens/sec
Step 1897 | loss: 3.544178 | lr:5.9450e-04 | norm 0.3185 | dt 337.78ms | 1552136.30 tokens/sec
Step 1898 | loss: 3.570011 | lr:5.9449e-04 | norm 0.3028 | dt 338.17ms | 1550389.78 tokens/sec
Step 1899 | loss: 3.563167 | lr:5.9448e-04 | norm 0.3059 | dt 932.62ms | 562169.50 tokens/sec
Step 1900 | loss: 3.671635 | lr:5.9447e-04 | norm 0.3168 | dt 336.30ms | 1558978.43 tokens/sec
Step 1901 | loss: 3.672476 | lr:5.9446e-04 | norm 0.3601 | dt 337.99ms | 1551181.58 tokens/sec
Step 1902 | loss: 3.665121 | lr:5.9445e-04 | norm 0.3216 | dt 337.51ms | 1553397.20 tokens/sec
Step 1903 | loss: 3.749545 | lr:5.9444e-04 | norm 0.2936 | dt 337.34ms | 1554203.06 tokens/sec
Step 1904 | loss: 3.719874 | lr:5.9443e-04 | norm 0.3195 | dt 339.01ms | 1546544.05 tokens/sec
Step 1905 | loss: 3.772011 | lr:5.9442e-04 | norm 0.3400 | dt 337.84ms | 1551866.85 tokens/sec
Step 1906 | loss: 3.757529 | lr:5.9441e-04 | norm 0.3364 | dt 337.31ms | 1554314.01 tokens/sec
Step 1907 | loss: 3.712402 | lr:5.9440e-04 | norm 0.3401 | dt 339.07ms | 1546263.49 tokens/sec
Step 1908 | loss: 3.677612 | lr:5.9439e-04 | norm 0.3063 | dt 338.80ms | 1547478.92 tokens/sec
Step 1909 | loss: 3.710871 | lr:5.9438e-04 | norm 0.2915 | dt 338.32ms | 1549686.16 tokens/sec
Step 1910 | loss: 3.652728 | lr:5.9437e-04 | norm 0.2926 | dt 339.81ms | 1542887.29 tokens/sec
Step 1911 | loss: 3.664422 | lr:5.9436e-04 | norm 0.2820 | dt 338.34ms | 1549590.06 tokens/sec
Step 1912 | loss: 3.643323 | lr:5.9436e-04 | norm 0.3182 | dt 338.83ms | 1547344.99 tokens/sec
Step 1913 | loss: 3.726923 | lr:5.9435e-04 | norm 0.3799 | dt 338.91ms | 1546996.65 tokens/sec
Step 1914 | loss: 3.676242 | lr:5.9434e-04 | norm 0.3483 | dt 338.63ms | 1548243.76 tokens/sec
Step 1915 | loss: 3.692036 | lr:5.9433e-04 | norm 0.3248 | dt 339.63ms | 1543709.37 tokens/sec
Step 1916 | loss: 3.679034 | lr:5.9432e-04 | norm 0.3114 | dt 339.60ms | 1543850.26 tokens/sec
Step 1917 | loss: 3.699241 | lr:5.9431e-04 | norm 0.3245 | dt 338.51ms | 1548800.98 tokens/sec
Step 1918 | loss: 3.673211 | lr:5.9430e-04 | norm 0.3173 | dt 338.57ms | 1548526.14 tokens/sec
Step 1919 | loss: 3.624898 | lr:5.9429e-04 | norm 0.2797 | dt 339.85ms | 1542684.89 tokens/sec
Step 1920 | loss: 3.677016 | lr:5.9428e-04 | norm 0.3178 | dt 338.97ms | 1546724.63 tokens/sec
Step 1921 | loss: 3.680392 | lr:5.9427e-04 | norm 0.3274 | dt 338.85ms | 1547278.57 tokens/sec
Step 1922 | loss: 3.639276 | lr:5.9426e-04 | norm 0.3170 | dt 341.87ms | 1533606.66 tokens/sec
Step 1923 | loss: 3.593520 | lr:5.9425e-04 | norm 0.3196 | dt 338.88ms | 1547130.53 tokens/sec
Step 1924 | loss: 3.616100 | lr:5.9424e-04 | norm 0.3223 | dt 338.06ms | 1550888.39 tokens/sec
Step 1925 | loss: 3.597501 | lr:5.9423e-04 | norm 0.3456 | dt 339.24ms | 1545478.88 tokens/sec
Step 1926 | loss: 3.629364 | lr:5.9422e-04 | norm 0.3735 | dt 339.55ms | 1544045.38 tokens/sec
Step 1927 | loss: 3.632943 | lr:5.9421e-04 | norm 0.3696 | dt 338.44ms | 1549142.49 tokens/sec
Step 1928 | loss: 3.623905 | lr:5.9420e-04 | norm 0.3124 | dt 338.22ms | 1550127.49 tokens/sec
Step 1929 | loss: 3.644439 | lr:5.9419e-04 | norm 0.3061 | dt 338.58ms | 1548484.70 tokens/sec
Step 1930 | loss: 3.625984 | lr:5.9418e-04 | norm 0.3044 | dt 338.79ms | 1547527.93 tokens/sec
Step 1931 | loss: 3.649577 | lr:5.9418e-04 | norm 0.2958 | dt 337.68ms | 1552618.49 tokens/sec
Step 1932 | loss: 3.593880 | lr:5.9417e-04 | norm 0.2849 | dt 337.52ms | 1553366.48 tokens/sec
Step 1933 | loss: 3.614310 | lr:5.9416e-04 | norm 0.2975 | dt 339.42ms | 1544667.94 tokens/sec
Step 1934 | loss: 3.561495 | lr:5.9415e-04 | norm 0.3036 | dt 337.61ms | 1552948.53 tokens/sec
Step 1935 | loss: 3.528926 | lr:5.9414e-04 | norm 0.3024 | dt 339.44ms | 1544572.46 tokens/sec
Step 1936 | loss: 3.577891 | lr:5.9413e-04 | norm 0.3113 | dt 338.93ms | 1546893.27 tokens/sec
Step 1937 | loss: 3.566243 | lr:5.9412e-04 | norm 0.2959 | dt 338.70ms | 1547958.22 tokens/sec
Step 1938 | loss: 3.574200 | lr:5.9411e-04 | norm 0.2996 | dt 338.76ms | 1547682.59 tokens/sec
Step 1939 | loss: 3.528901 | lr:5.9410e-04 | norm 0.2700 | dt 339.18ms | 1545771.11 tokens/sec
Step 1940 | loss: 3.596860 | lr:5.9409e-04 | norm 0.2701 | dt 338.16ms | 1550431.32 tokens/sec
Step 1941 | loss: 3.581984 | lr:5.9408e-04 | norm 0.2820 | dt 338.76ms | 1547654.27 tokens/sec
Step 1942 | loss: 3.572435 | lr:5.9407e-04 | norm 0.2904 | dt 338.99ms | 1546597.35 tokens/sec
Step 1943 | loss: 3.560292 | lr:5.9406e-04 | norm 0.2843 | dt 338.54ms | 1548669.00 tokens/sec
Step 1944 | loss: 3.532169 | lr:5.9405e-04 | norm 0.2891 | dt 338.57ms | 1548551.22 tokens/sec
Step 1945 | loss: 3.562131 | lr:5.9404e-04 | norm 0.3245 | dt 339.37ms | 1544886.06 tokens/sec
Step 1946 | loss: 3.577542 | lr:5.9403e-04 | norm 0.3111 | dt 339.08ms | 1546208.04 tokens/sec
Step 1947 | loss: 3.686251 | lr:5.9402e-04 | norm 0.3728 | dt 337.19ms | 1554894.30 tokens/sec
Step 1948 | loss: 3.718960 | lr:5.9401e-04 | norm 0.4167 | dt 339.16ms | 1545826.53 tokens/sec
Step 1949 | loss: 3.669429 | lr:5.9400e-04 | norm 0.4208 | dt 337.85ms | 1551829.61 tokens/sec
Step 1950 | loss: 3.670749 | lr:5.9399e-04 | norm 0.3909 | dt 337.68ms | 1552618.49 tokens/sec
Step 1951 | loss: 3.711031 | lr:5.9398e-04 | norm 0.3790 | dt 337.58ms | 1553064.78 tokens/sec
Step 1952 | loss: 3.714244 | lr:5.9397e-04 | norm 0.3500 | dt 337.59ms | 1553016.52 tokens/sec
Step 1953 | loss: 3.663293 | lr:5.9396e-04 | norm 0.3147 | dt 337.44ms | 1553716.59 tokens/sec
Step 1954 | loss: 3.676549 | lr:5.9395e-04 | norm 0.2858 | dt 337.43ms | 1553770.38 tokens/sec
Step 1955 | loss: 3.688781 | lr:5.9394e-04 | norm 0.2802 | dt 337.66ms | 1552704.00 tokens/sec
Step 1956 | loss: 3.747909 | lr:5.9393e-04 | norm 0.2836 | dt 337.58ms | 1553071.37 tokens/sec
Step 1957 | loss: 3.691157 | lr:5.9392e-04 | norm 0.3417 | dt 337.65ms | 1552752.24 tokens/sec
Step 1958 | loss: 3.716023 | lr:5.9391e-04 | norm 0.3597 | dt 338.30ms | 1549756.06 tokens/sec
Step 1959 | loss: 3.681046 | lr:5.9390e-04 | norm 0.3707 | dt 337.24ms | 1554663.45 tokens/sec
Step 1960 | loss: 3.637010 | lr:5.9390e-04 | norm 0.3822 | dt 337.81ms | 1552039.90 tokens/sec
Step 1961 | loss: 3.764889 | lr:5.9389e-04 | norm 0.3536 | dt 337.38ms | 1553996.57 tokens/sec
Step 1962 | loss: 3.726401 | lr:5.9388e-04 | norm 0.3363 | dt 337.69ms | 1552561.49 tokens/sec
Step 1963 | loss: 3.695849 | lr:5.9387e-04 | norm 0.3374 | dt 336.60ms | 1557595.91 tokens/sec
Step 1964 | loss: 3.620778 | lr:5.9386e-04 | norm 0.3116 | dt 337.25ms | 1554615.09 tokens/sec
Step 1965 | loss: 3.652803 | lr:5.9385e-04 | norm 0.3196 | dt 338.11ms | 1550634.67 tokens/sec
Step 1966 | loss: 3.645794 | lr:5.9384e-04 | norm 0.2906 | dt 337.51ms | 1553404.88 tokens/sec
Step 1967 | loss: 3.678520 | lr:5.9383e-04 | norm 0.2842 | dt 338.48ms | 1548928.62 tokens/sec
Step 1968 | loss: 3.619971 | lr:5.9382e-04 | norm 0.2842 | dt 338.56ms | 1548585.02 tokens/sec
Step 1969 | loss: 3.686316 | lr:5.9381e-04 | norm 0.2948 | dt 337.46ms | 1553640.85 tokens/sec
Step 1970 | loss: 3.583452 | lr:5.9380e-04 | norm 0.3510 | dt 336.55ms | 1557825.43 tokens/sec
Step 1971 | loss: 3.589240 | lr:5.9379e-04 | norm 0.3522 | dt 338.28ms | 1549846.71 tokens/sec
Step 1972 | loss: 3.618739 | lr:5.9378e-04 | norm 0.3989 | dt 338.01ms | 1551096.23 tokens/sec
Step 1973 | loss: 3.616762 | lr:5.9377e-04 | norm 0.3722 | dt 337.43ms | 1553763.80 tokens/sec
Step 1974 | loss: 3.566856 | lr:5.9376e-04 | norm 0.3621 | dt 337.25ms | 1554590.91 tokens/sec
Step 1975 | loss: 3.559692 | lr:5.9375e-04 | norm 0.3299 | dt 338.48ms | 1548931.89 tokens/sec
Step 1976 | loss: 3.601598 | lr:5.9374e-04 | norm 0.2808 | dt 337.98ms | 1551239.57 tokens/sec
Step 1977 | loss: 3.572381 | lr:5.9373e-04 | norm 0.2733 | dt 337.77ms | 1552197.66 tokens/sec
Step 1978 | loss: 3.575224 | lr:5.9372e-04 | norm 0.2762 | dt 338.08ms | 1550792.14 tokens/sec
Step 1979 | loss: 3.632752 | lr:5.9371e-04 | norm 0.2672 | dt 338.49ms | 1548915.53 tokens/sec
Step 1980 | loss: 3.578585 | lr:5.9370e-04 | norm 0.2833 | dt 337.84ms | 1551890.94 tokens/sec
Step 1981 | loss: 3.484347 | lr:5.9369e-04 | norm 0.2877 | dt 337.99ms | 1551216.59 tokens/sec
Step 1982 | loss: 3.546753 | lr:5.9368e-04 | norm 0.2913 | dt 337.97ms | 1551307.42 tokens/sec
Step 1983 | loss: 3.679914 | lr:5.9367e-04 | norm 0.3377 | dt 339.17ms | 1545788.50 tokens/sec
Step 1984 | loss: 3.564563 | lr:5.9366e-04 | norm 0.3927 | dt 338.56ms | 1548597.02 tokens/sec
Step 1985 | loss: 3.571704 | lr:5.9365e-04 | norm 0.3567 | dt 337.83ms | 1551906.27 tokens/sec
Step 1986 | loss: 3.592850 | lr:5.9364e-04 | norm 0.3286 | dt 338.03ms | 1551005.43 tokens/sec
Step 1987 | loss: 3.611588 | lr:5.9363e-04 | norm 0.3475 | dt 337.93ms | 1551465.03 tokens/sec
Step 1988 | loss: 3.574095 | lr:5.9362e-04 | norm 0.3322 | dt 337.22ms | 1554752.48 tokens/sec
Step 1989 | loss: 3.616444 | lr:5.9361e-04 | norm 0.2983 | dt 337.50ms | 1553426.83 tokens/sec
Step 1990 | loss: 3.556882 | lr:5.9360e-04 | norm 0.2819 | dt 338.38ms | 1549393.53 tokens/sec
Step 1991 | loss: 3.555479 | lr:5.9359e-04 | norm 0.2979 | dt 337.96ms | 1551331.50 tokens/sec
Step 1992 | loss: 3.640017 | lr:5.9358e-04 | norm 0.2784 | dt 336.98ms | 1555845.90 tokens/sec
Step 1993 | loss: 3.639243 | lr:5.9357e-04 | norm 0.2937 | dt 338.84ms | 1547292.73 tokens/sec
Step 1994 | loss: 3.676326 | lr:5.9356e-04 | norm 0.3115 | dt 339.16ms | 1545859.13 tokens/sec
Step 1995 | loss: 3.701274 | lr:5.9355e-04 | norm 0.3534 | dt 338.46ms | 1549053.01 tokens/sec
Step 1996 | loss: 3.679872 | lr:5.9354e-04 | norm 0.3849 | dt 338.71ms | 1547875.41 tokens/sec
Step 1997 | loss: 3.698984 | lr:5.9353e-04 | norm 0.3376 | dt 338.62ms | 1548309.17 tokens/sec
Step 1998 | loss: 3.680343 | lr:5.9352e-04 | norm 0.3426 | dt 338.20ms | 1550234.58 tokens/sec
Step 1999 | loss: 3.704838 | lr:5.9351e-04 | norm 0.3529 | dt 338.32ms | 1549663.23 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 2000: 3.6678
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2620/10042=0.2609


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not saying anything. Just type "x" on a diagram next to me or call on a "first
rank 3 sample 1 >Hello, I'm a language model, and this book is a rich set of useful ideas for exploring the world of language.
P.S. Eliot:
rank 3 sample 2 >Hello, I'm a language model, and it will be like what an author knows...
In our example of what happens when your first sentence is missing,
rank 3 sample 3 >Hello, I'm a language model, and he's the ultimate test that I'm making in college, but it's about that, and all the things he




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, let's talk of the things you should be aware of in the first person's opinion.
In the early days,

rank 2 sample 1 >Hello, I'm a language model, one of those people who have done very significant work in the world with some of the greatest language and science teachers. So

rank 2 sample 2 >Hello, I'm a language model, but I think you've got some problems with your code. We have done the right thing.<|endoftext|>There are at least
ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 3 >Hello, I'm a language model, so we created a simple model of a certain line: the point of the line in question "Who's going to use



rank 7 sample 0 >Hello, I'm a language model, and I'm a very much more computer intensive, but I'm not yet good at using it since it is a "
rank 5 sample 0 >Hello, I'm a language model, so this model is only a beginner. (The more I'm teaching my language models, the better I am. "
rank 5 sample 1 >Hello, I'm a language model, of some type: in general, that you really want to understand or do not recognize the words, that would make sense
rank 7 sample 1 >Hello, I'm a language model, who'll come up with it from around the world without us. This is a powerful way to keep learning about languages without
rank 5 sample 2 >Hello, I'm a language model, but it makes sense both in a linear and a linear way. But for any given purpose, I'm not sure where
rank 7 sample 2 >Hello, I'm a language model, and I've just never heard many English translations. A little more than I can imagine.
I'm not really good
rank 5 sample 3 >Hello, I'm a language model, we are just talking about "hating." It would help us understand how this has worked, our knowledge of how to


rank 7 sample 3 >Hello, I'm a language model, but it's not my only language I was really interested in, and in the language of the subject, but I had




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I do not want to be a math fan of the real world to have a strong computer/mathematician,
rank 4 sample 1 >Hello, I'm a language model, however, and do my best! Now the language uses vocabulary, and its syntax makes it easy to remember and recognize words
rank 4 sample 2 >Hello, I'm a language model, I'm used to build something that looks very similar to the language, but I did find that a big difference here,
rank 4 sample 3 >Hello, I'm a language model, and the kids come back to it after it and will work around the age of one.
If your child finds a




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and there are many examples, some will help your student remember which language he or she should have used. For example,
rank 1 sample 1 >Hello, I'm a language model, a model of how to use language to produce a set system. What's it about designing and the language that's used
rank 1 sample 2 >Hello, I'm a language model, so any kind of language that you can use to express the concepts and understand them is a language, so that is a
rank 1 sample 3 >Hello, I'm a language model, and I'm also interested in ways to write a new programming language based on other languages" i.e. how to




ddp_rank 6: ####### Printing generated samples ####### 



ddp_rank 0: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where every element of one component of another element is in one or two parts; a single element of one component of another
rank 6 sample 1 >Hello, I'm a language model, which could explain the different types of communication skills. But, I'm curious to know what it's all:
-
rank 0 sample 0 >Hello, I'm a language model, and I'd like a simple and fast way to build something that requires me to do, but what's important is a
rank 6 sample 2 >Hello, I'm a language model, so here are a few things I will cover for you:
- Language Models
- Language Models
- Understanding the
rank 0 sample 1 >Hello, I'm a language model, but with a few basic exercises to aid in understanding all sorts of concepts and how they relate to the learning experience.

rank 6 sample 3 >Hello, I'm a language model, and that's when the language of a real computer is used. The language structure of an environment is always set apart from


rank 0 sample 2 >Hello, I'm a language model, but I understand how to teach our subject. You can use that for example, you can use the full-color pencil
rank 0 sample 3 >Hello, I'm a language model, and when I'm done, it's all as simple as that. And it may even be that your reader has only


Step 2000 | loss: 3.646067 | lr:5.9350e-04 | norm 0.3819 | dt 12420.26ms | 42212.33 tokens/sec
Step 2001 | loss: 3.682219 | lr:5.9349e-04 | norm 0.3340 | dt 334.26ms | 1568490.20 tokens/sec
Step 2002 | loss: 3.626649 | lr:5.9348e-04 | norm 0.2995 | dt 336.69ms | 1557180.09 tokens/sec
Step 2003 | loss: 3.603565 | lr:5.9347e-04 | norm 0.3033 | dt 336.35ms | 1558762.94 tokens/sec
Step 2004 | loss: 3.681257 | lr:5.9346e-04 | norm 0.2823 | dt 335.31ms | 1563569.75 tokens/sec
Step 2005 | loss: 3.613179 | lr:5.9345e-04 | norm 0.2621 | dt 336.76ms | 1556852.67 tokens/sec
Step 2006 | loss: 3.687613 | lr:5.9344e-04 | norm 0.3008 | dt 336.30ms | 1558990.58 tokens/sec
Step 2007 | loss: 3.666165 | lr:5.9343e-04 | norm 0.3202 | dt 335.30ms | 1563628.67 tokens/sec
Step 2008 | loss: 3.582006 | lr:5.9342e-04 | norm 0.3505 | dt 337.91ms | 1551569.02 tokens/sec
Step 2009 | loss: 3.641836 | lr:5.9341e-04 | norm 0.3109 | dt 336.44ms | 1558342.08 tokens/sec
Step 2010 | loss: 3.681909 | lr:5.9340e-04 | norm 0.3525 | dt 336.17ms | 1559573.26 tokens/sec
Step 2011 | loss: 3.677532 | lr:5.9339e-04 | norm 0.3275 | dt 336.63ms | 1557456.91 tokens/sec
Step 2012 | loss: 3.621096 | lr:5.9338e-04 | norm 0.2771 | dt 336.30ms | 1558990.58 tokens/sec
Step 2013 | loss: 3.621224 | lr:5.9337e-04 | norm 0.2815 | dt 336.15ms | 1559692.73 tokens/sec
Step 2014 | loss: 3.685306 | lr:5.9336e-04 | norm 0.3437 | dt 337.16ms | 1555007.55 tokens/sec
Step 2015 | loss: 3.651237 | lr:5.9335e-04 | norm 0.3139 | dt 337.15ms | 1555041.64 tokens/sec
Step 2016 | loss: 3.627047 | lr:5.9334e-04 | norm 0.3453 | dt 336.69ms | 1557180.09 tokens/sec
Step 2017 | loss: 3.592766 | lr:5.9333e-04 | norm 0.3855 | dt 337.70ms | 1552526.42 tokens/sec
Step 2018 | loss: 3.620425 | lr:5.9332e-04 | norm 0.3975 | dt 336.91ms | 1556181.71 tokens/sec
Step 2019 | loss: 3.615131 | lr:5.9331e-04 | norm 0.3850 | dt 337.48ms | 1553558.53 tokens/sec
Step 2020 | loss: 3.653110 | lr:5.9330e-04 | norm 0.3722 | dt 338.10ms | 1550691.53 tokens/sec
Step 2021 | loss: 3.602535 | lr:5.9328e-04 | norm 0.3396 | dt 336.31ms | 1558944.17 tokens/sec
Step 2022 | loss: 3.607286 | lr:5.9327e-04 | norm 0.3152 | dt 338.55ms | 1548628.65 tokens/sec
Step 2023 | loss: 3.599375 | lr:5.9326e-04 | norm 0.3142 | dt 336.85ms | 1556451.57 tokens/sec
Step 2024 | loss: 3.611444 | lr:5.9325e-04 | norm 0.2842 | dt 337.03ms | 1555616.97 tokens/sec
Step 2025 | loss: 3.620792 | lr:5.9324e-04 | norm 0.3171 | dt 337.76ms | 1552244.77 tokens/sec
Step 2026 | loss: 3.559651 | lr:5.9323e-04 | norm 0.2986 | dt 340.85ms | 1538172.19 tokens/sec
Step 2027 | loss: 3.571433 | lr:5.9322e-04 | norm 0.2806 | dt 337.61ms | 1552917.82 tokens/sec
Step 2028 | loss: 3.520175 | lr:5.9321e-04 | norm 0.2815 | dt 338.01ms | 1551122.49 tokens/sec
Step 2029 | loss: 3.544875 | lr:5.9320e-04 | norm 0.2634 | dt 337.55ms | 1553219.46 tokens/sec
Step 2030 | loss: 3.564744 | lr:5.9319e-04 | norm 0.2743 | dt 337.36ms | 1554091.02 tokens/sec
Step 2031 | loss: 3.575949 | lr:5.9318e-04 | norm 0.2877 | dt 337.56ms | 1553179.96 tokens/sec
Step 2032 | loss: 3.502521 | lr:5.9317e-04 | norm 0.3021 | dt 337.54ms | 1553256.76 tokens/sec
Step 2033 | loss: 3.530476 | lr:5.9316e-04 | norm 0.2756 | dt 338.52ms | 1548781.35 tokens/sec
Step 2034 | loss: 3.515369 | lr:5.9315e-04 | norm 0.2860 | dt 337.54ms | 1553250.18 tokens/sec
Step 2035 | loss: 3.556637 | lr:5.9314e-04 | norm 0.2970 | dt 337.59ms | 1553019.82 tokens/sec
Step 2036 | loss: 3.517664 | lr:5.9313e-04 | norm 0.2742 | dt 337.60ms | 1552970.46 tokens/sec
Step 2037 | loss: 3.532951 | lr:5.9312e-04 | norm 0.2929 | dt 338.34ms | 1549611.90 tokens/sec
Step 2038 | loss: 3.549392 | lr:5.9311e-04 | norm 0.3130 | dt 338.05ms | 1550903.70 tokens/sec
Step 2039 | loss: 3.717496 | lr:5.9310e-04 | norm 0.3490 | dt 338.32ms | 1549664.32 tokens/sec
Step 2040 | loss: 3.665253 | lr:5.9309e-04 | norm 0.3301 | dt 337.68ms | 1552610.82 tokens/sec
Step 2041 | loss: 3.668561 | lr:5.9308e-04 | norm 0.3368 | dt 338.13ms | 1550561.42 tokens/sec
Step 2042 | loss: 3.663084 | lr:5.9307e-04 | norm 0.3429 | dt 337.41ms | 1553865.90 tokens/sec
Step 2043 | loss: 3.646061 | lr:5.9306e-04 | norm 0.3288 | dt 337.42ms | 1553792.34 tokens/sec
Step 2044 | loss: 3.665553 | lr:5.9305e-04 | norm 0.3306 | dt 338.26ms | 1549949.40 tokens/sec
Step 2045 | loss: 3.650471 | lr:5.9304e-04 | norm 0.3525 | dt 338.26ms | 1549945.03 tokens/sec
Step 2046 | loss: 3.630143 | lr:5.9303e-04 | norm 0.3400 | dt 338.11ms | 1550647.79 tokens/sec
Step 2047 | loss: 3.652834 | lr:5.9302e-04 | norm 0.3904 | dt 338.07ms | 1550839.17 tokens/sec
Step 2048 | loss: 3.740819 | lr:5.9301e-04 | norm 0.3574 | dt 337.86ms | 1551800.04 tokens/sec
Step 2049 | loss: 3.680356 | lr:5.9300e-04 | norm 0.3078 | dt 337.96ms | 1551309.61 tokens/sec
Step 2050 | loss: 3.666020 | lr:5.9298e-04 | norm 0.3566 | dt 338.02ms | 1551038.25 tokens/sec
Step 2051 | loss: 3.631249 | lr:5.9297e-04 | norm 0.3266 | dt 337.79ms | 1552135.21 tokens/sec
Step 2052 | loss: 3.627367 | lr:5.9296e-04 | norm 0.3334 | dt 338.17ms | 1550375.57 tokens/sec
Step 2053 | loss: 3.640323 | lr:5.9295e-04 | norm 0.3372 | dt 338.95ms | 1546812.75 tokens/sec
Step 2054 | loss: 3.640870 | lr:5.9294e-04 | norm 0.3046 | dt 339.41ms | 1544687.47 tokens/sec
Step 2055 | loss: 3.640125 | lr:5.9293e-04 | norm 0.3296 | dt 337.85ms | 1551846.04 tokens/sec
Step 2056 | loss: 3.712598 | lr:5.9292e-04 | norm 0.3947 | dt 338.62ms | 1548301.54 tokens/sec
Step 2057 | loss: 3.646792 | lr:5.9291e-04 | norm 0.3888 | dt 338.31ms | 1549713.46 tokens/sec
Step 2058 | loss: 3.695140 | lr:5.9290e-04 | norm 0.3254 | dt 338.92ms | 1546953.12 tokens/sec
Step 2059 | loss: 3.606308 | lr:5.9289e-04 | norm 0.2998 | dt 338.25ms | 1550017.13 tokens/sec
Step 2060 | loss: 3.671103 | lr:5.9288e-04 | norm 0.3109 | dt 337.55ms | 1553211.78 tokens/sec
Step 2061 | loss: 3.670538 | lr:5.9287e-04 | norm 0.3045 | dt 339.36ms | 1544912.11 tokens/sec
Step 2062 | loss: 3.589514 | lr:5.9286e-04 | norm 0.3054 | dt 338.57ms | 1548543.59 tokens/sec
Step 2063 | loss: 3.677006 | lr:5.9285e-04 | norm 0.3374 | dt 338.70ms | 1547960.40 tokens/sec
Step 2064 | loss: 3.654286 | lr:5.9284e-04 | norm 0.3636 | dt 339.29ms | 1545251.90 tokens/sec
Step 2065 | loss: 3.609439 | lr:5.9283e-04 | norm 0.3676 | dt 337.55ms | 1553227.14 tokens/sec
Step 2066 | loss: 3.623924 | lr:5.9282e-04 | norm 0.3165 | dt 337.62ms | 1552887.11 tokens/sec
Step 2067 | loss: 3.554956 | lr:5.9281e-04 | norm 0.3103 | dt 338.05ms | 1550916.83 tokens/sec
Step 2068 | loss: 3.610919 | lr:5.9279e-04 | norm 0.3388 | dt 338.19ms | 1550299.06 tokens/sec
Step 2069 | loss: 3.586453 | lr:5.9278e-04 | norm 0.4060 | dt 337.77ms | 1552204.23 tokens/sec
Step 2070 | loss: 3.572377 | lr:5.9277e-04 | norm 0.3814 | dt 337.05ms | 1555517.93 tokens/sec
Step 2071 | loss: 3.561295 | lr:5.9276e-04 | norm 0.3238 | dt 339.04ms | 1546397.23 tokens/sec
Step 2072 | loss: 3.590577 | lr:5.9275e-04 | norm 0.3516 | dt 339.31ms | 1545138.98 tokens/sec
Step 2073 | loss: 3.614425 | lr:5.9274e-04 | norm 0.3279 | dt 338.17ms | 1550374.48 tokens/sec
Step 2074 | loss: 3.582992 | lr:5.9273e-04 | norm 0.2959 | dt 338.28ms | 1549846.71 tokens/sec
Step 2075 | loss: 3.581511 | lr:5.9272e-04 | norm 0.2661 | dt 339.15ms | 1545899.34 tokens/sec
Step 2076 | loss: 3.532405 | lr:5.9271e-04 | norm 0.2672 | dt 339.03ms | 1546435.29 tokens/sec
Step 2077 | loss: 3.608161 | lr:5.9270e-04 | norm 0.2696 | dt 340.69ms | 1538917.08 tokens/sec
Step 2078 | loss: 3.544375 | lr:5.9269e-04 | norm 0.2884 | dt 912.63ms | 574483.15 tokens/sec
Step 2079 | loss: 3.577103 | lr:5.9268e-04 | norm 0.2981 | dt 335.40ms | 1563160.73 tokens/sec
Step 2080 | loss: 3.487331 | lr:5.9267e-04 | norm 0.2920 | dt 338.46ms | 1549019.18 tokens/sec
Step 2081 | loss: 3.543577 | lr:5.9266e-04 | norm 0.2926 | dt 341.01ms | 1537436.61 tokens/sec
Step 2082 | loss: 3.544043 | lr:5.9265e-04 | norm 0.2896 | dt 339.00ms | 1546567.98 tokens/sec
Step 2083 | loss: 3.526078 | lr:5.9264e-04 | norm 0.3101 | dt 338.39ms | 1549368.43 tokens/sec
Step 2084 | loss: 3.537180 | lr:5.9262e-04 | norm 0.2769 | dt 340.44ms | 1540017.45 tokens/sec
Step 2085 | loss: 3.642629 | lr:5.9261e-04 | norm 0.2918 | dt 338.77ms | 1547617.23 tokens/sec
Step 2086 | loss: 3.686091 | lr:5.9260e-04 | norm 0.3012 | dt 338.23ms | 1550083.78 tokens/sec
Step 2087 | loss: 3.705269 | lr:5.9259e-04 | norm 0.2762 | dt 339.47ms | 1544419.50 tokens/sec
Step 2088 | loss: 3.660726 | lr:5.9258e-04 | norm 0.3165 | dt 339.13ms | 1545969.98 tokens/sec
Step 2089 | loss: 3.676592 | lr:5.9257e-04 | norm 0.4127 | dt 1004.80ms | 521783.64 tokens/sec
Step 2090 | loss: 3.618008 | lr:5.9256e-04 | norm 0.5178 | dt 337.04ms | 1555565.25 tokens/sec
Step 2091 | loss: 3.691403 | lr:5.9255e-04 | norm 0.4815 | dt 339.63ms | 1543714.79 tokens/sec
Step 2092 | loss: 3.760870 | lr:5.9254e-04 | norm 0.4106 | dt 338.06ms | 1550858.86 tokens/sec
Step 2093 | loss: 3.703538 | lr:5.9253e-04 | norm 0.4665 | dt 337.22ms | 1554725.00 tokens/sec
Step 2094 | loss: 3.670293 | lr:5.9252e-04 | norm 0.4023 | dt 337.50ms | 1553457.56 tokens/sec
Step 2095 | loss: 3.629152 | lr:5.9251e-04 | norm 0.3619 | dt 337.38ms | 1553994.38 tokens/sec
Step 2096 | loss: 3.708007 | lr:5.9250e-04 | norm 0.3098 | dt 337.61ms | 1552920.01 tokens/sec
Step 2097 | loss: 3.676323 | lr:5.9248e-04 | norm 0.3038 | dt 337.24ms | 1554650.26 tokens/sec
Step 2098 | loss: 3.591290 | lr:5.9247e-04 | norm 0.2882 | dt 337.87ms | 1551755.15 tokens/sec
Step 2099 | loss: 3.624035 | lr:5.9246e-04 | norm 0.2521 | dt 337.80ms | 1552084.82 tokens/sec
Step 2100 | loss: 3.647509 | lr:5.9245e-04 | norm 0.2926 | dt 336.95ms | 1555959.29 tokens/sec
Step 2101 | loss: 3.583585 | lr:5.9244e-04 | norm 0.3095 | dt 337.56ms | 1553171.19 tokens/sec
Step 2102 | loss: 3.638881 | lr:5.9243e-04 | norm 0.2947 | dt 337.50ms | 1553432.32 tokens/sec
Step 2103 | loss: 3.593327 | lr:5.9242e-04 | norm 0.2832 | dt 338.18ms | 1550303.43 tokens/sec
Step 2104 | loss: 3.657649 | lr:5.9241e-04 | norm 0.2801 | dt 337.70ms | 1552511.07 tokens/sec
Step 2105 | loss: 3.647767 | lr:5.9240e-04 | norm 0.2879 | dt 338.17ms | 1550388.69 tokens/sec
Step 2106 | loss: 3.618978 | lr:5.9239e-04 | norm 0.2889 | dt 338.24ms | 1550067.39 tokens/sec
Step 2107 | loss: 3.650969 | lr:5.9238e-04 | norm 0.2778 | dt 338.19ms | 1550283.76 tokens/sec
Step 2108 | loss: 3.596038 | lr:5.9236e-04 | norm 0.2969 | dt 338.23ms | 1550080.50 tokens/sec
Step 2109 | loss: 3.536085 | lr:5.9235e-04 | norm 0.2651 | dt 337.57ms | 1553110.85 tokens/sec
Step 2110 | loss: 3.573716 | lr:5.9234e-04 | norm 0.2548 | dt 337.91ms | 1551577.78 tokens/sec
Step 2111 | loss: 3.549462 | lr:5.9233e-04 | norm 0.3140 | dt 338.02ms | 1551067.79 tokens/sec
Step 2112 | loss: 3.529550 | lr:5.9232e-04 | norm 0.3302 | dt 338.51ms | 1548803.16 tokens/sec
Step 2113 | loss: 3.583199 | lr:5.9231e-04 | norm 0.3277 | dt 339.11ms | 1546052.58 tokens/sec
Step 2114 | loss: 3.535638 | lr:5.9230e-04 | norm 0.3259 | dt 339.46ms | 1544471.57 tokens/sec
Step 2115 | loss: 3.562810 | lr:5.9229e-04 | norm 0.2705 | dt 338.01ms | 1551101.70 tokens/sec
Step 2116 | loss: 3.563766 | lr:5.9228e-04 | norm 0.2955 | dt 338.84ms | 1547310.15 tokens/sec
Step 2117 | loss: 3.582629 | lr:5.9227e-04 | norm 0.3179 | dt 338.25ms | 1549988.73 tokens/sec
Step 2118 | loss: 3.572437 | lr:5.9226e-04 | norm 0.3095 | dt 338.04ms | 1550972.61 tokens/sec
Step 2119 | loss: 3.547028 | lr:5.9224e-04 | norm 0.3085 | dt 338.68ms | 1548021.42 tokens/sec
Step 2120 | loss: 3.534527 | lr:5.9223e-04 | norm 0.3061 | dt 338.76ms | 1547649.91 tokens/sec
Step 2121 | loss: 3.529869 | lr:5.9222e-04 | norm 0.3046 | dt 338.55ms | 1548639.55 tokens/sec
Step 2122 | loss: 3.535580 | lr:5.9221e-04 | norm 0.2802 | dt 338.32ms | 1549662.13 tokens/sec
Step 2123 | loss: 3.507698 | lr:5.9220e-04 | norm 0.2735 | dt 338.49ms | 1548888.25 tokens/sec
Step 2124 | loss: 3.522395 | lr:5.9219e-04 | norm 0.2944 | dt 337.98ms | 1551249.42 tokens/sec
Step 2125 | loss: 3.549289 | lr:5.9218e-04 | norm 0.2998 | dt 338.47ms | 1548991.90 tokens/sec
Step 2126 | loss: 3.472315 | lr:5.9217e-04 | norm 0.3192 | dt 338.03ms | 1551022.93 tokens/sec
Step 2127 | loss: 3.562532 | lr:5.9216e-04 | norm 0.3632 | dt 338.08ms | 1550758.24 tokens/sec
Step 2128 | loss: 3.416920 | lr:5.9214e-04 | norm 0.3184 | dt 338.43ms | 1549162.13 tokens/sec
Step 2129 | loss: 3.511148 | lr:5.9213e-04 | norm 0.3198 | dt 338.46ms | 1549034.46 tokens/sec
Step 2130 | loss: 3.431764 | lr:5.9212e-04 | norm 0.3090 | dt 337.34ms | 1554172.30 tokens/sec
Step 2131 | loss: 3.636359 | lr:5.9211e-04 | norm 0.3307 | dt 338.00ms | 1551134.53 tokens/sec
Step 2132 | loss: 3.655530 | lr:5.9210e-04 | norm 0.3418 | dt 339.46ms | 1544455.30 tokens/sec
Step 2133 | loss: 3.690476 | lr:5.9209e-04 | norm 0.3698 | dt 338.28ms | 1549867.47 tokens/sec
Step 2134 | loss: 3.679266 | lr:5.9208e-04 | norm 0.3922 | dt 337.75ms | 1552273.26 tokens/sec
Step 2135 | loss: 3.820267 | lr:5.9207e-04 | norm 0.5625 | dt 337.70ms | 1552538.47 tokens/sec
Step 2136 | loss: 3.664014 | lr:5.9206e-04 | norm 0.5895 | dt 338.61ms | 1548356.05 tokens/sec
Step 2137 | loss: 3.665718 | lr:5.9205e-04 | norm 0.4447 | dt 338.12ms | 1550598.59 tokens/sec
Step 2138 | loss: 3.616287 | lr:5.9203e-04 | norm 0.3622 | dt 338.20ms | 1550224.75 tokens/sec
Step 2139 | loss: 3.646260 | lr:5.9202e-04 | norm 0.3168 | dt 337.76ms | 1552236.00 tokens/sec
Step 2140 | loss: 3.664704 | lr:5.9201e-04 | norm 0.3111 | dt 337.96ms | 1551309.61 tokens/sec
Step 2141 | loss: 3.690604 | lr:5.9200e-04 | norm 0.3068 | dt 338.74ms | 1547752.30 tokens/sec
Step 2142 | loss: 3.676609 | lr:5.9199e-04 | norm 0.2783 | dt 338.28ms | 1549856.54 tokens/sec
Step 2143 | loss: 3.683241 | lr:5.9198e-04 | norm 0.2756 | dt 338.15ms | 1550476.14 tokens/sec
Step 2144 | loss: 3.610577 | lr:5.9197e-04 | norm 0.2604 | dt 338.09ms | 1550735.27 tokens/sec
Step 2145 | loss: 3.604867 | lr:5.9196e-04 | norm 0.2670 | dt 337.78ms | 1552170.27 tokens/sec
Step 2146 | loss: 3.605237 | lr:5.9194e-04 | norm 0.2734 | dt 338.56ms | 1548571.94 tokens/sec
Step 2147 | loss: 3.626472 | lr:5.9193e-04 | norm 0.2668 | dt 336.90ms | 1556190.52 tokens/sec
Step 2148 | loss: 3.695899 | lr:5.9192e-04 | norm 0.2988 | dt 338.21ms | 1550179.94 tokens/sec
Step 2149 | loss: 3.663486 | lr:5.9191e-04 | norm 0.3035 | dt 338.47ms | 1548996.27 tokens/sec
Step 2150 | loss: 3.608733 | lr:5.9190e-04 | norm 0.2956 | dt 337.28ms | 1554479.92 tokens/sec
Step 2151 | loss: 3.647460 | lr:5.9189e-04 | norm 0.2904 | dt 337.83ms | 1551946.80 tokens/sec
Step 2152 | loss: 3.630597 | lr:5.9188e-04 | norm 0.2819 | dt 337.37ms | 1554038.31 tokens/sec
Step 2153 | loss: 3.628678 | lr:5.9187e-04 | norm 0.2728 | dt 338.16ms | 1550394.16 tokens/sec
Step 2154 | loss: 3.566536 | lr:5.9185e-04 | norm 0.2831 | dt 338.17ms | 1550353.71 tokens/sec
Step 2155 | loss: 3.559624 | lr:5.9184e-04 | norm 0.3014 | dt 337.56ms | 1553183.25 tokens/sec
Step 2156 | loss: 3.512246 | lr:5.9183e-04 | norm 0.2600 | dt 337.98ms | 1551237.38 tokens/sec
Step 2157 | loss: 3.518899 | lr:5.9182e-04 | norm 0.2787 | dt 338.71ms | 1547896.11 tokens/sec
Step 2158 | loss: 3.545947 | lr:5.9181e-04 | norm 0.3024 | dt 337.42ms | 1553791.24 tokens/sec
Step 2159 | loss: 3.578545 | lr:5.9180e-04 | norm 0.3160 | dt 337.09ms | 1555330.90 tokens/sec
Step 2160 | loss: 3.525842 | lr:5.9179e-04 | norm 0.2980 | dt 338.81ms | 1547451.70 tokens/sec
Step 2161 | loss: 3.500026 | lr:5.9178e-04 | norm 0.2648 | dt 337.36ms | 1554086.63 tokens/sec
Step 2162 | loss: 3.545727 | lr:5.9176e-04 | norm 0.2657 | dt 337.23ms | 1554690.93 tokens/sec
Step 2163 | loss: 3.548587 | lr:5.9175e-04 | norm 0.2776 | dt 337.91ms | 1551573.40 tokens/sec
Step 2164 | loss: 3.617700 | lr:5.9174e-04 | norm 0.3249 | dt 337.38ms | 1554009.75 tokens/sec
Step 2165 | loss: 3.547091 | lr:5.9173e-04 | norm 0.3241 | dt 337.60ms | 1553006.65 tokens/sec
Step 2166 | loss: 3.458827 | lr:5.9172e-04 | norm 0.3075 | dt 337.60ms | 1553006.65 tokens/sec
Step 2167 | loss: 3.520770 | lr:5.9171e-04 | norm 0.3152 | dt 337.39ms | 1553966.92 tokens/sec
Step 2168 | loss: 3.414536 | lr:5.9170e-04 | norm 0.3089 | dt 338.34ms | 1549605.35 tokens/sec
Step 2169 | loss: 3.474233 | lr:5.9168e-04 | norm 0.3336 | dt 338.70ms | 1547964.76 tokens/sec
Step 2170 | loss: 3.487883 | lr:5.9167e-04 | norm 0.3269 | dt 337.55ms | 1553207.39 tokens/sec
Step 2171 | loss: 3.405593 | lr:5.9166e-04 | norm 0.3216 | dt 337.86ms | 1551785.81 tokens/sec
Step 2172 | loss: 3.490642 | lr:5.9165e-04 | norm 0.3174 | dt 338.39ms | 1549372.79 tokens/sec
Step 2173 | loss: 3.422559 | lr:5.9164e-04 | norm 0.3421 | dt 337.40ms | 1553899.94 tokens/sec
Step 2174 | loss: 3.518413 | lr:5.9163e-04 | norm 0.2918 | dt 337.82ms | 1551953.37 tokens/sec
Step 2175 | loss: 3.487483 | lr:5.9162e-04 | norm 0.3291 | dt 337.39ms | 1553929.59 tokens/sec
Step 2176 | loss: 3.425704 | lr:5.9161e-04 | norm 0.3213 | dt 337.85ms | 1551829.61 tokens/sec
Step 2177 | loss: 3.452013 | lr:5.9159e-04 | norm 0.3488 | dt 338.00ms | 1551147.66 tokens/sec
Step 2178 | loss: 3.646096 | lr:5.9158e-04 | norm 0.3030 | dt 337.90ms | 1551588.73 tokens/sec
Step 2179 | loss: 3.655705 | lr:5.9157e-04 | norm 0.3235 | dt 338.92ms | 1546954.21 tokens/sec
Step 2180 | loss: 3.660779 | lr:5.9156e-04 | norm 0.3539 | dt 337.45ms | 1553662.80 tokens/sec
Step 2181 | loss: 3.859025 | lr:5.9155e-04 | norm 0.3956 | dt 337.52ms | 1553375.26 tokens/sec
Step 2182 | loss: 3.633012 | lr:5.9154e-04 | norm 0.4045 | dt 337.58ms | 1553060.40 tokens/sec
Step 2183 | loss: 3.718353 | lr:5.9152e-04 | norm 0.5026 | dt 338.83ms | 1547329.74 tokens/sec
Step 2184 | loss: 3.667659 | lr:5.9151e-04 | norm 0.4470 | dt 337.94ms | 1551431.09 tokens/sec
Step 2185 | loss: 3.648521 | lr:5.9150e-04 | norm 0.3698 | dt 337.67ms | 1552672.21 tokens/sec
Step 2186 | loss: 3.637635 | lr:5.9149e-04 | norm 0.3283 | dt 337.18ms | 1554917.39 tokens/sec
Step 2187 | loss: 3.653802 | lr:5.9148e-04 | norm 0.3305 | dt 339.22ms | 1545561.43 tokens/sec
Step 2188 | loss: 3.639293 | lr:5.9147e-04 | norm 0.3184 | dt 338.70ms | 1547945.14 tokens/sec
Step 2189 | loss: 3.628955 | lr:5.9146e-04 | norm 0.3071 | dt 337.94ms | 1551430.00 tokens/sec
Step 2190 | loss: 3.618866 | lr:5.9144e-04 | norm 0.2613 | dt 338.22ms | 1550120.93 tokens/sec
Step 2191 | loss: 3.663357 | lr:5.9143e-04 | norm 0.2557 | dt 337.75ms | 1552298.46 tokens/sec
Step 2192 | loss: 3.644983 | lr:5.9142e-04 | norm 0.2541 | dt 337.59ms | 1553030.78 tokens/sec
Step 2193 | loss: 3.558776 | lr:5.9141e-04 | norm 0.2923 | dt 338.53ms | 1548708.27 tokens/sec
Step 2194 | loss: 3.644691 | lr:5.9140e-04 | norm 0.2714 | dt 337.95ms | 1551361.05 tokens/sec
Step 2195 | loss: 3.597322 | lr:5.9139e-04 | norm 0.2700 | dt 337.33ms | 1554209.65 tokens/sec
Step 2196 | loss: 3.642453 | lr:5.9137e-04 | norm 0.2837 | dt 337.96ms | 1551317.27 tokens/sec
Step 2197 | loss: 3.607570 | lr:5.9136e-04 | norm 0.2674 | dt 337.91ms | 1551553.69 tokens/sec
Step 2198 | loss: 3.616827 | lr:5.9135e-04 | norm 0.2628 | dt 338.39ms | 1549379.34 tokens/sec
Step 2199 | loss: 3.609289 | lr:5.9134e-04 | norm 0.2892 | dt 338.12ms | 1550576.72 tokens/sec
Step 2200 | loss: 3.609147 | lr:5.9133e-04 | norm 0.2523 | dt 337.82ms | 1551997.18 tokens/sec
Step 2201 | loss: 3.562438 | lr:5.9132e-04 | norm 0.2611 | dt 337.58ms | 1553054.91 tokens/sec
Step 2202 | loss: 3.490331 | lr:5.9131e-04 | norm 0.2676 | dt 337.71ms | 1552458.46 tokens/sec
Step 2203 | loss: 3.595926 | lr:5.9129e-04 | norm 0.2758 | dt 338.11ms | 1550623.74 tokens/sec
Step 2204 | loss: 3.526164 | lr:5.9128e-04 | norm 0.2971 | dt 338.07ms | 1550828.23 tokens/sec
Step 2205 | loss: 3.530936 | lr:5.9127e-04 | norm 0.3093 | dt 337.32ms | 1554295.33 tokens/sec
Step 2206 | loss: 3.509377 | lr:5.9126e-04 | norm 0.3424 | dt 337.23ms | 1554706.32 tokens/sec
Step 2207 | loss: 3.586161 | lr:5.9125e-04 | norm 0.3683 | dt 338.69ms | 1548009.44 tokens/sec
Step 2208 | loss: 3.488669 | lr:5.9124e-04 | norm 0.3261 | dt 338.76ms | 1547690.21 tokens/sec
Step 2209 | loss: 3.547735 | lr:5.9122e-04 | norm 0.3479 | dt 337.50ms | 1553446.58 tokens/sec
Step 2210 | loss: 3.511470 | lr:5.9121e-04 | norm 0.3359 | dt 337.41ms | 1553850.53 tokens/sec
Step 2211 | loss: 3.556463 | lr:5.9120e-04 | norm 0.2743 | dt 338.29ms | 1549820.50 tokens/sec
Step 2212 | loss: 3.516797 | lr:5.9119e-04 | norm 0.3010 | dt 338.20ms | 1550219.28 tokens/sec
Step 2213 | loss: 3.527609 | lr:5.9118e-04 | norm 0.2658 | dt 338.94ms | 1546860.63 tokens/sec
Step 2214 | loss: 3.500734 | lr:5.9117e-04 | norm 0.2715 | dt 337.75ms | 1552305.04 tokens/sec
Step 2215 | loss: 3.498967 | lr:5.9115e-04 | norm 0.2897 | dt 338.41ms | 1549284.37 tokens/sec
Step 2216 | loss: 3.456855 | lr:5.9114e-04 | norm 0.3031 | dt 338.81ms | 1547421.21 tokens/sec
Step 2217 | loss: 3.431417 | lr:5.9113e-04 | norm 0.3148 | dt 338.35ms | 1549557.30 tokens/sec
Step 2218 | loss: 3.516494 | lr:5.9112e-04 | norm 0.3385 | dt 338.24ms | 1550043.35 tokens/sec
Step 2219 | loss: 3.480964 | lr:5.9111e-04 | norm 0.3258 | dt 338.76ms | 1547689.12 tokens/sec
Step 2220 | loss: 3.493638 | lr:5.9109e-04 | norm 0.3192 | dt 337.86ms | 1551804.42 tokens/sec
Step 2221 | loss: 3.490523 | lr:5.9108e-04 | norm 0.3007 | dt 338.59ms | 1548444.36 tokens/sec
Step 2222 | loss: 3.494120 | lr:5.9107e-04 | norm 0.3089 | dt 338.46ms | 1549026.82 tokens/sec
Step 2223 | loss: 3.499596 | lr:5.9106e-04 | norm 0.3124 | dt 338.21ms | 1550163.55 tokens/sec
Step 2224 | loss: 3.482533 | lr:5.9105e-04 | norm 0.3084 | dt 338.56ms | 1548597.02 tokens/sec
Step 2225 | loss: 3.654844 | lr:5.9104e-04 | norm 0.2842 | dt 337.88ms | 1551690.55 tokens/sec
Step 2226 | loss: 3.668761 | lr:5.9102e-04 | norm 0.3027 | dt 338.16ms | 1550416.02 tokens/sec
Step 2227 | loss: 3.620702 | lr:5.9101e-04 | norm 0.3084 | dt 338.62ms | 1548317.89 tokens/sec
Step 2228 | loss: 3.690094 | lr:5.9100e-04 | norm 0.3721 | dt 337.55ms | 1553219.46 tokens/sec
Step 2229 | loss: 3.609492 | lr:5.9099e-04 | norm 0.3848 | dt 337.14ms | 1555094.42 tokens/sec
Step 2230 | loss: 3.590811 | lr:5.9098e-04 | norm 0.3666 | dt 337.76ms | 1552229.43 tokens/sec
Step 2231 | loss: 3.676169 | lr:5.9096e-04 | norm 0.3306 | dt 338.87ms | 1547187.13 tokens/sec
Step 2232 | loss: 3.656260 | lr:5.9095e-04 | norm 0.3406 | dt 337.82ms | 1551967.61 tokens/sec
Step 2233 | loss: 3.604051 | lr:5.9094e-04 | norm 0.3298 | dt 338.50ms | 1548848.98 tokens/sec
Step 2234 | loss: 3.601723 | lr:5.9093e-04 | norm 0.3246 | dt 338.50ms | 1548862.07 tokens/sec
Step 2235 | loss: 3.597448 | lr:5.9092e-04 | norm 0.3336 | dt 338.70ms | 1547961.49 tokens/sec
Step 2236 | loss: 3.660963 | lr:5.9091e-04 | norm 0.3389 | dt 338.06ms | 1550850.11 tokens/sec
Step 2237 | loss: 3.577644 | lr:5.9089e-04 | norm 0.3119 | dt 338.41ms | 1549250.54 tokens/sec
Step 2238 | loss: 3.643190 | lr:5.9088e-04 | norm 0.3159 | dt 337.97ms | 1551272.40 tokens/sec
Step 2239 | loss: 3.620047 | lr:5.9087e-04 | norm 0.3083 | dt 338.29ms | 1549811.76 tokens/sec
Step 2240 | loss: 3.594908 | lr:5.9086e-04 | norm 0.2608 | dt 338.38ms | 1549384.80 tokens/sec
Step 2241 | loss: 3.668969 | lr:5.9085e-04 | norm 0.2849 | dt 338.74ms | 1547742.50 tokens/sec
Step 2242 | loss: 3.592548 | lr:5.9083e-04 | norm 0.2943 | dt 337.15ms | 1555062.53 tokens/sec
Step 2243 | loss: 3.541060 | lr:5.9082e-04 | norm 0.3432 | dt 337.47ms | 1553569.50 tokens/sec
Step 2244 | loss: 3.571480 | lr:5.9081e-04 | norm 0.3320 | dt 339.60ms | 1543838.34 tokens/sec
Step 2245 | loss: 3.611362 | lr:5.9080e-04 | norm 0.2994 | dt 337.54ms | 1553273.22 tokens/sec
Step 2246 | loss: 3.618219 | lr:5.9079e-04 | norm 0.2695 | dt 338.19ms | 1550285.95 tokens/sec
Step 2247 | loss: 3.530099 | lr:5.9077e-04 | norm 0.2744 | dt 338.32ms | 1549690.53 tokens/sec
Step 2248 | loss: 3.507766 | lr:5.9076e-04 | norm 0.2995 | dt 338.06ms | 1550853.39 tokens/sec
Step 2249 | loss: 3.596322 | lr:5.9075e-04 | norm 0.3089 | dt 338.23ms | 1550092.52 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 2250: 3.6185
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2632/10042=0.2621


ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, trying to find words that are difficult to read and understand them in a novel, in a novel.
There are lots
rank 2 sample 1 >Hello, I'm a language model, but I understand how different the language was so different.
In English, the word "L", or the word "
rank 2 sample 2 >Hello, I'm a language model, so I'll show you a little explanation of the concept of how language processing affects your writing style at least in depth with
rank 2 sample 3 >Hello, I'm a language model, but in my experience there is a lot of discussion with language-based languages. It is more prevalent than in any other




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I think it's just a bit crazy, so I think it's good for language processing. That's because I
rank 4 sample 1 >Hello, I'm a language model, at first. Like many languages on the web, they contain a lot of data but do some pretty useful functions such as
rank 4 sample 2 >Hello, I'm a language model, I're all too busy thinking about my language.
When my children are old, the time and context of your child
rank 4 sample 3 >Hello, I'm a language model, and you learned many words, and also you will do a whole class math and business analysis on some topics. There's




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I was able to work with my system. I know most of my ideas lie ahead of me, and I do
rank 7 sample 1 >Hello, I'm a language model, having trouble in the classroom, you couldn't figure out how to build a language classroom using this method. So you just
rank 7 sample 2 >Hello, I'm a language model, where you have already mapped out other classes.
My language is called MARY, which is a kind of network where
rank 7 sample 3 >Hello, I'm a language model, no one wants to make anything worse while playing this game. I've learned a lot about the "word" pattern.




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this isn't one of them. One thing, it's something called a 'semi' where a string of
rank 5 sample 1 >Hello, I'm a language model, using Java with its GUI (I'll do in the first article). I've had some experience with some things in the
rank 5 sample 2 >Hello, I'm a language model, so you get the feel of reading some of the language.
Now that we're on the journey of learning, what
rank 5 sample 3 >Hello, I'm a language model, there is a special version of this language, the word 'Hello' in that this means "Yes" and "That




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I'd like you to do everything yourself.
And that sounds as it sounds and music like that, but the
rank 0 sample 1 >Hello, I'm a language model, but how do you use it for computer programming in business context?
(I don't know how well, but I
rank 0 sample 2 >Hello, I'm a language model, so I found this language I learnt earlier. We went through an experiment and found it as a whole.
The lesson
rank 0 sample 3 >Hello, I'm a language model, which says that we can have the letters of several different letters, just like in each letters. So for example, for




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, or you can call my or your school, or if your language does, that's a game.
I'm pretty
rank 6 sample 1 >Hello, I'm a language model, which looks and feels as though they're making it work.
If this is the case, the model can be installed
rank 6 sample 2 >Hello, I'm a language model, but there is no other model that makes me believe that I have this one more than in any other language. For me
rank 6 sample 3 >Hello, I'm a language model, and they were taught. I'll look at them in a future newspaper. And they were on the first computer's head




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not familiar with it...
The "Biological Model A" in C language modeling comes in two major

ddp_rank 1: ####### Printing generated samples ####### 


rank 3 sample 1 >Hello, I'm a language model, and a few of the real life applications that come with this. I'm not a language model, but I was anrank 1 sample 0 >Hello, I'm a language model, an international forum for people who are at a different pace working on a project.
"It really is an international forum

rank 1 sample 1 >Hello, I'm a language model, a model, a model, a model, a model . . . . the thing you're given is that you can
rank 3 sample 2 >Hello, I'm a language model, and it can be tricky to translate a document back to a particular language, because one of the words in a system called
rank 1 sample 2 >Hello, I'm a language model, but on that occasion, I'm not a language model or my language. We're not language. It is very straightforward
rank 3 sample 3 >Hello, I'm a language model, and as you go, you are essentially the most common way you are seeing the future. As you make your way around


rank 1 sample 3 >Hello, I'm a language model, and I'm interested to consider a specific field of study based on two factors.
Well, you can create a simple


Step 2250 | loss: 3.474047 | lr:5.9074e-04 | norm 0.3267 | dt 18638.75ms | 28128.93 tokens/sec
Step 2251 | loss: 3.487182 | lr:5.9073e-04 | norm 0.3008 | dt 334.52ms | 1567272.82 tokens/sec
Step 2252 | loss: 3.503487 | lr:5.9071e-04 | norm 0.4168 | dt 335.23ms | 1563973.41 tokens/sec
Step 2253 | loss: 3.604276 | lr:5.9070e-04 | norm 0.4365 | dt 336.30ms | 1559007.16 tokens/sec
Step 2254 | loss: 3.483121 | lr:5.9069e-04 | norm 0.4242 | dt 337.19ms | 1554863.51 tokens/sec
Step 2255 | loss: 3.649301 | lr:5.9068e-04 | norm 0.3747 | dt 337.42ms | 1553816.49 tokens/sec
Step 2256 | loss: 3.526114 | lr:5.9067e-04 | norm 0.3652 | dt 336.52ms | 1557958.97 tokens/sec
Step 2257 | loss: 3.525090 | lr:5.9065e-04 | norm 0.2925 | dt 337.28ms | 1554439.26 tokens/sec
Step 2258 | loss: 3.500267 | lr:5.9064e-04 | norm 0.2848 | dt 335.81ms | 1561269.60 tokens/sec
Step 2259 | loss: 3.539903 | lr:5.9063e-04 | norm 0.2853 | dt 336.58ms | 1557681.97 tokens/sec
Step 2260 | loss: 3.452674 | lr:5.9062e-04 | norm 0.3026 | dt 337.46ms | 1553622.19 tokens/sec
Step 2261 | loss: 3.490692 | lr:5.9061e-04 | norm 0.2844 | dt 336.01ms | 1560329.07 tokens/sec
Step 2262 | loss: 3.473057 | lr:5.9059e-04 | norm 0.2661 | dt 336.59ms | 1557642.25 tokens/sec
Step 2263 | loss: 3.524316 | lr:5.9058e-04 | norm 0.2773 | dt 337.39ms | 1553940.57 tokens/sec
Step 2264 | loss: 3.677514 | lr:5.9057e-04 | norm 0.3816 | dt 336.87ms | 1556353.53 tokens/sec
Step 2265 | loss: 3.520901 | lr:5.9056e-04 | norm 0.5488 | dt 336.53ms | 1557907.10 tokens/sec
Step 2266 | loss: 3.476227 | lr:5.9055e-04 | norm 0.5872 | dt 337.22ms | 1554757.98 tokens/sec
Step 2267 | loss: 3.535393 | lr:5.9053e-04 | norm 0.4494 | dt 1006.65ms | 520825.15 tokens/sec
Step 2268 | loss: 3.494349 | lr:5.9052e-04 | norm 0.3771 | dt 335.60ms | 1562258.99 tokens/sec
Step 2269 | loss: 3.581510 | lr:5.9051e-04 | norm 0.3332 | dt 336.15ms | 1559673.92 tokens/sec
Step 2270 | loss: 3.490250 | lr:5.9050e-04 | norm 0.3604 | dt 337.28ms | 1554437.07 tokens/sec
Step 2271 | loss: 3.507648 | lr:5.9048e-04 | norm 0.3594 | dt 336.46ms | 1558257.05 tokens/sec
Step 2272 | loss: 3.616753 | lr:5.9047e-04 | norm 0.3690 | dt 336.36ms | 1558704.38 tokens/sec
Step 2273 | loss: 3.634600 | lr:5.9046e-04 | norm 0.3139 | dt 337.46ms | 1553638.65 tokens/sec
Step 2274 | loss: 3.706292 | lr:5.9045e-04 | norm 0.3050 | dt 336.79ms | 1556743.56 tokens/sec
Step 2275 | loss: 3.633874 | lr:5.9044e-04 | norm 0.2869 | dt 336.94ms | 1556023.14 tokens/sec
Step 2276 | loss: 3.619267 | lr:5.9042e-04 | norm 0.3744 | dt 336.85ms | 1556444.96 tokens/sec
Step 2277 | loss: 3.634348 | lr:5.9041e-04 | norm 0.2978 | dt 337.09ms | 1555355.10 tokens/sec
Step 2278 | loss: 3.641705 | lr:5.9040e-04 | norm 0.4437 | dt 337.33ms | 1554211.85 tokens/sec
Step 2279 | loss: 3.661159 | lr:5.9039e-04 | norm 0.2752 | dt 1040.25ms | 504002.59 tokens/sec
Step 2280 | loss: 3.597437 | lr:5.9037e-04 | norm 0.2836 | dt 334.27ms | 1568461.11 tokens/sec
Step 2281 | loss: 3.683327 | lr:5.9036e-04 | norm 0.2525 | dt 337.15ms | 1555036.14 tokens/sec
Step 2282 | loss: 3.647002 | lr:5.9035e-04 | norm 0.2581 | dt 337.07ms | 1555443.11 tokens/sec
Step 2283 | loss: 3.593718 | lr:5.9034e-04 | norm 0.2597 | dt 337.11ms | 1555240.70 tokens/sec
Step 2284 | loss: 3.622337 | lr:5.9033e-04 | norm 0.2509 | dt 337.09ms | 1555324.30 tokens/sec
Step 2285 | loss: 3.589601 | lr:5.9031e-04 | norm 0.2617 | dt 336.95ms | 1556000.02 tokens/sec
Step 2286 | loss: 3.679907 | lr:5.9030e-04 | norm 0.2658 | dt 337.51ms | 1553418.05 tokens/sec
Step 2287 | loss: 3.616471 | lr:5.9029e-04 | norm 0.2727 | dt 337.81ms | 1552022.38 tokens/sec
Step 2288 | loss: 3.671971 | lr:5.9028e-04 | norm 0.2744 | dt 337.66ms | 1552724.83 tokens/sec
Step 2289 | loss: 3.650882 | lr:5.9026e-04 | norm 0.2858 | dt 338.35ms | 1549533.28 tokens/sec
Step 2290 | loss: 3.601900 | lr:5.9025e-04 | norm 0.2776 | dt 337.07ms | 1555415.61 tokens/sec
Step 2291 | loss: 3.570030 | lr:5.9024e-04 | norm 0.2973 | dt 338.89ms | 1547093.52 tokens/sec
Step 2292 | loss: 3.604175 | lr:5.9023e-04 | norm 0.3199 | dt 337.81ms | 1552001.56 tokens/sec
Step 2293 | loss: 3.519902 | lr:5.9022e-04 | norm 0.3148 | dt 337.40ms | 1553921.90 tokens/sec
Step 2294 | loss: 3.536143 | lr:5.9020e-04 | norm 0.3251 | dt 340.10ms | 1541564.50 tokens/sec
Step 2295 | loss: 3.542038 | lr:5.9019e-04 | norm 0.3308 | dt 337.42ms | 1553816.49 tokens/sec
Step 2296 | loss: 3.511350 | lr:5.9018e-04 | norm 0.3197 | dt 337.67ms | 1552666.73 tokens/sec
Step 2297 | loss: 3.517592 | lr:5.9017e-04 | norm 0.3200 | dt 339.23ms | 1545506.03 tokens/sec
Step 2298 | loss: 3.465092 | lr:5.9015e-04 | norm 0.3072 | dt 337.61ms | 1552955.11 tokens/sec
Step 2299 | loss: 3.547746 | lr:5.9014e-04 | norm 0.3238 | dt 337.74ms | 1552325.86 tokens/sec
Step 2300 | loss: 3.496235 | lr:5.9013e-04 | norm 0.3311 | dt 338.35ms | 1549531.10 tokens/sec
Step 2301 | loss: 3.565172 | lr:5.9012e-04 | norm 0.3194 | dt 338.42ms | 1549245.08 tokens/sec
Step 2302 | loss: 3.547249 | lr:5.9010e-04 | norm 0.2802 | dt 338.70ms | 1547944.05 tokens/sec
Step 2303 | loss: 3.554193 | lr:5.9009e-04 | norm 0.2842 | dt 338.04ms | 1550948.55 tokens/sec
Step 2304 | loss: 3.608553 | lr:5.9008e-04 | norm 0.3283 | dt 337.94ms | 1551423.43 tokens/sec
Step 2305 | loss: 3.472210 | lr:5.9007e-04 | norm 0.3190 | dt 338.70ms | 1547920.08 tokens/sec
Step 2306 | loss: 3.442977 | lr:5.9005e-04 | norm 0.3688 | dt 338.35ms | 1549547.48 tokens/sec
Step 2307 | loss: 3.489256 | lr:5.9004e-04 | norm 0.2840 | dt 338.19ms | 1550289.23 tokens/sec
Step 2308 | loss: 3.468007 | lr:5.9003e-04 | norm 0.2635 | dt 338.57ms | 1548520.69 tokens/sec
Step 2309 | loss: 3.463424 | lr:5.9002e-04 | norm 0.2805 | dt 338.25ms | 1550010.58 tokens/sec
Step 2310 | loss: 3.461843 | lr:5.9000e-04 | norm 0.2816 | dt 338.96ms | 1546738.77 tokens/sec
Step 2311 | loss: 3.477646 | lr:5.8999e-04 | norm 0.3098 | dt 338.24ms | 1550067.39 tokens/sec
Step 2312 | loss: 3.489930 | lr:5.8998e-04 | norm 0.3182 | dt 338.26ms | 1549957.04 tokens/sec
Step 2313 | loss: 3.502490 | lr:5.8997e-04 | norm 0.3308 | dt 338.16ms | 1550407.27 tokens/sec
Step 2314 | loss: 3.464066 | lr:5.8995e-04 | norm 0.3373 | dt 338.29ms | 1549808.48 tokens/sec
Step 2315 | loss: 3.464658 | lr:5.8994e-04 | norm 0.3826 | dt 339.12ms | 1546021.06 tokens/sec
Step 2316 | loss: 3.470655 | lr:5.8993e-04 | norm 0.3988 | dt 338.83ms | 1547340.63 tokens/sec
Step 2317 | loss: 3.574785 | lr:5.8992e-04 | norm 0.3199 | dt 338.20ms | 1550208.35 tokens/sec
Step 2318 | loss: 3.633461 | lr:5.8990e-04 | norm 0.2947 | dt 338.17ms | 1550352.62 tokens/sec
Step 2319 | loss: 3.632064 | lr:5.8989e-04 | norm 0.2858 | dt 339.32ms | 1545115.10 tokens/sec
Step 2320 | loss: 3.628552 | lr:5.8988e-04 | norm 0.2892 | dt 338.29ms | 1549804.11 tokens/sec
Step 2321 | loss: 3.618358 | lr:5.8987e-04 | norm 0.2694 | dt 338.38ms | 1549427.38 tokens/sec
Step 2322 | loss: 3.662098 | lr:5.8985e-04 | norm 0.2796 | dt 338.68ms | 1548051.94 tokens/sec
Step 2323 | loss: 3.614868 | lr:5.8984e-04 | norm 0.2718 | dt 338.30ms | 1549787.73 tokens/sec
Step 2324 | loss: 3.647058 | lr:5.8983e-04 | norm 0.2874 | dt 338.69ms | 1547982.19 tokens/sec
Step 2325 | loss: 3.793619 | lr:5.8982e-04 | norm 0.3017 | dt 338.41ms | 1549281.10 tokens/sec
Step 2326 | loss: 3.620505 | lr:5.8980e-04 | norm 0.2914 | dt 338.53ms | 1548708.27 tokens/sec
Step 2327 | loss: 3.610641 | lr:5.8979e-04 | norm 0.2979 | dt 338.68ms | 1548045.40 tokens/sec
Step 2328 | loss: 3.595389 | lr:5.8978e-04 | norm 0.2936 | dt 338.76ms | 1547690.21 tokens/sec
Step 2329 | loss: 3.585758 | lr:5.8977e-04 | norm 0.2789 | dt 338.42ms | 1549203.61 tokens/sec
Step 2330 | loss: 3.585389 | lr:5.8975e-04 | norm 0.2845 | dt 339.23ms | 1545536.44 tokens/sec
Step 2331 | loss: 3.706971 | lr:5.8974e-04 | norm 0.3115 | dt 339.47ms | 1544409.74 tokens/sec
Step 2332 | loss: 3.630681 | lr:5.8973e-04 | norm 0.3141 | dt 339.32ms | 1545122.70 tokens/sec
Step 2333 | loss: 3.577821 | lr:5.8972e-04 | norm 0.3116 | dt 339.08ms | 1546228.69 tokens/sec
Step 2334 | loss: 3.560178 | lr:5.8970e-04 | norm 0.2863 | dt 339.51ms | 1544251.40 tokens/sec
Step 2335 | loss: 3.576911 | lr:5.8969e-04 | norm 0.3225 | dt 339.60ms | 1543824.25 tokens/sec
Step 2336 | loss: 3.568696 | lr:5.8968e-04 | norm 0.3356 | dt 338.40ms | 1549314.94 tokens/sec
Step 2337 | loss: 3.617918 | lr:5.8967e-04 | norm 0.3142 | dt 338.44ms | 1549151.22 tokens/sec
Step 2338 | loss: 3.571342 | lr:5.8965e-04 | norm 0.2871 | dt 339.05ms | 1546348.30 tokens/sec
Step 2339 | loss: 3.493755 | lr:5.8964e-04 | norm 0.2750 | dt 337.33ms | 1554220.63 tokens/sec
Step 2340 | loss: 3.525274 | lr:5.8963e-04 | norm 0.2870 | dt 338.72ms | 1547869.96 tokens/sec
Step 2341 | loss: 3.504250 | lr:5.8961e-04 | norm 0.2775 | dt 339.21ms | 1545624.44 tokens/sec
Step 2342 | loss: 3.517449 | lr:5.8960e-04 | norm 0.3033 | dt 337.99ms | 1551205.65 tokens/sec
Step 2343 | loss: 3.541723 | lr:5.8959e-04 | norm 0.3107 | dt 338.16ms | 1550411.64 tokens/sec
Step 2344 | loss: 3.502800 | lr:5.8958e-04 | norm 0.2875 | dt 338.31ms | 1549709.09 tokens/sec
Step 2345 | loss: 3.506540 | lr:5.8956e-04 | norm 0.3055 | dt 337.86ms | 1551782.52 tokens/sec
Step 2346 | loss: 3.451962 | lr:5.8955e-04 | norm 0.3205 | dt 338.72ms | 1547862.34 tokens/sec
Step 2347 | loss: 3.508616 | lr:5.8954e-04 | norm 0.3071 | dt 338.42ms | 1549211.25 tokens/sec
Step 2348 | loss: 3.511648 | lr:5.8953e-04 | norm 0.2929 | dt 338.55ms | 1548649.37 tokens/sec
Step 2349 | loss: 3.600997 | lr:5.8951e-04 | norm 0.3243 | dt 337.83ms | 1551947.89 tokens/sec
Step 2350 | loss: 3.505236 | lr:5.8950e-04 | norm 0.3009 | dt 338.07ms | 1550804.17 tokens/sec
Step 2351 | loss: 3.462273 | lr:5.8949e-04 | norm 0.2833 | dt 338.21ms | 1550162.46 tokens/sec
Step 2352 | loss: 3.458285 | lr:5.8947e-04 | norm 0.2923 | dt 338.38ms | 1549404.45 tokens/sec
Step 2353 | loss: 3.434554 | lr:5.8946e-04 | norm 0.3039 | dt 338.00ms | 1551152.03 tokens/sec
Step 2354 | loss: 3.440754 | lr:5.8945e-04 | norm 0.2725 | dt 337.88ms | 1551699.30 tokens/sec
Step 2355 | loss: 3.457966 | lr:5.8944e-04 | norm 0.2560 | dt 337.70ms | 1552536.28 tokens/sec
Step 2356 | loss: 3.471680 | lr:5.8942e-04 | norm 0.2742 | dt 339.11ms | 1546075.41 tokens/sec
Step 2357 | loss: 3.411178 | lr:5.8941e-04 | norm 0.2848 | dt 337.52ms | 1553345.63 tokens/sec
Step 2358 | loss: 3.427178 | lr:5.8940e-04 | norm 0.2961 | dt 337.83ms | 1551925.99 tokens/sec
Step 2359 | loss: 3.431847 | lr:5.8939e-04 | norm 0.2932 | dt 339.86ms | 1542638.35 tokens/sec
Step 2360 | loss: 3.466872 | lr:5.8937e-04 | norm 0.2906 | dt 338.08ms | 1550761.52 tokens/sec
Step 2361 | loss: 3.466843 | lr:5.8936e-04 | norm 0.2913 | dt 338.03ms | 1551004.34 tokens/sec
Step 2362 | loss: 3.575066 | lr:5.8935e-04 | norm 0.2783 | dt 338.17ms | 1550354.81 tokens/sec
Step 2363 | loss: 3.656517 | lr:5.8933e-04 | norm 0.2762 | dt 338.20ms | 1550228.02 tokens/sec
Step 2364 | loss: 3.644699 | lr:5.8932e-04 | norm 0.2955 | dt 338.64ms | 1548207.79 tokens/sec
Step 2365 | loss: 3.637082 | lr:5.8931e-04 | norm 0.3236 | dt 338.04ms | 1550970.43 tokens/sec
Step 2366 | loss: 3.616042 | lr:5.8930e-04 | norm 0.3819 | dt 338.39ms | 1549344.41 tokens/sec
Step 2367 | loss: 3.619596 | lr:5.8928e-04 | norm 0.3494 | dt 338.32ms | 1549687.25 tokens/sec
Step 2368 | loss: 3.592964 | lr:5.8927e-04 | norm 0.3140 | dt 338.26ms | 1549951.58 tokens/sec
Step 2369 | loss: 3.598201 | lr:5.8926e-04 | norm 0.2944 | dt 338.25ms | 1550023.69 tokens/sec
Step 2370 | loss: 3.618241 | lr:5.8924e-04 | norm 0.3406 | dt 337.50ms | 1553434.51 tokens/sec
Step 2371 | loss: 3.769722 | lr:5.8923e-04 | norm 0.4092 | dt 338.15ms | 1550472.86 tokens/sec
Step 2372 | loss: 3.607438 | lr:5.8922e-04 | norm 0.4095 | dt 338.75ms | 1547735.96 tokens/sec
Step 2373 | loss: 3.597744 | lr:5.8920e-04 | norm 0.3358 | dt 337.86ms | 1551807.71 tokens/sec
Step 2374 | loss: 3.519057 | lr:5.8919e-04 | norm 0.3367 | dt 337.80ms | 1552058.52 tokens/sec
Step 2375 | loss: 3.568984 | lr:5.8918e-04 | norm 0.3357 | dt 338.92ms | 1546934.62 tokens/sec
Step 2376 | loss: 3.592527 | lr:5.8917e-04 | norm 0.3523 | dt 338.30ms | 1549770.25 tokens/sec
Step 2377 | loss: 3.590436 | lr:5.8915e-04 | norm 0.3179 | dt 338.51ms | 1548822.80 tokens/sec
Step 2378 | loss: 3.683197 | lr:5.8914e-04 | norm 0.2830 | dt 337.99ms | 1551199.08 tokens/sec
Step 2379 | loss: 3.548337 | lr:5.8913e-04 | norm 0.2891 | dt 337.66ms | 1552701.81 tokens/sec
Step 2380 | loss: 3.547800 | lr:5.8911e-04 | norm 0.2813 | dt 337.84ms | 1551863.56 tokens/sec
Step 2381 | loss: 3.608900 | lr:5.8910e-04 | norm 0.2882 | dt 339.07ms | 1546249.35 tokens/sec
Step 2382 | loss: 3.585093 | lr:5.8909e-04 | norm 0.2921 | dt 337.55ms | 1553230.43 tokens/sec
Step 2383 | loss: 3.535330 | lr:5.8907e-04 | norm 0.2783 | dt 338.11ms | 1550662.01 tokens/sec
Step 2384 | loss: 3.476529 | lr:5.8906e-04 | norm 0.2824 | dt 337.73ms | 1552403.66 tokens/sec
Step 2385 | loss: 3.537698 | lr:5.8905e-04 | norm 0.2754 | dt 338.26ms | 1549975.62 tokens/sec
Step 2386 | loss: 3.536287 | lr:5.8904e-04 | norm 0.2869 | dt 338.35ms | 1549559.49 tokens/sec
Step 2387 | loss: 3.567323 | lr:5.8902e-04 | norm 0.2495 | dt 337.75ms | 1552294.08 tokens/sec
Step 2388 | loss: 3.437789 | lr:5.8901e-04 | norm 0.2793 | dt 338.31ms | 1549729.84 tokens/sec
Step 2389 | loss: 3.501627 | lr:5.8900e-04 | norm 0.2923 | dt 337.98ms | 1551248.33 tokens/sec
Step 2390 | loss: 3.485611 | lr:5.8898e-04 | norm 0.2851 | dt 337.72ms | 1552423.39 tokens/sec
Step 2391 | loss: 3.487880 | lr:5.8897e-04 | norm 0.2794 | dt 338.52ms | 1548756.26 tokens/sec
Step 2392 | loss: 3.465883 | lr:5.8896e-04 | norm 0.2506 | dt 338.00ms | 1551127.96 tokens/sec
Step 2393 | loss: 3.504204 | lr:5.8894e-04 | norm 0.2676 | dt 338.12ms | 1550585.47 tokens/sec
Step 2394 | loss: 3.558897 | lr:5.8893e-04 | norm 0.3114 | dt 339.19ms | 1545725.47 tokens/sec
Step 2395 | loss: 3.546398 | lr:5.8892e-04 | norm 0.3266 | dt 337.35ms | 1554159.12 tokens/sec
Step 2396 | loss: 3.433316 | lr:5.8891e-04 | norm 0.3268 | dt 338.12ms | 1550604.06 tokens/sec
Step 2397 | loss: 3.404898 | lr:5.8889e-04 | norm 0.3135 | dt 338.28ms | 1549849.99 tokens/sec
Step 2398 | loss: 3.479934 | lr:5.8888e-04 | norm 0.3043 | dt 338.41ms | 1549282.19 tokens/sec
Step 2399 | loss: 3.429954 | lr:5.8887e-04 | norm 0.3524 | dt 338.15ms | 1550459.74 tokens/sec
Step 2400 | loss: 3.370291 | lr:5.8885e-04 | norm 0.3666 | dt 338.91ms | 1546990.12 tokens/sec
Step 2401 | loss: 3.389689 | lr:5.8884e-04 | norm 0.3608 | dt 338.62ms | 1548315.71 tokens/sec
Step 2402 | loss: 3.408390 | lr:5.8883e-04 | norm 0.3200 | dt 338.06ms | 1550881.83 tokens/sec
Step 2403 | loss: 3.418358 | lr:5.8881e-04 | norm 0.3326 | dt 338.35ms | 1549521.27 tokens/sec
Step 2404 | loss: 3.422506 | lr:5.8880e-04 | norm 0.2967 | dt 338.44ms | 1549117.39 tokens/sec
Step 2405 | loss: 3.440120 | lr:5.8879e-04 | norm 0.2973 | dt 338.33ms | 1549651.21 tokens/sec
Step 2406 | loss: 3.490800 | lr:5.8877e-04 | norm 0.2813 | dt 338.16ms | 1550424.76 tokens/sec
Step 2407 | loss: 3.498419 | lr:5.8876e-04 | norm 0.2867 | dt 338.61ms | 1548356.05 tokens/sec
Step 2408 | loss: 3.560840 | lr:5.8875e-04 | norm 0.2965 | dt 338.32ms | 1549703.63 tokens/sec
Step 2409 | loss: 3.603545 | lr:5.8873e-04 | norm 0.2860 | dt 337.49ms | 1553475.12 tokens/sec
Step 2410 | loss: 3.562346 | lr:5.8872e-04 | norm 0.2577 | dt 338.40ms | 1549333.49 tokens/sec
Step 2411 | loss: 3.572656 | lr:5.8871e-04 | norm 0.2870 | dt 338.27ms | 1549930.83 tokens/sec
Step 2412 | loss: 3.592169 | lr:5.8869e-04 | norm 0.3043 | dt 338.47ms | 1548972.26 tokens/sec
Step 2413 | loss: 3.589170 | lr:5.8868e-04 | norm 0.3097 | dt 337.66ms | 1552710.58 tokens/sec
Step 2414 | loss: 3.535465 | lr:5.8867e-04 | norm 0.2901 | dt 337.73ms | 1552401.47 tokens/sec
Step 2415 | loss: 3.614101 | lr:5.8865e-04 | norm 0.2810 | dt 339.23ms | 1545515.81 tokens/sec
Step 2416 | loss: 3.596897 | lr:5.8864e-04 | norm 0.2944 | dt 337.79ms | 1552124.25 tokens/sec
Step 2417 | loss: 3.584472 | lr:5.8863e-04 | norm 0.2984 | dt 337.83ms | 1551919.42 tokens/sec
Step 2418 | loss: 3.560774 | lr:5.8861e-04 | norm 0.2893 | dt 338.82ms | 1547408.14 tokens/sec
Step 2419 | loss: 3.583429 | lr:5.8860e-04 | norm 0.2991 | dt 338.07ms | 1550810.73 tokens/sec
Step 2420 | loss: 3.633168 | lr:5.8859e-04 | norm 0.2828 | dt 337.69ms | 1552581.22 tokens/sec
Step 2421 | loss: 3.559589 | lr:5.8858e-04 | norm 0.2960 | dt 338.50ms | 1548858.80 tokens/sec
Step 2422 | loss: 3.573341 | lr:5.8856e-04 | norm 0.3404 | dt 338.27ms | 1549894.78 tokens/sec
Step 2423 | loss: 3.575302 | lr:5.8855e-04 | norm 0.3514 | dt 337.91ms | 1551552.60 tokens/sec
Step 2424 | loss: 3.591724 | lr:5.8854e-04 | norm 0.3587 | dt 338.95ms | 1546777.94 tokens/sec
Step 2425 | loss: 3.564670 | lr:5.8852e-04 | norm 0.3450 | dt 338.31ms | 1549706.91 tokens/sec
Step 2426 | loss: 3.589320 | lr:5.8851e-04 | norm 0.2773 | dt 337.74ms | 1552342.29 tokens/sec
Step 2427 | loss: 3.612087 | lr:5.8850e-04 | norm 0.3519 | dt 338.24ms | 1550024.78 tokens/sec
Step 2428 | loss: 3.629562 | lr:5.8848e-04 | norm 0.4166 | dt 338.12ms | 1550598.59 tokens/sec
Step 2429 | loss: 3.678097 | lr:5.8847e-04 | norm 0.4464 | dt 338.03ms | 1550994.49 tokens/sec
Step 2430 | loss: 3.586543 | lr:5.8846e-04 | norm 0.4315 | dt 338.20ms | 1550222.56 tokens/sec
Step 2431 | loss: 3.557538 | lr:5.8844e-04 | norm 0.3507 | dt 337.86ms | 1551808.81 tokens/sec
Step 2432 | loss: 3.529486 | lr:5.8843e-04 | norm 0.3110 | dt 338.11ms | 1550660.91 tokens/sec
Step 2433 | loss: 3.523298 | lr:5.8841e-04 | norm 0.2994 | dt 338.16ms | 1550420.39 tokens/sec
Step 2434 | loss: 3.497341 | lr:5.8840e-04 | norm 0.2760 | dt 338.77ms | 1547629.21 tokens/sec
Step 2435 | loss: 3.539946 | lr:5.8839e-04 | norm 0.2848 | dt 337.67ms | 1552667.82 tokens/sec
Step 2436 | loss: 3.466825 | lr:5.8837e-04 | norm 0.2614 | dt 338.45ms | 1549089.02 tokens/sec
Step 2437 | loss: 3.501017 | lr:5.8836e-04 | norm 0.2654 | dt 338.37ms | 1549429.56 tokens/sec
Step 2438 | loss: 3.535325 | lr:5.8835e-04 | norm 0.2836 | dt 338.31ms | 1549747.32 tokens/sec
Step 2439 | loss: 3.523753 | lr:5.8833e-04 | norm 0.2598 | dt 337.17ms | 1554985.56 tokens/sec
Step 2440 | loss: 3.513280 | lr:5.8832e-04 | norm 0.2958 | dt 338.59ms | 1548448.72 tokens/sec
Step 2441 | loss: 3.524557 | lr:5.8831e-04 | norm 0.3203 | dt 338.84ms | 1547314.50 tokens/sec
Step 2442 | loss: 3.452901 | lr:5.8829e-04 | norm 0.3219 | dt 337.73ms | 1552410.24 tokens/sec
Step 2443 | loss: 3.493051 | lr:5.8828e-04 | norm 0.2998 | dt 338.45ms | 1549077.01 tokens/sec
Step 2444 | loss: 3.456431 | lr:5.8827e-04 | norm 0.3137 | dt 337.92ms | 1551508.81 tokens/sec
Step 2445 | loss: 3.408799 | lr:5.8825e-04 | norm 0.3032 | dt 338.42ms | 1549241.81 tokens/sec
Step 2446 | loss: 3.399600 | lr:5.8824e-04 | norm 0.2691 | dt 338.32ms | 1549669.78 tokens/sec
Step 2447 | loss: 3.484238 | lr:5.8823e-04 | norm 0.2746 | dt 337.79ms | 1552122.06 tokens/sec
Step 2448 | loss: 3.490494 | lr:5.8821e-04 | norm 0.2675 | dt 337.38ms | 1553984.49 tokens/sec
Step 2449 | loss: 3.399625 | lr:5.8820e-04 | norm 0.2641 | dt 338.30ms | 1549759.33 tokens/sec
Step 2450 | loss: 3.445815 | lr:5.8819e-04 | norm 0.2796 | dt 337.95ms | 1551388.41 tokens/sec
Step 2451 | loss: 3.387911 | lr:5.8817e-04 | norm 0.2811 | dt 339.58ms | 1543926.13 tokens/sec
Step 2452 | loss: 3.415694 | lr:5.8816e-04 | norm 0.2682 | dt 338.67ms | 1548099.89 tokens/sec
Step 2453 | loss: 3.446481 | lr:5.8815e-04 | norm 0.2944 | dt 338.81ms | 1547453.87 tokens/sec
Step 2454 | loss: 3.395497 | lr:5.8813e-04 | norm 0.2408 | dt 338.81ms | 1547416.85 tokens/sec
Step 2455 | loss: 3.490988 | lr:5.8812e-04 | norm 0.2592 | dt 338.09ms | 1550726.52 tokens/sec
Step 2456 | loss: 3.604574 | lr:5.8810e-04 | norm 0.2566 | dt 1023.48ms | 512259.69 tokens/sec
Step 2457 | loss: 3.544606 | lr:5.8809e-04 | norm 0.2679 | dt 335.52ms | 1562598.69 tokens/sec
Step 2458 | loss: 3.578609 | lr:5.8808e-04 | norm 0.2895 | dt 338.12ms | 1550580.00 tokens/sec
Step 2459 | loss: 3.584402 | lr:5.8806e-04 | norm 0.3073 | dt 338.44ms | 1549128.30 tokens/sec
Step 2460 | loss: 3.566957 | lr:5.8805e-04 | norm 0.3509 | dt 337.64ms | 1552823.51 tokens/sec
Step 2461 | loss: 3.585043 | lr:5.8804e-04 | norm 0.3605 | dt 338.02ms | 1551062.32 tokens/sec
Step 2462 | loss: 3.612329 | lr:5.8802e-04 | norm 0.3167 | dt 337.70ms | 1552509.97 tokens/sec
Step 2463 | loss: 3.578960 | lr:5.8801e-04 | norm 0.3138 | dt 338.25ms | 1549985.45 tokens/sec
Step 2464 | loss: 3.519469 | lr:5.8800e-04 | norm 0.2685 | dt 338.34ms | 1549600.98 tokens/sec
Step 2465 | loss: 3.568789 | lr:5.8798e-04 | norm 0.2755 | dt 339.11ms | 1546078.67 tokens/sec
Step 2466 | loss: 3.570608 | lr:5.8797e-04 | norm 0.2978 | dt 338.89ms | 1547094.61 tokens/sec
Step 2467 | loss: 3.516522 | lr:5.8796e-04 | norm 0.2903 | dt 339.12ms | 1546028.67 tokens/sec
Step 2468 | loss: 3.564532 | lr:5.8794e-04 | norm 0.3388 | dt 338.84ms | 1547318.86 tokens/sec
Step 2469 | loss: 3.562119 | lr:5.8793e-04 | norm 0.3498 | dt 1023.34ms | 512328.07 tokens/sec
Step 2470 | loss: 3.513933 | lr:5.8791e-04 | norm 0.3328 | dt 337.69ms | 1552550.53 tokens/sec
Step 2471 | loss: 3.550317 | lr:5.8790e-04 | norm 0.3241 | dt 340.25ms | 1540891.53 tokens/sec
Step 2472 | loss: 3.576373 | lr:5.8789e-04 | norm 0.2860 | dt 339.07ms | 1546268.92 tokens/sec
Step 2473 | loss: 3.571683 | lr:5.8787e-04 | norm 0.2913 | dt 338.86ms | 1547203.46 tokens/sec
Step 2474 | loss: 3.583467 | lr:5.8786e-04 | norm 0.2822 | dt 338.39ms | 1549380.43 tokens/sec
Step 2475 | loss: 3.527760 | lr:5.8785e-04 | norm 0.2803 | dt 338.91ms | 1546990.12 tokens/sec
Step 2476 | loss: 3.567142 | lr:5.8783e-04 | norm 0.3154 | dt 339.68ms | 1543491.58 tokens/sec
Step 2477 | loss: 3.513358 | lr:5.8782e-04 | norm 0.3181 | dt 339.05ms | 1546338.51 tokens/sec
Step 2478 | loss: 3.475013 | lr:5.8780e-04 | norm 0.3628 | dt 339.08ms | 1546187.38 tokens/sec
Step 2479 | loss: 3.515874 | lr:5.8779e-04 | norm 0.3584 | dt 339.49ms | 1544360.93 tokens/sec
Step 2480 | loss: 3.516747 | lr:5.8778e-04 | norm 0.3266 | dt 339.22ms | 1545577.72 tokens/sec
Step 2481 | loss: 3.513748 | lr:5.8776e-04 | norm 0.3352 | dt 339.40ms | 1544736.30 tokens/sec
Step 2482 | loss: 3.523033 | lr:5.8775e-04 | norm 0.2663 | dt 339.58ms | 1543953.23 tokens/sec
Step 2483 | loss: 3.504845 | lr:5.8774e-04 | norm 0.2927 | dt 338.95ms | 1546785.55 tokens/sec
Step 2484 | loss: 3.505632 | lr:5.8772e-04 | norm 0.2990 | dt 339.26ms | 1545401.76 tokens/sec
Step 2485 | loss: 3.450134 | lr:5.8771e-04 | norm 0.2769 | dt 340.04ms | 1541860.66 tokens/sec
Step 2486 | loss: 3.559840 | lr:5.8769e-04 | norm 0.3018 | dt 339.13ms | 1545992.80 tokens/sec
Step 2487 | loss: 3.516544 | lr:5.8768e-04 | norm 0.3208 | dt 339.78ms | 1543038.86 tokens/sec
Step 2488 | loss: 3.540017 | lr:5.8767e-04 | norm 0.2739 | dt 339.60ms | 1543856.76 tokens/sec
Step 2489 | loss: 3.553196 | lr:5.8765e-04 | norm 0.3223 | dt 338.90ms | 1547032.57 tokens/sec
Step 2490 | loss: 3.450349 | lr:5.8764e-04 | norm 0.3020 | dt 339.24ms | 1545458.24 tokens/sec
Step 2491 | loss: 3.436221 | lr:5.8763e-04 | norm 0.3060 | dt 338.99ms | 1546632.16 tokens/sec
Step 2492 | loss: 3.476780 | lr:5.8761e-04 | norm 0.3171 | dt 339.07ms | 1546260.22 tokens/sec
Step 2493 | loss: 3.409895 | lr:5.8760e-04 | norm 0.3031 | dt 339.80ms | 1542952.25 tokens/sec
Step 2494 | loss: 3.459055 | lr:5.8758e-04 | norm 0.2988 | dt 338.80ms | 1547464.76 tokens/sec
Step 2495 | loss: 3.432172 | lr:5.8757e-04 | norm 0.3331 | dt 338.08ms | 1550766.99 tokens/sec
Step 2496 | loss: 3.384482 | lr:5.8756e-04 | norm 0.3232 | dt 338.81ms | 1547431.01 tokens/sec
Step 2497 | loss: 3.521844 | lr:5.8754e-04 | norm 0.3221 | dt 338.04ms | 1550961.67 tokens/sec
Step 2498 | loss: 3.364197 | lr:5.8753e-04 | norm 0.3474 | dt 338.14ms | 1550502.38 tokens/sec
Step 2499 | loss: 3.409737 | lr:5.8751e-04 | norm 0.3567 | dt 338.10ms | 1550669.66 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 2500: 3.5771
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2656/10042=0.2645


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm going into that article of speech and speech. To start things is to ask how well. I know some
rank 3 sample 1 >Hello, I'm a language model, and this one is a series about a language model. It's not a language model, but is a framework where a
rank 3 sample 2 >Hello, I'm a language model, and it’s about getting a set of language skills from the beginning! They’re all about using that
rank 3 sample 3 >Hello, I'm a language model, and many of these are considered in a way that would give you an insight into different concepts and areas across different domains.




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, so we'll be building a small database to help you learn this in real life.
I have a big job to
rank 5 sample 1 >Hello, I'm a language model, to say we want to see if our data could be interpreted from and we don't have a map at once, but
rank 5 sample 2 >Hello, I'm a language model, but it still doesn't have the correct pronunciation, but it's good for reading the same language.
I'm sorry
rank 5 sample 3 >Hello, I'm a language model, there is an interaction between them, and it's like the human hand. This happens to be some sort of language model




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, or the one for that.
A model can be any language model or a data model. It can range from the
rank 2 sample 1 >Hello, I'm a language model, this is where it is a little girl by the age of 13. They are all born by age five, and each
rank 2 sample 2 >Hello, I'm a language model, but I've always used it for our analysis. I'm very happy to know this so, when you get in to
rank 2 sample 3 >Hello, I'm a language model, so the process is not easy. But since I had no problems with the approach, I made mistakes and had no thought




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'd like to talk about it one way. But I'm getting good hands on language projects like that. I
rank 7 sample 1 >Hello, I'm a language model, really very easy to understand, that language is very very sophisticated and not very simple on all the other two sides, I
rank 7 sample 2 >Hello, I'm a language model, and I am so tired of asking and “does this language be any other language?” It is so hard
rank 7 sample 3 >Hello, I'm a language model, having been taught in a different language as a result of the Second World War. The study of Language Modeling was inspired




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I don't want to say so on now.
But here it has to be known what the languages are.
rank 0 sample 1 >Hello, I'm a language model, so a whole lot of people use in their work right now, I wanted to do it, but most people don't
rank 0 sample 2 >Hello, I'm a language model, but I wasn't kidding with this one. What happens when people speak and talk about each other?
I think everyone
rank 0 sample 3 >Hello, I'm a language model, and as I was writing my book, it taught me to write sentences with sentences similar to my first ones. It became




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and then it's built.
Coding can help simplify operations, simplify problem-solving or even solve problems.
rank 1 sample 1 >Hello, I'm a language model, which I have seen over the years. What I like this is that I like this since it didn’t exist
rank 1 sample 2 >Hello, I'm a language model, so anyone interested in learning Python can follow me on Twitter and see how they will be used. The language models that are
rank 1 sample 3 >Hello, I'm a language model, and I'm pretty happy if I get along. I started studying to code just like every other language model. The first




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I didn't even need to be fluent. I'm just adding more idioms of Spanish, such as: "
rank 4 sample 1 >Hello, I'm a language model, who knows how there are multiple expressions. This helps explain everything about the whole. This kind of language makes me learn to
rank 4 sample 2 >Hello, I'm a language model, I see it was created so that when it was created, that the object was introduced by the child without the aid of
rank 4 sample 3 >Hello, I'm a language model, and you shouldn't need to know one, you do not need a technical knowledge to understand the code, but to be




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, for my students to know to make use of the same word, like this one.
Let's start by asking each
rank 6 sample 1 >Hello, I'm a language model, a web site that uses a framework that looks like that.
A simple example is a Web site, that the creator
rank 6 sample 2 >Hello, I'm a language model, so this is the second of two ways I would be able to get the information I want to use by creating your model
rank 6 sample 3 >Hello, I'm a language model, and a couple of questions. How can you know what I mean by this, if it starts with 2 words each person


Step 2500 | loss: 3.438669 | lr:5.8750e-04 | norm 0.3223 | dt 18875.46ms | 27776.16 tokens/sec
Step 2501 | loss: 3.409473 | lr:5.8749e-04 | norm 0.3139 | dt 333.99ms | 1569758.77 tokens/sec
Step 2502 | loss: 3.610726 | lr:5.8747e-04 | norm 0.2957 | dt 335.41ms | 1563120.73 tokens/sec
Step 2503 | loss: 3.546011 | lr:5.8746e-04 | norm 0.2907 | dt 336.40ms | 1558522.10 tokens/sec
Step 2504 | loss: 3.563557 | lr:5.8745e-04 | norm 0.2759 | dt 336.52ms | 1557979.94 tokens/sec
Step 2505 | loss: 3.581816 | lr:5.8743e-04 | norm 0.2962 | dt 336.52ms | 1557956.76 tokens/sec
Step 2506 | loss: 3.553499 | lr:5.8742e-04 | norm 0.2831 | dt 336.10ms | 1559901.84 tokens/sec
Step 2507 | loss: 3.576167 | lr:5.8740e-04 | norm 0.2939 | dt 336.98ms | 1555845.90 tokens/sec
Step 2508 | loss: 3.560772 | lr:5.8739e-04 | norm 0.2836 | dt 336.31ms | 1558959.64 tokens/sec
Step 2509 | loss: 3.575079 | lr:5.8738e-04 | norm 0.3203 | dt 336.85ms | 1556452.67 tokens/sec
Step 2510 | loss: 3.644551 | lr:5.8736e-04 | norm 0.3196 | dt 337.08ms | 1555371.60 tokens/sec
Step 2511 | loss: 3.608065 | lr:5.8735e-04 | norm 0.3548 | dt 337.71ms | 1552470.52 tokens/sec
Step 2512 | loss: 3.552102 | lr:5.8733e-04 | norm 0.3422 | dt 335.65ms | 1562018.18 tokens/sec
Step 2513 | loss: 3.569383 | lr:5.8732e-04 | norm 0.3314 | dt 336.61ms | 1557544.06 tokens/sec
Step 2514 | loss: 3.543663 | lr:5.8731e-04 | norm 0.3117 | dt 338.27ms | 1549925.36 tokens/sec
Step 2515 | loss: 3.540223 | lr:5.8729e-04 | norm 0.3037 | dt 336.99ms | 1555807.37 tokens/sec
Step 2516 | loss: 3.594486 | lr:5.8728e-04 | norm 0.2813 | dt 338.35ms | 1549525.64 tokens/sec
Step 2517 | loss: 3.491761 | lr:5.8726e-04 | norm 0.3029 | dt 337.68ms | 1552627.26 tokens/sec
Step 2518 | loss: 3.597479 | lr:5.8725e-04 | norm 0.2797 | dt 337.80ms | 1552078.24 tokens/sec
Step 2519 | loss: 3.679451 | lr:5.8724e-04 | norm 0.3233 | dt 337.32ms | 1554259.08 tokens/sec
Step 2520 | loss: 3.528780 | lr:5.8722e-04 | norm 0.2913 | dt 338.27ms | 1549890.41 tokens/sec
Step 2521 | loss: 3.525397 | lr:5.8721e-04 | norm 0.3080 | dt 337.28ms | 1554442.56 tokens/sec
Step 2522 | loss: 3.552090 | lr:5.8719e-04 | norm 0.3166 | dt 336.85ms | 1556461.48 tokens/sec
Step 2523 | loss: 3.494827 | lr:5.8718e-04 | norm 0.3206 | dt 337.66ms | 1552708.39 tokens/sec
Step 2524 | loss: 3.501024 | lr:5.8717e-04 | norm 0.3088 | dt 338.43ms | 1549154.49 tokens/sec
Step 2525 | loss: 3.473397 | lr:5.8715e-04 | norm 0.2902 | dt 338.54ms | 1548677.73 tokens/sec
Step 2526 | loss: 3.460206 | lr:5.8714e-04 | norm 0.2742 | dt 338.26ms | 1549942.84 tokens/sec
Step 2527 | loss: 3.504990 | lr:5.8712e-04 | norm 0.2900 | dt 338.52ms | 1548754.08 tokens/sec
Step 2528 | loss: 3.459425 | lr:5.8711e-04 | norm 0.2793 | dt 338.37ms | 1549474.32 tokens/sec
Step 2529 | loss: 3.422609 | lr:5.8709e-04 | norm 0.2760 | dt 337.85ms | 1551853.70 tokens/sec
Step 2530 | loss: 3.479158 | lr:5.8708e-04 | norm 0.2698 | dt 338.54ms | 1548689.72 tokens/sec
Step 2531 | loss: 3.463235 | lr:5.8707e-04 | norm 0.2762 | dt 339.24ms | 1545479.96 tokens/sec
Step 2532 | loss: 3.526112 | lr:5.8705e-04 | norm 0.2879 | dt 337.24ms | 1554629.38 tokens/sec
Step 2533 | loss: 3.476818 | lr:5.8704e-04 | norm 0.2641 | dt 337.18ms | 1554936.08 tokens/sec
Step 2534 | loss: 3.474408 | lr:5.8702e-04 | norm 0.2403 | dt 338.28ms | 1549843.44 tokens/sec
Step 2535 | loss: 3.533170 | lr:5.8701e-04 | norm 0.2462 | dt 337.55ms | 1553221.65 tokens/sec
Step 2536 | loss: 3.376622 | lr:5.8700e-04 | norm 0.2702 | dt 337.62ms | 1552902.47 tokens/sec
Step 2537 | loss: 3.411568 | lr:5.8698e-04 | norm 0.2838 | dt 339.57ms | 1543995.51 tokens/sec
Step 2538 | loss: 3.446064 | lr:5.8697e-04 | norm 0.2690 | dt 337.89ms | 1551663.17 tokens/sec
Step 2539 | loss: 3.395243 | lr:5.8695e-04 | norm 0.2606 | dt 337.32ms | 1554274.46 tokens/sec
Step 2540 | loss: 3.461647 | lr:5.8694e-04 | norm 0.2660 | dt 338.78ms | 1547569.31 tokens/sec
Step 2541 | loss: 3.381963 | lr:5.8692e-04 | norm 0.2679 | dt 339.66ms | 1543545.75 tokens/sec
Step 2542 | loss: 3.426735 | lr:5.8691e-04 | norm 0.3059 | dt 338.33ms | 1549626.10 tokens/sec
Step 2543 | loss: 3.391306 | lr:5.8690e-04 | norm 0.3359 | dt 338.42ms | 1549209.06 tokens/sec
Step 2544 | loss: 3.479238 | lr:5.8688e-04 | norm 0.3372 | dt 338.64ms | 1548202.34 tokens/sec
Step 2545 | loss: 3.438488 | lr:5.8687e-04 | norm 0.3380 | dt 338.84ms | 1547302.53 tokens/sec
Step 2546 | loss: 3.389859 | lr:5.8685e-04 | norm 0.3465 | dt 339.35ms | 1544989.17 tokens/sec
Step 2547 | loss: 3.413684 | lr:5.8684e-04 | norm 0.3438 | dt 338.38ms | 1549391.35 tokens/sec
Step 2548 | loss: 3.601420 | lr:5.8683e-04 | norm 0.3312 | dt 339.07ms | 1546245.00 tokens/sec
Step 2549 | loss: 3.604548 | lr:5.8681e-04 | norm 0.3528 | dt 338.27ms | 1549904.61 tokens/sec
Step 2550 | loss: 3.595606 | lr:5.8680e-04 | norm 0.3463 | dt 338.91ms | 1546998.83 tokens/sec
Step 2551 | loss: 3.613228 | lr:5.8678e-04 | norm 0.3375 | dt 339.44ms | 1544582.22 tokens/sec
Step 2552 | loss: 3.599402 | lr:5.8677e-04 | norm 0.2910 | dt 338.94ms | 1546864.98 tokens/sec
Step 2553 | loss: 3.612679 | lr:5.8675e-04 | norm 0.3138 | dt 338.74ms | 1547739.23 tokens/sec
Step 2554 | loss: 3.522967 | lr:5.8674e-04 | norm 0.2738 | dt 339.20ms | 1545653.77 tokens/sec
Step 2555 | loss: 3.589078 | lr:5.8673e-04 | norm 0.2688 | dt 338.64ms | 1548229.59 tokens/sec
Step 2556 | loss: 3.510361 | lr:5.8671e-04 | norm 0.2998 | dt 338.81ms | 1547426.65 tokens/sec
Step 2557 | loss: 3.626341 | lr:5.8670e-04 | norm 0.2898 | dt 338.99ms | 1546634.33 tokens/sec
Step 2558 | loss: 3.585946 | lr:5.8668e-04 | norm 0.3163 | dt 338.88ms | 1547097.87 tokens/sec
Step 2559 | loss: 3.563354 | lr:5.8667e-04 | norm 0.3030 | dt 337.83ms | 1551906.27 tokens/sec
Step 2560 | loss: 3.516062 | lr:5.8665e-04 | norm 0.3237 | dt 338.73ms | 1547793.70 tokens/sec
Step 2561 | loss: 3.529182 | lr:5.8664e-04 | norm 0.2918 | dt 338.77ms | 1547616.14 tokens/sec
Step 2562 | loss: 3.558130 | lr:5.8662e-04 | norm 0.2804 | dt 338.28ms | 1549869.65 tokens/sec
Step 2563 | loss: 3.543437 | lr:5.8661e-04 | norm 0.3076 | dt 338.28ms | 1549848.90 tokens/sec
Step 2564 | loss: 3.540047 | lr:5.8660e-04 | norm 0.3083 | dt 338.49ms | 1548912.25 tokens/sec
Step 2565 | loss: 3.535325 | lr:5.8658e-04 | norm 0.2623 | dt 338.70ms | 1547959.31 tokens/sec
Step 2566 | loss: 3.552747 | lr:5.8657e-04 | norm 0.3198 | dt 338.83ms | 1547359.14 tokens/sec
Step 2567 | loss: 3.564330 | lr:5.8655e-04 | norm 0.3071 | dt 338.20ms | 1550237.86 tokens/sec
Step 2568 | loss: 3.572902 | lr:5.8654e-04 | norm 0.2985 | dt 338.16ms | 1550419.30 tokens/sec
Step 2569 | loss: 3.531400 | lr:5.8652e-04 | norm 0.3321 | dt 338.42ms | 1549224.34 tokens/sec
Step 2570 | loss: 3.520743 | lr:5.8651e-04 | norm 0.3327 | dt 338.42ms | 1549217.79 tokens/sec
Step 2571 | loss: 3.476964 | lr:5.8650e-04 | norm 0.2987 | dt 338.82ms | 1547404.87 tokens/sec
Step 2572 | loss: 3.535049 | lr:5.8648e-04 | norm 0.3052 | dt 337.92ms | 1551528.51 tokens/sec
Step 2573 | loss: 3.508471 | lr:5.8647e-04 | norm 0.3132 | dt 338.59ms | 1548462.89 tokens/sec
Step 2574 | loss: 3.473103 | lr:5.8645e-04 | norm 0.2930 | dt 338.32ms | 1549698.17 tokens/sec
Step 2575 | loss: 3.445299 | lr:5.8644e-04 | norm 0.2751 | dt 338.36ms | 1549499.43 tokens/sec
Step 2576 | loss: 3.453726 | lr:5.8642e-04 | norm 0.2915 | dt 338.52ms | 1548764.98 tokens/sec
Step 2577 | loss: 3.451073 | lr:5.8641e-04 | norm 0.2644 | dt 338.44ms | 1549122.85 tokens/sec
Step 2578 | loss: 3.475994 | lr:5.8639e-04 | norm 0.2873 | dt 337.84ms | 1551867.94 tokens/sec
Step 2579 | loss: 3.474639 | lr:5.8638e-04 | norm 0.3056 | dt 338.46ms | 1549046.46 tokens/sec
Step 2580 | loss: 3.452002 | lr:5.8637e-04 | norm 0.3050 | dt 338.19ms | 1550272.83 tokens/sec
Step 2581 | loss: 3.478702 | lr:5.8635e-04 | norm 0.2583 | dt 337.94ms | 1551410.30 tokens/sec
Step 2582 | loss: 3.498147 | lr:5.8634e-04 | norm 0.2588 | dt 338.03ms | 1551026.22 tokens/sec
Step 2583 | loss: 3.393263 | lr:5.8632e-04 | norm 0.2702 | dt 337.77ms | 1552219.57 tokens/sec
Step 2584 | loss: 3.429861 | lr:5.8631e-04 | norm 0.2705 | dt 338.97ms | 1546706.13 tokens/sec
Step 2585 | loss: 3.420478 | lr:5.8629e-04 | norm 0.2575 | dt 337.67ms | 1552655.77 tokens/sec
Step 2586 | loss: 3.598144 | lr:5.8628e-04 | norm 0.2982 | dt 338.02ms | 1551038.25 tokens/sec
Step 2587 | loss: 3.386211 | lr:5.8626e-04 | norm 0.2902 | dt 338.16ms | 1550395.25 tokens/sec
Step 2588 | loss: 3.359733 | lr:5.8625e-04 | norm 0.3152 | dt 338.73ms | 1547796.97 tokens/sec
Step 2589 | loss: 3.350019 | lr:5.8623e-04 | norm 0.3053 | dt 337.61ms | 1552925.50 tokens/sec
Step 2590 | loss: 3.391485 | lr:5.8622e-04 | norm 0.2981 | dt 338.31ms | 1549721.11 tokens/sec
Step 2591 | loss: 3.420296 | lr:5.8621e-04 | norm 0.3210 | dt 339.01ms | 1546535.35 tokens/sec
Step 2592 | loss: 3.341552 | lr:5.8619e-04 | norm 0.3140 | dt 337.86ms | 1551812.09 tokens/sec
Step 2593 | loss: 3.408140 | lr:5.8618e-04 | norm 0.3090 | dt 337.69ms | 1552569.16 tokens/sec
Step 2594 | loss: 3.427863 | lr:5.8616e-04 | norm 0.2776 | dt 338.55ms | 1548612.29 tokens/sec
Step 2595 | loss: 3.590490 | lr:5.8615e-04 | norm 0.2957 | dt 338.11ms | 1550634.67 tokens/sec
Step 2596 | loss: 3.540412 | lr:5.8613e-04 | norm 0.3085 | dt 337.46ms | 1553635.36 tokens/sec
Step 2597 | loss: 3.498444 | lr:5.8612e-04 | norm 0.2900 | dt 338.98ms | 1546673.50 tokens/sec
Step 2598 | loss: 3.669812 | lr:5.8610e-04 | norm 0.3205 | dt 337.53ms | 1553294.06 tokens/sec
Step 2599 | loss: 3.571831 | lr:5.8609e-04 | norm 0.3440 | dt 337.95ms | 1551381.84 tokens/sec
Step 2600 | loss: 3.659519 | lr:5.8607e-04 | norm 0.3247 | dt 338.19ms | 1550289.23 tokens/sec
Step 2601 | loss: 3.612784 | lr:5.8606e-04 | norm 0.3328 | dt 338.53ms | 1548738.81 tokens/sec
Step 2602 | loss: 3.562467 | lr:5.8604e-04 | norm 0.3765 | dt 337.71ms | 1552480.38 tokens/sec
Step 2603 | loss: 3.625091 | lr:5.8603e-04 | norm 0.3797 | dt 338.45ms | 1549080.29 tokens/sec
Step 2604 | loss: 3.572261 | lr:5.8602e-04 | norm 0.3400 | dt 338.28ms | 1549868.56 tokens/sec
Step 2605 | loss: 3.557991 | lr:5.8600e-04 | norm 0.3312 | dt 337.78ms | 1552170.27 tokens/sec
Step 2606 | loss: 3.610533 | lr:5.8599e-04 | norm 0.3241 | dt 338.41ms | 1549282.19 tokens/sec
Step 2607 | loss: 3.589244 | lr:5.8597e-04 | norm 0.3076 | dt 337.78ms | 1552153.83 tokens/sec
Step 2608 | loss: 3.596696 | lr:5.8596e-04 | norm 0.2891 | dt 338.11ms | 1550637.95 tokens/sec
Step 2609 | loss: 3.581683 | lr:5.8594e-04 | norm 0.2741 | dt 338.75ms | 1547732.69 tokens/sec
Step 2610 | loss: 3.549315 | lr:5.8593e-04 | norm 0.2742 | dt 337.56ms | 1553172.28 tokens/sec
Step 2611 | loss: 3.553505 | lr:5.8591e-04 | norm 0.2838 | dt 337.78ms | 1552153.83 tokens/sec
Step 2612 | loss: 3.541104 | lr:5.8590e-04 | norm 0.2639 | dt 338.65ms | 1548170.73 tokens/sec
Step 2613 | loss: 3.568770 | lr:5.8588e-04 | norm 0.2836 | dt 338.38ms | 1549416.46 tokens/sec
Step 2614 | loss: 3.609644 | lr:5.8587e-04 | norm 0.3058 | dt 337.57ms | 1553105.37 tokens/sec
Step 2615 | loss: 3.587698 | lr:5.8585e-04 | norm 0.3021 | dt 338.63ms | 1548281.91 tokens/sec
Step 2616 | loss: 3.558224 | lr:5.8584e-04 | norm 0.2801 | dt 338.32ms | 1549683.97 tokens/sec
Step 2617 | loss: 3.549857 | lr:5.8582e-04 | norm 0.2871 | dt 337.30ms | 1554382.13 tokens/sec
Step 2618 | loss: 3.472972 | lr:5.8581e-04 | norm 0.2928 | dt 338.47ms | 1548995.18 tokens/sec
Step 2619 | loss: 3.462358 | lr:5.8579e-04 | norm 0.2785 | dt 338.14ms | 1550487.07 tokens/sec
Step 2620 | loss: 3.504749 | lr:5.8578e-04 | norm 0.2934 | dt 338.26ms | 1549973.43 tokens/sec
Step 2621 | loss: 3.467578 | lr:5.8576e-04 | norm 0.2828 | dt 338.62ms | 1548295.00 tokens/sec
Step 2622 | loss: 3.452609 | lr:5.8575e-04 | norm 0.2737 | dt 337.82ms | 1551984.04 tokens/sec
Step 2623 | loss: 3.463121 | lr:5.8573e-04 | norm 0.2975 | dt 337.84ms | 1551866.85 tokens/sec
Step 2624 | loss: 3.470246 | lr:5.8572e-04 | norm 0.2993 | dt 337.78ms | 1552164.79 tokens/sec
Step 2625 | loss: 3.478521 | lr:5.8571e-04 | norm 0.2630 | dt 338.02ms | 1551052.47 tokens/sec
Step 2626 | loss: 3.466804 | lr:5.8569e-04 | norm 0.2714 | dt 337.88ms | 1551698.21 tokens/sec
Step 2627 | loss: 3.491562 | lr:5.8568e-04 | norm 0.2828 | dt 337.85ms | 1551815.38 tokens/sec
Step 2628 | loss: 3.479822 | lr:5.8566e-04 | norm 0.2757 | dt 338.57ms | 1548552.31 tokens/sec
Step 2629 | loss: 3.531670 | lr:5.8565e-04 | norm 0.2751 | dt 337.79ms | 1552118.78 tokens/sec
Step 2630 | loss: 3.486807 | lr:5.8563e-04 | norm 0.3141 | dt 337.72ms | 1552438.73 tokens/sec
Step 2631 | loss: 3.448806 | lr:5.8562e-04 | norm 0.3155 | dt 337.80ms | 1552046.47 tokens/sec
Step 2632 | loss: 3.370183 | lr:5.8560e-04 | norm 0.3558 | dt 338.60ms | 1548400.75 tokens/sec
Step 2633 | loss: 3.457811 | lr:5.8559e-04 | norm 0.3957 | dt 337.92ms | 1551528.51 tokens/sec
Step 2634 | loss: 3.438967 | lr:5.8557e-04 | norm 0.3394 | dt 337.41ms | 1553876.88 tokens/sec
Step 2635 | loss: 3.412548 | lr:5.8556e-04 | norm 0.3227 | dt 338.18ms | 1550308.90 tokens/sec
Step 2636 | loss: 3.445123 | lr:5.8554e-04 | norm 0.3317 | dt 338.66ms | 1548141.30 tokens/sec
Step 2637 | loss: 3.394341 | lr:5.8553e-04 | norm 0.2877 | dt 337.66ms | 1552710.58 tokens/sec
Step 2638 | loss: 3.374377 | lr:5.8551e-04 | norm 0.2896 | dt 337.44ms | 1553710.00 tokens/sec
Step 2639 | loss: 3.411413 | lr:5.8550e-04 | norm 0.2885 | dt 338.10ms | 1550674.04 tokens/sec
Step 2640 | loss: 3.395136 | lr:5.8548e-04 | norm 0.2704 | dt 337.86ms | 1551800.04 tokens/sec
Step 2641 | loss: 3.482868 | lr:5.8547e-04 | norm 0.2715 | dt 338.57ms | 1548549.04 tokens/sec
Step 2642 | loss: 3.419050 | lr:5.8545e-04 | norm 0.2864 | dt 338.18ms | 1550339.50 tokens/sec
Step 2643 | loss: 3.467500 | lr:5.8544e-04 | norm 0.2866 | dt 337.60ms | 1552983.62 tokens/sec
Step 2644 | loss: 3.540109 | lr:5.8542e-04 | norm 0.2937 | dt 337.99ms | 1551214.40 tokens/sec
Step 2645 | loss: 3.550622 | lr:5.8541e-04 | norm 0.3002 | dt 903.72ms | 580146.80 tokens/sec
Step 2646 | loss: 3.554990 | lr:5.8539e-04 | norm 0.2958 | dt 335.19ms | 1564146.96 tokens/sec
Step 2647 | loss: 3.597381 | lr:5.8538e-04 | norm 0.3362 | dt 338.61ms | 1548347.32 tokens/sec
Step 2648 | loss: 3.548746 | lr:5.8536e-04 | norm 0.3447 | dt 339.34ms | 1545028.25 tokens/sec
Step 2649 | loss: 3.558077 | lr:5.8535e-04 | norm 0.3445 | dt 336.87ms | 1556341.41 tokens/sec
Step 2650 | loss: 3.514063 | lr:5.8533e-04 | norm 0.3466 | dt 337.61ms | 1552945.24 tokens/sec
Step 2651 | loss: 3.522371 | lr:5.8532e-04 | norm 0.3497 | dt 339.20ms | 1545683.10 tokens/sec
Step 2652 | loss: 3.583917 | lr:5.8530e-04 | norm 0.3147 | dt 338.80ms | 1547491.99 tokens/sec
Step 2653 | loss: 3.572728 | lr:5.8529e-04 | norm 0.2629 | dt 340.11ms | 1541520.19 tokens/sec
Step 2654 | loss: 3.587031 | lr:5.8527e-04 | norm 0.3107 | dt 338.55ms | 1548623.19 tokens/sec
Step 2655 | loss: 3.508548 | lr:5.8526e-04 | norm 0.3478 | dt 338.68ms | 1548042.13 tokens/sec
Step 2656 | loss: 3.581739 | lr:5.8524e-04 | norm 0.3146 | dt 338.73ms | 1547801.33 tokens/sec
Step 2657 | loss: 3.532717 | lr:5.8523e-04 | norm 0.3309 | dt 339.45ms | 1544517.13 tokens/sec
Step 2658 | loss: 3.550397 | lr:5.8521e-04 | norm 0.3297 | dt 339.11ms | 1546078.67 tokens/sec
Step 2659 | loss: 3.534623 | lr:5.8520e-04 | norm 0.3155 | dt 992.62ms | 528187.79 tokens/sec
Step 2660 | loss: 3.556003 | lr:5.8518e-04 | norm 0.3086 | dt 337.60ms | 1552993.49 tokens/sec
Step 2661 | loss: 3.568605 | lr:5.8517e-04 | norm 0.2888 | dt 338.06ms | 1550876.36 tokens/sec
Step 2662 | loss: 3.523363 | lr:5.8515e-04 | norm 0.2671 | dt 337.65ms | 1552735.80 tokens/sec
Step 2663 | loss: 3.566200 | lr:5.8514e-04 | norm 0.2872 | dt 338.48ms | 1548967.90 tokens/sec
Step 2664 | loss: 3.487654 | lr:5.8512e-04 | norm 0.3471 | dt 337.93ms | 1551454.08 tokens/sec
Step 2665 | loss: 3.588600 | lr:5.8511e-04 | norm 0.2723 | dt 337.59ms | 1553052.72 tokens/sec
Step 2666 | loss: 3.482232 | lr:5.8509e-04 | norm 0.2582 | dt 337.67ms | 1552677.69 tokens/sec
Step 2667 | loss: 3.475057 | lr:5.8508e-04 | norm 0.3112 | dt 338.44ms | 1549137.03 tokens/sec
Step 2668 | loss: 3.487407 | lr:5.8506e-04 | norm 0.2751 | dt 337.93ms | 1551483.63 tokens/sec
Step 2669 | loss: 3.555886 | lr:5.8505e-04 | norm 0.2873 | dt 338.05ms | 1550899.33 tokens/sec
Step 2670 | loss: 3.495602 | lr:5.8503e-04 | norm 0.2775 | dt 338.30ms | 1549775.72 tokens/sec
Step 2671 | loss: 3.463800 | lr:5.8501e-04 | norm 0.2991 | dt 337.30ms | 1554351.36 tokens/sec
Step 2672 | loss: 3.451149 | lr:5.8500e-04 | norm 0.2772 | dt 337.89ms | 1551637.99 tokens/sec
Step 2673 | loss: 3.445401 | lr:5.8498e-04 | norm 0.2720 | dt 339.04ms | 1546412.46 tokens/sec
Step 2674 | loss: 3.490981 | lr:5.8497e-04 | norm 0.2917 | dt 337.87ms | 1551733.25 tokens/sec
Step 2675 | loss: 3.480829 | lr:5.8495e-04 | norm 0.3155 | dt 338.73ms | 1547820.94 tokens/sec
Step 2676 | loss: 3.544201 | lr:5.8494e-04 | norm 0.3260 | dt 338.05ms | 1550923.39 tokens/sec
Step 2677 | loss: 3.514430 | lr:5.8492e-04 | norm 0.3011 | dt 338.07ms | 1550847.92 tokens/sec
Step 2678 | loss: 3.522891 | lr:5.8491e-04 | norm 0.2824 | dt 338.36ms | 1549491.79 tokens/sec
Step 2679 | loss: 3.384298 | lr:5.8489e-04 | norm 0.2684 | dt 338.71ms | 1547875.41 tokens/sec
Step 2680 | loss: 3.433316 | lr:5.8488e-04 | norm 0.2713 | dt 337.51ms | 1553379.65 tokens/sec
Step 2681 | loss: 3.396720 | lr:5.8486e-04 | norm 0.2660 | dt 337.83ms | 1551927.08 tokens/sec
Step 2682 | loss: 3.328938 | lr:5.8485e-04 | norm 0.2579 | dt 337.53ms | 1553306.13 tokens/sec
Step 2683 | loss: 3.353759 | lr:5.8483e-04 | norm 0.2633 | dt 338.41ms | 1549282.19 tokens/sec
Step 2684 | loss: 3.402430 | lr:5.8482e-04 | norm 0.2783 | dt 337.68ms | 1552626.17 tokens/sec
Step 2685 | loss: 3.408568 | lr:5.8480e-04 | norm 0.3094 | dt 338.59ms | 1548462.89 tokens/sec
Step 2686 | loss: 3.414162 | lr:5.8479e-04 | norm 0.3232 | dt 337.50ms | 1553447.68 tokens/sec
Step 2687 | loss: 3.437227 | lr:5.8477e-04 | norm 0.3173 | dt 337.77ms | 1552193.27 tokens/sec
Step 2688 | loss: 3.402543 | lr:5.8476e-04 | norm 0.2967 | dt 337.61ms | 1552940.85 tokens/sec
Step 2689 | loss: 3.384218 | lr:5.8474e-04 | norm 0.2866 | dt 337.56ms | 1553172.28 tokens/sec
Step 2690 | loss: 3.432571 | lr:5.8473e-04 | norm 0.2996 | dt 338.13ms | 1550569.07 tokens/sec
Step 2691 | loss: 3.563066 | lr:5.8471e-04 | norm 0.2716 | dt 337.79ms | 1552133.02 tokens/sec
Step 2692 | loss: 3.602105 | lr:5.8469e-04 | norm 0.2596 | dt 337.55ms | 1553220.55 tokens/sec
Step 2693 | loss: 3.524995 | lr:5.8468e-04 | norm 0.2754 | dt 338.38ms | 1549418.64 tokens/sec
Step 2694 | loss: 3.532840 | lr:5.8466e-04 | norm 0.2776 | dt 337.54ms | 1553266.63 tokens/sec
Step 2695 | loss: 3.584750 | lr:5.8465e-04 | norm 0.2829 | dt 339.33ms | 1545087.95 tokens/sec
Step 2696 | loss: 3.565763 | lr:5.8463e-04 | norm 0.3049 | dt 339.08ms | 1546194.99 tokens/sec
Step 2697 | loss: 3.586517 | lr:5.8462e-04 | norm 0.3284 | dt 339.15ms | 1545901.51 tokens/sec
Step 2698 | loss: 3.519458 | lr:5.8460e-04 | norm 0.3025 | dt 339.28ms | 1545306.19 tokens/sec
Step 2699 | loss: 3.599752 | lr:5.8459e-04 | norm 0.3285 | dt 340.27ms | 1540779.25 tokens/sec
Step 2700 | loss: 3.494201 | lr:5.8457e-04 | norm 0.3403 | dt 338.95ms | 1546805.14 tokens/sec
Step 2701 | loss: 3.493907 | lr:5.8456e-04 | norm 0.2801 | dt 339.54ms | 1544115.85 tokens/sec
Step 2702 | loss: 3.532243 | lr:5.8454e-04 | norm 0.2969 | dt 338.88ms | 1547129.44 tokens/sec
Step 2703 | loss: 3.510961 | lr:5.8453e-04 | norm 0.3206 | dt 338.98ms | 1546650.65 tokens/sec
Step 2704 | loss: 3.530896 | lr:5.8451e-04 | norm 0.2929 | dt 338.73ms | 1547790.43 tokens/sec
Step 2705 | loss: 3.535584 | lr:5.8449e-04 | norm 0.2839 | dt 339.30ms | 1545184.58 tokens/sec
Step 2706 | loss: 3.535177 | lr:5.8448e-04 | norm 0.3274 | dt 339.07ms | 1546262.40 tokens/sec
Step 2707 | loss: 3.494707 | lr:5.8446e-04 | norm 0.3745 | dt 339.42ms | 1544672.28 tokens/sec
Step 2708 | loss: 3.625834 | lr:5.8445e-04 | norm 0.3565 | dt 339.23ms | 1545520.15 tokens/sec
Step 2709 | loss: 3.559846 | lr:5.8443e-04 | norm 0.4406 | dt 339.02ms | 1546469.01 tokens/sec
Step 2710 | loss: 3.570877 | lr:5.8442e-04 | norm 0.3933 | dt 338.79ms | 1547551.89 tokens/sec
Step 2711 | loss: 3.528135 | lr:5.8440e-04 | norm 0.3570 | dt 339.26ms | 1545400.68 tokens/sec
Step 2712 | loss: 3.513655 | lr:5.8439e-04 | norm 0.3032 | dt 340.02ms | 1541951.47 tokens/sec
Step 2713 | loss: 3.524401 | lr:5.8437e-04 | norm 0.2870 | dt 339.01ms | 1546536.44 tokens/sec
Step 2714 | loss: 3.513610 | lr:5.8436e-04 | norm 0.3311 | dt 338.51ms | 1548830.43 tokens/sec
Step 2715 | loss: 3.485318 | lr:5.8434e-04 | norm 0.2857 | dt 338.83ms | 1547342.81 tokens/sec
Step 2716 | loss: 3.468618 | lr:5.8432e-04 | norm 0.2676 | dt 338.52ms | 1548787.89 tokens/sec
Step 2717 | loss: 3.568978 | lr:5.8431e-04 | norm 0.2877 | dt 338.74ms | 1547766.46 tokens/sec
Step 2718 | loss: 3.442664 | lr:5.8429e-04 | norm 0.3091 | dt 337.87ms | 1551727.77 tokens/sec
Step 2719 | loss: 3.430960 | lr:5.8428e-04 | norm 0.2812 | dt 338.11ms | 1550662.01 tokens/sec
Step 2720 | loss: 3.481950 | lr:5.8426e-04 | norm 0.2866 | dt 338.19ms | 1550280.48 tokens/sec
Step 2721 | loss: 3.474093 | lr:5.8425e-04 | norm 0.2683 | dt 338.33ms | 1549656.67 tokens/sec
Step 2722 | loss: 3.444495 | lr:5.8423e-04 | norm 0.2826 | dt 337.91ms | 1551553.69 tokens/sec
Step 2723 | loss: 3.475112 | lr:5.8422e-04 | norm 0.2873 | dt 337.96ms | 1551310.70 tokens/sec
Step 2724 | loss: 3.461133 | lr:5.8420e-04 | norm 0.2912 | dt 338.74ms | 1547771.91 tokens/sec
Step 2725 | loss: 3.568798 | lr:5.8418e-04 | norm 0.2875 | dt 338.30ms | 1549762.61 tokens/sec
Step 2726 | loss: 3.493179 | lr:5.8417e-04 | norm 0.2719 | dt 339.72ms | 1543290.10 tokens/sec
Step 2727 | loss: 3.440265 | lr:5.8415e-04 | norm 0.3121 | dt 339.24ms | 1545473.45 tokens/sec
Step 2728 | loss: 3.386818 | lr:5.8414e-04 | norm 0.2977 | dt 339.30ms | 1545216.07 tokens/sec
Step 2729 | loss: 3.375279 | lr:5.8412e-04 | norm 0.2683 | dt 339.29ms | 1545232.36 tokens/sec
Step 2730 | loss: 3.370642 | lr:5.8411e-04 | norm 0.2610 | dt 339.41ms | 1544710.25 tokens/sec
Step 2731 | loss: 3.447078 | lr:5.8409e-04 | norm 0.2613 | dt 339.37ms | 1544880.63 tokens/sec
Step 2732 | loss: 3.391543 | lr:5.8408e-04 | norm 0.2556 | dt 338.86ms | 1547203.46 tokens/sec
Step 2733 | loss: 3.378505 | lr:5.8406e-04 | norm 0.2854 | dt 337.96ms | 1551335.87 tokens/sec
Step 2734 | loss: 3.416303 | lr:5.8404e-04 | norm 0.2789 | dt 339.62ms | 1543755.97 tokens/sec
Step 2735 | loss: 3.425527 | lr:5.8403e-04 | norm 0.2616 | dt 339.34ms | 1545030.42 tokens/sec
Step 2736 | loss: 3.422626 | lr:5.8401e-04 | norm 0.2651 | dt 338.95ms | 1546806.22 tokens/sec
Step 2737 | loss: 3.388819 | lr:5.8400e-04 | norm 0.2787 | dt 339.36ms | 1544911.02 tokens/sec
Step 2738 | loss: 3.379636 | lr:5.8398e-04 | norm 0.2921 | dt 339.06ms | 1546301.54 tokens/sec
Step 2739 | loss: 3.589843 | lr:5.8397e-04 | norm 0.3087 | dt 339.55ms | 1544087.66 tokens/sec
Step 2740 | loss: 3.551529 | lr:5.8395e-04 | norm 0.3375 | dt 339.04ms | 1546380.92 tokens/sec
Step 2741 | loss: 3.603941 | lr:5.8393e-04 | norm 0.3379 | dt 339.29ms | 1545271.45 tokens/sec
Step 2742 | loss: 3.541703 | lr:5.8392e-04 | norm 0.4157 | dt 340.14ms | 1541409.98 tokens/sec
Step 2743 | loss: 3.536115 | lr:5.8390e-04 | norm 0.3729 | dt 338.96ms | 1546751.82 tokens/sec
Step 2744 | loss: 3.564048 | lr:5.8389e-04 | norm 0.3329 | dt 339.32ms | 1545100.98 tokens/sec
Step 2745 | loss: 3.681867 | lr:5.8387e-04 | norm 0.3702 | dt 339.50ms | 1544314.30 tokens/sec
Step 2746 | loss: 3.544883 | lr:5.8386e-04 | norm 0.3521 | dt 339.25ms | 1545416.97 tokens/sec
Step 2747 | loss: 3.647607 | lr:5.8384e-04 | norm 0.3279 | dt 338.67ms | 1548077.00 tokens/sec
Step 2748 | loss: 3.530462 | lr:5.8382e-04 | norm 0.2742 | dt 338.23ms | 1550093.61 tokens/sec
Step 2749 | loss: 3.513725 | lr:5.8381e-04 | norm 0.2907 | dt 338.23ms | 1550112.19 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 2750: 3.5437
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2656/10042=0.2645


ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, using some tools at the start. This makes me more intuitive.
One thing that I've learned is my own understanding


ddp_rank 5: ####### Printing generated samples ####### 

rank 2 sample 1 >Hello, I'm a language model, so I get my first language model up-to date. So if I'm a language modeler at all, I
rank 2 sample 2 >Hello, I'm a language model, but I think I have to get in more detail about it. It helps me understand it, how much time that the
rank 5 sample 0 >Hello, I'm a language model, so we have to start with a very broad set of languages. There's a very, very wide range of languages and
rank 2 sample 3 >Hello, I'm a language model, so what happened? It was a model of the World Wide Web. It was built using many types of computer systems,


rank 5 sample 1 >Hello, I'm a language model, we'd want to thank a big fan who called me, who got a strong response from me. That meant I was
rank 5 sample 2 >Hello, I'm a language model, but it’s a bunch of people, and I’m very interested in the relationship between the two concepts
rank 5 sample 3 >Hello, I'm a language model, though it's too much and it's too small for us to be able to explain, and people in general don't




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not really using the book and it was my favourite. He found a pretty impressive case at the back when

ddp_rank 7: ####### Printing generated samples ####### 


rank 7 sample 0 >Hello, I'm a language model, and I’m an expert at both languages, so I’m passionate about building trust between the people whorank 3 sample 1 >Hello, I'm a language model, and my understanding of the other programming languages I can read in the future is enhanced by the C++ programming language called R

rank 3 sample 2 >Hello, I'm a language model, and it’s based on the
the whole idea of the language itself.
This would be the simplest approach
rank 7 sample 1 >Hello, I'm a language model, now that you can build the data I need to process one. We're going to do it in just a little little
rank 3 sample 3 >Hello, I'm a language model, and of course a bunch of developers and programmers have come and gone to great lengths to help solve their job problems. I


rank 7 sample 2 >Hello, I'm a language model, and I've created a lot of data from the topic.
In most cases, the data is presented as "true
rank 7 sample 3 >Hello, I'm a language model, trying to create a basic object on all these domains. I've got a lot of clients, you can see me sitting




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, an interactive text editor, a dictionary file and more for anyone using the Internet. I am writing on the Web, and
rank 1 sample 1 >Hello, I'm a language model, a little weird way to get you right by the way—but just in case I wanted to come back to the basics
rank 1 sample 2 >Hello, I'm a language model, so I'll use it in a few different ways. It does a great job of explaining what is meant to you.
rank 1 sample 3 >Hello, I'm a language model, and I'm also an AI. That gives me a large place in your local language; with all the right tools,




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I don't think anyone understands any theory yet.
So this thing, I guess, and you are a good
rank 0 sample 1 >Hello, I'm a language model, but for a long time I'm on the same edge of a small community in the world.
As an example,
rank 0 sample 2 >Hello, I'm a language model, but I couldn't put down a whole bunch of reasons. First and foremost, there are many languages that are used in
rank 0 sample 3 >Hello, I'm a language model, and then I'm a language designer and a little bit of a community of developers I’m building. It started




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where is the time period: A (or a bit more than 20 milliseconds in the time period) is a time difference
rank 6 sample 1 >Hello, I'm a language model, you can have a couple different languages. Some of you are familiar with:
There is an app for your Python site
rank 6 sample 2 >Hello, I'm a language model, so I'm a language modeling, so I want to be in charge of all the code and I'm looking towards a


ddp_rank 4: ####### Printing generated samples ####### 

rank 6 sample 3 >Hello, I'm a language model, and that's in general. In fact, the language model is still very strong. But people are on that plateau!


rank 4 sample 0 >Hello, I'm a language model, but I'll be reading in the future."<|endoftext|>Last week's class I asked her a small group of parents who had
rank 4 sample 1 >Hello, I'm a language model, meaning the language or culture you already have. My new course is learning something that looks a bit like that of humans,
rank 4 sample 2 >Hello, I'm a language model, I haven't, it's a model that's been around long enough to be out of shape in space. If I
rank 4 sample 3 >Hello, I'm a language model, and you thought. You could be wondering where your friends were playing. Of course, you would also have to explain what


Step 2750 | loss: 3.533441 | lr:5.8379e-04 | norm 0.2967 | dt 19097.04ms | 27453.89 tokens/sec
Step 2751 | loss: 3.494729 | lr:5.8378e-04 | norm 0.2803 | dt 332.97ms | 1574583.00 tokens/sec
Step 2752 | loss: 3.543903 | lr:5.8376e-04 | norm 0.2951 | dt 335.25ms | 1563876.65 tokens/sec
Step 2753 | loss: 3.540341 | lr:5.8375e-04 | norm 0.2792 | dt 336.42ms | 1558452.52 tokens/sec
Step 2754 | loss: 3.526419 | lr:5.8373e-04 | norm 0.2935 | dt 336.13ms | 1559756.89 tokens/sec
Step 2755 | loss: 3.536097 | lr:5.8371e-04 | norm 0.2639 | dt 336.77ms | 1556835.03 tokens/sec
Step 2756 | loss: 3.484701 | lr:5.8370e-04 | norm 0.3325 | dt 335.37ms | 1563312.98 tokens/sec
Step 2757 | loss: 3.566457 | lr:5.8368e-04 | norm 0.3269 | dt 337.25ms | 1554583.22 tokens/sec
Step 2758 | loss: 3.471389 | lr:5.8367e-04 | norm 0.2949 | dt 336.58ms | 1557710.66 tokens/sec
Step 2759 | loss: 3.483893 | lr:5.8365e-04 | norm 0.2743 | dt 337.05ms | 1555506.93 tokens/sec
Step 2760 | loss: 3.491410 | lr:5.8363e-04 | norm 0.2998 | dt 337.06ms | 1555454.12 tokens/sec
Step 2761 | loss: 3.490822 | lr:5.8362e-04 | norm 0.3171 | dt 337.02ms | 1555644.48 tokens/sec
Step 2762 | loss: 3.533383 | lr:5.8360e-04 | norm 0.2726 | dt 338.58ms | 1548498.88 tokens/sec
Step 2763 | loss: 3.508073 | lr:5.8359e-04 | norm 0.2933 | dt 337.92ms | 1551536.18 tokens/sec
Step 2764 | loss: 3.427568 | lr:5.8357e-04 | norm 0.3147 | dt 337.31ms | 1554329.39 tokens/sec
Step 2765 | loss: 3.480226 | lr:5.8356e-04 | norm 0.2888 | dt 338.40ms | 1549321.49 tokens/sec
Step 2766 | loss: 3.452209 | lr:5.8354e-04 | norm 0.2703 | dt 337.15ms | 1555075.73 tokens/sec
Step 2767 | loss: 3.438993 | lr:5.8352e-04 | norm 0.2727 | dt 338.10ms | 1550690.44 tokens/sec
Step 2768 | loss: 3.523870 | lr:5.8351e-04 | norm 0.2861 | dt 337.25ms | 1554587.61 tokens/sec
Step 2769 | loss: 3.480887 | lr:5.8349e-04 | norm 0.3056 | dt 337.52ms | 1553370.87 tokens/sec
Step 2770 | loss: 3.472741 | lr:5.8348e-04 | norm 0.3024 | dt 337.16ms | 1554994.35 tokens/sec
Step 2771 | loss: 3.456067 | lr:5.8346e-04 | norm 0.2821 | dt 336.61ms | 1557540.75 tokens/sec
Step 2772 | loss: 3.506941 | lr:5.8344e-04 | norm 0.2817 | dt 338.15ms | 1550446.63 tokens/sec
Step 2773 | loss: 3.556503 | lr:5.8343e-04 | norm 0.2747 | dt 337.38ms | 1554014.14 tokens/sec
Step 2774 | loss: 3.463794 | lr:5.8341e-04 | norm 0.2918 | dt 337.67ms | 1552674.40 tokens/sec
Step 2775 | loss: 3.429748 | lr:5.8340e-04 | norm 0.2683 | dt 338.21ms | 1550169.01 tokens/sec
Step 2776 | loss: 3.413063 | lr:5.8338e-04 | norm 0.3051 | dt 337.20ms | 1554834.93 tokens/sec
Step 2777 | loss: 3.410321 | lr:5.8336e-04 | norm 0.3008 | dt 337.19ms | 1554888.80 tokens/sec
Step 2778 | loss: 3.397432 | lr:5.8335e-04 | norm 0.3359 | dt 337.50ms | 1553424.64 tokens/sec
Step 2779 | loss: 3.343068 | lr:5.8333e-04 | norm 0.3777 | dt 336.54ms | 1557893.85 tokens/sec
Step 2780 | loss: 3.320168 | lr:5.8332e-04 | norm 0.3549 | dt 337.02ms | 1555646.68 tokens/sec
Step 2781 | loss: 3.490872 | lr:5.8330e-04 | norm 0.3144 | dt 337.57ms | 1553127.31 tokens/sec
Step 2782 | loss: 3.440087 | lr:5.8328e-04 | norm 0.3335 | dt 336.37ms | 1558650.24 tokens/sec
Step 2783 | loss: 3.412755 | lr:5.8327e-04 | norm 0.3437 | dt 336.80ms | 1556693.96 tokens/sec
Step 2784 | loss: 3.387758 | lr:5.8325e-04 | norm 0.3373 | dt 336.71ms | 1557067.63 tokens/sec
Step 2785 | loss: 3.392548 | lr:5.8324e-04 | norm 0.3358 | dt 337.59ms | 1553040.65 tokens/sec
Step 2786 | loss: 3.451878 | lr:5.8322e-04 | norm 0.3355 | dt 336.58ms | 1557687.49 tokens/sec
Step 2787 | loss: 3.530265 | lr:5.8320e-04 | norm 0.3127 | dt 336.94ms | 1556014.34 tokens/sec
Step 2788 | loss: 3.475626 | lr:5.8319e-04 | norm 0.3032 | dt 337.58ms | 1553077.95 tokens/sec
Step 2789 | loss: 3.536057 | lr:5.8317e-04 | norm 0.3034 | dt 337.16ms | 1555026.24 tokens/sec
Step 2790 | loss: 3.570950 | lr:5.8316e-04 | norm 0.3149 | dt 337.01ms | 1555686.30 tokens/sec
Step 2791 | loss: 3.577884 | lr:5.8314e-04 | norm 0.3236 | dt 337.65ms | 1552734.70 tokens/sec
Step 2792 | loss: 3.546593 | lr:5.8312e-04 | norm 0.3034 | dt 337.87ms | 1551722.30 tokens/sec
Step 2793 | loss: 3.561689 | lr:5.8311e-04 | norm 0.2761 | dt 338.25ms | 1550009.48 tokens/sec
Step 2794 | loss: 3.577441 | lr:5.8309e-04 | norm 0.2963 | dt 337.03ms | 1555599.36 tokens/sec
Step 2795 | loss: 3.493939 | lr:5.8308e-04 | norm 0.2970 | dt 337.71ms | 1552471.61 tokens/sec
Step 2796 | loss: 3.667171 | lr:5.8306e-04 | norm 0.2949 | dt 337.37ms | 1554025.13 tokens/sec
Step 2797 | loss: 3.566273 | lr:5.8304e-04 | norm 0.3252 | dt 337.14ms | 1555121.92 tokens/sec
Step 2798 | loss: 3.518436 | lr:5.8303e-04 | norm 0.3375 | dt 338.18ms | 1550306.71 tokens/sec
Step 2799 | loss: 3.523573 | lr:5.8301e-04 | norm 0.3237 | dt 337.76ms | 1552254.63 tokens/sec
Step 2800 | loss: 3.530918 | lr:5.8299e-04 | norm 0.3483 | dt 337.30ms | 1554365.65 tokens/sec
Step 2801 | loss: 3.521811 | lr:5.8298e-04 | norm 0.3221 | dt 338.20ms | 1550249.88 tokens/sec
Step 2802 | loss: 3.505322 | lr:5.8296e-04 | norm 0.3153 | dt 337.13ms | 1555143.91 tokens/sec
Step 2803 | loss: 3.484791 | lr:5.8295e-04 | norm 0.3300 | dt 337.78ms | 1552158.22 tokens/sec
Step 2804 | loss: 3.561932 | lr:5.8293e-04 | norm 0.2894 | dt 337.56ms | 1553159.12 tokens/sec
Step 2805 | loss: 3.502256 | lr:5.8291e-04 | norm 0.2993 | dt 338.03ms | 1550989.02 tokens/sec
Step 2806 | loss: 3.555256 | lr:5.8290e-04 | norm 0.3026 | dt 337.58ms | 1553070.27 tokens/sec
Step 2807 | loss: 3.469426 | lr:5.8288e-04 | norm 0.2540 | dt 336.80ms | 1556663.11 tokens/sec
Step 2808 | loss: 3.483012 | lr:5.8287e-04 | norm 0.3028 | dt 339.30ms | 1545219.33 tokens/sec
Step 2809 | loss: 3.439588 | lr:5.8285e-04 | norm 0.2748 | dt 338.07ms | 1550818.39 tokens/sec
Step 2810 | loss: 3.468246 | lr:5.8283e-04 | norm 0.2575 | dt 337.53ms | 1553321.49 tokens/sec
Step 2811 | loss: 3.409298 | lr:5.8282e-04 | norm 0.2923 | dt 338.38ms | 1549395.72 tokens/sec
Step 2812 | loss: 3.426215 | lr:5.8280e-04 | norm 0.2593 | dt 338.25ms | 1549986.54 tokens/sec
Step 2813 | loss: 3.467404 | lr:5.8278e-04 | norm 0.2741 | dt 337.34ms | 1554164.61 tokens/sec
Step 2814 | loss: 3.458605 | lr:5.8277e-04 | norm 0.2555 | dt 337.53ms | 1553322.59 tokens/sec
Step 2815 | loss: 3.465366 | lr:5.8275e-04 | norm 0.2552 | dt 337.85ms | 1551835.09 tokens/sec
Step 2816 | loss: 3.465962 | lr:5.8274e-04 | norm 0.2829 | dt 337.39ms | 1553942.77 tokens/sec
Step 2817 | loss: 3.458995 | lr:5.8272e-04 | norm 0.2853 | dt 338.17ms | 1550361.36 tokens/sec
Step 2818 | loss: 3.460051 | lr:5.8270e-04 | norm 0.2602 | dt 337.41ms | 1553870.29 tokens/sec
Step 2819 | loss: 3.432970 | lr:5.8269e-04 | norm 0.2683 | dt 337.52ms | 1553344.53 tokens/sec
Step 2820 | loss: 3.467010 | lr:5.8267e-04 | norm 0.2739 | dt 338.04ms | 1550951.83 tokens/sec
Step 2821 | loss: 3.506021 | lr:5.8265e-04 | norm 0.2707 | dt 336.80ms | 1556674.13 tokens/sec
Step 2822 | loss: 3.465949 | lr:5.8264e-04 | norm 0.2661 | dt 338.35ms | 1549540.92 tokens/sec
Step 2823 | loss: 3.358551 | lr:5.8262e-04 | norm 0.3058 | dt 337.64ms | 1552805.97 tokens/sec
Step 2824 | loss: 3.334389 | lr:5.8261e-04 | norm 0.2909 | dt 337.59ms | 1553031.88 tokens/sec
Step 2825 | loss: 3.388274 | lr:5.8259e-04 | norm 0.2651 | dt 337.46ms | 1553646.34 tokens/sec
Step 2826 | loss: 3.331580 | lr:5.8257e-04 | norm 0.2753 | dt 338.79ms | 1547549.71 tokens/sec
Step 2827 | loss: 3.375923 | lr:5.8256e-04 | norm 0.2740 | dt 337.22ms | 1554741.49 tokens/sec
Step 2828 | loss: 3.399880 | lr:5.8254e-04 | norm 0.2618 | dt 340.36ms | 1540380.99 tokens/sec
Step 2829 | loss: 3.365960 | lr:5.8252e-04 | norm 0.2699 | dt 337.94ms | 1551415.77 tokens/sec
Step 2830 | loss: 3.398266 | lr:5.8251e-04 | norm 0.3077 | dt 337.80ms | 1552053.05 tokens/sec
Step 2831 | loss: 3.353078 | lr:5.8249e-04 | norm 0.3226 | dt 338.00ms | 1551141.09 tokens/sec
Step 2832 | loss: 3.318210 | lr:5.8247e-04 | norm 0.2878 | dt 337.35ms | 1554132.76 tokens/sec
Step 2833 | loss: 3.334517 | lr:5.8246e-04 | norm 0.3208 | dt 337.44ms | 1553718.79 tokens/sec
Step 2834 | loss: 3.325698 | lr:5.8244e-04 | norm 0.2831 | dt 891.87ms | 587853.10 tokens/sec
Step 2835 | loss: 3.558054 | lr:5.8243e-04 | norm 0.2751 | dt 334.81ms | 1565916.82 tokens/sec
Step 2836 | loss: 3.515573 | lr:5.8241e-04 | norm 0.3158 | dt 338.96ms | 1546734.42 tokens/sec
Step 2837 | loss: 3.572521 | lr:5.8239e-04 | norm 0.3344 | dt 339.54ms | 1544107.18 tokens/sec
Step 2838 | loss: 3.524337 | lr:5.8238e-04 | norm 0.2983 | dt 337.43ms | 1553758.31 tokens/sec
Step 2839 | loss: 3.509989 | lr:5.8236e-04 | norm 0.2979 | dt 337.83ms | 1551930.37 tokens/sec
Step 2840 | loss: 3.588280 | lr:5.8234e-04 | norm 0.2871 | dt 338.37ms | 1549438.29 tokens/sec
Step 2841 | loss: 3.500719 | lr:5.8233e-04 | norm 0.3209 | dt 337.95ms | 1551392.79 tokens/sec
Step 2842 | loss: 3.495674 | lr:5.8231e-04 | norm 0.3326 | dt 338.03ms | 1551032.78 tokens/sec
Step 2843 | loss: 3.517688 | lr:5.8229e-04 | norm 0.2883 | dt 336.64ms | 1557407.28 tokens/sec
Step 2844 | loss: 3.489777 | lr:5.8228e-04 | norm 0.2849 | dt 339.15ms | 1545899.34 tokens/sec
Step 2845 | loss: 3.481441 | lr:5.8226e-04 | norm 0.2756 | dt 337.48ms | 1553528.89 tokens/sec
Step 2846 | loss: 3.513803 | lr:5.8224e-04 | norm 0.2684 | dt 339.25ms | 1545414.79 tokens/sec
Step 2847 | loss: 3.525343 | lr:5.8223e-04 | norm 0.3030 | dt 337.57ms | 1553137.18 tokens/sec
Step 2848 | loss: 3.539603 | lr:5.8221e-04 | norm 0.3238 | dt 338.41ms | 1549271.28 tokens/sec
Step 2849 | loss: 3.527936 | lr:5.8220e-04 | norm 0.2958 | dt 946.38ms | 553991.77 tokens/sec
Step 2850 | loss: 3.466918 | lr:5.8218e-04 | norm 0.2899 | dt 336.93ms | 1556080.40 tokens/sec
Step 2851 | loss: 3.492546 | lr:5.8216e-04 | norm 0.2949 | dt 338.00ms | 1551160.79 tokens/sec
Step 2852 | loss: 3.524919 | lr:5.8215e-04 | norm 0.2964 | dt 337.76ms | 1552264.49 tokens/sec
Step 2853 | loss: 3.495578 | lr:5.8213e-04 | norm 0.2315 | dt 336.60ms | 1557603.64 tokens/sec
Step 2854 | loss: 3.473467 | lr:5.8211e-04 | norm 0.3031 | dt 337.64ms | 1552784.04 tokens/sec
Step 2855 | loss: 3.459894 | lr:5.8210e-04 | norm 0.3367 | dt 337.80ms | 1552087.01 tokens/sec
Step 2856 | loss: 3.459516 | lr:5.8208e-04 | norm 0.3230 | dt 336.72ms | 1557043.37 tokens/sec
Step 2857 | loss: 3.361434 | lr:5.8206e-04 | norm 0.2835 | dt 337.80ms | 1552083.72 tokens/sec
Step 2858 | loss: 3.454519 | lr:5.8205e-04 | norm 0.2699 | dt 337.95ms | 1551399.35 tokens/sec
Step 2859 | loss: 3.445391 | lr:5.8203e-04 | norm 0.2670 | dt 337.92ms | 1551520.85 tokens/sec
Step 2860 | loss: 3.517221 | lr:5.8201e-04 | norm 0.2462 | dt 337.64ms | 1552788.43 tokens/sec
Step 2861 | loss: 3.393752 | lr:5.8200e-04 | norm 0.2776 | dt 337.44ms | 1553740.74 tokens/sec
Step 2862 | loss: 3.442116 | lr:5.8198e-04 | norm 0.2812 | dt 338.16ms | 1550403.99 tokens/sec
Step 2863 | loss: 3.486552 | lr:5.8196e-04 | norm 0.2963 | dt 338.34ms | 1549604.26 tokens/sec
Step 2864 | loss: 3.468538 | lr:5.8195e-04 | norm 0.3171 | dt 337.49ms | 1553498.16 tokens/sec
Step 2865 | loss: 3.450397 | lr:5.8193e-04 | norm 0.2864 | dt 338.50ms | 1548869.71 tokens/sec
Step 2866 | loss: 3.485610 | lr:5.8191e-04 | norm 0.2854 | dt 338.63ms | 1548265.56 tokens/sec
Step 2867 | loss: 3.537715 | lr:5.8190e-04 | norm 0.2802 | dt 338.03ms | 1551029.50 tokens/sec
Step 2868 | loss: 3.523027 | lr:5.8188e-04 | norm 0.2730 | dt 338.16ms | 1550410.55 tokens/sec
Step 2869 | loss: 3.467350 | lr:5.8186e-04 | norm 0.2528 | dt 339.44ms | 1544582.22 tokens/sec
Step 2870 | loss: 3.378538 | lr:5.8185e-04 | norm 0.2726 | dt 337.97ms | 1551269.12 tokens/sec
Step 2871 | loss: 3.335577 | lr:5.8183e-04 | norm 0.2621 | dt 338.24ms | 1550031.34 tokens/sec
Step 2872 | loss: 3.439004 | lr:5.8181e-04 | norm 0.2808 | dt 339.03ms | 1546438.56 tokens/sec
Step 2873 | loss: 3.370932 | lr:5.8180e-04 | norm 0.2939 | dt 338.51ms | 1548795.53 tokens/sec
Step 2874 | loss: 3.374371 | lr:5.8178e-04 | norm 0.2963 | dt 338.39ms | 1549372.79 tokens/sec
Step 2875 | loss: 3.388530 | lr:5.8176e-04 | norm 0.3356 | dt 337.95ms | 1551371.99 tokens/sec
Step 2876 | loss: 3.389924 | lr:5.8175e-04 | norm 0.3090 | dt 338.63ms | 1548265.56 tokens/sec
Step 2877 | loss: 3.373593 | lr:5.8173e-04 | norm 0.3015 | dt 337.97ms | 1551278.97 tokens/sec
Step 2878 | loss: 3.426374 | lr:5.8171e-04 | norm 0.3399 | dt 338.45ms | 1549103.20 tokens/sec
Step 2879 | loss: 3.377029 | lr:5.8170e-04 | norm 0.3462 | dt 339.53ms | 1544178.74 tokens/sec
Step 2880 | loss: 3.429456 | lr:5.8168e-04 | norm 0.3615 | dt 339.32ms | 1545104.24 tokens/sec
Step 2881 | loss: 3.491953 | lr:5.8166e-04 | norm 0.3222 | dt 338.14ms | 1550525.34 tokens/sec
Step 2882 | loss: 3.488135 | lr:5.8165e-04 | norm 0.3313 | dt 338.74ms | 1547737.05 tokens/sec
Step 2883 | loss: 3.560460 | lr:5.8163e-04 | norm 0.3409 | dt 339.45ms | 1544534.49 tokens/sec
Step 2884 | loss: 3.518274 | lr:5.8161e-04 | norm 0.3172 | dt 337.70ms | 1552516.55 tokens/sec
Step 2885 | loss: 3.503197 | lr:5.8160e-04 | norm 0.2773 | dt 338.06ms | 1550884.01 tokens/sec
Step 2886 | loss: 3.494739 | lr:5.8158e-04 | norm 0.3042 | dt 338.76ms | 1547654.27 tokens/sec
Step 2887 | loss: 3.504639 | lr:5.8156e-04 | norm 0.2845 | dt 338.72ms | 1547843.81 tokens/sec
Step 2888 | loss: 3.498141 | lr:5.8155e-04 | norm 0.2875 | dt 338.08ms | 1550801.98 tokens/sec
Step 2889 | loss: 3.518972 | lr:5.8153e-04 | norm 0.2850 | dt 338.21ms | 1550203.98 tokens/sec
Step 2890 | loss: 3.519527 | lr:5.8151e-04 | norm 0.2977 | dt 338.61ms | 1548339.69 tokens/sec
Step 2891 | loss: 3.527985 | lr:5.8150e-04 | norm 0.3351 | dt 337.91ms | 1551570.11 tokens/sec
Step 2892 | loss: 3.492601 | lr:5.8148e-04 | norm 0.3087 | dt 339.48ms | 1544376.12 tokens/sec
Step 2893 | loss: 3.508361 | lr:5.8146e-04 | norm 0.2723 | dt 338.72ms | 1547850.35 tokens/sec
Step 2894 | loss: 3.503466 | lr:5.8145e-04 | norm 0.2887 | dt 337.06ms | 1555489.32 tokens/sec
Step 2895 | loss: 3.490329 | lr:5.8143e-04 | norm 0.2917 | dt 338.85ms | 1547236.12 tokens/sec
Step 2896 | loss: 3.538918 | lr:5.8141e-04 | norm 0.2794 | dt 339.08ms | 1546223.26 tokens/sec
Step 2897 | loss: 3.537933 | lr:5.8139e-04 | norm 0.2780 | dt 337.32ms | 1554297.53 tokens/sec
Step 2898 | loss: 3.527040 | lr:5.8138e-04 | norm 0.3056 | dt 337.47ms | 1553567.31 tokens/sec
Step 2899 | loss: 3.504726 | lr:5.8136e-04 | norm 0.3276 | dt 338.40ms | 1549332.40 tokens/sec
Step 2900 | loss: 3.557755 | lr:5.8134e-04 | norm 0.3149 | dt 338.28ms | 1549848.90 tokens/sec
Step 2901 | loss: 3.513230 | lr:5.8133e-04 | norm 0.2721 | dt 338.23ms | 1550075.04 tokens/sec
Step 2902 | loss: 3.453241 | lr:5.8131e-04 | norm 0.2600 | dt 338.11ms | 1550653.26 tokens/sec
Step 2903 | loss: 3.454690 | lr:5.8129e-04 | norm 0.2877 | dt 337.57ms | 1553117.44 tokens/sec
Step 2904 | loss: 3.433593 | lr:5.8128e-04 | norm 0.2747 | dt 338.86ms | 1547232.85 tokens/sec
Step 2905 | loss: 3.368823 | lr:5.8126e-04 | norm 0.2867 | dt 337.88ms | 1551711.35 tokens/sec
Step 2906 | loss: 3.423866 | lr:5.8124e-04 | norm 0.3198 | dt 337.38ms | 1554014.14 tokens/sec
Step 2907 | loss: 3.484627 | lr:5.8123e-04 | norm 0.3556 | dt 337.68ms | 1552617.40 tokens/sec
Step 2908 | loss: 3.413929 | lr:5.8121e-04 | norm 0.3014 | dt 337.96ms | 1551330.40 tokens/sec
Step 2909 | loss: 3.445036 | lr:5.8119e-04 | norm 0.2954 | dt 339.52ms | 1544213.44 tokens/sec
Step 2910 | loss: 3.514182 | lr:5.8117e-04 | norm 0.3431 | dt 338.42ms | 1549239.62 tokens/sec
Step 2911 | loss: 3.414291 | lr:5.8116e-04 | norm 0.3219 | dt 337.62ms | 1552891.50 tokens/sec
Step 2912 | loss: 3.462086 | lr:5.8114e-04 | norm 0.2980 | dt 338.50ms | 1548854.43 tokens/sec
Step 2913 | loss: 3.484020 | lr:5.8112e-04 | norm 0.2998 | dt 338.10ms | 1550669.66 tokens/sec
Step 2914 | loss: 3.476295 | lr:5.8111e-04 | norm 0.2889 | dt 336.74ms | 1556952.98 tokens/sec
Step 2915 | loss: 3.420330 | lr:5.8109e-04 | norm 0.3292 | dt 337.98ms | 1551225.35 tokens/sec
Step 2916 | loss: 3.479592 | lr:5.8107e-04 | norm 0.3156 | dt 337.81ms | 1552042.09 tokens/sec
Step 2917 | loss: 3.405362 | lr:5.8106e-04 | norm 0.2863 | dt 337.76ms | 1552267.78 tokens/sec
Step 2918 | loss: 3.421351 | lr:5.8104e-04 | norm 0.3099 | dt 337.52ms | 1553376.35 tokens/sec
Step 2919 | loss: 3.356358 | lr:5.8102e-04 | norm 0.2943 | dt 338.32ms | 1549701.45 tokens/sec
Step 2920 | loss: 3.376394 | lr:5.8100e-04 | norm 0.2732 | dt 337.57ms | 1553107.56 tokens/sec
Step 2921 | loss: 3.330255 | lr:5.8099e-04 | norm 0.2615 | dt 338.00ms | 1551156.41 tokens/sec
Step 2922 | loss: 3.387294 | lr:5.8097e-04 | norm 0.2936 | dt 337.75ms | 1552285.31 tokens/sec
Step 2923 | loss: 3.383914 | lr:5.8095e-04 | norm 0.2492 | dt 337.90ms | 1551615.00 tokens/sec
Step 2924 | loss: 3.353548 | lr:5.8094e-04 | norm 0.2934 | dt 337.67ms | 1552682.08 tokens/sec
Step 2925 | loss: 3.374525 | lr:5.8092e-04 | norm 0.2832 | dt 338.05ms | 1550898.23 tokens/sec
Step 2926 | loss: 3.346034 | lr:5.8090e-04 | norm 0.2862 | dt 337.91ms | 1551576.68 tokens/sec
Step 2927 | loss: 3.391438 | lr:5.8089e-04 | norm 0.3156 | dt 337.02ms | 1555651.08 tokens/sec
Step 2928 | loss: 3.358003 | lr:5.8087e-04 | norm 0.2778 | dt 337.95ms | 1551378.56 tokens/sec
Step 2929 | loss: 3.559384 | lr:5.8085e-04 | norm 0.2839 | dt 340.65ms | 1539092.65 tokens/sec
Step 2930 | loss: 3.497993 | lr:5.8083e-04 | norm 0.2971 | dt 338.03ms | 1550999.96 tokens/sec
Step 2931 | loss: 3.555584 | lr:5.8082e-04 | norm 0.2842 | dt 337.66ms | 1552732.51 tokens/sec
Step 2932 | loss: 3.522913 | lr:5.8080e-04 | norm 0.2938 | dt 337.82ms | 1551994.99 tokens/sec
Step 2933 | loss: 3.544601 | lr:5.8078e-04 | norm 0.3178 | dt 338.57ms | 1548545.77 tokens/sec
Step 2934 | loss: 3.551946 | lr:5.8077e-04 | norm 0.3622 | dt 338.03ms | 1551002.15 tokens/sec
Step 2935 | loss: 3.537717 | lr:5.8075e-04 | norm 0.3474 | dt 337.80ms | 1552065.10 tokens/sec
Step 2936 | loss: 3.584800 | lr:5.8073e-04 | norm 0.3226 | dt 337.50ms | 1553459.75 tokens/sec
Step 2937 | loss: 3.476298 | lr:5.8071e-04 | norm 0.3203 | dt 338.48ms | 1548954.81 tokens/sec
Step 2938 | loss: 3.475523 | lr:5.8070e-04 | norm 0.2889 | dt 338.22ms | 1550125.30 tokens/sec
Step 2939 | loss: 3.562789 | lr:5.8068e-04 | norm 0.2691 | dt 337.56ms | 1553179.96 tokens/sec
Step 2940 | loss: 3.513079 | lr:5.8066e-04 | norm 0.2660 | dt 338.52ms | 1548778.07 tokens/sec
Step 2941 | loss: 3.478518 | lr:5.8065e-04 | norm 0.2832 | dt 337.87ms | 1551748.58 tokens/sec
Step 2942 | loss: 3.501033 | lr:5.8063e-04 | norm 0.2764 | dt 336.81ms | 1556619.03 tokens/sec
Step 2943 | loss: 3.487046 | lr:5.8061e-04 | norm 0.2405 | dt 337.86ms | 1551812.09 tokens/sec
Step 2944 | loss: 3.553092 | lr:5.8059e-04 | norm 0.2701 | dt 338.67ms | 1548060.65 tokens/sec
Step 2945 | loss: 3.476267 | lr:5.8058e-04 | norm 0.2978 | dt 338.03ms | 1551008.71 tokens/sec
Step 2946 | loss: 3.484224 | lr:5.8056e-04 | norm 0.2861 | dt 338.98ms | 1546640.86 tokens/sec
Step 2947 | loss: 3.465213 | lr:5.8054e-04 | norm 0.2539 | dt 339.81ms | 1542907.86 tokens/sec
Step 2948 | loss: 3.473169 | lr:5.8053e-04 | norm 0.2856 | dt 338.31ms | 1549728.75 tokens/sec
Step 2949 | loss: 3.436285 | lr:5.8051e-04 | norm 0.2605 | dt 337.43ms | 1553750.62 tokens/sec
Step 2950 | loss: 3.507944 | lr:5.8049e-04 | norm 0.2516 | dt 338.16ms | 1550430.23 tokens/sec
Step 2951 | loss: 3.543200 | lr:5.8047e-04 | norm 0.2744 | dt 337.78ms | 1552138.49 tokens/sec
Step 2952 | loss: 3.396696 | lr:5.8046e-04 | norm 0.2501 | dt 338.15ms | 1550449.90 tokens/sec
Step 2953 | loss: 3.398065 | lr:5.8044e-04 | norm 0.2993 | dt 338.04ms | 1550944.17 tokens/sec
Step 2954 | loss: 3.423122 | lr:5.8042e-04 | norm 0.2800 | dt 337.48ms | 1553517.92 tokens/sec
Step 2955 | loss: 3.423799 | lr:5.8040e-04 | norm 0.2938 | dt 338.55ms | 1548635.19 tokens/sec
Step 2956 | loss: 3.436207 | lr:5.8039e-04 | norm 0.3313 | dt 337.58ms | 1553093.30 tokens/sec
Step 2957 | loss: 3.385665 | lr:5.8037e-04 | norm 0.3239 | dt 338.10ms | 1550666.38 tokens/sec
Step 2958 | loss: 3.435771 | lr:5.8035e-04 | norm 0.2720 | dt 337.54ms | 1553246.88 tokens/sec
Step 2959 | loss: 3.402543 | lr:5.8034e-04 | norm 0.2879 | dt 337.62ms | 1552912.34 tokens/sec
Step 2960 | loss: 3.499632 | lr:5.8032e-04 | norm 0.3190 | dt 338.36ms | 1549512.54 tokens/sec
Step 2961 | loss: 3.486513 | lr:5.8030e-04 | norm 0.3170 | dt 337.75ms | 1552295.17 tokens/sec
Step 2962 | loss: 3.443779 | lr:5.8028e-04 | norm 0.2705 | dt 339.10ms | 1546131.94 tokens/sec
Step 2963 | loss: 3.570134 | lr:5.8027e-04 | norm 0.3218 | dt 337.84ms | 1551899.70 tokens/sec
Step 2964 | loss: 3.319261 | lr:5.8025e-04 | norm 0.3399 | dt 338.26ms | 1549972.34 tokens/sec
Step 2965 | loss: 3.394403 | lr:5.8023e-04 | norm 0.3385 | dt 338.95ms | 1546794.26 tokens/sec
Step 2966 | loss: 3.358778 | lr:5.8021e-04 | norm 0.3143 | dt 338.47ms | 1549001.72 tokens/sec
Step 2967 | loss: 3.369579 | lr:5.8020e-04 | norm 0.2887 | dt 338.81ms | 1547440.81 tokens/sec
Step 2968 | loss: 3.333558 | lr:5.8018e-04 | norm 0.2693 | dt 338.44ms | 1549114.12 tokens/sec
Step 2969 | loss: 3.420292 | lr:5.8016e-04 | norm 0.2661 | dt 338.54ms | 1548673.36 tokens/sec
Step 2970 | loss: 3.416600 | lr:5.8014e-04 | norm 0.2720 | dt 337.02ms | 1555649.98 tokens/sec
Step 2971 | loss: 3.368204 | lr:5.8013e-04 | norm 0.2707 | dt 338.06ms | 1550877.45 tokens/sec
Step 2972 | loss: 3.352268 | lr:5.8011e-04 | norm 0.2743 | dt 338.32ms | 1549661.04 tokens/sec
Step 2973 | loss: 3.377641 | lr:5.8009e-04 | norm 0.2758 | dt 338.47ms | 1549010.45 tokens/sec
Step 2974 | loss: 3.358578 | lr:5.8007e-04 | norm 0.2610 | dt 336.95ms | 1555983.51 tokens/sec
Step 2975 | loss: 3.401094 | lr:5.8006e-04 | norm 0.2526 | dt 337.14ms | 1555096.62 tokens/sec
Step 2976 | loss: 3.593871 | lr:5.8004e-04 | norm 0.2771 | dt 338.96ms | 1546739.86 tokens/sec
Step 2977 | loss: 3.547350 | lr:5.8002e-04 | norm 0.2983 | dt 339.30ms | 1545195.44 tokens/sec
Step 2978 | loss: 3.505398 | lr:5.8001e-04 | norm 0.2883 | dt 339.25ms | 1545450.64 tokens/sec
Step 2979 | loss: 3.466295 | lr:5.7999e-04 | norm 0.2795 | dt 338.18ms | 1550343.87 tokens/sec
Step 2980 | loss: 3.563205 | lr:5.7997e-04 | norm 0.2808 | dt 338.36ms | 1549519.09 tokens/sec
Step 2981 | loss: 3.515282 | lr:5.7995e-04 | norm 0.2764 | dt 338.26ms | 1549952.67 tokens/sec
Step 2982 | loss: 3.521522 | lr:5.7994e-04 | norm 0.3475 | dt 337.67ms | 1552650.28 tokens/sec
Step 2983 | loss: 3.556415 | lr:5.7992e-04 | norm 0.3540 | dt 338.92ms | 1546925.92 tokens/sec
Step 2984 | loss: 3.491826 | lr:5.7990e-04 | norm 0.3816 | dt 337.97ms | 1551304.14 tokens/sec
Step 2985 | loss: 3.470568 | lr:5.7988e-04 | norm 0.3403 | dt 338.05ms | 1550934.33 tokens/sec
Step 2986 | loss: 3.507408 | lr:5.7987e-04 | norm 0.2797 | dt 338.88ms | 1547133.79 tokens/sec
Step 2987 | loss: 3.480695 | lr:5.7985e-04 | norm 0.2601 | dt 337.41ms | 1553868.10 tokens/sec
Step 2988 | loss: 3.474888 | lr:5.7983e-04 | norm 0.2838 | dt 338.05ms | 1550936.52 tokens/sec
Step 2989 | loss: 3.457154 | lr:5.7981e-04 | norm 0.2779 | dt 339.64ms | 1543648.68 tokens/sec
Step 2990 | loss: 3.526126 | lr:5.7980e-04 | norm 0.3038 | dt 337.81ms | 1552005.95 tokens/sec
Step 2991 | loss: 3.511303 | lr:5.7978e-04 | norm 0.2744 | dt 337.94ms | 1551415.77 tokens/sec
Step 2992 | loss: 3.492051 | lr:5.7976e-04 | norm 0.3363 | dt 338.67ms | 1548094.44 tokens/sec
Step 2993 | loss: 3.474181 | lr:5.7974e-04 | norm 0.3493 | dt 337.90ms | 1551600.77 tokens/sec
Step 2994 | loss: 3.512881 | lr:5.7973e-04 | norm 0.2878 | dt 337.56ms | 1553175.58 tokens/sec
Step 2995 | loss: 3.510986 | lr:5.7971e-04 | norm 0.3059 | dt 337.86ms | 1551792.38 tokens/sec
Step 2996 | loss: 3.429869 | lr:5.7969e-04 | norm 0.2904 | dt 338.07ms | 1550809.64 tokens/sec
Step 2997 | loss: 3.494241 | lr:5.7967e-04 | norm 0.2959 | dt 337.28ms | 1554465.63 tokens/sec
Step 2998 | loss: 3.523706 | lr:5.7965e-04 | norm 0.2988 | dt 338.38ms | 1549397.90 tokens/sec
Step 2999 | loss: 3.435716 | lr:5.7964e-04 | norm 0.3526 | dt 339.15ms | 1545904.77 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 3000: 3.5168
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2729/10042=0.2718


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what's the difference between the two? I would say that in terms of a given language, the two are a
rank 5 sample 1 >Hello, I'm a language model, using what I'm reading for a long time before I got my attention. And if it is right, if I'm
rank 5 sample 2 >Hello, I'm a language model, so I'd love looking at its ability to work with your ideas, but here is an example of what I mean.


ddp_rank 1: ####### Printing generated samples ####### 

rank 5 sample 3 >Hello, I'm a language model, i think, I'm looking for a new way of teaching in the classroom. For the most part, it is about


rank 1 sample 0 >Hello, I'm a language model, and I find it quite useful. Why do we do this?, and what do we know about our language and culture?
rank 1 sample 1 >Hello, I'm a language model, which is the process of creating a code or a structure out of context. You and each server of your computer, and
rank 1 sample 2 >Hello, I'm a language model, but all my lessons are always in the background. I am writing this course today, and I'll do my job all
rank 1 sample 3 >Hello, I'm a language model, and I'm always working under the sun like I did work out so, hopefully, how should I go about my life




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I want to get it right, you have to have your own script with lots of language examples (with some exceptions
rank 4 sample 1 >Hello, I'm a language model, let's say people want to write sentences like this.<|endoftext|>The latest findings have provided strong support for the idea that people
rank 4 sample 2 >Hello, I'm a language model, I created a system called Language Models and then implemented the model on the computer. At the same time, I am using
rank 4 sample 3 >Hello, I'm a language model, and my husband always makes mistakes. (My father would carelessly tell me, "I don't have a machine."




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I hope you've learned a little when you need to explain to him how programming works. With so much effort and
rank 7 sample 1 >Hello, I'm a language model, right? The answer is "You use it all the time. In the first cycle you can make new changes, which


ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 2 >Hello, I'm a language model, and I think these is a "game of the future." I believe "game" is a game of the future and
rank 7 sample 3 >Hello, I'm a language model, so it's a nice introduction to C++:
But, if you're using C++ C++, you really


rank 2 sample 0 >Hello, I'm a language model, we'll use all the options in the table, and you will see that you don't have to use this. In
rank 2 sample 1 >Hello, I'm a language model, but I use this to my advantage (so I prefer to say a single word, but I can call a single word
rank 2 sample 2 >Hello, I'm a language model, so I have a good idea for each sentence.
What do you really like with this method? Share your thoughts!
rank 2 sample 3 >Hello, I'm a language model, but if I'm going to do the things I like, and I'm at that time...why? If you know




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, just like I'm used to build my code. But it doesn't take anything kind of data to do it. Let
rank 6 sample 1 >Hello, I'm a language model, and I am a very professional linguist—and one who is a professor of English and one of my own. I
rank 6 sample 2 >Hello, I'm a language model, but in a sense, you don't have to be a linguist to work with these models. When you look at
rank 6 sample 3 >Hello, I'm a language model, and I am just beginning to teach my lesson here. I'm making my best friends and there are some of my questions




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm going to take the students outside to see how their brains used synch to represent these brain-sourced
rank 3 sample 1 >Hello, I'm a language model, and you have to be inspired! I have already explained to you that you can use the language you are using on an
rank 3 sample 2 >Hello, I'm a language model, and it seems to be a relatively new tool:
If you're interested in seeing how we change our lives or make
rank 3 sample 3 >Hello, I'm a language model, and have a couple of choices that I think should help in the design process.
A typical script
The script should




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I would like you to understand. Of course, I'm using those tools to do tasks and have an idea that
rank 0 sample 1 >Hello, I'm a language model, so when I was working on it then I thought like: "Well, we have been on a different project at different
rank 0 sample 2 >Hello, I'm a language model, so I used this question at the first post as a guide as well.
1. What is the difference between the
rank 0 sample 3 >Hello, I'm a language model, and then I have to write my course on something. I already know I can give the feedback. My course will not


Step 3000 | loss: 3.428494 | lr:5.7962e-04 | norm 0.3800 | dt 18627.31ms | 28146.20 tokens/sec
Step 3001 | loss: 3.507362 | lr:5.7960e-04 | norm 0.3377 | dt 336.61ms | 1557544.06 tokens/sec
Step 3002 | loss: 3.450272 | lr:5.7958e-04 | norm 0.2881 | dt 336.84ms | 1556469.19 tokens/sec
Step 3003 | loss: 3.409132 | lr:5.7957e-04 | norm 0.3199 | dt 336.49ms | 1558122.35 tokens/sec
Step 3004 | loss: 3.409216 | lr:5.7955e-04 | norm 0.2779 | dt 336.25ms | 1559222.72 tokens/sec
Step 3005 | loss: 3.418483 | lr:5.7953e-04 | norm 0.2808 | dt 336.43ms | 1558376.31 tokens/sec
Step 3006 | loss: 3.404952 | lr:5.7951e-04 | norm 0.3190 | dt 337.76ms | 1552239.29 tokens/sec
Step 3007 | loss: 3.437458 | lr:5.7950e-04 | norm 0.2939 | dt 336.05ms | 1560171.88 tokens/sec
Step 3008 | loss: 3.506002 | lr:5.7948e-04 | norm 0.3101 | dt 336.15ms | 1559688.30 tokens/sec
Step 3009 | loss: 3.394219 | lr:5.7946e-04 | norm 0.2626 | dt 336.31ms | 1558958.53 tokens/sec
Step 3010 | loss: 3.475135 | lr:5.7944e-04 | norm 0.2814 | dt 336.04ms | 1560186.27 tokens/sec
Step 3011 | loss: 3.391693 | lr:5.7943e-04 | norm 0.2778 | dt 336.34ms | 1558824.82 tokens/sec
Step 3012 | loss: 3.360925 | lr:5.7941e-04 | norm 0.2543 | dt 336.31ms | 1558943.06 tokens/sec
Step 3013 | loss: 3.370014 | lr:5.7939e-04 | norm 0.2681 | dt 336.64ms | 1557403.97 tokens/sec
Step 3014 | loss: 3.331045 | lr:5.7937e-04 | norm 0.2714 | dt 336.82ms | 1556570.55 tokens/sec
Step 3015 | loss: 3.335444 | lr:5.7935e-04 | norm 0.2662 | dt 336.60ms | 1557614.67 tokens/sec
Step 3016 | loss: 3.391538 | lr:5.7934e-04 | norm 0.2705 | dt 337.05ms | 1555511.33 tokens/sec
Step 3017 | loss: 3.305891 | lr:5.7932e-04 | norm 0.2716 | dt 336.98ms | 1555840.39 tokens/sec
Step 3018 | loss: 3.345563 | lr:5.7930e-04 | norm 0.2846 | dt 336.57ms | 1557736.04 tokens/sec
Step 3019 | loss: 3.326588 | lr:5.7928e-04 | norm 0.3349 | dt 337.40ms | 1553928.49 tokens/sec
Step 3020 | loss: 3.305902 | lr:5.7927e-04 | norm 0.3227 | dt 336.71ms | 1557108.42 tokens/sec
Step 3021 | loss: 3.329751 | lr:5.7925e-04 | norm 0.2908 | dt 336.84ms | 1556496.73 tokens/sec
Step 3022 | loss: 3.332238 | lr:5.7923e-04 | norm 0.2706 | dt 336.72ms | 1557034.55 tokens/sec
Step 3023 | loss: 3.512521 | lr:5.7921e-04 | norm 0.2892 | dt 898.00ms | 583836.70 tokens/sec
Step 3024 | loss: 3.525752 | lr:5.7919e-04 | norm 0.3057 | dt 334.65ms | 1566686.61 tokens/sec
Step 3025 | loss: 3.458616 | lr:5.7918e-04 | norm 0.3198 | dt 335.16ms | 1564289.38 tokens/sec
Step 3026 | loss: 3.483459 | lr:5.7916e-04 | norm 0.2866 | dt 338.33ms | 1549653.40 tokens/sec
Step 3027 | loss: 3.542923 | lr:5.7914e-04 | norm 0.3077 | dt 336.78ms | 1556756.78 tokens/sec
Step 3028 | loss: 3.531013 | lr:5.7912e-04 | norm 0.3381 | dt 336.40ms | 1558521.00 tokens/sec
Step 3029 | loss: 3.510578 | lr:5.7911e-04 | norm 0.3340 | dt 336.40ms | 1558547.51 tokens/sec
Step 3030 | loss: 3.495818 | lr:5.7909e-04 | norm 0.2990 | dt 337.04ms | 1555558.64 tokens/sec
Step 3031 | loss: 3.485207 | lr:5.7907e-04 | norm 0.3015 | dt 336.21ms | 1559412.90 tokens/sec
Step 3032 | loss: 3.515789 | lr:5.7905e-04 | norm 0.2909 | dt 336.12ms | 1559828.81 tokens/sec
Step 3033 | loss: 3.584394 | lr:5.7903e-04 | norm 0.2716 | dt 337.74ms | 1552349.96 tokens/sec
Step 3034 | loss: 3.517593 | lr:5.7902e-04 | norm 0.2601 | dt 336.81ms | 1556612.42 tokens/sec
Step 3035 | loss: 3.529765 | lr:5.7900e-04 | norm 0.2780 | dt 337.13ms | 1555154.91 tokens/sec
Step 3036 | loss: 3.462185 | lr:5.7898e-04 | norm 0.2739 | dt 337.27ms | 1554506.29 tokens/sec
Step 3037 | loss: 3.428256 | lr:5.7896e-04 | norm 0.2707 | dt 337.65ms | 1552750.05 tokens/sec
Step 3038 | loss: 3.455534 | lr:5.7895e-04 | norm 0.2704 | dt 338.61ms | 1548340.78 tokens/sec
Step 3039 | loss: 3.511612 | lr:5.7893e-04 | norm 0.2735 | dt 1002.64ms | 522906.40 tokens/sec
Step 3040 | loss: 3.461014 | lr:5.7891e-04 | norm 0.2829 | dt 337.71ms | 1552493.53 tokens/sec
Step 3041 | loss: 3.443649 | lr:5.7889e-04 | norm 0.3091 | dt 338.00ms | 1551134.53 tokens/sec
Step 3042 | loss: 3.447030 | lr:5.7887e-04 | norm 0.2927 | dt 338.62ms | 1548318.98 tokens/sec
Step 3043 | loss: 3.450073 | lr:5.7886e-04 | norm 0.2874 | dt 337.21ms | 1554804.15 tokens/sec
Step 3044 | loss: 3.437788 | lr:5.7884e-04 | norm 0.2674 | dt 337.09ms | 1555316.60 tokens/sec
Step 3045 | loss: 3.370220 | lr:5.7882e-04 | norm 0.2471 | dt 337.82ms | 1551962.13 tokens/sec
Step 3046 | loss: 3.441971 | lr:5.7880e-04 | norm 0.2885 | dt 340.03ms | 1541884.44 tokens/sec
Step 3047 | loss: 3.405867 | lr:5.7878e-04 | norm 0.2587 | dt 337.55ms | 1553196.42 tokens/sec
Step 3048 | loss: 3.393484 | lr:5.7877e-04 | norm 0.2580 | dt 337.81ms | 1552007.04 tokens/sec
Step 3049 | loss: 3.408065 | lr:5.7875e-04 | norm 0.2834 | dt 337.75ms | 1552296.27 tokens/sec
Step 3050 | loss: 3.391333 | lr:5.7873e-04 | norm 0.2558 | dt 338.48ms | 1548926.44 tokens/sec
Step 3051 | loss: 3.367115 | lr:5.7871e-04 | norm 0.2638 | dt 337.36ms | 1554084.43 tokens/sec
Step 3052 | loss: 3.505865 | lr:5.7869e-04 | norm 0.2703 | dt 339.10ms | 1546133.02 tokens/sec
Step 3053 | loss: 3.479522 | lr:5.7868e-04 | norm 0.2596 | dt 337.60ms | 1552989.11 tokens/sec
Step 3054 | loss: 3.417653 | lr:5.7866e-04 | norm 0.2779 | dt 337.98ms | 1551250.51 tokens/sec
Step 3055 | loss: 3.486980 | lr:5.7864e-04 | norm 0.2587 | dt 338.08ms | 1550779.02 tokens/sec
Step 3056 | loss: 3.383989 | lr:5.7862e-04 | norm 0.2686 | dt 338.24ms | 1550037.89 tokens/sec
Step 3057 | loss: 3.357379 | lr:5.7860e-04 | norm 0.2805 | dt 337.74ms | 1552345.58 tokens/sec
Step 3058 | loss: 3.348188 | lr:5.7859e-04 | norm 0.2857 | dt 338.26ms | 1549936.29 tokens/sec
Step 3059 | loss: 3.367175 | lr:5.7857e-04 | norm 0.2875 | dt 337.89ms | 1551656.60 tokens/sec
Step 3060 | loss: 3.352781 | lr:5.7855e-04 | norm 0.2850 | dt 339.88ms | 1542575.59 tokens/sec
Step 3061 | loss: 3.316536 | lr:5.7853e-04 | norm 0.2673 | dt 339.08ms | 1546226.52 tokens/sec
Step 3062 | loss: 3.397385 | lr:5.7851e-04 | norm 0.2592 | dt 339.33ms | 1545090.13 tokens/sec
Step 3063 | loss: 3.330327 | lr:5.7850e-04 | norm 0.2756 | dt 338.48ms | 1548929.71 tokens/sec
Step 3064 | loss: 3.346714 | lr:5.7848e-04 | norm 0.3545 | dt 338.62ms | 1548299.36 tokens/sec
Step 3065 | loss: 3.374624 | lr:5.7846e-04 | norm 0.3218 | dt 338.61ms | 1548346.23 tokens/sec
Step 3066 | loss: 3.308654 | lr:5.7844e-04 | norm 0.2717 | dt 339.90ms | 1542457.65 tokens/sec
Step 3067 | loss: 3.305238 | lr:5.7842e-04 | norm 0.3107 | dt 339.00ms | 1546560.37 tokens/sec
Step 3068 | loss: 3.423594 | lr:5.7841e-04 | norm 0.3089 | dt 338.45ms | 1549094.47 tokens/sec
Step 3069 | loss: 3.544824 | lr:5.7839e-04 | norm 0.3894 | dt 338.17ms | 1550366.83 tokens/sec
Step 3070 | loss: 3.471196 | lr:5.7837e-04 | norm 0.3308 | dt 338.09ms | 1550740.74 tokens/sec
Step 3071 | loss: 3.550798 | lr:5.7835e-04 | norm 0.3396 | dt 338.36ms | 1549513.63 tokens/sec
Step 3072 | loss: 3.499193 | lr:5.7833e-04 | norm 0.3795 | dt 337.97ms | 1551289.91 tokens/sec
Step 3073 | loss: 3.556959 | lr:5.7831e-04 | norm 0.3771 | dt 339.74ms | 1543185.04 tokens/sec
Step 3074 | loss: 3.498477 | lr:5.7830e-04 | norm 0.3248 | dt 338.51ms | 1548830.43 tokens/sec
Step 3075 | loss: 3.450440 | lr:5.7828e-04 | norm 0.3080 | dt 338.03ms | 1551010.90 tokens/sec
Step 3076 | loss: 3.514263 | lr:5.7826e-04 | norm 0.2984 | dt 338.66ms | 1548117.33 tokens/sec
Step 3077 | loss: 3.490208 | lr:5.7824e-04 | norm 0.3128 | dt 338.16ms | 1550435.69 tokens/sec
Step 3078 | loss: 3.434967 | lr:5.7822e-04 | norm 0.3112 | dt 338.51ms | 1548793.35 tokens/sec
Step 3079 | loss: 3.564268 | lr:5.7821e-04 | norm 0.3052 | dt 338.91ms | 1547003.18 tokens/sec
Step 3080 | loss: 3.539560 | lr:5.7819e-04 | norm 0.3188 | dt 338.99ms | 1546616.93 tokens/sec
Step 3081 | loss: 3.438523 | lr:5.7817e-04 | norm 0.3095 | dt 339.67ms | 1543533.83 tokens/sec
Step 3082 | loss: 3.522842 | lr:5.7815e-04 | norm 0.2747 | dt 338.78ms | 1547568.22 tokens/sec
Step 3083 | loss: 3.489913 | lr:5.7813e-04 | norm 0.2746 | dt 339.10ms | 1546112.37 tokens/sec
Step 3084 | loss: 3.444531 | lr:5.7811e-04 | norm 0.2469 | dt 338.64ms | 1548227.41 tokens/sec
Step 3085 | loss: 3.459206 | lr:5.7810e-04 | norm 0.2689 | dt 338.80ms | 1547478.92 tokens/sec
Step 3086 | loss: 3.452152 | lr:5.7808e-04 | norm 0.2608 | dt 346.72ms | 1512148.12 tokens/sec
Step 3087 | loss: 3.428727 | lr:5.7806e-04 | norm 0.2753 | dt 338.99ms | 1546631.07 tokens/sec
Step 3088 | loss: 3.453375 | lr:5.7804e-04 | norm 0.2832 | dt 338.78ms | 1547561.69 tokens/sec
Step 3089 | loss: 3.482903 | lr:5.7802e-04 | norm 0.2743 | dt 339.30ms | 1545204.12 tokens/sec
Step 3090 | loss: 3.494956 | lr:5.7801e-04 | norm 0.3163 | dt 345.11ms | 1519211.19 tokens/sec
Step 3091 | loss: 3.357394 | lr:5.7799e-04 | norm 0.2889 | dt 338.55ms | 1548610.11 tokens/sec
Step 3092 | loss: 3.339464 | lr:5.7797e-04 | norm 0.2723 | dt 339.16ms | 1545856.95 tokens/sec
Step 3093 | loss: 3.422563 | lr:5.7795e-04 | norm 0.2935 | dt 339.08ms | 1546204.78 tokens/sec
Step 3094 | loss: 3.389555 | lr:5.7793e-04 | norm 0.2631 | dt 338.40ms | 1549301.84 tokens/sec
Step 3095 | loss: 3.433629 | lr:5.7791e-04 | norm 0.2737 | dt 339.59ms | 1543874.10 tokens/sec
Step 3096 | loss: 3.419038 | lr:5.7790e-04 | norm 0.2878 | dt 340.86ms | 1538133.45 tokens/sec
Step 3097 | loss: 3.427513 | lr:5.7788e-04 | norm 0.2640 | dt 339.12ms | 1546023.24 tokens/sec
Step 3098 | loss: 3.489644 | lr:5.7786e-04 | norm 0.2621 | dt 338.74ms | 1547752.30 tokens/sec
Step 3099 | loss: 3.453217 | lr:5.7784e-04 | norm 0.2989 | dt 338.81ms | 1547461.50 tokens/sec
Step 3100 | loss: 3.445607 | lr:5.7782e-04 | norm 0.3197 | dt 339.26ms | 1545369.18 tokens/sec
Step 3101 | loss: 3.476790 | lr:5.7780e-04 | norm 0.2948 | dt 340.11ms | 1541540.72 tokens/sec
Step 3102 | loss: 3.452204 | lr:5.7779e-04 | norm 0.2890 | dt 338.62ms | 1548292.82 tokens/sec
Step 3103 | loss: 3.464639 | lr:5.7777e-04 | norm 0.2797 | dt 338.60ms | 1548380.03 tokens/sec
Step 3104 | loss: 3.334365 | lr:5.7775e-04 | norm 0.2802 | dt 339.82ms | 1542862.39 tokens/sec
Step 3105 | loss: 3.300398 | lr:5.7773e-04 | norm 0.2930 | dt 339.28ms | 1545276.88 tokens/sec
Step 3106 | loss: 3.329540 | lr:5.7771e-04 | norm 0.2742 | dt 338.70ms | 1547961.49 tokens/sec
Step 3107 | loss: 3.352175 | lr:5.7769e-04 | norm 0.3382 | dt 339.49ms | 1544357.68 tokens/sec
Step 3108 | loss: 3.390613 | lr:5.7768e-04 | norm 0.2962 | dt 337.56ms | 1553170.09 tokens/sec
Step 3109 | loss: 3.315081 | lr:5.7766e-04 | norm 0.3027 | dt 339.02ms | 1546494.02 tokens/sec
Step 3110 | loss: 3.297493 | lr:5.7764e-04 | norm 0.2704 | dt 338.15ms | 1550451.00 tokens/sec
Step 3111 | loss: 3.370759 | lr:5.7762e-04 | norm 0.2690 | dt 338.57ms | 1548558.85 tokens/sec
Step 3112 | loss: 3.345597 | lr:5.7760e-04 | norm 0.2896 | dt 338.18ms | 1550314.36 tokens/sec
Step 3113 | loss: 3.297595 | lr:5.7758e-04 | norm 0.2378 | dt 338.28ms | 1549875.11 tokens/sec
Step 3114 | loss: 3.340942 | lr:5.7756e-04 | norm 0.2674 | dt 338.79ms | 1547543.17 tokens/sec
Step 3115 | loss: 3.387681 | lr:5.7755e-04 | norm 0.2996 | dt 338.33ms | 1549650.12 tokens/sec
Step 3116 | loss: 3.506401 | lr:5.7753e-04 | norm 0.3505 | dt 338.25ms | 1549989.82 tokens/sec
Step 3117 | loss: 3.550194 | lr:5.7751e-04 | norm 0.3485 | dt 339.14ms | 1545935.20 tokens/sec
Step 3118 | loss: 3.441218 | lr:5.7749e-04 | norm 0.2827 | dt 338.04ms | 1550957.30 tokens/sec
Step 3119 | loss: 3.495013 | lr:5.7747e-04 | norm 0.2874 | dt 337.77ms | 1552207.52 tokens/sec
Step 3120 | loss: 3.447103 | lr:5.7745e-04 | norm 0.3271 | dt 338.38ms | 1549417.55 tokens/sec
Step 3121 | loss: 3.511047 | lr:5.7744e-04 | norm 0.2933 | dt 338.29ms | 1549837.97 tokens/sec
Step 3122 | loss: 3.471600 | lr:5.7742e-04 | norm 0.3198 | dt 338.53ms | 1548704.99 tokens/sec
Step 3123 | loss: 3.436152 | lr:5.7740e-04 | norm 0.3348 | dt 340.11ms | 1541528.84 tokens/sec
Step 3124 | loss: 3.434113 | lr:5.7738e-04 | norm 0.2964 | dt 337.97ms | 1551307.42 tokens/sec
Step 3125 | loss: 3.429503 | lr:5.7736e-04 | norm 0.2882 | dt 338.09ms | 1550713.40 tokens/sec
Step 3126 | loss: 3.466387 | lr:5.7734e-04 | norm 0.2741 | dt 338.62ms | 1548287.37 tokens/sec
Step 3127 | loss: 3.425548 | lr:5.7732e-04 | norm 0.3477 | dt 338.28ms | 1549845.62 tokens/sec
Step 3128 | loss: 3.525783 | lr:5.7731e-04 | norm 0.3634 | dt 338.10ms | 1550686.06 tokens/sec
Step 3129 | loss: 3.493209 | lr:5.7729e-04 | norm 0.3095 | dt 339.65ms | 1543627.01 tokens/sec
Step 3130 | loss: 3.541314 | lr:5.7727e-04 | norm 0.3136 | dt 338.92ms | 1546945.50 tokens/sec
Step 3131 | loss: 3.482433 | lr:5.7725e-04 | norm 0.3012 | dt 337.76ms | 1552241.48 tokens/sec
Step 3132 | loss: 3.506175 | lr:5.7723e-04 | norm 0.2902 | dt 339.65ms | 1543602.09 tokens/sec
Step 3133 | loss: 3.550107 | lr:5.7721e-04 | norm 0.2927 | dt 337.97ms | 1551288.82 tokens/sec
Step 3134 | loss: 3.493817 | lr:5.7719e-04 | norm 0.2850 | dt 338.33ms | 1549642.48 tokens/sec
Step 3135 | loss: 3.487905 | lr:5.7718e-04 | norm 0.2765 | dt 340.86ms | 1538120.54 tokens/sec
Step 3136 | loss: 3.462165 | lr:5.7716e-04 | norm 0.2691 | dt 339.29ms | 1545238.87 tokens/sec
Step 3137 | loss: 3.439606 | lr:5.7714e-04 | norm 0.2664 | dt 338.21ms | 1550190.87 tokens/sec
Step 3138 | loss: 3.444995 | lr:5.7712e-04 | norm 0.3181 | dt 338.21ms | 1550171.20 tokens/sec
Step 3139 | loss: 3.456450 | lr:5.7710e-04 | norm 0.3121 | dt 338.57ms | 1548549.04 tokens/sec
Step 3140 | loss: 3.422842 | lr:5.7708e-04 | norm 0.2905 | dt 339.94ms | 1542295.38 tokens/sec
Step 3141 | loss: 3.384886 | lr:5.7706e-04 | norm 0.2806 | dt 338.50ms | 1548870.80 tokens/sec
Step 3142 | loss: 3.409737 | lr:5.7705e-04 | norm 0.2820 | dt 341.01ms | 1537464.56 tokens/sec
Step 3143 | loss: 3.423183 | lr:5.7703e-04 | norm 0.2813 | dt 340.29ms | 1540706.92 tokens/sec
Step 3144 | loss: 3.461440 | lr:5.7701e-04 | norm 0.2822 | dt 338.60ms | 1548388.75 tokens/sec
Step 3145 | loss: 3.443239 | lr:5.7699e-04 | norm 0.2868 | dt 338.77ms | 1547627.04 tokens/sec
Step 3146 | loss: 3.447349 | lr:5.7697e-04 | norm 0.2580 | dt 341.31ms | 1536109.19 tokens/sec
Step 3147 | loss: 3.394938 | lr:5.7695e-04 | norm 0.2624 | dt 339.65ms | 1543624.84 tokens/sec
Step 3148 | loss: 3.465111 | lr:5.7693e-04 | norm 0.2738 | dt 338.86ms | 1547223.05 tokens/sec
Step 3149 | loss: 3.470908 | lr:5.7691e-04 | norm 0.2823 | dt 338.79ms | 1547508.32 tokens/sec
Step 3150 | loss: 3.466824 | lr:5.7690e-04 | norm 0.2681 | dt 338.69ms | 1547975.65 tokens/sec
Step 3151 | loss: 3.289859 | lr:5.7688e-04 | norm 0.2820 | dt 338.13ms | 1550547.20 tokens/sec
Step 3152 | loss: 3.356462 | lr:5.7686e-04 | norm 0.2853 | dt 337.96ms | 1551328.21 tokens/sec
Step 3153 | loss: 3.398487 | lr:5.7684e-04 | norm 0.2843 | dt 338.66ms | 1548121.69 tokens/sec
Step 3154 | loss: 3.318325 | lr:5.7682e-04 | norm 0.2503 | dt 338.68ms | 1548026.87 tokens/sec
Step 3155 | loss: 3.314665 | lr:5.7680e-04 | norm 0.2734 | dt 337.44ms | 1553718.79 tokens/sec
Step 3156 | loss: 3.349120 | lr:5.7678e-04 | norm 0.2801 | dt 340.15ms | 1541346.23 tokens/sec
Step 3157 | loss: 3.286231 | lr:5.7676e-04 | norm 0.2881 | dt 339.57ms | 1543964.07 tokens/sec
Step 3158 | loss: 3.307750 | lr:5.7675e-04 | norm 0.2647 | dt 338.48ms | 1548953.71 tokens/sec
Step 3159 | loss: 3.360136 | lr:5.7673e-04 | norm 0.2790 | dt 339.37ms | 1544902.34 tokens/sec
Step 3160 | loss: 3.299240 | lr:5.7671e-04 | norm 0.2667 | dt 339.29ms | 1545262.76 tokens/sec
Step 3161 | loss: 3.290389 | lr:5.7669e-04 | norm 0.2702 | dt 337.44ms | 1553727.57 tokens/sec
Step 3162 | loss: 3.379810 | lr:5.7667e-04 | norm 0.2767 | dt 338.06ms | 1550868.70 tokens/sec
Step 3163 | loss: 3.438758 | lr:5.7665e-04 | norm 0.2945 | dt 338.87ms | 1547177.33 tokens/sec
Step 3164 | loss: 3.472295 | lr:5.7663e-04 | norm 0.2854 | dt 336.65ms | 1557366.47 tokens/sec
Step 3165 | loss: 3.438996 | lr:5.7661e-04 | norm 0.2666 | dt 338.59ms | 1548458.53 tokens/sec
Step 3166 | loss: 3.458104 | lr:5.7660e-04 | norm 0.3130 | dt 338.54ms | 1548685.36 tokens/sec
Step 3167 | loss: 3.464367 | lr:5.7658e-04 | norm 0.2794 | dt 337.59ms | 1553050.53 tokens/sec
Step 3168 | loss: 3.474079 | lr:5.7656e-04 | norm 0.2893 | dt 337.56ms | 1553177.77 tokens/sec
Step 3169 | loss: 3.479293 | lr:5.7654e-04 | norm 0.2803 | dt 337.53ms | 1553294.06 tokens/sec
Step 3170 | loss: 3.417453 | lr:5.7652e-04 | norm 0.2779 | dt 338.02ms | 1551039.34 tokens/sec
Step 3171 | loss: 3.471337 | lr:5.7650e-04 | norm 0.2643 | dt 337.70ms | 1552548.34 tokens/sec
Step 3172 | loss: 3.529905 | lr:5.7648e-04 | norm 0.2934 | dt 337.54ms | 1553251.27 tokens/sec
Step 3173 | loss: 3.457101 | lr:5.7646e-04 | norm 0.3033 | dt 338.36ms | 1549499.43 tokens/sec
Step 3174 | loss: 3.457234 | lr:5.7645e-04 | norm 0.3196 | dt 338.00ms | 1551136.72 tokens/sec
Step 3175 | loss: 3.460701 | lr:5.7643e-04 | norm 0.3195 | dt 337.91ms | 1551549.31 tokens/sec
Step 3176 | loss: 3.480608 | lr:5.7641e-04 | norm 0.3167 | dt 338.18ms | 1550313.27 tokens/sec
Step 3177 | loss: 3.450309 | lr:5.7639e-04 | norm 0.3103 | dt 338.31ms | 1549735.30 tokens/sec
Step 3178 | loss: 3.390689 | lr:5.7637e-04 | norm 0.3919 | dt 337.85ms | 1551815.38 tokens/sec
Step 3179 | loss: 3.541178 | lr:5.7635e-04 | norm 0.3911 | dt 338.25ms | 1550000.74 tokens/sec
Step 3180 | loss: 3.481801 | lr:5.7633e-04 | norm 0.4091 | dt 338.66ms | 1548107.52 tokens/sec
Step 3181 | loss: 3.435925 | lr:5.7631e-04 | norm 0.3790 | dt 337.57ms | 1553142.67 tokens/sec
Step 3182 | loss: 3.424060 | lr:5.7629e-04 | norm 0.3110 | dt 338.25ms | 1549985.45 tokens/sec
Step 3183 | loss: 3.420568 | lr:5.7627e-04 | norm 0.3182 | dt 338.19ms | 1550288.13 tokens/sec
Step 3184 | loss: 3.433262 | lr:5.7626e-04 | norm 0.3262 | dt 338.34ms | 1549571.50 tokens/sec
Step 3185 | loss: 3.423965 | lr:5.7624e-04 | norm 0.2937 | dt 339.68ms | 1543485.08 tokens/sec
Step 3186 | loss: 3.351161 | lr:5.7622e-04 | norm 0.2832 | dt 338.38ms | 1549396.81 tokens/sec
Step 3187 | loss: 3.416674 | lr:5.7620e-04 | norm 0.2798 | dt 338.05ms | 1550901.51 tokens/sec
Step 3188 | loss: 3.399139 | lr:5.7618e-04 | norm 0.2687 | dt 338.87ms | 1547169.71 tokens/sec
Step 3189 | loss: 3.361731 | lr:5.7616e-04 | norm 0.3074 | dt 337.97ms | 1551268.02 tokens/sec
Step 3190 | loss: 3.385453 | lr:5.7614e-04 | norm 0.2841 | dt 337.72ms | 1552425.58 tokens/sec
Step 3191 | loss: 3.376639 | lr:5.7612e-04 | norm 0.2804 | dt 337.72ms | 1552456.27 tokens/sec
Step 3192 | loss: 3.390322 | lr:5.7610e-04 | norm 0.2718 | dt 337.49ms | 1553475.12 tokens/sec
Step 3193 | loss: 3.477300 | lr:5.7609e-04 | norm 0.2803 | dt 339.17ms | 1545777.63 tokens/sec
Step 3194 | loss: 3.412176 | lr:5.7607e-04 | norm 0.2786 | dt 338.03ms | 1550994.49 tokens/sec
Step 3195 | loss: 3.545295 | lr:5.7605e-04 | norm 0.3144 | dt 339.21ms | 1545594.02 tokens/sec
Step 3196 | loss: 3.482917 | lr:5.7603e-04 | norm 0.3021 | dt 337.99ms | 1551208.93 tokens/sec
Step 3197 | loss: 3.400045 | lr:5.7601e-04 | norm 0.2843 | dt 338.80ms | 1547472.39 tokens/sec
Step 3198 | loss: 3.351572 | lr:5.7599e-04 | norm 0.2567 | dt 338.15ms | 1550452.09 tokens/sec
Step 3199 | loss: 3.405465 | lr:5.7597e-04 | norm 0.2685 | dt 337.93ms | 1551466.12 tokens/sec
Step 3200 | loss: 3.316280 | lr:5.7595e-04 | norm 0.2731 | dt 337.43ms | 1553776.97 tokens/sec
Step 3201 | loss: 3.300575 | lr:5.7593e-04 | norm 0.2798 | dt 338.11ms | 1550654.35 tokens/sec
Step 3202 | loss: 3.276346 | lr:5.7591e-04 | norm 0.2628 | dt 337.64ms | 1552803.78 tokens/sec
Step 3203 | loss: 3.320913 | lr:5.7589e-04 | norm 0.2821 | dt 337.53ms | 1553324.78 tokens/sec
Step 3204 | loss: 3.327323 | lr:5.7588e-04 | norm 0.3106 | dt 337.86ms | 1551773.76 tokens/sec
Step 3205 | loss: 3.347036 | lr:5.7586e-04 | norm 0.3002 | dt 338.38ms | 1549397.90 tokens/sec
Step 3206 | loss: 3.370069 | lr:5.7584e-04 | norm 0.2827 | dt 337.77ms | 1552198.75 tokens/sec
Step 3207 | loss: 3.323753 | lr:5.7582e-04 | norm 0.2687 | dt 337.73ms | 1552386.13 tokens/sec
Step 3208 | loss: 3.300666 | lr:5.7580e-04 | norm 0.2726 | dt 337.24ms | 1554657.95 tokens/sec
Step 3209 | loss: 3.412614 | lr:5.7578e-04 | norm 0.2935 | dt 339.55ms | 1544057.31 tokens/sec
Step 3210 | loss: 3.494706 | lr:5.7576e-04 | norm 0.2793 | dt 339.76ms | 1543108.16 tokens/sec
Step 3211 | loss: 3.510752 | lr:5.7574e-04 | norm 0.2513 | dt 338.67ms | 1548073.73 tokens/sec
Step 3212 | loss: 3.485279 | lr:5.7572e-04 | norm 0.2769 | dt 1033.61ms | 507240.44 tokens/sec
Step 3213 | loss: 3.448419 | lr:5.7570e-04 | norm 0.2620 | dt 337.16ms | 1555011.95 tokens/sec
Step 3214 | loss: 3.485685 | lr:5.7568e-04 | norm 0.2812 | dt 336.93ms | 1556060.58 tokens/sec
Step 3215 | loss: 3.433765 | lr:5.7567e-04 | norm 0.2824 | dt 339.88ms | 1542573.42 tokens/sec
Step 3216 | loss: 3.508282 | lr:5.7565e-04 | norm 0.2827 | dt 337.90ms | 1551605.15 tokens/sec
Step 3217 | loss: 3.492608 | lr:5.7563e-04 | norm 0.2897 | dt 337.68ms | 1552598.76 tokens/sec
Step 3218 | loss: 3.416564 | lr:5.7561e-04 | norm 0.2782 | dt 340.67ms | 1538968.78 tokens/sec
Step 3219 | loss: 3.510700 | lr:5.7559e-04 | norm 0.2838 | dt 338.32ms | 1549671.96 tokens/sec
Step 3220 | loss: 3.483062 | lr:5.7557e-04 | norm 0.2887 | dt 337.56ms | 1553170.09 tokens/sec
Step 3221 | loss: 3.436882 | lr:5.7555e-04 | norm 0.2959 | dt 337.91ms | 1551556.98 tokens/sec
Step 3222 | loss: 3.455979 | lr:5.7553e-04 | norm 0.2719 | dt 338.23ms | 1550085.97 tokens/sec
Step 3223 | loss: 3.575846 | lr:5.7551e-04 | norm 0.2569 | dt 337.50ms | 1553445.49 tokens/sec
Step 3224 | loss: 3.483570 | lr:5.7549e-04 | norm 0.2787 | dt 338.75ms | 1547707.64 tokens/sec
Step 3225 | loss: 3.417696 | lr:5.7547e-04 | norm 0.2840 | dt 339.09ms | 1546154.77 tokens/sec
Step 3226 | loss: 3.456394 | lr:5.7545e-04 | norm 0.2613 | dt 338.82ms | 1547391.81 tokens/sec
Step 3227 | loss: 3.418014 | lr:5.7543e-04 | norm 0.2804 | dt 337.72ms | 1552456.27 tokens/sec
Step 3228 | loss: 3.496801 | lr:5.7542e-04 | norm 0.2653 | dt 337.92ms | 1551531.80 tokens/sec
Step 3229 | loss: 3.446428 | lr:5.7540e-04 | norm 0.3211 | dt 999.37ms | 524619.21 tokens/sec
Step 3230 | loss: 3.417267 | lr:5.7538e-04 | norm 0.3710 | dt 336.94ms | 1556048.47 tokens/sec
Step 3231 | loss: 3.420400 | lr:5.7536e-04 | norm 0.3440 | dt 337.51ms | 1553413.66 tokens/sec
Step 3232 | loss: 3.415403 | lr:5.7534e-04 | norm 0.3079 | dt 338.31ms | 1549734.21 tokens/sec
Step 3233 | loss: 3.363008 | lr:5.7532e-04 | norm 0.3106 | dt 337.42ms | 1553796.73 tokens/sec
Step 3234 | loss: 3.368813 | lr:5.7530e-04 | norm 0.2797 | dt 337.65ms | 1552764.30 tokens/sec
Step 3235 | loss: 3.350528 | lr:5.7528e-04 | norm 0.2937 | dt 337.51ms | 1553377.45 tokens/sec
Step 3236 | loss: 3.378583 | lr:5.7526e-04 | norm 0.2630 | dt 338.37ms | 1549452.49 tokens/sec
Step 3237 | loss: 3.431564 | lr:5.7524e-04 | norm 0.2999 | dt 337.63ms | 1552838.87 tokens/sec
Step 3238 | loss: 3.438138 | lr:5.7522e-04 | norm 0.2654 | dt 337.81ms | 1552008.14 tokens/sec
Step 3239 | loss: 3.435593 | lr:5.7520e-04 | norm 0.2532 | dt 338.14ms | 1550498.01 tokens/sec
Step 3240 | loss: 3.413556 | lr:5.7518e-04 | norm 0.2544 | dt 337.91ms | 1551549.31 tokens/sec
Step 3241 | loss: 3.428815 | lr:5.7516e-04 | norm 0.2555 | dt 338.90ms | 1547026.04 tokens/sec
Step 3242 | loss: 3.442164 | lr:5.7514e-04 | norm 0.2923 | dt 337.55ms | 1553208.49 tokens/sec
Step 3243 | loss: 3.390188 | lr:5.7513e-04 | norm 0.2711 | dt 337.52ms | 1553341.24 tokens/sec
Step 3244 | loss: 3.359309 | lr:5.7511e-04 | norm 0.2755 | dt 338.82ms | 1547372.21 tokens/sec
Step 3245 | loss: 3.284082 | lr:5.7509e-04 | norm 0.3107 | dt 337.70ms | 1552509.97 tokens/sec
Step 3246 | loss: 3.308232 | lr:5.7507e-04 | norm 0.2686 | dt 337.83ms | 1551932.56 tokens/sec
Step 3247 | loss: 3.340654 | lr:5.7505e-04 | norm 0.2982 | dt 338.40ms | 1549316.03 tokens/sec
Step 3248 | loss: 3.337829 | lr:5.7503e-04 | norm 0.2948 | dt 338.08ms | 1550801.98 tokens/sec
Step 3249 | loss: 3.302332 | lr:5.7501e-04 | norm 0.2896 | dt 338.03ms | 1551024.03 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 3250: 3.4852
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2706/10042=0.2695


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not gonna explain to you anything. So I got a series of articles to cover you (I am writing
rank 3 sample 1 >Hello, I'm a language model, and the code is pretty great!"
The video below goes into a fascinating experiment.
“This is great because
rank 3 sample 2 >Hello, I'm a language model, and it doesn't know what other languages were on the table.
So it gets a little fun, but in reality
rank 3 sample 3 >Hello, I'm a language model, and how I am going to help you with your own project? If I'm a language learner, what's my




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, so this was a really good look at my own grammar and I hope I've found it to be a really good start
rank 5 sample 1 >Hello, I'm a language model, because when you think of an interactive, I could call it 'My' language 'I' or 'We' and

rank 5 sample 2 >Hello, I'm a language model, I think we're creating a real-world environment. I think you're trying to develop a real language, but how

ddp_rank 7: ####### Printing generated samples ####### 

rank 5 sample 3 >Hello, I'm a language model, no, but the
(1) = the same thing. (2) = "the product" (2)


rank 7 sample 0 >Hello, I'm a language model, and I hope to do something about it
I know I'm a huge fan of it. Every time you go

rank 7 sample 1 >Hello, I'm a language model, is quite complex. So I've heard you say, The language is a language; The language is like that, so
rank 7 sample 2 >Hello, I'm a language model, so I can help you to keep your class organised around the same framework:
- The class structure is a hierarchical framework
rank 7 sample 3 >Hello, I'm a language model, do I get a new thing? Is it fun?
Why is it so cool?
When you're done on




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, since my students, I expect them to learn the basics and how to get the students to learn the basics quickly. The
rank 2 sample 1 >Hello, I'm a language model, so I feel a little bit surprised this past week this week when I saw the same thing I was looking for in class
rank 2 sample 2 >Hello, I'm a language model, I'm an architect.
But here's the thing: "Well, we are trying to work our way down to
rank 2 sample 3 >Hello, I'm a language model, so my guess is it is a language class. We're using a language group of learners – children and adults – to




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, that you can use to create a macro to show a range of values. One of these is:
A macro is
rank 1 sample 1 >Hello, I'm a language model, it is my job to do that. To do that then I go to an Ecosystem or Myself.
I
rank 1 sample 2 >Hello, I'm a language model, so hopefully, that's a good one.
A few simple ways of explaining the concept:
- If there's
rank 1 sample 3 >Hello, I'm a language model, and I'm in a situation where I try to work really hard. In any case, trying to work with it is




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, I'm sure you can, but you will be asking for more information as well. This part of the website is for
rank 6 sample 0 >Hello, I'm a language model, what would I do about it?". I think you could be an engineer to this. But I think I'm getting
rank 4 sample 1 >Hello, I'm a language model, or maybe a developer's programmer can build and deploy Java API.
Now let's move to Java for Java Tutorial.
rank 6 sample 1 >Hello, I'm a language model, a language. I don’t worry about being a language model programmer, I just don’t know how
rank 6 sample 2 >Hello, I'm a language model, so the students are given an overview of the concept, and the main ideas.
And so, to start. I
rank 4 sample 2 >Hello, I'm a language model, but do you work? So, it's better to make this an attractive language so you can be on your list and
rank 6 sample 3 >Hello, I'm a language model, but what about writing your code so that it doesn't work? One of these is that that you ask. To keep


rank 4 sample 3 >Hello, I'm a language model, and my current machine will not be your first programming, so in my I have to go through what my machine says and




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I like to think like the world doesn't necessarily want to happen: a language, at least for us, can
rank 0 sample 1 >Hello, I'm a language model, so now I'm gonna think that goes to a post: "How to make a language change the learning environment?"

rank 0 sample 2 >Hello, I'm a language model, I'm having the data my programming gets. Now I'm in that position. I have a data model, and one
rank 0 sample 3 >Hello, I'm a language model, so why not start with a language like C++?
Hi, I've given me a post:
Hi if


Step 3250 | loss: 3.354913 | lr:5.7499e-04 | norm 0.3154 | dt 12319.62ms | 42557.16 tokens/sec
Step 3251 | loss: 3.302947 | lr:5.7497e-04 | norm 0.3077 | dt 335.72ms | 1561680.96 tokens/sec
Step 3252 | loss: 3.273939 | lr:5.7495e-04 | norm 0.2887 | dt 336.42ms | 1558431.53 tokens/sec
Step 3253 | loss: 3.320175 | lr:5.7493e-04 | norm 0.3074 | dt 336.42ms | 1558420.49 tokens/sec
Step 3254 | loss: 3.366765 | lr:5.7491e-04 | norm 0.2875 | dt 336.65ms | 1557368.67 tokens/sec
Step 3255 | loss: 3.433329 | lr:5.7489e-04 | norm 0.2997 | dt 335.74ms | 1561590.02 tokens/sec
Step 3256 | loss: 3.493570 | lr:5.7487e-04 | norm 0.2798 | dt 337.57ms | 1553137.18 tokens/sec
Step 3257 | loss: 3.442160 | lr:5.7485e-04 | norm 0.2913 | dt 336.50ms | 1558084.81 tokens/sec
Step 3258 | loss: 3.458006 | lr:5.7483e-04 | norm 0.3081 | dt 336.10ms | 1559914.01 tokens/sec
Step 3259 | loss: 3.531930 | lr:5.7481e-04 | norm 0.2840 | dt 336.61ms | 1557536.34 tokens/sec
Step 3260 | loss: 3.478713 | lr:5.7480e-04 | norm 0.2563 | dt 337.02ms | 1555649.98 tokens/sec
Step 3261 | loss: 3.522200 | lr:5.7478e-04 | norm 0.2617 | dt 337.32ms | 1554253.59 tokens/sec
Step 3262 | loss: 3.412202 | lr:5.7476e-04 | norm 0.2870 | dt 336.72ms | 1557066.53 tokens/sec
Step 3263 | loss: 3.436017 | lr:5.7474e-04 | norm 0.2891 | dt 336.38ms | 1558639.20 tokens/sec
Step 3264 | loss: 3.471501 | lr:5.7472e-04 | norm 0.2955 | dt 337.90ms | 1551618.28 tokens/sec
Step 3265 | loss: 3.506355 | lr:5.7470e-04 | norm 0.2562 | dt 336.75ms | 1556922.11 tokens/sec
Step 3266 | loss: 3.522553 | lr:5.7468e-04 | norm 0.2625 | dt 336.76ms | 1556876.92 tokens/sec
Step 3267 | loss: 3.433545 | lr:5.7466e-04 | norm 0.2748 | dt 338.72ms | 1547873.23 tokens/sec
Step 3268 | loss: 3.406615 | lr:5.7464e-04 | norm 0.2520 | dt 337.05ms | 1555509.13 tokens/sec
Step 3269 | loss: 3.459336 | lr:5.7462e-04 | norm 0.2767 | dt 337.72ms | 1552436.54 tokens/sec
Step 3270 | loss: 3.480938 | lr:5.7460e-04 | norm 0.3041 | dt 337.42ms | 1553832.96 tokens/sec
Step 3271 | loss: 3.461493 | lr:5.7458e-04 | norm 0.2857 | dt 337.70ms | 1552528.61 tokens/sec
Step 3272 | loss: 3.385347 | lr:5.7456e-04 | norm 0.2851 | dt 337.75ms | 1552312.71 tokens/sec
Step 3273 | loss: 3.366175 | lr:5.7454e-04 | norm 0.2523 | dt 337.13ms | 1555134.01 tokens/sec
Step 3274 | loss: 3.468819 | lr:5.7452e-04 | norm 0.2658 | dt 337.56ms | 1553177.77 tokens/sec
Step 3275 | loss: 3.383869 | lr:5.7450e-04 | norm 0.2766 | dt 337.52ms | 1553357.70 tokens/sec
Step 3276 | loss: 3.470095 | lr:5.7448e-04 | norm 0.2677 | dt 337.19ms | 1554865.71 tokens/sec
Step 3277 | loss: 3.391892 | lr:5.7446e-04 | norm 0.2759 | dt 337.45ms | 1553672.68 tokens/sec
Step 3278 | loss: 3.359921 | lr:5.7444e-04 | norm 0.2764 | dt 338.17ms | 1550358.08 tokens/sec
Step 3279 | loss: 3.414496 | lr:5.7442e-04 | norm 0.2938 | dt 338.55ms | 1548614.47 tokens/sec
Step 3280 | loss: 3.381163 | lr:5.7440e-04 | norm 0.3069 | dt 337.73ms | 1552405.85 tokens/sec
Step 3281 | loss: 3.412706 | lr:5.7438e-04 | norm 0.3158 | dt 338.37ms | 1549430.65 tokens/sec
Step 3282 | loss: 3.421668 | lr:5.7436e-04 | norm 0.3275 | dt 338.23ms | 1550104.54 tokens/sec
Step 3283 | loss: 3.457003 | lr:5.7434e-04 | norm 0.2946 | dt 338.04ms | 1550949.64 tokens/sec
Step 3284 | loss: 3.426180 | lr:5.7433e-04 | norm 0.2669 | dt 338.52ms | 1548745.35 tokens/sec
Step 3285 | loss: 3.433532 | lr:5.7431e-04 | norm 0.2766 | dt 338.38ms | 1549405.54 tokens/sec
Step 3286 | loss: 3.465842 | lr:5.7429e-04 | norm 0.2843 | dt 337.51ms | 1553421.34 tokens/sec
Step 3287 | loss: 3.428275 | lr:5.7427e-04 | norm 0.2676 | dt 338.58ms | 1548479.25 tokens/sec
Step 3288 | loss: 3.468129 | lr:5.7425e-04 | norm 0.2698 | dt 338.50ms | 1548858.80 tokens/sec
Step 3289 | loss: 3.335179 | lr:5.7423e-04 | norm 0.2988 | dt 338.15ms | 1550438.97 tokens/sec
Step 3290 | loss: 3.286517 | lr:5.7421e-04 | norm 0.3112 | dt 337.54ms | 1553261.15 tokens/sec
Step 3291 | loss: 3.346874 | lr:5.7419e-04 | norm 0.3206 | dt 337.72ms | 1552429.97 tokens/sec
Step 3292 | loss: 3.319476 | lr:5.7417e-04 | norm 0.2532 | dt 338.72ms | 1547867.78 tokens/sec
Step 3293 | loss: 3.337121 | lr:5.7415e-04 | norm 0.2646 | dt 337.73ms | 1552372.98 tokens/sec
Step 3294 | loss: 3.354919 | lr:5.7413e-04 | norm 0.2631 | dt 337.79ms | 1552133.02 tokens/sec
Step 3295 | loss: 3.271014 | lr:5.7411e-04 | norm 0.2425 | dt 338.12ms | 1550614.99 tokens/sec
Step 3296 | loss: 3.273011 | lr:5.7409e-04 | norm 0.2677 | dt 338.43ms | 1549175.23 tokens/sec
Step 3297 | loss: 3.332185 | lr:5.7407e-04 | norm 0.2748 | dt 338.11ms | 1550649.98 tokens/sec
Step 3298 | loss: 3.338536 | lr:5.7405e-04 | norm 0.2757 | dt 338.53ms | 1548706.08 tokens/sec
Step 3299 | loss: 3.277179 | lr:5.7403e-04 | norm 0.3576 | dt 338.06ms | 1550858.86 tokens/sec
Step 3300 | loss: 3.310649 | lr:5.7401e-04 | norm 0.3847 | dt 339.88ms | 1542571.26 tokens/sec
Step 3301 | loss: 3.470413 | lr:5.7399e-04 | norm 0.3607 | dt 339.87ms | 1542615.63 tokens/sec
Step 3302 | loss: 3.486373 | lr:5.7397e-04 | norm 0.3525 | dt 339.30ms | 1545199.78 tokens/sec
Step 3303 | loss: 3.488976 | lr:5.7395e-04 | norm 0.3231 | dt 339.60ms | 1543858.93 tokens/sec
Step 3304 | loss: 3.495883 | lr:5.7393e-04 | norm 0.3395 | dt 338.55ms | 1548625.38 tokens/sec
Step 3305 | loss: 3.497864 | lr:5.7391e-04 | norm 0.3416 | dt 339.74ms | 1543218.62 tokens/sec
Step 3306 | loss: 3.465399 | lr:5.7389e-04 | norm 0.2867 | dt 339.22ms | 1545571.21 tokens/sec
Step 3307 | loss: 3.454451 | lr:5.7387e-04 | norm 0.3341 | dt 339.02ms | 1546462.48 tokens/sec
Step 3308 | loss: 3.461650 | lr:5.7385e-04 | norm 0.3655 | dt 339.06ms | 1546314.59 tokens/sec
Step 3309 | loss: 3.443361 | lr:5.7383e-04 | norm 0.3708 | dt 339.35ms | 1544993.51 tokens/sec
Step 3310 | loss: 3.523695 | lr:5.7381e-04 | norm 0.3298 | dt 338.35ms | 1549554.03 tokens/sec
Step 3311 | loss: 3.522605 | lr:5.7379e-04 | norm 0.3294 | dt 338.62ms | 1548308.08 tokens/sec
Step 3312 | loss: 3.470052 | lr:5.7377e-04 | norm 0.3089 | dt 339.85ms | 1542688.13 tokens/sec
Step 3313 | loss: 3.391786 | lr:5.7375e-04 | norm 0.2708 | dt 338.85ms | 1547242.65 tokens/sec
Step 3314 | loss: 3.445274 | lr:5.7373e-04 | norm 0.2743 | dt 338.47ms | 1549012.63 tokens/sec
Step 3315 | loss: 3.472608 | lr:5.7371e-04 | norm 0.2764 | dt 338.90ms | 1547051.07 tokens/sec
Step 3316 | loss: 3.458137 | lr:5.7369e-04 | norm 0.2816 | dt 339.10ms | 1546114.54 tokens/sec
Step 3317 | loss: 3.430904 | lr:5.7367e-04 | norm 0.2677 | dt 338.11ms | 1550624.83 tokens/sec
Step 3318 | loss: 3.417153 | lr:5.7365e-04 | norm 0.2632 | dt 341.07ms | 1537164.71 tokens/sec
Step 3319 | loss: 3.461516 | lr:5.7363e-04 | norm 0.2611 | dt 337.94ms | 1551414.68 tokens/sec
Step 3320 | loss: 3.400579 | lr:5.7361e-04 | norm 0.2552 | dt 338.74ms | 1547743.59 tokens/sec
Step 3321 | loss: 3.408200 | lr:5.7359e-04 | norm 0.2840 | dt 338.57ms | 1548520.69 tokens/sec
Step 3322 | loss: 3.387950 | lr:5.7357e-04 | norm 0.2844 | dt 339.03ms | 1546416.81 tokens/sec
Step 3323 | loss: 3.431610 | lr:5.7355e-04 | norm 0.2592 | dt 338.98ms | 1546677.85 tokens/sec
Step 3324 | loss: 3.385058 | lr:5.7353e-04 | norm 0.3449 | dt 339.18ms | 1545753.72 tokens/sec
Step 3325 | loss: 3.417738 | lr:5.7351e-04 | norm 0.2926 | dt 339.38ms | 1544828.54 tokens/sec
Step 3326 | loss: 3.342731 | lr:5.7349e-04 | norm 0.3280 | dt 339.31ms | 1545168.29 tokens/sec
Step 3327 | loss: 3.436355 | lr:5.7347e-04 | norm 0.3088 | dt 339.11ms | 1546071.06 tokens/sec
Step 3328 | loss: 3.387252 | lr:5.7345e-04 | norm 0.2728 | dt 338.76ms | 1547646.64 tokens/sec
Step 3329 | loss: 3.409063 | lr:5.7343e-04 | norm 0.2768 | dt 339.44ms | 1544545.34 tokens/sec
Step 3330 | loss: 3.404665 | lr:5.7341e-04 | norm 0.2845 | dt 339.60ms | 1543857.85 tokens/sec
Step 3331 | loss: 3.484681 | lr:5.7339e-04 | norm 0.2573 | dt 338.85ms | 1547277.49 tokens/sec
Step 3332 | loss: 3.468155 | lr:5.7337e-04 | norm 0.2751 | dt 338.97ms | 1546707.22 tokens/sec
Step 3333 | loss: 3.442003 | lr:5.7335e-04 | norm 0.2888 | dt 339.48ms | 1544403.23 tokens/sec
Step 3334 | loss: 3.413586 | lr:5.7333e-04 | norm 0.2838 | dt 339.91ms | 1542414.37 tokens/sec
Step 3335 | loss: 3.312797 | lr:5.7331e-04 | norm 0.2833 | dt 338.35ms | 1549552.94 tokens/sec
Step 3336 | loss: 3.266218 | lr:5.7329e-04 | norm 0.2796 | dt 337.46ms | 1553612.31 tokens/sec
Step 3337 | loss: 3.294587 | lr:5.7327e-04 | norm 0.2868 | dt 338.48ms | 1548942.80 tokens/sec
Step 3338 | loss: 3.313009 | lr:5.7325e-04 | norm 0.2777 | dt 337.15ms | 1555039.44 tokens/sec
Step 3339 | loss: 3.302292 | lr:5.7323e-04 | norm 0.2443 | dt 337.57ms | 1553121.82 tokens/sec
Step 3340 | loss: 3.317094 | lr:5.7321e-04 | norm 0.2746 | dt 337.71ms | 1552481.48 tokens/sec
Step 3341 | loss: 3.362197 | lr:5.7319e-04 | norm 0.3126 | dt 338.49ms | 1548920.98 tokens/sec
Step 3342 | loss: 3.351806 | lr:5.7317e-04 | norm 0.2860 | dt 337.63ms | 1552855.31 tokens/sec
Step 3343 | loss: 3.323624 | lr:5.7315e-04 | norm 0.2500 | dt 339.50ms | 1544316.47 tokens/sec
Step 3344 | loss: 3.398060 | lr:5.7313e-04 | norm 0.3197 | dt 338.16ms | 1550393.06 tokens/sec
Step 3345 | loss: 3.295638 | lr:5.7311e-04 | norm 0.3062 | dt 339.51ms | 1544262.24 tokens/sec
Step 3346 | loss: 3.424830 | lr:5.7309e-04 | norm 0.3266 | dt 338.17ms | 1550375.57 tokens/sec
Step 3347 | loss: 3.478608 | lr:5.7307e-04 | norm 0.2858 | dt 337.58ms | 1553077.95 tokens/sec
Step 3348 | loss: 3.557386 | lr:5.7305e-04 | norm 0.3346 | dt 337.83ms | 1551941.32 tokens/sec
Step 3349 | loss: 3.513796 | lr:5.7303e-04 | norm 0.2899 | dt 338.11ms | 1550621.55 tokens/sec
Step 3350 | loss: 3.469314 | lr:5.7301e-04 | norm 0.2893 | dt 337.43ms | 1553755.01 tokens/sec
Step 3351 | loss: 3.634375 | lr:5.7299e-04 | norm 0.2877 | dt 338.07ms | 1550823.86 tokens/sec
Step 3352 | loss: 3.464066 | lr:5.7297e-04 | norm 0.3403 | dt 337.64ms | 1552808.16 tokens/sec
Step 3353 | loss: 3.433683 | lr:5.7295e-04 | norm 0.3189 | dt 338.16ms | 1550398.53 tokens/sec
Step 3354 | loss: 3.455750 | lr:5.7293e-04 | norm 0.3039 | dt 338.42ms | 1549245.08 tokens/sec
Step 3355 | loss: 3.472317 | lr:5.7291e-04 | norm 0.3382 | dt 338.12ms | 1550586.56 tokens/sec
Step 3356 | loss: 3.435723 | lr:5.7289e-04 | norm 0.3130 | dt 338.45ms | 1549084.65 tokens/sec
Step 3357 | loss: 3.418933 | lr:5.7287e-04 | norm 0.3089 | dt 339.08ms | 1546206.95 tokens/sec
Step 3358 | loss: 3.494187 | lr:5.7285e-04 | norm 0.2742 | dt 337.94ms | 1551414.68 tokens/sec
Step 3359 | loss: 3.569740 | lr:5.7283e-04 | norm 0.3221 | dt 338.98ms | 1546672.41 tokens/sec
Step 3360 | loss: 3.478189 | lr:5.7281e-04 | norm 0.2990 | dt 337.79ms | 1552126.44 tokens/sec
Step 3361 | loss: 3.445194 | lr:5.7279e-04 | norm 0.2865 | dt 338.17ms | 1550381.04 tokens/sec
Step 3362 | loss: 3.442533 | lr:5.7277e-04 | norm 0.5407 | dt 338.18ms | 1550335.13 tokens/sec
Step 3363 | loss: 3.533016 | lr:5.7275e-04 | norm 0.4152 | dt 339.02ms | 1546474.45 tokens/sec
Step 3364 | loss: 3.434987 | lr:5.7273e-04 | norm 0.3982 | dt 339.18ms | 1545764.59 tokens/sec
Step 3365 | loss: 3.539783 | lr:5.7271e-04 | norm 0.3165 | dt 339.13ms | 1545973.24 tokens/sec
Step 3366 | loss: 3.441031 | lr:5.7269e-04 | norm 0.3383 | dt 338.24ms | 1550046.63 tokens/sec
Step 3367 | loss: 3.430450 | lr:5.7267e-04 | norm 0.3047 | dt 338.98ms | 1546673.50 tokens/sec
Step 3368 | loss: 3.470847 | lr:5.7265e-04 | norm 0.3109 | dt 338.53ms | 1548723.54 tokens/sec
Step 3369 | loss: 3.425714 | lr:5.7263e-04 | norm 0.2948 | dt 337.86ms | 1551811.00 tokens/sec
Step 3370 | loss: 3.392111 | lr:5.7261e-04 | norm 0.3243 | dt 338.68ms | 1548039.95 tokens/sec
Step 3371 | loss: 3.405054 | lr:5.7259e-04 | norm 0.3035 | dt 338.28ms | 1549848.90 tokens/sec
Step 3372 | loss: 3.351125 | lr:5.7257e-04 | norm 0.2770 | dt 338.17ms | 1550384.32 tokens/sec
Step 3373 | loss: 3.339039 | lr:5.7255e-04 | norm 0.2941 | dt 339.31ms | 1545171.55 tokens/sec
Step 3374 | loss: 3.439067 | lr:5.7253e-04 | norm 0.2789 | dt 337.66ms | 1552706.20 tokens/sec
Step 3375 | loss: 3.387172 | lr:5.7251e-04 | norm 0.2631 | dt 338.18ms | 1550335.13 tokens/sec
Step 3376 | loss: 3.478687 | lr:5.7249e-04 | norm 0.2840 | dt 338.48ms | 1548929.71 tokens/sec
Step 3377 | loss: 3.473914 | lr:5.7247e-04 | norm 0.2972 | dt 339.09ms | 1546156.94 tokens/sec
Step 3378 | loss: 3.439643 | lr:5.7245e-04 | norm 0.2639 | dt 338.63ms | 1548274.28 tokens/sec
Step 3379 | loss: 3.458068 | lr:5.7242e-04 | norm 0.2985 | dt 337.90ms | 1551625.95 tokens/sec
Step 3380 | loss: 3.427718 | lr:5.7240e-04 | norm 0.2833 | dt 339.76ms | 1543109.24 tokens/sec
Step 3381 | loss: 3.416833 | lr:5.7238e-04 | norm 0.2725 | dt 339.61ms | 1543800.40 tokens/sec
Step 3382 | loss: 3.304907 | lr:5.7236e-04 | norm 0.2777 | dt 337.84ms | 1551900.80 tokens/sec
Step 3383 | loss: 3.284257 | lr:5.7234e-04 | norm 0.2535 | dt 337.74ms | 1552342.29 tokens/sec
Step 3384 | loss: 3.345301 | lr:5.7232e-04 | norm 0.2388 | dt 338.60ms | 1548389.84 tokens/sec
Step 3385 | loss: 3.361034 | lr:5.7230e-04 | norm 0.2835 | dt 337.91ms | 1551570.11 tokens/sec
Step 3386 | loss: 3.310353 | lr:5.7228e-04 | norm 0.2974 | dt 338.77ms | 1547640.11 tokens/sec
Step 3387 | loss: 3.310834 | lr:5.7226e-04 | norm 0.3199 | dt 338.40ms | 1549307.30 tokens/sec
Step 3388 | loss: 3.336643 | lr:5.7224e-04 | norm 0.3345 | dt 338.28ms | 1549882.76 tokens/sec
Step 3389 | loss: 3.348460 | lr:5.7222e-04 | norm 0.2850 | dt 338.62ms | 1548314.62 tokens/sec
Step 3390 | loss: 3.327593 | lr:5.7220e-04 | norm 0.2702 | dt 338.45ms | 1549068.28 tokens/sec
Step 3391 | loss: 3.396873 | lr:5.7218e-04 | norm 0.3296 | dt 338.22ms | 1550148.25 tokens/sec
Step 3392 | loss: 3.398791 | lr:5.7216e-04 | norm 0.3120 | dt 338.16ms | 1550401.81 tokens/sec
Step 3393 | loss: 3.278480 | lr:5.7214e-04 | norm 0.2697 | dt 337.70ms | 1552532.99 tokens/sec
Step 3394 | loss: 3.498028 | lr:5.7212e-04 | norm 0.2877 | dt 338.45ms | 1549066.10 tokens/sec
Step 3395 | loss: 3.544479 | lr:5.7210e-04 | norm 0.2829 | dt 338.57ms | 1548522.87 tokens/sec
Step 3396 | loss: 3.456413 | lr:5.7208e-04 | norm 0.2718 | dt 338.19ms | 1550289.23 tokens/sec
Step 3397 | loss: 3.506732 | lr:5.7206e-04 | norm 0.2835 | dt 338.40ms | 1549299.66 tokens/sec
Step 3398 | loss: 3.464674 | lr:5.7204e-04 | norm 0.2827 | dt 338.24ms | 1550061.93 tokens/sec
Step 3399 | loss: 3.485776 | lr:5.7202e-04 | norm 0.2780 | dt 338.32ms | 1549663.23 tokens/sec
Step 3400 | loss: 3.442611 | lr:5.7200e-04 | norm 0.2784 | dt 338.78ms | 1547592.18 tokens/sec
Step 3401 | loss: 3.515650 | lr:5.7198e-04 | norm 0.2752 | dt 897.52ms | 584150.60 tokens/sec
Step 3402 | loss: 3.432084 | lr:5.7196e-04 | norm 0.2803 | dt 337.23ms | 1554704.12 tokens/sec
Step 3403 | loss: 3.457683 | lr:5.7193e-04 | norm 0.2947 | dt 338.60ms | 1548401.84 tokens/sec
Step 3404 | loss: 3.408427 | lr:5.7191e-04 | norm 0.2749 | dt 337.93ms | 1551479.26 tokens/sec
Step 3405 | loss: 3.451018 | lr:5.7189e-04 | norm 0.2865 | dt 337.90ms | 1551627.04 tokens/sec
Step 3406 | loss: 3.443725 | lr:5.7187e-04 | norm 0.2969 | dt 338.08ms | 1550776.83 tokens/sec
Step 3407 | loss: 3.494325 | lr:5.7185e-04 | norm 0.3136 | dt 337.65ms | 1552767.59 tokens/sec
Step 3408 | loss: 3.448167 | lr:5.7183e-04 | norm 0.3084 | dt 338.10ms | 1550687.16 tokens/sec
Step 3409 | loss: 3.402978 | lr:5.7181e-04 | norm 0.3263 | dt 338.26ms | 1549959.23 tokens/sec
Step 3410 | loss: 3.460178 | lr:5.7179e-04 | norm 0.3243 | dt 338.45ms | 1549102.11 tokens/sec
Step 3411 | loss: 3.444020 | lr:5.7177e-04 | norm 0.3218 | dt 338.05ms | 1550934.33 tokens/sec
Step 3412 | loss: 3.400775 | lr:5.7175e-04 | norm 0.3535 | dt 338.21ms | 1550207.26 tokens/sec
Step 3413 | loss: 3.386467 | lr:5.7173e-04 | norm 0.3282 | dt 338.15ms | 1550443.35 tokens/sec
Step 3414 | loss: 3.428119 | lr:5.7171e-04 | norm 0.3184 | dt 337.95ms | 1551390.60 tokens/sec
Step 3415 | loss: 3.384456 | lr:5.7169e-04 | norm 0.3361 | dt 337.75ms | 1552297.37 tokens/sec
Step 3416 | loss: 3.386663 | lr:5.7167e-04 | norm 0.2980 | dt 337.44ms | 1553745.13 tokens/sec
Step 3417 | loss: 3.385062 | lr:5.7165e-04 | norm 0.3045 | dt 337.73ms | 1552383.94 tokens/sec
Step 3418 | loss: 3.358514 | lr:5.7163e-04 | norm 0.2860 | dt 338.69ms | 1547982.19 tokens/sec
Step 3419 | loss: 3.415611 | lr:5.7161e-04 | norm 0.2861 | dt 932.83ms | 562041.62 tokens/sec
Step 3420 | loss: 3.372755 | lr:5.7158e-04 | norm 0.2980 | dt 336.58ms | 1557698.52 tokens/sec
Step 3421 | loss: 3.367622 | lr:5.7156e-04 | norm 0.2701 | dt 338.45ms | 1549080.29 tokens/sec
Step 3422 | loss: 3.407863 | lr:5.7154e-04 | norm 0.2863 | dt 336.89ms | 1556253.29 tokens/sec
Step 3423 | loss: 3.430542 | lr:5.7152e-04 | norm 0.2751 | dt 337.86ms | 1551782.52 tokens/sec
Step 3424 | loss: 3.515680 | lr:5.7150e-04 | norm 0.2823 | dt 337.86ms | 1551782.52 tokens/sec
Step 3425 | loss: 3.411889 | lr:5.7148e-04 | norm 0.2803 | dt 337.35ms | 1554136.05 tokens/sec
Step 3426 | loss: 3.402935 | lr:5.7146e-04 | norm 0.2832 | dt 337.49ms | 1553512.43 tokens/sec
Step 3427 | loss: 3.457346 | lr:5.7144e-04 | norm 0.2957 | dt 337.64ms | 1552780.75 tokens/sec
Step 3428 | loss: 3.387399 | lr:5.7142e-04 | norm 0.3053 | dt 337.21ms | 1554794.26 tokens/sec
Step 3429 | loss: 3.310137 | lr:5.7140e-04 | norm 0.3071 | dt 338.72ms | 1547843.81 tokens/sec
Step 3430 | loss: 3.336485 | lr:5.7138e-04 | norm 0.3142 | dt 338.53ms | 1548697.36 tokens/sec
Step 3431 | loss: 3.257943 | lr:5.7136e-04 | norm 0.3079 | dt 337.99ms | 1551200.18 tokens/sec
Step 3432 | loss: 3.276618 | lr:5.7134e-04 | norm 0.2679 | dt 338.76ms | 1547676.05 tokens/sec
Step 3433 | loss: 3.328112 | lr:5.7132e-04 | norm 0.2631 | dt 338.95ms | 1546801.87 tokens/sec
Step 3434 | loss: 3.277301 | lr:5.7130e-04 | norm 0.2926 | dt 338.11ms | 1550625.92 tokens/sec
Step 3435 | loss: 3.314227 | lr:5.7127e-04 | norm 0.3155 | dt 338.33ms | 1549639.20 tokens/sec
Step 3436 | loss: 3.287585 | lr:5.7125e-04 | norm 0.3335 | dt 339.09ms | 1546155.85 tokens/sec
Step 3437 | loss: 3.289216 | lr:5.7123e-04 | norm 0.3310 | dt 341.31ms | 1536089.88 tokens/sec
Step 3438 | loss: 3.293500 | lr:5.7121e-04 | norm 0.2664 | dt 338.37ms | 1549461.22 tokens/sec
Step 3439 | loss: 3.388095 | lr:5.7119e-04 | norm 0.2861 | dt 338.36ms | 1549497.25 tokens/sec
Step 3440 | loss: 3.312593 | lr:5.7117e-04 | norm 0.2688 | dt 337.62ms | 1552910.14 tokens/sec
Step 3441 | loss: 3.478951 | lr:5.7115e-04 | norm 0.2760 | dt 337.47ms | 1553579.38 tokens/sec
Step 3442 | loss: 3.360698 | lr:5.7113e-04 | norm 0.3165 | dt 337.85ms | 1551828.52 tokens/sec
Step 3443 | loss: 3.488029 | lr:5.7111e-04 | norm 0.2868 | dt 338.21ms | 1550178.85 tokens/sec
Step 3444 | loss: 3.461389 | lr:5.7109e-04 | norm 0.2715 | dt 338.39ms | 1549369.52 tokens/sec
Step 3445 | loss: 3.419522 | lr:5.7107e-04 | norm 0.2855 | dt 338.47ms | 1548996.27 tokens/sec
Step 3446 | loss: 3.412491 | lr:5.7105e-04 | norm 0.2821 | dt 337.88ms | 1551719.01 tokens/sec
Step 3447 | loss: 3.458575 | lr:5.7103e-04 | norm 0.2945 | dt 337.98ms | 1551258.17 tokens/sec
Step 3448 | loss: 3.436943 | lr:5.7100e-04 | norm 0.2769 | dt 338.31ms | 1549713.46 tokens/sec
Step 3449 | loss: 3.479414 | lr:5.7098e-04 | norm 0.2723 | dt 338.46ms | 1549058.46 tokens/sec
Step 3450 | loss: 3.416885 | lr:5.7096e-04 | norm 0.2836 | dt 340.21ms | 1541067.55 tokens/sec
Step 3451 | loss: 3.480925 | lr:5.7094e-04 | norm 0.2682 | dt 337.54ms | 1553276.51 tokens/sec
Step 3452 | loss: 3.467009 | lr:5.7092e-04 | norm 0.2773 | dt 337.96ms | 1551321.65 tokens/sec
Step 3453 | loss: 3.425880 | lr:5.7090e-04 | norm 0.2948 | dt 338.50ms | 1548875.16 tokens/sec
Step 3454 | loss: 3.416680 | lr:5.7088e-04 | norm 0.2627 | dt 337.46ms | 1553652.92 tokens/sec
Step 3455 | loss: 3.454103 | lr:5.7086e-04 | norm 0.2851 | dt 338.43ms | 1549177.41 tokens/sec
Step 3456 | loss: 3.471707 | lr:5.7084e-04 | norm 0.2605 | dt 337.99ms | 1551212.22 tokens/sec
Step 3457 | loss: 3.455364 | lr:5.7082e-04 | norm 0.2968 | dt 337.80ms | 1552071.67 tokens/sec
Step 3458 | loss: 3.370479 | lr:5.7080e-04 | norm 0.3372 | dt 338.26ms | 1549950.49 tokens/sec
Step 3459 | loss: 3.392845 | lr:5.7077e-04 | norm 0.2957 | dt 338.79ms | 1547536.64 tokens/sec
Step 3460 | loss: 3.418527 | lr:5.7075e-04 | norm 0.2851 | dt 338.79ms | 1547531.19 tokens/sec
Step 3461 | loss: 3.412577 | lr:5.7073e-04 | norm 0.2815 | dt 338.16ms | 1550433.51 tokens/sec
Step 3462 | loss: 3.380591 | lr:5.7071e-04 | norm 0.2781 | dt 337.86ms | 1551773.76 tokens/sec
Step 3463 | loss: 3.383920 | lr:5.7069e-04 | norm 0.2921 | dt 337.95ms | 1551387.31 tokens/sec
Step 3464 | loss: 3.420048 | lr:5.7067e-04 | norm 0.2947 | dt 337.48ms | 1553545.36 tokens/sec
Step 3465 | loss: 3.391306 | lr:5.7065e-04 | norm 0.2784 | dt 338.02ms | 1551063.41 tokens/sec
Step 3466 | loss: 3.359929 | lr:5.7063e-04 | norm 0.2786 | dt 338.38ms | 1549394.63 tokens/sec
Step 3467 | loss: 3.358591 | lr:5.7061e-04 | norm 0.2518 | dt 338.06ms | 1550888.39 tokens/sec
Step 3468 | loss: 3.454231 | lr:5.7059e-04 | norm 0.3050 | dt 337.28ms | 1554455.75 tokens/sec
Step 3469 | loss: 3.468459 | lr:5.7057e-04 | norm 0.3699 | dt 337.94ms | 1551400.45 tokens/sec
Step 3470 | loss: 3.419217 | lr:5.7054e-04 | norm 0.3075 | dt 337.52ms | 1553373.06 tokens/sec
Step 3471 | loss: 3.450095 | lr:5.7052e-04 | norm 0.3162 | dt 338.05ms | 1550934.33 tokens/sec
Step 3472 | loss: 3.457818 | lr:5.7050e-04 | norm 0.2977 | dt 337.80ms | 1552067.29 tokens/sec
Step 3473 | loss: 3.409864 | lr:5.7048e-04 | norm 0.2919 | dt 337.95ms | 1551382.94 tokens/sec
Step 3474 | loss: 3.460036 | lr:5.7046e-04 | norm 0.3048 | dt 337.62ms | 1552886.02 tokens/sec
Step 3475 | loss: 3.396462 | lr:5.7044e-04 | norm 0.3163 | dt 337.97ms | 1551293.19 tokens/sec
Step 3476 | loss: 3.223304 | lr:5.7042e-04 | norm 0.3169 | dt 337.91ms | 1551570.11 tokens/sec
Step 3477 | loss: 3.301286 | lr:5.7040e-04 | norm 0.3002 | dt 337.12ms | 1555182.41 tokens/sec
Step 3478 | loss: 3.289064 | lr:5.7038e-04 | norm 0.3135 | dt 338.56ms | 1548589.39 tokens/sec
Step 3479 | loss: 3.282860 | lr:5.7036e-04 | norm 0.2875 | dt 338.45ms | 1549066.10 tokens/sec
Step 3480 | loss: 3.292740 | lr:5.7033e-04 | norm 0.2707 | dt 337.98ms | 1551242.85 tokens/sec
Step 3481 | loss: 3.329940 | lr:5.7031e-04 | norm 0.2832 | dt 337.84ms | 1551873.42 tokens/sec
Step 3482 | loss: 3.235711 | lr:5.7029e-04 | norm 0.2425 | dt 338.19ms | 1550271.74 tokens/sec
Step 3483 | loss: 3.296217 | lr:5.7027e-04 | norm 0.2955 | dt 339.65ms | 1543629.18 tokens/sec
Step 3484 | loss: 3.288312 | lr:5.7025e-04 | norm 0.2657 | dt 337.57ms | 1553102.08 tokens/sec
Step 3485 | loss: 3.282719 | lr:5.7023e-04 | norm 0.2733 | dt 337.40ms | 1553887.86 tokens/sec
Step 3486 | loss: 3.330784 | lr:5.7021e-04 | norm 0.3029 | dt 337.71ms | 1552470.52 tokens/sec
Step 3487 | loss: 3.528467 | lr:5.7019e-04 | norm 0.2943 | dt 337.69ms | 1552566.97 tokens/sec
Step 3488 | loss: 3.465410 | lr:5.7017e-04 | norm 0.3388 | dt 337.58ms | 1553090.01 tokens/sec
Step 3489 | loss: 3.474440 | lr:5.7014e-04 | norm 0.2872 | dt 339.04ms | 1546401.58 tokens/sec
Step 3490 | loss: 3.425468 | lr:5.7012e-04 | norm 0.2969 | dt 337.99ms | 1551171.73 tokens/sec
Step 3491 | loss: 3.434787 | lr:5.7010e-04 | norm 0.2973 | dt 338.89ms | 1547065.22 tokens/sec
Step 3492 | loss: 3.448438 | lr:5.7008e-04 | norm 0.2989 | dt 337.58ms | 1553070.27 tokens/sec
Step 3493 | loss: 3.445728 | lr:5.7006e-04 | norm 0.2726 | dt 337.92ms | 1551514.28 tokens/sec
Step 3494 | loss: 3.447883 | lr:5.7004e-04 | norm 0.2931 | dt 338.16ms | 1550405.09 tokens/sec
Step 3495 | loss: 3.422489 | lr:5.7002e-04 | norm 0.2957 | dt 337.94ms | 1551431.09 tokens/sec
Step 3496 | loss: 3.424651 | lr:5.7000e-04 | norm 0.2608 | dt 338.02ms | 1551051.38 tokens/sec
Step 3497 | loss: 3.546404 | lr:5.6998e-04 | norm 0.2680 | dt 337.15ms | 1555065.83 tokens/sec
Step 3498 | loss: 3.460534 | lr:5.6995e-04 | norm 0.2674 | dt 337.62ms | 1552896.98 tokens/sec
Step 3499 | loss: 3.403417 | lr:5.6993e-04 | norm 0.2599 | dt 337.73ms | 1552374.07 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 3500: 3.4564
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2701/10042=0.2690


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but that doesn't mean that it can not do this, because if the language was changed, you would be considered "
rank 5 sample 1 >Hello, I'm a language model, now for everyone! There you will have the experience of learning the correct way. If you want to have good fun,
rank 5 sample 2 >Hello, I'm a language model, so I would say from the experience of a real world, where languages are defined in very simple terms.
I don
rank 5 sample 3 >Hello, I'm a language model, for instance, and I need to know how to talk with my readers. This helps them to write and understand the material




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, having no knowledge with the words, this makes it possible to use the structure to define the language, and not to mention
rank 2 sample 1 >Hello, I'm a language model, but I mean a good understanding of languages without knowing just how many. There are many languages and their different forms. Some
rank 2 sample 2 >Hello, I'm a language model, so I can do the math and add text and graphics.<|endoftext|>In 1777, the British made the first transatlantic
rank 2 sample 3 >Hello, I'm a language model, but in future I'm a language model!
A language learning model is pretty simple to comprehend because you don't have




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not an advocate of English; but if you follow me when one comes on campus, why do I try
rank 3 sample 1 >Hello, I'm a language model, and so forth.
1/4: If you'd like a read, I'd love to hear it with me
rank 3 sample 2 >Hello, I'm a language model, and it works so well. Now I had to spend some time with the people who are in here, and if anyone
rank 3 sample 3 >Hello, I'm a language model, and with a very clever concept we can use that as one language for a language.<|endoftext|>There are hundreds and thousands of




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I would like to write a little song about the future of the American political revolution. So just how much do we
rank 7 sample 1 >Hello, I'm a language model, however I do not think it that easy, but I try to avoid the fact that it is not what it is!
rank 7 sample 2 >Hello, I'm a language model, but I don't think I did or am not comfortable with the concepts as I was trying to understand them. This seems
rank 7 sample 3 >Hello, I'm a language model, meaning that all the components should be separated out before the application. I was surprised at the time: now I'm really




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, what would you say that means nothing if it were possible for a word or concept to be spelled out? I'm really
rank 6 sample 1 >Hello, I'm a language model, it's pretty simple, and that's an advantage that I'm really good at, because it allows me to say that
rank 6 sample 2 >Hello, I'm a language model, but if I get some feedback from others, the code will not appear to exist.
So, do you find the


ddp_rank 1: ####### Printing generated samples ####### 

rank 6 sample 3 >Hello, I'm a language model, and I have tried to make them more interactive and more efficient.
I think it would very well be helpful to write


rank 1 sample 0 >Hello, I'm a language model, there is no need to know anything even though a word doesn't have to be a problem. And I'll post a


ddp_rank 0: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, a computer, and a computer. My guess is that as a language model, it must be flexible and flexible.

rank 1 sample 2 >Hello, I'm a language model, but have no idea what it is?
I'm an interpreter and it's not a native. But it doesn't
rank 0 sample 0 >Hello, I'm a language model, and I'd like you to use an easy approach to think of all the steps you'll need to get a better grasp
rank 1 sample 3 >Hello, I'm a language model, and I'm pretty excited this week." On the other hand, one of my students I’m a language model


rank 0 sample 1 >Hello, I'm a language model, but a pretty good question. If we can do much more, what do we need in order to find what we want
rank 0 sample 2 >Hello, I'm a language model, so I did some pretty hard work and I would ask you questions and I would probably ask you a question. I mean


ddp_rank 4: ####### Printing generated samples ####### 

rank 0 sample 3 >Hello, I'm a language model, but since I'm a language teacher, you learn as much as you did,” wrote a paper describing what would


rank 4 sample 0 >Hello, I'm a language model, so I need to work on it. Let me give you the instructions so you can make everything just for a quick review
rank 4 sample 1 >Hello, I'm a language model, too."
Kirby wrote a lot of advice in the journal How to Read in the 21st Century for a
rank 4 sample 2 >Hello, I'm a language model, I get asked - how I'm supposed to put together a new function?
1. Suppose a command is empty.
rank 4 sample 3 >Hello, I'm a language model, and the class that has an extra edition (of ours! A really creative, right? I would be a part)


Step 3500 | loss: 3.405216 | lr:5.6991e-04 | norm 0.2626 | dt 12256.70ms | 42775.63 tokens/sec
Step 3501 | loss: 3.465392 | lr:5.6989e-04 | norm 0.2960 | dt 335.69ms | 1561807.40 tokens/sec
Step 3502 | loss: 3.419242 | lr:5.6987e-04 | norm 0.2969 | dt 337.74ms | 1552322.57 tokens/sec
Step 3503 | loss: 3.468907 | lr:5.6985e-04 | norm 0.3158 | dt 336.02ms | 1560272.61 tokens/sec
Step 3504 | loss: 3.435308 | lr:5.6983e-04 | norm 0.2876 | dt 338.54ms | 1548654.82 tokens/sec
Step 3505 | loss: 3.392800 | lr:5.6981e-04 | norm 0.2867 | dt 336.39ms | 1558591.69 tokens/sec
Step 3506 | loss: 3.347047 | lr:5.6978e-04 | norm 0.2893 | dt 336.53ms | 1557935.79 tokens/sec
Step 3507 | loss: 3.353262 | lr:5.6976e-04 | norm 0.2555 | dt 336.33ms | 1558849.13 tokens/sec
Step 3508 | loss: 3.344414 | lr:5.6974e-04 | norm 0.2673 | dt 336.78ms | 1556768.90 tokens/sec
Step 3509 | loss: 3.456640 | lr:5.6972e-04 | norm 0.2775 | dt 336.83ms | 1556533.09 tokens/sec
Step 3510 | loss: 3.405748 | lr:5.6970e-04 | norm 0.2738 | dt 340.43ms | 1540090.79 tokens/sec
Step 3511 | loss: 3.383754 | lr:5.6968e-04 | norm 0.2729 | dt 336.44ms | 1558326.62 tokens/sec
Step 3512 | loss: 3.390409 | lr:5.6966e-04 | norm 0.2894 | dt 336.94ms | 1556050.67 tokens/sec
Step 3513 | loss: 3.336733 | lr:5.6964e-04 | norm 0.2996 | dt 337.20ms | 1554849.22 tokens/sec
Step 3514 | loss: 3.335363 | lr:5.6961e-04 | norm 0.2924 | dt 336.81ms | 1556630.05 tokens/sec
Step 3515 | loss: 3.342153 | lr:5.6959e-04 | norm 0.2596 | dt 337.59ms | 1553030.78 tokens/sec
Step 3516 | loss: 3.391115 | lr:5.6957e-04 | norm 0.2755 | dt 339.57ms | 1543955.40 tokens/sec
Step 3517 | loss: 3.388586 | lr:5.6955e-04 | norm 0.2906 | dt 339.00ms | 1546584.30 tokens/sec
Step 3518 | loss: 3.381311 | lr:5.6953e-04 | norm 0.2848 | dt 338.07ms | 1550845.73 tokens/sec
Step 3519 | loss: 3.416992 | lr:5.6951e-04 | norm 0.2801 | dt 337.72ms | 1552427.77 tokens/sec
Step 3520 | loss: 3.481110 | lr:5.6949e-04 | norm 0.2846 | dt 338.71ms | 1547904.83 tokens/sec
Step 3521 | loss: 3.531730 | lr:5.6947e-04 | norm 0.2958 | dt 338.56ms | 1548569.76 tokens/sec
Step 3522 | loss: 3.337564 | lr:5.6944e-04 | norm 0.2796 | dt 338.82ms | 1547408.14 tokens/sec
Step 3523 | loss: 3.343806 | lr:5.6942e-04 | norm 0.3335 | dt 338.53ms | 1548698.45 tokens/sec
Step 3524 | loss: 3.336264 | lr:5.6940e-04 | norm 0.2785 | dt 338.57ms | 1548554.49 tokens/sec
Step 3525 | loss: 3.282752 | lr:5.6938e-04 | norm 0.2944 | dt 339.19ms | 1545710.26 tokens/sec
Step 3526 | loss: 3.301481 | lr:5.6936e-04 | norm 0.3017 | dt 338.90ms | 1547046.72 tokens/sec
Step 3527 | loss: 3.336470 | lr:5.6934e-04 | norm 0.2573 | dt 338.21ms | 1550196.33 tokens/sec
Step 3528 | loss: 3.316990 | lr:5.6932e-04 | norm 0.2742 | dt 337.13ms | 1555139.51 tokens/sec
Step 3529 | loss: 3.275163 | lr:5.6929e-04 | norm 0.2574 | dt 338.54ms | 1548671.18 tokens/sec
Step 3530 | loss: 3.334080 | lr:5.6927e-04 | norm 0.2647 | dt 340.75ms | 1538615.59 tokens/sec
Step 3531 | loss: 3.252415 | lr:5.6925e-04 | norm 0.3238 | dt 337.66ms | 1552691.94 tokens/sec
Step 3532 | loss: 3.319739 | lr:5.6923e-04 | norm 0.3084 | dt 338.40ms | 1549334.59 tokens/sec
Step 3533 | loss: 3.345544 | lr:5.6921e-04 | norm 0.4433 | dt 337.90ms | 1551620.47 tokens/sec
Step 3534 | loss: 3.353006 | lr:5.6919e-04 | norm 0.3460 | dt 339.69ms | 1543449.33 tokens/sec
Step 3535 | loss: 3.417235 | lr:5.6917e-04 | norm 0.3040 | dt 339.42ms | 1544640.81 tokens/sec
Step 3536 | loss: 3.402452 | lr:5.6914e-04 | norm 0.3080 | dt 339.62ms | 1543739.71 tokens/sec
Step 3537 | loss: 3.446474 | lr:5.6912e-04 | norm 0.3062 | dt 339.15ms | 1545873.25 tokens/sec
Step 3538 | loss: 3.483040 | lr:5.6910e-04 | norm 0.3223 | dt 339.09ms | 1546164.55 tokens/sec
Step 3539 | loss: 3.461257 | lr:5.6908e-04 | norm 0.3446 | dt 339.74ms | 1543202.37 tokens/sec
Step 3540 | loss: 3.544530 | lr:5.6906e-04 | norm 0.3300 | dt 339.17ms | 1545806.97 tokens/sec
Step 3541 | loss: 3.428839 | lr:5.6904e-04 | norm 0.3023 | dt 339.34ms | 1545022.82 tokens/sec
Step 3542 | loss: 3.453036 | lr:5.6902e-04 | norm 0.3320 | dt 339.31ms | 1545156.35 tokens/sec
Step 3543 | loss: 3.390375 | lr:5.6899e-04 | norm 0.3318 | dt 339.55ms | 1544074.65 tokens/sec
Step 3544 | loss: 3.430179 | lr:5.6897e-04 | norm 0.2997 | dt 339.00ms | 1546549.49 tokens/sec
Step 3545 | loss: 3.515260 | lr:5.6895e-04 | norm 0.2783 | dt 339.62ms | 1543752.72 tokens/sec
Step 3546 | loss: 3.470126 | lr:5.6893e-04 | norm 0.3377 | dt 341.76ms | 1534069.91 tokens/sec
Step 3547 | loss: 3.359234 | lr:5.6891e-04 | norm 0.2954 | dt 339.66ms | 1543575.00 tokens/sec
Step 3548 | loss: 3.489542 | lr:5.6889e-04 | norm 0.2897 | dt 339.64ms | 1543645.43 tokens/sec
Step 3549 | loss: 3.470418 | lr:5.6886e-04 | norm 0.2914 | dt 338.13ms | 1550555.95 tokens/sec
Step 3550 | loss: 3.418798 | lr:5.6884e-04 | norm 0.2841 | dt 337.81ms | 1552033.33 tokens/sec
Step 3551 | loss: 3.400400 | lr:5.6882e-04 | norm 0.3003 | dt 338.33ms | 1549650.12 tokens/sec
Step 3552 | loss: 3.424304 | lr:5.6880e-04 | norm 0.2808 | dt 338.50ms | 1548856.62 tokens/sec
Step 3553 | loss: 3.408437 | lr:5.6878e-04 | norm 0.2911 | dt 338.57ms | 1548519.60 tokens/sec
Step 3554 | loss: 3.434738 | lr:5.6876e-04 | norm 0.2734 | dt 338.09ms | 1550728.71 tokens/sec
Step 3555 | loss: 3.432382 | lr:5.6874e-04 | norm 0.2716 | dt 338.23ms | 1550088.15 tokens/sec
Step 3556 | loss: 3.410256 | lr:5.6871e-04 | norm 0.2583 | dt 338.54ms | 1548662.46 tokens/sec
Step 3557 | loss: 3.357488 | lr:5.6869e-04 | norm 0.2596 | dt 338.24ms | 1550036.80 tokens/sec
Step 3558 | loss: 3.341227 | lr:5.6867e-04 | norm 0.2702 | dt 338.15ms | 1550475.05 tokens/sec
Step 3559 | loss: 3.348560 | lr:5.6865e-04 | norm 0.2801 | dt 338.23ms | 1550082.69 tokens/sec
Step 3560 | loss: 3.350780 | lr:5.6863e-04 | norm 0.2892 | dt 338.85ms | 1547269.86 tokens/sec
Step 3561 | loss: 3.336321 | lr:5.6861e-04 | norm 0.3150 | dt 338.52ms | 1548748.62 tokens/sec
Step 3562 | loss: 3.345311 | lr:5.6858e-04 | norm 0.2969 | dt 338.24ms | 1550038.98 tokens/sec
Step 3563 | loss: 3.425187 | lr:5.6856e-04 | norm 0.2966 | dt 338.87ms | 1547174.07 tokens/sec
Step 3564 | loss: 3.429561 | lr:5.6854e-04 | norm 0.2836 | dt 338.20ms | 1550248.79 tokens/sec
Step 3565 | loss: 3.403478 | lr:5.6852e-04 | norm 0.2960 | dt 338.09ms | 1550741.83 tokens/sec
Step 3566 | loss: 3.417061 | lr:5.6850e-04 | norm 0.2817 | dt 338.36ms | 1549487.42 tokens/sec
Step 3567 | loss: 3.412219 | lr:5.6848e-04 | norm 0.3066 | dt 338.21ms | 1550202.89 tokens/sec
Step 3568 | loss: 3.486750 | lr:5.6845e-04 | norm 0.2573 | dt 338.75ms | 1547734.87 tokens/sec
Step 3569 | loss: 3.365812 | lr:5.6843e-04 | norm 0.3097 | dt 339.74ms | 1543226.20 tokens/sec
Step 3570 | loss: 3.285098 | lr:5.6841e-04 | norm 0.3395 | dt 337.82ms | 1551952.27 tokens/sec
Step 3571 | loss: 3.350950 | lr:5.6839e-04 | norm 0.3046 | dt 338.02ms | 1551054.66 tokens/sec
Step 3572 | loss: 3.257274 | lr:5.6837e-04 | norm 0.2954 | dt 338.03ms | 1550996.68 tokens/sec
Step 3573 | loss: 3.343283 | lr:5.6835e-04 | norm 0.3052 | dt 337.98ms | 1551255.99 tokens/sec
Step 3574 | loss: 3.343400 | lr:5.6832e-04 | norm 0.2688 | dt 337.67ms | 1552678.79 tokens/sec
Step 3575 | loss: 3.319909 | lr:5.6830e-04 | norm 0.2476 | dt 338.10ms | 1550684.97 tokens/sec
Step 3576 | loss: 3.250259 | lr:5.6828e-04 | norm 0.2482 | dt 337.52ms | 1553359.90 tokens/sec
Step 3577 | loss: 3.324905 | lr:5.6826e-04 | norm 0.2574 | dt 338.67ms | 1548057.38 tokens/sec
Step 3578 | loss: 3.352855 | lr:5.6824e-04 | norm 0.2666 | dt 338.90ms | 1547026.04 tokens/sec
Step 3579 | loss: 3.315223 | lr:5.6822e-04 | norm 0.2759 | dt 337.44ms | 1553701.22 tokens/sec
Step 3580 | loss: 3.273706 | lr:5.6819e-04 | norm 0.2814 | dt 338.40ms | 1549296.38 tokens/sec
Step 3581 | loss: 3.488353 | lr:5.6817e-04 | norm 0.3236 | dt 338.37ms | 1549459.04 tokens/sec
Step 3582 | loss: 3.451655 | lr:5.6815e-04 | norm 0.3461 | dt 337.76ms | 1552268.88 tokens/sec
Step 3583 | loss: 3.494064 | lr:5.6813e-04 | norm 0.3426 | dt 337.88ms | 1551698.21 tokens/sec
Step 3584 | loss: 3.494949 | lr:5.6811e-04 | norm 0.3272 | dt 338.43ms | 1549181.78 tokens/sec
Step 3585 | loss: 3.471559 | lr:5.6808e-04 | norm 0.2927 | dt 338.09ms | 1550726.52 tokens/sec
Step 3586 | loss: 3.405854 | lr:5.6806e-04 | norm 0.2732 | dt 338.10ms | 1550675.13 tokens/sec
Step 3587 | loss: 3.456615 | lr:5.6804e-04 | norm 0.2822 | dt 337.92ms | 1551524.14 tokens/sec
Step 3588 | loss: 3.507162 | lr:5.6802e-04 | norm 0.2670 | dt 338.52ms | 1548749.71 tokens/sec
Step 3589 | loss: 3.437264 | lr:5.6800e-04 | norm 0.2845 | dt 338.37ms | 1549436.11 tokens/sec
Step 3590 | loss: 3.513429 | lr:5.6798e-04 | norm 0.2781 | dt 889.74ms | 589257.90 tokens/sec
Step 3591 | loss: 3.469902 | lr:5.6795e-04 | norm 0.3098 | dt 334.42ms | 1567730.93 tokens/sec
Step 3592 | loss: 3.430152 | lr:5.6793e-04 | norm 0.3041 | dt 337.03ms | 1555593.86 tokens/sec
Step 3593 | loss: 3.466424 | lr:5.6791e-04 | norm 0.2650 | dt 339.38ms | 1544829.62 tokens/sec
Step 3594 | loss: 3.529810 | lr:5.6789e-04 | norm 0.2800 | dt 336.76ms | 1556844.95 tokens/sec
Step 3595 | loss: 3.415983 | lr:5.6787e-04 | norm 0.3085 | dt 337.01ms | 1555705.01 tokens/sec
Step 3596 | loss: 3.384427 | lr:5.6784e-04 | norm 0.2968 | dt 339.54ms | 1544097.42 tokens/sec
Step 3597 | loss: 3.414609 | lr:5.6782e-04 | norm 0.2906 | dt 338.55ms | 1548647.19 tokens/sec
Step 3598 | loss: 3.419378 | lr:5.6780e-04 | norm 0.2761 | dt 337.68ms | 1552609.72 tokens/sec
Step 3599 | loss: 3.384279 | lr:5.6778e-04 | norm 0.2679 | dt 338.08ms | 1550775.74 tokens/sec
Step 3600 | loss: 3.381955 | lr:5.6776e-04 | norm 0.2889 | dt 338.72ms | 1547855.80 tokens/sec
Step 3601 | loss: 3.422970 | lr:5.6774e-04 | norm 0.2726 | dt 338.54ms | 1548691.90 tokens/sec
Step 3602 | loss: 3.385737 | lr:5.6771e-04 | norm 0.2526 | dt 337.65ms | 1552768.69 tokens/sec
Step 3603 | loss: 3.370739 | lr:5.6769e-04 | norm 0.2910 | dt 338.49ms | 1548905.71 tokens/sec
Step 3604 | loss: 3.353217 | lr:5.6767e-04 | norm 0.2714 | dt 338.81ms | 1547423.38 tokens/sec
Step 3605 | loss: 3.391056 | lr:5.6765e-04 | norm 0.2700 | dt 338.52ms | 1548766.08 tokens/sec
Step 3606 | loss: 3.376377 | lr:5.6763e-04 | norm 0.2619 | dt 338.13ms | 1550536.27 tokens/sec
Step 3607 | loss: 3.488123 | lr:5.6760e-04 | norm 0.2910 | dt 338.05ms | 1550925.58 tokens/sec
Step 3608 | loss: 3.383479 | lr:5.6758e-04 | norm 0.3037 | dt 338.86ms | 1547192.57 tokens/sec
Step 3609 | loss: 3.447630 | lr:5.6756e-04 | norm 0.2876 | dt 932.53ms | 562223.69 tokens/sec
Step 3610 | loss: 3.387154 | lr:5.6754e-04 | norm 0.2878 | dt 336.52ms | 1557972.22 tokens/sec
Step 3611 | loss: 3.404129 | lr:5.6752e-04 | norm 0.2750 | dt 339.61ms | 1543810.16 tokens/sec
Step 3612 | loss: 3.445794 | lr:5.6749e-04 | norm 0.2660 | dt 340.06ms | 1541746.07 tokens/sec
Step 3613 | loss: 3.413374 | lr:5.6747e-04 | norm 0.2719 | dt 340.21ms | 1541071.87 tokens/sec
Step 3614 | loss: 3.361030 | lr:5.6745e-04 | norm 0.2567 | dt 338.59ms | 1548424.73 tokens/sec
Step 3615 | loss: 3.260477 | lr:5.6743e-04 | norm 0.2877 | dt 338.98ms | 1546685.46 tokens/sec
Step 3616 | loss: 3.343691 | lr:5.6741e-04 | norm 0.3036 | dt 339.33ms | 1545052.13 tokens/sec
Step 3617 | loss: 3.291935 | lr:5.6738e-04 | norm 0.2943 | dt 338.91ms | 1546974.89 tokens/sec
Step 3618 | loss: 3.222690 | lr:5.6736e-04 | norm 0.2906 | dt 339.30ms | 1545187.84 tokens/sec
Step 3619 | loss: 3.323556 | lr:5.6734e-04 | norm 0.2899 | dt 338.82ms | 1547415.76 tokens/sec
Step 3620 | loss: 3.287766 | lr:5.6732e-04 | norm 0.2764 | dt 338.65ms | 1548156.56 tokens/sec
Step 3621 | loss: 3.263483 | lr:5.6730e-04 | norm 0.2656 | dt 339.89ms | 1542543.12 tokens/sec
Step 3622 | loss: 3.284129 | lr:5.6727e-04 | norm 0.2742 | dt 338.98ms | 1546666.97 tokens/sec
Step 3623 | loss: 3.310344 | lr:5.6725e-04 | norm 0.2631 | dt 338.94ms | 1546826.90 tokens/sec
Step 3624 | loss: 3.232086 | lr:5.6723e-04 | norm 0.2609 | dt 340.11ms | 1541537.48 tokens/sec
Step 3625 | loss: 3.245826 | lr:5.6721e-04 | norm 0.3080 | dt 338.88ms | 1547135.97 tokens/sec
Step 3626 | loss: 3.397475 | lr:5.6719e-04 | norm 0.3102 | dt 339.32ms | 1545098.81 tokens/sec
Step 3627 | loss: 3.485343 | lr:5.6716e-04 | norm 0.3018 | dt 338.81ms | 1547416.85 tokens/sec
Step 3628 | loss: 3.452799 | lr:5.6714e-04 | norm 0.3064 | dt 339.44ms | 1544552.93 tokens/sec
Step 3629 | loss: 3.411468 | lr:5.6712e-04 | norm 0.3163 | dt 340.16ms | 1541296.54 tokens/sec
Step 3630 | loss: 3.534676 | lr:5.6710e-04 | norm 0.3062 | dt 342.46ms | 1530944.92 tokens/sec
Step 3631 | loss: 3.437420 | lr:5.6707e-04 | norm 0.3078 | dt 338.15ms | 1550479.42 tokens/sec
Step 3632 | loss: 3.422403 | lr:5.6705e-04 | norm 0.2830 | dt 338.51ms | 1548821.71 tokens/sec
Step 3633 | loss: 3.460633 | lr:5.6703e-04 | norm 0.2832 | dt 340.19ms | 1541161.51 tokens/sec
Step 3634 | loss: 3.451250 | lr:5.6701e-04 | norm 0.3077 | dt 338.97ms | 1546700.69 tokens/sec
Step 3635 | loss: 3.473962 | lr:5.6699e-04 | norm 0.2941 | dt 339.90ms | 1542487.94 tokens/sec
Step 3636 | loss: 3.450030 | lr:5.6696e-04 | norm 0.2868 | dt 338.99ms | 1546615.84 tokens/sec
Step 3637 | loss: 3.450215 | lr:5.6694e-04 | norm 0.3151 | dt 339.22ms | 1545574.46 tokens/sec
Step 3638 | loss: 3.458240 | lr:5.6692e-04 | norm 0.2981 | dt 340.67ms | 1539007.55 tokens/sec
Step 3639 | loss: 3.406184 | lr:5.6690e-04 | norm 0.2643 | dt 339.48ms | 1544373.95 tokens/sec
Step 3640 | loss: 3.451228 | lr:5.6688e-04 | norm 0.2702 | dt 339.40ms | 1544730.87 tokens/sec
Step 3641 | loss: 3.358535 | lr:5.6685e-04 | norm 0.2679 | dt 339.35ms | 1544989.17 tokens/sec
Step 3642 | loss: 3.428040 | lr:5.6683e-04 | norm 0.2756 | dt 339.84ms | 1542740.08 tokens/sec
Step 3643 | loss: 3.391084 | lr:5.6681e-04 | norm 0.2882 | dt 339.23ms | 1545538.62 tokens/sec
Step 3644 | loss: 3.381680 | lr:5.6679e-04 | norm 0.2633 | dt 338.19ms | 1550293.60 tokens/sec
Step 3645 | loss: 3.340944 | lr:5.6676e-04 | norm 0.2669 | dt 339.43ms | 1544634.30 tokens/sec
Step 3646 | loss: 3.408245 | lr:5.6674e-04 | norm 0.2631 | dt 339.85ms | 1542693.54 tokens/sec
Step 3647 | loss: 3.367864 | lr:5.6672e-04 | norm 0.3230 | dt 339.10ms | 1546134.11 tokens/sec
Step 3648 | loss: 3.411432 | lr:5.6670e-04 | norm 0.3250 | dt 338.89ms | 1547082.63 tokens/sec
Step 3649 | loss: 3.372304 | lr:5.6668e-04 | norm 0.3141 | dt 338.86ms | 1547227.41 tokens/sec
Step 3650 | loss: 3.382118 | lr:5.6665e-04 | norm 0.3324 | dt 338.20ms | 1550214.91 tokens/sec
Step 3651 | loss: 3.388425 | lr:5.6663e-04 | norm 0.2987 | dt 340.17ms | 1541247.92 tokens/sec
Step 3652 | loss: 3.322027 | lr:5.6661e-04 | norm 0.3175 | dt 338.69ms | 1548010.53 tokens/sec
Step 3653 | loss: 3.404865 | lr:5.6659e-04 | norm 0.2488 | dt 338.57ms | 1548531.59 tokens/sec
Step 3654 | loss: 3.392128 | lr:5.6656e-04 | norm 0.2959 | dt 338.78ms | 1547563.87 tokens/sec
Step 3655 | loss: 3.397631 | lr:5.6654e-04 | norm 0.3017 | dt 339.49ms | 1544319.72 tokens/sec
Step 3656 | loss: 3.356312 | lr:5.6652e-04 | norm 0.2869 | dt 339.30ms | 1545224.75 tokens/sec
Step 3657 | loss: 3.391803 | lr:5.6650e-04 | norm 0.2718 | dt 339.01ms | 1546534.26 tokens/sec
Step 3658 | loss: 3.428824 | lr:5.6648e-04 | norm 0.2552 | dt 339.07ms | 1546261.31 tokens/sec
Step 3659 | loss: 3.350661 | lr:5.6645e-04 | norm 0.2814 | dt 338.09ms | 1550741.83 tokens/sec
Step 3660 | loss: 3.402348 | lr:5.6643e-04 | norm 0.2853 | dt 339.47ms | 1544418.42 tokens/sec
Step 3661 | loss: 3.413172 | lr:5.6641e-04 | norm 0.2824 | dt 339.26ms | 1545375.70 tokens/sec
Step 3662 | loss: 3.310555 | lr:5.6639e-04 | norm 0.2726 | dt 339.01ms | 1546547.32 tokens/sec
Step 3663 | loss: 3.289184 | lr:5.6636e-04 | norm 0.2822 | dt 340.40ms | 1540211.60 tokens/sec
Step 3664 | loss: 3.289618 | lr:5.6634e-04 | norm 0.2750 | dt 338.87ms | 1547144.68 tokens/sec
Step 3665 | loss: 3.247402 | lr:5.6632e-04 | norm 0.2718 | dt 338.56ms | 1548603.56 tokens/sec
Step 3666 | loss: 3.275440 | lr:5.6630e-04 | norm 0.2712 | dt 338.92ms | 1546950.95 tokens/sec
Step 3667 | loss: 3.306014 | lr:5.6627e-04 | norm 0.2762 | dt 339.13ms | 1545997.15 tokens/sec
Step 3668 | loss: 3.344100 | lr:5.6625e-04 | norm 0.2659 | dt 340.29ms | 1540714.48 tokens/sec
Step 3669 | loss: 3.266308 | lr:5.6623e-04 | norm 0.2754 | dt 338.01ms | 1551094.05 tokens/sec
Step 3670 | loss: 3.315625 | lr:5.6621e-04 | norm 0.2813 | dt 339.49ms | 1544333.82 tokens/sec
Step 3671 | loss: 3.360175 | lr:5.6618e-04 | norm 0.2888 | dt 339.53ms | 1544163.56 tokens/sec
Step 3672 | loss: 3.316198 | lr:5.6616e-04 | norm 0.3284 | dt 339.20ms | 1545638.56 tokens/sec
Step 3673 | loss: 3.298955 | lr:5.6614e-04 | norm 0.2963 | dt 339.38ms | 1544846.99 tokens/sec
Step 3674 | loss: 3.455584 | lr:5.6612e-04 | norm 0.3053 | dt 338.55ms | 1548647.19 tokens/sec
Step 3675 | loss: 3.468901 | lr:5.6610e-04 | norm 0.2814 | dt 339.59ms | 1543901.20 tokens/sec
Step 3676 | loss: 3.403493 | lr:5.6607e-04 | norm 0.3068 | dt 338.51ms | 1548814.07 tokens/sec
Step 3677 | loss: 3.422691 | lr:5.6605e-04 | norm 0.2763 | dt 339.06ms | 1546280.88 tokens/sec
Step 3678 | loss: 3.426308 | lr:5.6603e-04 | norm 0.2998 | dt 338.51ms | 1548814.07 tokens/sec
Step 3679 | loss: 3.391008 | lr:5.6601e-04 | norm 0.2852 | dt 339.32ms | 1545134.64 tokens/sec
Step 3680 | loss: 3.401648 | lr:5.6598e-04 | norm 0.3095 | dt 339.90ms | 1542482.53 tokens/sec
Step 3681 | loss: 3.473791 | lr:5.6596e-04 | norm 0.2634 | dt 339.68ms | 1543475.33 tokens/sec
Step 3682 | loss: 3.476089 | lr:5.6594e-04 | norm 0.2660 | dt 339.12ms | 1546010.19 tokens/sec
Step 3683 | loss: 3.402484 | lr:5.6592e-04 | norm 0.2870 | dt 339.49ms | 1544350.09 tokens/sec
Step 3684 | loss: 3.454184 | lr:5.6589e-04 | norm 0.2473 | dt 339.84ms | 1542757.40 tokens/sec
Step 3685 | loss: 3.367999 | lr:5.6587e-04 | norm 0.2598 | dt 339.17ms | 1545816.75 tokens/sec
Step 3686 | loss: 3.421655 | lr:5.6585e-04 | norm 0.3191 | dt 339.37ms | 1544887.14 tokens/sec
Step 3687 | loss: 3.368806 | lr:5.6583e-04 | norm 0.8985 | dt 339.02ms | 1546469.01 tokens/sec
Step 3688 | loss: 3.380177 | lr:5.6580e-04 | norm 0.3774 | dt 339.47ms | 1544421.67 tokens/sec
Step 3689 | loss: 3.376035 | lr:5.6578e-04 | norm 0.3554 | dt 339.51ms | 1544256.82 tokens/sec
Step 3690 | loss: 3.418105 | lr:5.6576e-04 | norm 0.3007 | dt 342.31ms | 1531622.02 tokens/sec
Step 3691 | loss: 3.354157 | lr:5.6574e-04 | norm 0.3865 | dt 339.55ms | 1544087.66 tokens/sec
Step 3692 | loss: 3.414387 | lr:5.6571e-04 | norm 0.3167 | dt 338.96ms | 1546769.23 tokens/sec
Step 3693 | loss: 3.418053 | lr:5.6569e-04 | norm 0.3020 | dt 339.37ms | 1544887.14 tokens/sec
Step 3694 | loss: 3.465712 | lr:5.6567e-04 | norm 0.3161 | dt 339.23ms | 1545511.46 tokens/sec
Step 3695 | loss: 3.432227 | lr:5.6565e-04 | norm 0.2786 | dt 338.67ms | 1548057.38 tokens/sec
Step 3696 | loss: 3.493133 | lr:5.6562e-04 | norm 0.2756 | dt 338.72ms | 1547845.99 tokens/sec
Step 3697 | loss: 3.362339 | lr:5.6560e-04 | norm 0.2794 | dt 340.13ms | 1541420.78 tokens/sec
Step 3698 | loss: 3.375823 | lr:5.6558e-04 | norm 0.3041 | dt 338.97ms | 1546717.01 tokens/sec
Step 3699 | loss: 3.337029 | lr:5.6556e-04 | norm 0.2690 | dt 338.24ms | 1550053.19 tokens/sec
Step 3700 | loss: 3.315329 | lr:5.6553e-04 | norm 0.2746 | dt 337.88ms | 1551690.55 tokens/sec
Step 3701 | loss: 3.398996 | lr:5.6551e-04 | norm 0.2836 | dt 338.57ms | 1548527.23 tokens/sec
Step 3702 | loss: 3.395183 | lr:5.6549e-04 | norm 0.3358 | dt 337.61ms | 1552923.30 tokens/sec
Step 3703 | loss: 3.410419 | lr:5.6546e-04 | norm 0.3090 | dt 338.43ms | 1549161.04 tokens/sec
Step 3704 | loss: 3.407950 | lr:5.6544e-04 | norm 0.2560 | dt 337.91ms | 1551554.79 tokens/sec
Step 3705 | loss: 3.378319 | lr:5.6542e-04 | norm 0.2789 | dt 338.48ms | 1548947.17 tokens/sec
Step 3706 | loss: 3.463281 | lr:5.6540e-04 | norm 0.3007 | dt 337.73ms | 1552402.57 tokens/sec
Step 3707 | loss: 3.394785 | lr:5.6537e-04 | norm 0.2795 | dt 337.24ms | 1554655.76 tokens/sec
Step 3708 | loss: 3.424856 | lr:5.6535e-04 | norm 0.3025 | dt 339.62ms | 1543753.80 tokens/sec
Step 3709 | loss: 3.274271 | lr:5.6533e-04 | norm 0.3219 | dt 339.14ms | 1545936.29 tokens/sec
Step 3710 | loss: 3.365632 | lr:5.6531e-04 | norm 0.3059 | dt 338.27ms | 1549928.64 tokens/sec
Step 3711 | loss: 3.280329 | lr:5.6528e-04 | norm 0.3270 | dt 339.48ms | 1544377.20 tokens/sec
Step 3712 | loss: 3.304885 | lr:5.6526e-04 | norm 0.3488 | dt 339.67ms | 1543533.83 tokens/sec
Step 3713 | loss: 3.350029 | lr:5.6524e-04 | norm 0.4717 | dt 338.58ms | 1548501.06 tokens/sec
Step 3714 | loss: 3.330323 | lr:5.6522e-04 | norm 0.3330 | dt 338.41ms | 1549284.37 tokens/sec
Step 3715 | loss: 3.326521 | lr:5.6519e-04 | norm 0.3022 | dt 339.10ms | 1546096.06 tokens/sec
Step 3716 | loss: 3.293570 | lr:5.6517e-04 | norm 0.2864 | dt 337.50ms | 1553452.07 tokens/sec
Step 3717 | loss: 3.331847 | lr:5.6515e-04 | norm 0.2903 | dt 338.02ms | 1551050.28 tokens/sec
Step 3718 | loss: 3.259645 | lr:5.6512e-04 | norm 0.2816 | dt 339.61ms | 1543783.06 tokens/sec
Step 3719 | loss: 3.260226 | lr:5.6510e-04 | norm 0.2759 | dt 338.40ms | 1549305.11 tokens/sec
Step 3720 | loss: 3.285928 | lr:5.6508e-04 | norm 0.3075 | dt 337.70ms | 1552534.09 tokens/sec
Step 3721 | loss: 3.508069 | lr:5.6506e-04 | norm 0.3032 | dt 338.71ms | 1547910.28 tokens/sec
Step 3722 | loss: 3.475420 | lr:5.6503e-04 | norm 0.2901 | dt 336.75ms | 1556924.31 tokens/sec
Step 3723 | loss: 3.447522 | lr:5.6501e-04 | norm 0.3149 | dt 337.24ms | 1554630.48 tokens/sec
Step 3724 | loss: 3.482299 | lr:5.6499e-04 | norm 0.3190 | dt 338.37ms | 1549449.21 tokens/sec
Step 3725 | loss: 3.516802 | lr:5.6497e-04 | norm 0.3050 | dt 337.92ms | 1551511.00 tokens/sec
Step 3726 | loss: 3.492960 | lr:5.6494e-04 | norm 0.2763 | dt 340.53ms | 1539626.05 tokens/sec
Step 3727 | loss: 3.489133 | lr:5.6492e-04 | norm 0.2836 | dt 338.39ms | 1549381.53 tokens/sec
Step 3728 | loss: 3.436933 | lr:5.6490e-04 | norm 0.2973 | dt 339.23ms | 1545511.46 tokens/sec
Step 3729 | loss: 3.425354 | lr:5.6487e-04 | norm 0.2934 | dt 340.67ms | 1539010.79 tokens/sec
Step 3730 | loss: 3.419989 | lr:5.6485e-04 | norm 0.2984 | dt 338.20ms | 1550218.19 tokens/sec
Step 3731 | loss: 3.422696 | lr:5.6483e-04 | norm 0.2790 | dt 338.65ms | 1548180.54 tokens/sec
Step 3732 | loss: 3.428119 | lr:5.6481e-04 | norm 0.2998 | dt 338.60ms | 1548392.02 tokens/sec
Step 3733 | loss: 3.397409 | lr:5.6478e-04 | norm 0.2859 | dt 338.73ms | 1547803.50 tokens/sec
Step 3734 | loss: 3.411936 | lr:5.6476e-04 | norm 0.2777 | dt 339.52ms | 1544193.92 tokens/sec
Step 3735 | loss: 3.356494 | lr:5.6474e-04 | norm 0.2859 | dt 339.03ms | 1546429.86 tokens/sec
Step 3736 | loss: 3.388973 | lr:5.6472e-04 | norm 0.2986 | dt 339.07ms | 1546267.84 tokens/sec
Step 3737 | loss: 3.375674 | lr:5.6469e-04 | norm 0.2535 | dt 338.98ms | 1546673.50 tokens/sec
Step 3738 | loss: 3.407225 | lr:5.6467e-04 | norm 0.2652 | dt 339.09ms | 1546139.55 tokens/sec
Step 3739 | loss: 3.331742 | lr:5.6465e-04 | norm 0.2591 | dt 338.75ms | 1547691.30 tokens/sec
Step 3740 | loss: 3.386349 | lr:5.6462e-04 | norm 0.2778 | dt 338.32ms | 1549678.51 tokens/sec
Step 3741 | loss: 3.419619 | lr:5.6460e-04 | norm 0.2875 | dt 338.90ms | 1547016.24 tokens/sec
Step 3742 | loss: 3.416819 | lr:5.6458e-04 | norm 0.2638 | dt 338.61ms | 1548341.87 tokens/sec
Step 3743 | loss: 3.361836 | lr:5.6456e-04 | norm 0.2570 | dt 339.20ms | 1545661.37 tokens/sec
Step 3744 | loss: 3.365645 | lr:5.6453e-04 | norm 0.2605 | dt 337.76ms | 1552231.62 tokens/sec
Step 3745 | loss: 3.301628 | lr:5.6451e-04 | norm 0.2679 | dt 339.51ms | 1544245.98 tokens/sec
Step 3746 | loss: 3.335383 | lr:5.6449e-04 | norm 0.3007 | dt 338.86ms | 1547195.84 tokens/sec
Step 3747 | loss: 3.417670 | lr:5.6446e-04 | norm 0.2975 | dt 338.49ms | 1548920.98 tokens/sec
Step 3748 | loss: 3.413853 | lr:5.6444e-04 | norm 0.3021 | dt 339.76ms | 1543118.99 tokens/sec
Step 3749 | loss: 3.422442 | lr:5.6442e-04 | norm 0.2969 | dt 339.38ms | 1544860.01 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 3750: 3.4387
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2758/10042=0.2746




ddp_rank 5: ####### Printing generated samples ####### 

ddp_rank 7: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what's the focus of this book? What do I need to do to make this book interesting?
To answer
rank 7 sample 0 >Hello, I'm a language model, and I'm a computer program. I prefer to have a machine learning perspective, particularly since I didn't know the software
rank 5 sample 1 >Hello, I'm a language model, what will this look like to your classroom? -
The second chapter is important! The second chapter provides suggestions on how
rank 7 sample 1 >Hello, I'm a language model, thanks for sharing the results from two examples.
2) The idea of the class
This is something we've never
rank 5 sample 2 >Hello, I'm a language model, so let's take off. What kind of model is it? Can we write the whole thing? What is the role
rank 7 sample 2 >Hello, I'm a language model, how do I apply the concept of inter-language phonology to understand some of the most common phonological and vowel-
rank 5 sample 3 >Hello, I'm a language model, working in Python with a native language. I'm using it to generate a project code that will save the project and then


rank 7 sample 3 >Hello, I'm a language model, no one, but one to get good grades (though I should have seen it) and the following post:
First




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not saying the same thing. I can be confused, there are a few people who will be happy or
rank 3 sample 1 >Hello, I'm a language model, and a teacher, and just about everything else I wanted to know about it.
I'm looking at the grammar.
rank 3 sample 2 >Hello, I'm a language model, and it looks like its vocabulary doesn't come very well with this. I'll let you know on a deeper levels how
rank 3 sample 3 >Hello, I'm a language model, and its two models are often misused, or misinterpreted, to say that 'I was really lucky'.
What




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, using this framework's own programming language. I'm working on the following models, as well as using the "language model
rank 2 sample 1 >Hello, I'm a language model, in which i've been working on that course. Every time you look at the word, you'll find a different word
rank 2 sample 2 >Hello, I'm a language model, so I can show you the structure on a web page. We've made some adjustments so we decided to show you something
rank 2 sample 3 >Hello, I'm a language model, but if I'm going to be more concrete, so what if I'm still trying to go my way? I guess




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I think it's going to be easier for us to do it if we're really there. That's the point
rank 4 sample 1 >Hello, I'm a language model, we have to integrate with a system we want to know we're here because I wanted to understand. This is known as


ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 2 >Hello, I'm a language model, I'm already getting some suggestions. Here's a link to this post.
Language and Philosophy
Here's how to
rank 4 sample 3 >Hello, I'm a language model, and you thought a bit differently about different kinds of topics, we'll need a set of components we're going to be


rank 6 sample 0 >Hello, I'm a language model, just like the other three words in English, but that is what they are and are used to describe. I'm in
rank 6 sample 1 >Hello, I'm a language model, not a computer science geek, but a writer, writer, and blogger of the computer science world. I love the experience
rank 6 sample 2 >Hello, I'm a language model, but for now I'm a bit different. In fact, the more you've got to know about it, the better
rank 6 sample 3 >Hello, I'm a language model, and that's okay.
If you know a little bit about machine learning or machine-computing, and they may




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, writing and thinking about the problem. She has to think with herself, and ask questions and try something new.
I
rank 1 sample 1 >Hello, I'm a language model, a linguistics lab, and a computational linguistics lab."
And he also was working with C.S. Lewis
rank 1 sample 2 >Hello, I'm a language model, but once we know what it is, we can better understand language, how it works, and so many more languages to
rank 1 sample 3 >Hello, I'm a language model, and I'm glad I saw the data the way I made my thesis: my thesis? -- a thesis that I'm




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I can't remember which way my school does. I don't expect students to take calculus for the next year or
rank 0 sample 1 >Hello, I'm a language model, so i'm going to see that with the same properties here, it appears like a good candidate. However, a better
rank 0 sample 2 >Hello, I'm a language model, so I’m in the University of South Florida. You have to be a programmer to be able to do that
rank 0 sample 3 >Hello, I'm a language model, so a lot of the vocabulary and ideas that some of the material here are so simple and hard to implement. But then


Step 3750 | loss: 3.425782 | lr:5.6439e-04 | norm 0.2796 | dt 13503.71ms | 38825.47 tokens/sec
Step 3751 | loss: 3.417543 | lr:5.6437e-04 | norm 0.3303 | dt 334.75ms | 1566201.22 tokens/sec
Step 3752 | loss: 3.371987 | lr:5.6435e-04 | norm 0.3671 | dt 336.62ms | 1557510.97 tokens/sec
Step 3753 | loss: 3.353774 | lr:5.6433e-04 | norm 0.3072 | dt 337.02ms | 1555671.99 tokens/sec
Step 3754 | loss: 3.425619 | lr:5.6430e-04 | norm 0.2682 | dt 336.58ms | 1557694.11 tokens/sec
Step 3755 | loss: 3.270823 | lr:5.6428e-04 | norm 0.2912 | dt 335.65ms | 1561995.99 tokens/sec
Step 3756 | loss: 3.235387 | lr:5.6426e-04 | norm 0.2997 | dt 336.36ms | 1558722.06 tokens/sec
Step 3757 | loss: 3.366200 | lr:5.6423e-04 | norm 0.2791 | dt 336.55ms | 1557811.08 tokens/sec
Step 3758 | loss: 3.304326 | lr:5.6421e-04 | norm 0.3270 | dt 336.08ms | 1560023.56 tokens/sec
Step 3759 | loss: 3.250614 | lr:5.6419e-04 | norm 0.3117 | dt 337.20ms | 1554840.43 tokens/sec
Step 3760 | loss: 3.257144 | lr:5.6416e-04 | norm 0.2835 | dt 336.95ms | 1556001.12 tokens/sec
Step 3761 | loss: 3.293245 | lr:5.6414e-04 | norm 0.2543 | dt 336.24ms | 1559282.42 tokens/sec
Step 3762 | loss: 3.335952 | lr:5.6412e-04 | norm 0.2868 | dt 337.66ms | 1552718.26 tokens/sec
Step 3763 | loss: 3.303388 | lr:5.6410e-04 | norm 0.2959 | dt 337.03ms | 1555599.36 tokens/sec
Step 3764 | loss: 3.278691 | lr:5.6407e-04 | norm 0.2807 | dt 337.01ms | 1555716.01 tokens/sec
Step 3765 | loss: 3.262795 | lr:5.6405e-04 | norm 0.2659 | dt 337.31ms | 1554299.73 tokens/sec
Step 3766 | loss: 3.392492 | lr:5.6403e-04 | norm 0.2621 | dt 337.13ms | 1555170.31 tokens/sec
Step 3767 | loss: 3.450673 | lr:5.6400e-04 | norm 0.2900 | dt 339.62ms | 1543748.38 tokens/sec
Step 3768 | loss: 3.419165 | lr:5.6398e-04 | norm 0.2678 | dt 337.99ms | 1551187.05 tokens/sec
Step 3769 | loss: 3.415590 | lr:5.6396e-04 | norm 0.2646 | dt 337.96ms | 1551323.84 tokens/sec
Step 3770 | loss: 3.455904 | lr:5.6393e-04 | norm 0.2674 | dt 339.01ms | 1546514.69 tokens/sec
Step 3771 | loss: 3.401928 | lr:5.6391e-04 | norm 0.2610 | dt 338.01ms | 1551115.93 tokens/sec
Step 3772 | loss: 3.424360 | lr:5.6389e-04 | norm 0.2850 | dt 340.07ms | 1541716.89 tokens/sec
Step 3773 | loss: 3.461730 | lr:5.6387e-04 | norm 0.3010 | dt 339.08ms | 1546203.69 tokens/sec
Step 3774 | loss: 3.411940 | lr:5.6384e-04 | norm 0.2695 | dt 337.55ms | 1553204.10 tokens/sec
Step 3775 | loss: 3.383070 | lr:5.6382e-04 | norm 0.2564 | dt 338.06ms | 1550855.57 tokens/sec
Step 3776 | loss: 3.428401 | lr:5.6380e-04 | norm 0.2574 | dt 338.31ms | 1549742.95 tokens/sec
Step 3777 | loss: 3.441513 | lr:5.6377e-04 | norm 0.2727 | dt 337.70ms | 1552509.97 tokens/sec
Step 3778 | loss: 3.400747 | lr:5.6375e-04 | norm 0.2827 | dt 338.02ms | 1551054.66 tokens/sec
Step 3779 | loss: 3.443999 | lr:5.6373e-04 | norm 0.2909 | dt 893.44ms | 586816.82 tokens/sec
Step 3780 | loss: 3.421056 | lr:5.6370e-04 | norm 0.3232 | dt 341.49ms | 1535314.48 tokens/sec
Step 3781 | loss: 3.366065 | lr:5.6368e-04 | norm 0.2836 | dt 338.87ms | 1547152.30 tokens/sec
Step 3782 | loss: 3.410120 | lr:5.6366e-04 | norm 0.2696 | dt 338.75ms | 1547713.09 tokens/sec
Step 3783 | loss: 3.350285 | lr:5.6363e-04 | norm 0.2966 | dt 337.63ms | 1552845.44 tokens/sec
Step 3784 | loss: 3.375460 | lr:5.6361e-04 | norm 0.2990 | dt 337.92ms | 1551512.09 tokens/sec
Step 3785 | loss: 3.409779 | lr:5.6359e-04 | norm 0.2726 | dt 337.83ms | 1551924.89 tokens/sec
Step 3786 | loss: 3.408466 | lr:5.6356e-04 | norm 0.3056 | dt 337.64ms | 1552814.74 tokens/sec
Step 3787 | loss: 3.362023 | lr:5.6354e-04 | norm 0.2727 | dt 337.44ms | 1553739.64 tokens/sec
Step 3788 | loss: 3.376472 | lr:5.6352e-04 | norm 0.2800 | dt 339.31ms | 1545154.18 tokens/sec
Step 3789 | loss: 3.330097 | lr:5.6350e-04 | norm 0.3095 | dt 337.19ms | 1554853.62 tokens/sec
Step 3790 | loss: 3.384602 | lr:5.6347e-04 | norm 0.2637 | dt 336.94ms | 1556008.83 tokens/sec
Step 3791 | loss: 3.343014 | lr:5.6345e-04 | norm 0.2776 | dt 338.80ms | 1547476.74 tokens/sec
Step 3792 | loss: 3.375787 | lr:5.6343e-04 | norm 0.2924 | dt 337.78ms | 1552148.35 tokens/sec
Step 3793 | loss: 3.414094 | lr:5.6340e-04 | norm 0.2865 | dt 337.82ms | 1551956.66 tokens/sec
Step 3794 | loss: 3.418458 | lr:5.6338e-04 | norm 0.2583 | dt 337.85ms | 1551832.90 tokens/sec
Step 3795 | loss: 3.386521 | lr:5.6336e-04 | norm 0.2481 | dt 338.20ms | 1550221.47 tokens/sec
Step 3796 | loss: 3.364787 | lr:5.6333e-04 | norm 0.2724 | dt 338.46ms | 1549030.09 tokens/sec
Step 3797 | loss: 3.423076 | lr:5.6331e-04 | norm 0.2658 | dt 338.11ms | 1550655.45 tokens/sec
Step 3798 | loss: 3.428736 | lr:5.6329e-04 | norm 0.2654 | dt 338.09ms | 1550722.15 tokens/sec
Step 3799 | loss: 3.365891 | lr:5.6326e-04 | norm 0.2611 | dt 936.45ms | 559866.16 tokens/sec
Step 3800 | loss: 3.311956 | lr:5.6324e-04 | norm 0.2532 | dt 337.57ms | 1553115.24 tokens/sec
Step 3801 | loss: 3.264670 | lr:5.6322e-04 | norm 0.2555 | dt 338.09ms | 1550729.81 tokens/sec
Step 3802 | loss: 3.305222 | lr:5.6319e-04 | norm 0.2753 | dt 337.03ms | 1555594.96 tokens/sec
Step 3803 | loss: 3.269990 | lr:5.6317e-04 | norm 0.3238 | dt 338.73ms | 1547817.67 tokens/sec
Step 3804 | loss: 3.246580 | lr:5.6315e-04 | norm 0.2857 | dt 336.49ms | 1558119.04 tokens/sec
Step 3805 | loss: 3.273930 | lr:5.6312e-04 | norm 0.2813 | dt 337.91ms | 1551575.59 tokens/sec
Step 3806 | loss: 3.315497 | lr:5.6310e-04 | norm 0.3043 | dt 338.54ms | 1548689.72 tokens/sec
Step 3807 | loss: 3.286734 | lr:5.6308e-04 | norm 0.3085 | dt 338.07ms | 1550840.26 tokens/sec
Step 3808 | loss: 3.321089 | lr:5.6305e-04 | norm 0.3351 | dt 337.95ms | 1551396.07 tokens/sec
Step 3809 | loss: 3.239328 | lr:5.6303e-04 | norm 0.2606 | dt 338.54ms | 1548677.73 tokens/sec
Step 3810 | loss: 3.281983 | lr:5.6301e-04 | norm 0.2664 | dt 338.39ms | 1549356.42 tokens/sec
Step 3811 | loss: 3.260966 | lr:5.6298e-04 | norm 0.2901 | dt 337.15ms | 1555047.14 tokens/sec
Step 3812 | loss: 3.406156 | lr:5.6296e-04 | norm 0.3415 | dt 337.86ms | 1551779.24 tokens/sec
Step 3813 | loss: 3.406858 | lr:5.6294e-04 | norm 0.3279 | dt 337.83ms | 1551938.04 tokens/sec
Step 3814 | loss: 3.381467 | lr:5.6291e-04 | norm 0.3623 | dt 340.57ms | 1539462.22 tokens/sec
Step 3815 | loss: 3.423255 | lr:5.6289e-04 | norm 0.3398 | dt 337.54ms | 1553266.63 tokens/sec
Step 3816 | loss: 3.435048 | lr:5.6287e-04 | norm 0.3066 | dt 337.77ms | 1552192.18 tokens/sec
Step 3817 | loss: 3.468105 | lr:5.6284e-04 | norm 0.2846 | dt 338.15ms | 1550456.46 tokens/sec
Step 3818 | loss: 3.433481 | lr:5.6282e-04 | norm 0.2772 | dt 338.06ms | 1550858.86 tokens/sec
Step 3819 | loss: 3.418610 | lr:5.6280e-04 | norm 0.2742 | dt 337.70ms | 1552509.97 tokens/sec
Step 3820 | loss: 3.411187 | lr:5.6277e-04 | norm 0.2718 | dt 337.82ms | 1551956.66 tokens/sec
Step 3821 | loss: 3.369475 | lr:5.6275e-04 | norm 0.2557 | dt 338.32ms | 1549702.54 tokens/sec
Step 3822 | loss: 3.441301 | lr:5.6273e-04 | norm 0.2568 | dt 339.77ms | 1543080.01 tokens/sec
Step 3823 | loss: 3.546761 | lr:5.6270e-04 | norm 0.2997 | dt 338.93ms | 1546896.54 tokens/sec
Step 3824 | loss: 3.394831 | lr:5.6268e-04 | norm 0.3009 | dt 340.13ms | 1541448.87 tokens/sec
Step 3825 | loss: 3.446432 | lr:5.6266e-04 | norm 0.3320 | dt 339.38ms | 1544823.11 tokens/sec
Step 3826 | loss: 3.339811 | lr:5.6263e-04 | norm 0.3012 | dt 338.81ms | 1547453.87 tokens/sec
Step 3827 | loss: 3.412215 | lr:5.6261e-04 | norm 0.3605 | dt 339.58ms | 1543908.79 tokens/sec
Step 3828 | loss: 3.390086 | lr:5.6259e-04 | norm 0.3093 | dt 343.04ms | 1528347.63 tokens/sec
Step 3829 | loss: 3.429293 | lr:5.6256e-04 | norm 0.3348 | dt 338.97ms | 1546724.63 tokens/sec
Step 3830 | loss: 3.423635 | lr:5.6254e-04 | norm 0.3260 | dt 339.69ms | 1543438.49 tokens/sec
Step 3831 | loss: 3.396388 | lr:5.6251e-04 | norm 0.3081 | dt 339.36ms | 1544928.39 tokens/sec
Step 3832 | loss: 3.321245 | lr:5.6249e-04 | norm 0.2963 | dt 339.65ms | 1543610.76 tokens/sec
Step 3833 | loss: 3.406372 | lr:5.6247e-04 | norm 0.3009 | dt 339.31ms | 1545158.52 tokens/sec
Step 3834 | loss: 3.386204 | lr:5.6244e-04 | norm 0.2976 | dt 339.13ms | 1545992.80 tokens/sec
Step 3835 | loss: 3.320635 | lr:5.6242e-04 | norm 0.3131 | dt 339.17ms | 1545815.66 tokens/sec
Step 3836 | loss: 3.335237 | lr:5.6240e-04 | norm 0.2975 | dt 338.51ms | 1548797.71 tokens/sec
Step 3837 | loss: 3.355954 | lr:5.6237e-04 | norm 0.2961 | dt 339.43ms | 1544616.94 tokens/sec
Step 3838 | loss: 3.312232 | lr:5.6235e-04 | norm 0.2820 | dt 338.31ms | 1549748.41 tokens/sec
Step 3839 | loss: 3.411102 | lr:5.6233e-04 | norm 0.2782 | dt 338.93ms | 1546882.39 tokens/sec
Step 3840 | loss: 3.433887 | lr:5.6230e-04 | norm 0.2752 | dt 339.15ms | 1545904.77 tokens/sec
Step 3841 | loss: 3.416729 | lr:5.6228e-04 | norm 0.3051 | dt 338.64ms | 1548226.32 tokens/sec
Step 3842 | loss: 3.285216 | lr:5.6226e-04 | norm 0.3085 | dt 338.94ms | 1546823.63 tokens/sec
Step 3843 | loss: 3.402833 | lr:5.6223e-04 | norm 0.2893 | dt 338.16ms | 1550430.23 tokens/sec
Step 3844 | loss: 3.365620 | lr:5.6221e-04 | norm 0.3040 | dt 339.80ms | 1542924.10 tokens/sec
Step 3845 | loss: 3.399770 | lr:5.6219e-04 | norm 0.3126 | dt 339.45ms | 1544531.23 tokens/sec
Step 3846 | loss: 3.359883 | lr:5.6216e-04 | norm 0.2761 | dt 337.76ms | 1552265.59 tokens/sec
Step 3847 | loss: 3.285749 | lr:5.6214e-04 | norm 0.2849 | dt 339.01ms | 1546540.79 tokens/sec
Step 3848 | loss: 3.222272 | lr:5.6211e-04 | norm 0.2857 | dt 340.19ms | 1541145.31 tokens/sec
Step 3849 | loss: 3.340939 | lr:5.6209e-04 | norm 0.2840 | dt 339.56ms | 1544021.53 tokens/sec
Step 3850 | loss: 3.291071 | lr:5.6207e-04 | norm 0.2591 | dt 338.94ms | 1546842.13 tokens/sec
Step 3851 | loss: 3.306759 | lr:5.6204e-04 | norm 0.2663 | dt 339.27ms | 1545348.55 tokens/sec
Step 3852 | loss: 3.329142 | lr:5.6202e-04 | norm 0.2513 | dt 339.02ms | 1546475.53 tokens/sec
Step 3853 | loss: 3.355668 | lr:5.6200e-04 | norm 0.2959 | dt 338.97ms | 1546730.07 tokens/sec
Step 3854 | loss: 3.324239 | lr:5.6197e-04 | norm 0.2886 | dt 341.30ms | 1536144.60 tokens/sec
Step 3855 | loss: 3.312456 | lr:5.6195e-04 | norm 0.2967 | dt 341.66ms | 1534542.01 tokens/sec
Step 3856 | loss: 3.303604 | lr:5.6193e-04 | norm 0.2778 | dt 340.29ms | 1540720.95 tokens/sec
Step 3857 | loss: 3.294244 | lr:5.6190e-04 | norm 0.3061 | dt 339.67ms | 1543521.91 tokens/sec
Step 3858 | loss: 3.236047 | lr:5.6188e-04 | norm 0.3079 | dt 340.13ms | 1541454.28 tokens/sec
Step 3859 | loss: 3.366918 | lr:5.6185e-04 | norm 0.2940 | dt 338.43ms | 1549156.68 tokens/sec
Step 3860 | loss: 3.420969 | lr:5.6183e-04 | norm 0.3240 | dt 339.67ms | 1543501.33 tokens/sec
Step 3861 | loss: 3.426263 | lr:5.6181e-04 | norm 0.2919 | dt 338.95ms | 1546816.02 tokens/sec
Step 3862 | loss: 3.437178 | lr:5.6178e-04 | norm 0.2739 | dt 339.18ms | 1545750.47 tokens/sec
Step 3863 | loss: 3.341152 | lr:5.6176e-04 | norm 0.2729 | dt 339.63ms | 1543722.37 tokens/sec
Step 3864 | loss: 3.424261 | lr:5.6174e-04 | norm 0.2612 | dt 339.83ms | 1542792.04 tokens/sec
Step 3865 | loss: 3.449458 | lr:5.6171e-04 | norm 0.2386 | dt 338.21ms | 1550172.29 tokens/sec
Step 3866 | loss: 3.428916 | lr:5.6169e-04 | norm 0.2700 | dt 339.33ms | 1545081.44 tokens/sec
Step 3867 | loss: 3.344261 | lr:5.6166e-04 | norm 0.2580 | dt 339.56ms | 1544041.04 tokens/sec
Step 3868 | loss: 3.450256 | lr:5.6164e-04 | norm 0.2638 | dt 339.33ms | 1545089.04 tokens/sec
Step 3869 | loss: 3.411291 | lr:5.6162e-04 | norm 0.2552 | dt 339.76ms | 1543095.16 tokens/sec
Step 3870 | loss: 3.389165 | lr:5.6159e-04 | norm 0.2722 | dt 339.79ms | 1542961.99 tokens/sec
Step 3871 | loss: 3.317896 | lr:5.6157e-04 | norm 0.2815 | dt 339.04ms | 1546400.49 tokens/sec
Step 3872 | loss: 3.399878 | lr:5.6155e-04 | norm 0.2648 | dt 339.36ms | 1544928.39 tokens/sec
Step 3873 | loss: 3.445734 | lr:5.6152e-04 | norm 0.2983 | dt 338.73ms | 1547822.03 tokens/sec
Step 3874 | loss: 3.402722 | lr:5.6150e-04 | norm 0.2766 | dt 339.58ms | 1543952.15 tokens/sec
Step 3875 | loss: 3.338716 | lr:5.6147e-04 | norm 0.2659 | dt 339.19ms | 1545689.62 tokens/sec
Step 3876 | loss: 3.430925 | lr:5.6145e-04 | norm 0.2809 | dt 338.93ms | 1546913.95 tokens/sec
Step 3877 | loss: 3.366724 | lr:5.6143e-04 | norm 0.3186 | dt 338.72ms | 1547843.81 tokens/sec
Step 3878 | loss: 3.383126 | lr:5.6140e-04 | norm 0.3081 | dt 338.58ms | 1548503.24 tokens/sec
Step 3879 | loss: 3.387627 | lr:5.6138e-04 | norm 0.3073 | dt 338.35ms | 1549524.55 tokens/sec
Step 3880 | loss: 3.335946 | lr:5.6136e-04 | norm 0.3216 | dt 337.64ms | 1552800.49 tokens/sec
Step 3881 | loss: 3.419847 | lr:5.6133e-04 | norm 0.3288 | dt 337.58ms | 1553091.11 tokens/sec
Step 3882 | loss: 3.412850 | lr:5.6131e-04 | norm 0.3242 | dt 338.46ms | 1549054.10 tokens/sec
Step 3883 | loss: 3.355373 | lr:5.6128e-04 | norm 0.2637 | dt 339.43ms | 1544623.45 tokens/sec
Step 3884 | loss: 3.344315 | lr:5.6126e-04 | norm 0.2795 | dt 340.06ms | 1541760.12 tokens/sec
Step 3885 | loss: 3.322303 | lr:5.6124e-04 | norm 0.2629 | dt 337.98ms | 1551260.36 tokens/sec
Step 3886 | loss: 3.390340 | lr:5.6121e-04 | norm 0.2475 | dt 337.34ms | 1554204.16 tokens/sec
Step 3887 | loss: 3.385388 | lr:5.6119e-04 | norm 0.2743 | dt 338.99ms | 1546608.23 tokens/sec
Step 3888 | loss: 3.392304 | lr:5.6116e-04 | norm 0.2625 | dt 337.90ms | 1551601.86 tokens/sec
Step 3889 | loss: 3.467488 | lr:5.6114e-04 | norm 0.2943 | dt 337.45ms | 1553697.93 tokens/sec
Step 3890 | loss: 3.418962 | lr:5.6112e-04 | norm 0.2770 | dt 338.98ms | 1546643.04 tokens/sec
Step 3891 | loss: 3.361191 | lr:5.6109e-04 | norm 0.2668 | dt 337.91ms | 1551547.12 tokens/sec
Step 3892 | loss: 3.394671 | lr:5.6107e-04 | norm 0.2594 | dt 339.68ms | 1543495.91 tokens/sec
Step 3893 | loss: 3.469844 | lr:5.6105e-04 | norm 0.2747 | dt 338.11ms | 1550648.89 tokens/sec
Step 3894 | loss: 3.389845 | lr:5.6102e-04 | norm 0.2864 | dt 338.81ms | 1547420.12 tokens/sec
Step 3895 | loss: 3.294942 | lr:5.6100e-04 | norm 0.2651 | dt 338.09ms | 1550727.62 tokens/sec
Step 3896 | loss: 3.246840 | lr:5.6097e-04 | norm 0.2930 | dt 337.46ms | 1553638.65 tokens/sec
Step 3897 | loss: 3.303576 | lr:5.6095e-04 | norm 0.3338 | dt 338.68ms | 1548029.05 tokens/sec
Step 3898 | loss: 3.287677 | lr:5.6093e-04 | norm 0.3398 | dt 338.46ms | 1549020.27 tokens/sec
Step 3899 | loss: 3.271638 | lr:5.6090e-04 | norm 0.2954 | dt 337.55ms | 1553198.61 tokens/sec
Step 3900 | loss: 3.271363 | lr:5.6088e-04 | norm 0.2931 | dt 338.70ms | 1547942.97 tokens/sec
Step 3901 | loss: 3.258961 | lr:5.6085e-04 | norm 0.2936 | dt 339.43ms | 1544593.07 tokens/sec
Step 3902 | loss: 3.252592 | lr:5.6083e-04 | norm 0.2863 | dt 339.61ms | 1543789.56 tokens/sec
Step 3903 | loss: 3.268996 | lr:5.6081e-04 | norm 0.3485 | dt 338.69ms | 1547980.01 tokens/sec
Step 3904 | loss: 3.288975 | lr:5.6078e-04 | norm 0.3540 | dt 339.64ms | 1543657.35 tokens/sec
Step 3905 | loss: 3.262026 | lr:5.6076e-04 | norm 0.2932 | dt 339.50ms | 1544282.85 tokens/sec
Step 3906 | loss: 3.275154 | lr:5.6073e-04 | norm 0.3206 | dt 338.76ms | 1547688.03 tokens/sec
Step 3907 | loss: 3.424029 | lr:5.6071e-04 | norm 0.2770 | dt 339.27ms | 1545323.57 tokens/sec
Step 3908 | loss: 3.406806 | lr:5.6069e-04 | norm 0.2958 | dt 339.03ms | 1546448.34 tokens/sec
Step 3909 | loss: 3.395047 | lr:5.6066e-04 | norm 0.2790 | dt 339.08ms | 1546210.21 tokens/sec
Step 3910 | loss: 3.399944 | lr:5.6064e-04 | norm 0.3046 | dt 338.76ms | 1547690.21 tokens/sec
Step 3911 | loss: 3.388034 | lr:5.6061e-04 | norm 0.2915 | dt 339.04ms | 1546368.96 tokens/sec
Step 3912 | loss: 3.394351 | lr:5.6059e-04 | norm 0.3064 | dt 341.08ms | 1537151.82 tokens/sec
Step 3913 | loss: 3.420633 | lr:5.6057e-04 | norm 0.3250 | dt 342.89ms | 1529040.52 tokens/sec
Step 3914 | loss: 3.405192 | lr:5.6054e-04 | norm 0.2835 | dt 339.39ms | 1544780.79 tokens/sec
Step 3915 | loss: 3.401139 | lr:5.6052e-04 | norm 0.2834 | dt 338.72ms | 1547855.80 tokens/sec
Step 3916 | loss: 3.389248 | lr:5.6049e-04 | norm 0.2922 | dt 338.25ms | 1550023.69 tokens/sec
Step 3917 | loss: 3.380206 | lr:5.6047e-04 | norm 0.2919 | dt 337.67ms | 1552650.28 tokens/sec
Step 3918 | loss: 3.454772 | lr:5.6045e-04 | norm 0.2667 | dt 337.95ms | 1551384.03 tokens/sec
Step 3919 | loss: 3.452140 | lr:5.6042e-04 | norm 0.2826 | dt 339.40ms | 1544751.49 tokens/sec
Step 3920 | loss: 3.403739 | lr:5.6040e-04 | norm 0.2763 | dt 338.87ms | 1547150.12 tokens/sec
Step 3921 | loss: 3.390873 | lr:5.6037e-04 | norm 0.2805 | dt 337.96ms | 1551327.12 tokens/sec
Step 3922 | loss: 3.338600 | lr:5.6035e-04 | norm 0.2882 | dt 338.95ms | 1546805.14 tokens/sec
Step 3923 | loss: 3.368358 | lr:5.6033e-04 | norm 0.2774 | dt 338.26ms | 1549971.25 tokens/sec
Step 3924 | loss: 3.362869 | lr:5.6030e-04 | norm 0.2552 | dt 338.51ms | 1548826.07 tokens/sec
Step 3925 | loss: 3.336420 | lr:5.6028e-04 | norm 0.2525 | dt 337.94ms | 1551400.45 tokens/sec
Step 3926 | loss: 3.422554 | lr:5.6025e-04 | norm 0.2695 | dt 339.51ms | 1544267.66 tokens/sec
Step 3927 | loss: 3.402649 | lr:5.6023e-04 | norm 0.2698 | dt 339.69ms | 1543440.66 tokens/sec
Step 3928 | loss: 3.373884 | lr:5.6020e-04 | norm 0.2943 | dt 338.53ms | 1548699.54 tokens/sec
Step 3929 | loss: 3.309354 | lr:5.6018e-04 | norm 0.2857 | dt 338.01ms | 1551121.40 tokens/sec
Step 3930 | loss: 3.331032 | lr:5.6016e-04 | norm 0.3003 | dt 338.21ms | 1550167.92 tokens/sec
Step 3931 | loss: 3.317022 | lr:5.6013e-04 | norm 0.3145 | dt 338.86ms | 1547190.39 tokens/sec
Step 3932 | loss: 3.309869 | lr:5.6011e-04 | norm 0.2967 | dt 337.69ms | 1552553.82 tokens/sec
Step 3933 | loss: 3.360099 | lr:5.6008e-04 | norm 0.2564 | dt 337.98ms | 1551236.29 tokens/sec
Step 3934 | loss: 3.406189 | lr:5.6006e-04 | norm 0.2640 | dt 338.31ms | 1549726.57 tokens/sec
Step 3935 | loss: 3.412832 | lr:5.6004e-04 | norm 0.2714 | dt 337.81ms | 1552035.52 tokens/sec
Step 3936 | loss: 3.365489 | lr:5.6001e-04 | norm 0.2910 | dt 337.98ms | 1551224.25 tokens/sec
Step 3937 | loss: 3.402879 | lr:5.5999e-04 | norm 0.3117 | dt 338.28ms | 1549870.74 tokens/sec
Step 3938 | loss: 3.421524 | lr:5.5996e-04 | norm 0.3100 | dt 337.97ms | 1551304.14 tokens/sec
Step 3939 | loss: 3.375988 | lr:5.5994e-04 | norm 0.2894 | dt 337.72ms | 1552438.73 tokens/sec
Step 3940 | loss: 3.377821 | lr:5.5991e-04 | norm 0.2773 | dt 338.61ms | 1548338.60 tokens/sec
Step 3941 | loss: 3.446826 | lr:5.5989e-04 | norm 0.2782 | dt 338.45ms | 1549091.20 tokens/sec
Step 3942 | loss: 3.302577 | lr:5.5987e-04 | norm 0.2643 | dt 339.90ms | 1542484.70 tokens/sec
Step 3943 | loss: 3.282525 | lr:5.5984e-04 | norm 0.2805 | dt 337.64ms | 1552803.78 tokens/sec
Step 3944 | loss: 3.287416 | lr:5.5982e-04 | norm 0.2916 | dt 338.26ms | 1549943.93 tokens/sec
Step 3945 | loss: 3.209994 | lr:5.5979e-04 | norm 0.2858 | dt 339.75ms | 1543178.55 tokens/sec
Step 3946 | loss: 3.203490 | lr:5.5977e-04 | norm 0.2876 | dt 338.96ms | 1546765.97 tokens/sec
Step 3947 | loss: 3.285806 | lr:5.5974e-04 | norm 0.2637 | dt 339.24ms | 1545484.31 tokens/sec
Step 3948 | loss: 3.225504 | lr:5.5972e-04 | norm 0.2514 | dt 338.57ms | 1548519.60 tokens/sec
Step 3949 | loss: 3.254341 | lr:5.5970e-04 | norm 0.2700 | dt 338.29ms | 1549798.65 tokens/sec
Step 3950 | loss: 3.315995 | lr:5.5967e-04 | norm 0.2689 | dt 338.40ms | 1549292.02 tokens/sec
Step 3951 | loss: 3.354636 | lr:5.5965e-04 | norm 0.2903 | dt 338.36ms | 1549516.90 tokens/sec
Step 3952 | loss: 3.246744 | lr:5.5962e-04 | norm 0.3097 | dt 338.10ms | 1550704.65 tokens/sec
Step 3953 | loss: 3.353154 | lr:5.5960e-04 | norm 0.3143 | dt 338.58ms | 1548505.42 tokens/sec
Step 3954 | loss: 3.355331 | lr:5.5957e-04 | norm 0.3061 | dt 338.78ms | 1547593.27 tokens/sec
Step 3955 | loss: 3.414351 | lr:5.5955e-04 | norm 0.3014 | dt 339.58ms | 1543927.22 tokens/sec
Step 3956 | loss: 3.428045 | lr:5.5953e-04 | norm 0.3522 | dt 339.46ms | 1544462.89 tokens/sec
Step 3957 | loss: 3.408042 | lr:5.5950e-04 | norm 0.3040 | dt 338.81ms | 1547450.61 tokens/sec
Step 3958 | loss: 3.409003 | lr:5.5948e-04 | norm 0.2977 | dt 338.32ms | 1549695.99 tokens/sec
Step 3959 | loss: 3.439018 | lr:5.5945e-04 | norm 0.3043 | dt 339.19ms | 1545689.62 tokens/sec
Step 3960 | loss: 3.392120 | lr:5.5943e-04 | norm 0.3016 | dt 338.72ms | 1547859.07 tokens/sec
Step 3961 | loss: 3.368116 | lr:5.5940e-04 | norm 0.2834 | dt 339.23ms | 1545516.89 tokens/sec
Step 3962 | loss: 3.411936 | lr:5.5938e-04 | norm 0.3164 | dt 339.28ms | 1545315.97 tokens/sec
Step 3963 | loss: 3.334424 | lr:5.5936e-04 | norm 0.3133 | dt 338.16ms | 1550409.46 tokens/sec
Step 3964 | loss: 3.410136 | lr:5.5933e-04 | norm 0.3061 | dt 339.13ms | 1545990.63 tokens/sec
Step 3965 | loss: 3.437360 | lr:5.5931e-04 | norm 0.3185 | dt 338.86ms | 1547219.79 tokens/sec
Step 3966 | loss: 3.433706 | lr:5.5928e-04 | norm 0.2761 | dt 338.58ms | 1548504.33 tokens/sec
Step 3967 | loss: 3.437822 | lr:5.5926e-04 | norm 0.2757 | dt 339.64ms | 1543676.86 tokens/sec
Step 3968 | loss: 3.369268 | lr:5.5923e-04 | norm 0.2858 | dt 1021.41ms | 513297.93 tokens/sec
Step 3969 | loss: 3.312361 | lr:5.5921e-04 | norm 0.3223 | dt 337.07ms | 1555432.11 tokens/sec
Step 3970 | loss: 3.396892 | lr:5.5918e-04 | norm 0.2969 | dt 340.01ms | 1541962.29 tokens/sec
Step 3971 | loss: 3.440138 | lr:5.5916e-04 | norm 0.2977 | dt 340.64ms | 1539114.19 tokens/sec
Step 3972 | loss: 3.381828 | lr:5.5914e-04 | norm 0.2730 | dt 339.15ms | 1545896.08 tokens/sec
Step 3973 | loss: 3.377197 | lr:5.5911e-04 | norm 0.3018 | dt 338.77ms | 1547633.57 tokens/sec
Step 3974 | loss: 3.387260 | lr:5.5909e-04 | norm 0.3125 | dt 338.59ms | 1548462.89 tokens/sec
Step 3975 | loss: 3.334144 | lr:5.5906e-04 | norm 0.2876 | dt 337.73ms | 1552408.05 tokens/sec
Step 3976 | loss: 3.313506 | lr:5.5904e-04 | norm 0.2645 | dt 337.84ms | 1551893.13 tokens/sec
Step 3977 | loss: 3.349221 | lr:5.5901e-04 | norm 0.2706 | dt 337.61ms | 1552925.50 tokens/sec
Step 3978 | loss: 3.416463 | lr:5.5899e-04 | norm 0.2783 | dt 338.50ms | 1548839.16 tokens/sec
Step 3979 | loss: 3.438473 | lr:5.5896e-04 | norm 0.2702 | dt 338.26ms | 1549959.23 tokens/sec
Step 3980 | loss: 3.369753 | lr:5.5894e-04 | norm 0.2963 | dt 338.23ms | 1550075.04 tokens/sec
Step 3981 | loss: 3.353857 | lr:5.5892e-04 | norm 0.2959 | dt 337.90ms | 1551629.23 tokens/sec
Step 3982 | loss: 3.419843 | lr:5.5889e-04 | norm 0.2763 | dt 338.37ms | 1549461.22 tokens/sec
Step 3983 | loss: 3.392970 | lr:5.5887e-04 | norm 0.2550 | dt 338.26ms | 1549949.40 tokens/sec
Step 3984 | loss: 3.377287 | lr:5.5884e-04 | norm 0.2948 | dt 338.81ms | 1547429.92 tokens/sec
Step 3985 | loss: 3.370645 | lr:5.5882e-04 | norm 0.2934 | dt 338.40ms | 1549330.22 tokens/sec
Step 3986 | loss: 3.347798 | lr:5.5879e-04 | norm 0.2563 | dt 337.96ms | 1551328.21 tokens/sec
Step 3987 | loss: 3.381321 | lr:5.5877e-04 | norm 0.2478 | dt 341.00ms | 1537488.21 tokens/sec
Step 3988 | loss: 3.342400 | lr:5.5874e-04 | norm 0.2759 | dt 338.18ms | 1550324.20 tokens/sec
Step 3989 | loss: 3.302668 | lr:5.5872e-04 | norm 0.2950 | dt 913.74ms | 573784.17 tokens/sec
Step 3990 | loss: 3.251878 | lr:5.5869e-04 | norm 0.2768 | dt 336.75ms | 1556913.29 tokens/sec
Step 3991 | loss: 3.259887 | lr:5.5867e-04 | norm 0.2505 | dt 338.17ms | 1550378.85 tokens/sec
Step 3992 | loss: 3.335767 | lr:5.5865e-04 | norm 0.2764 | dt 337.01ms | 1555718.22 tokens/sec
Step 3993 | loss: 3.267631 | lr:5.5862e-04 | norm 0.2900 | dt 339.05ms | 1546324.38 tokens/sec
Step 3994 | loss: 3.236090 | lr:5.5860e-04 | norm 0.2839 | dt 338.72ms | 1547850.35 tokens/sec
Step 3995 | loss: 3.240622 | lr:5.5857e-04 | norm 0.2729 | dt 338.32ms | 1549658.86 tokens/sec
Step 3996 | loss: 3.284120 | lr:5.5855e-04 | norm 0.2510 | dt 337.93ms | 1551469.40 tokens/sec
Step 3997 | loss: 3.282623 | lr:5.5852e-04 | norm 0.2850 | dt 337.65ms | 1552773.08 tokens/sec
Step 3998 | loss: 3.251487 | lr:5.5850e-04 | norm 0.2881 | dt 337.49ms | 1553499.26 tokens/sec
Step 3999 | loss: 3.284093 | lr:5.5847e-04 | norm 0.2676 | dt 339.41ms | 1544682.04 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 4000: 3.4228
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2702/10042=0.2691


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but if it's in a language model, it's a language model, and if it's in a language model,
rank 5 sample 1 >Hello, I'm a language model, one that does some type of calculation that can explain the result in both a rational and rational form. That does make sense
rank 5 sample 2 >Hello, I'm a language model, so I use the standard language from C++, but I'm doing the most basic math.
So, I am
rank 5 sample 3 >Hello, I'm a language model, for that reason my question "what is a language modeling?".
Here's a table to show both of them:





ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'd like to be able to try out a language model that matches the style of the source, so I can


ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, we can understand the context and language levels of the content domain. But I'm assuming you're the real world. The
rank 7 sample 2 >Hello, I'm a language model, and I've spent ten years modeling/visualizing everything, so I started to use the language model to help them to
rank 2 sample 0 >Hello, I'm a language model, please do not speak to you, because you are not going to be making any modifications to your code. For example,
rank 7 sample 3 >Hello, I'm a language model, so you can see a bit of "real" in the original language.
In the language model, you can add


rank 2 sample 1 >Hello, I'm a language model, so I should use it to model our grammar. Of course it has not been taught in my classes since we were born
rank 2 sample 2 >Hello, I'm a language model, so I can work on a little while of this course. And then another time I have a child who was 3/
rank 2 sample 3 >Hello, I'm a language model, but there are a couple of things I just figured out: The first thing I noticed, just going through it and figuring




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I am going to make a short movie, but I don't want to tell that if you like the example above
rank 4 sample 1 >Hello, I'm a language model, working from a pre-built program to an intelligent AI development platform. That way, AI can be deployed and brought into
rank 4 sample 2 >Hello, I'm a language model, I had to develop some good language learning models, and I have just seen the students that I had made a difference in
rank 4 sample 3 >Hello, I'm a language model, and it may just be one of the best ones too, when I leave the code to the next generation. My kids






ddp_rank 1: ####### Printing generated samples ####### 

ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm going to take my model a step back. (You take more than 2 MB.) We'll talk and
rank 1 sample 0 >Hello, I'm a language model, and the word "fantastic", and so forth. When I'm about to go, there's a lot of
rank 1 sample 1 >Hello, I'm a language model, which I'll start with. So you may have to make a change to that. My focus is to make sure thatrank 3 sample 1 >Hello, I'm a language model, and the reason I'm very intrigued with it is that the language is very easy to learn, and it's just as

rank 1 sample 2 >Hello, I'm a language model, but by now I'm going to teach you how to write program in this workshop.
In a previous post (werank 3 sample 2 >Hello, I'm a language model, and you have to define it like this:
whereas = "hello,"("hello"), is a constant number;

rank 1 sample 3 >Hello, I'm a language model, and I'm glad I found a post telling me some real principles to teach people how to define the language.
I


rank 3 sample 3 >Hello, I'm a language model, and hope you have a way to share it, but there are plenty of other skills. In this first post, we




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, because that's where all the difference between a language model and an ML-to-ML model are. I'm here
rank 6 sample 1 >Hello, I'm a language model, and I can't find a solution. Just take one of the above expressions and go back and reread.
Step
rank 6 sample 2 >Hello, I'm a language model, but this one is going to change to this next.
In fact, a language model is going to change and I
rank 6 sample 3 >Hello, I'm a language model, and so this is just the kind of thing that I'm going to talk about here in any language format (see it




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I love it! My job is to talk to kids and teachers about my work with "real-world" people
rank 0 sample 1 >Hello, I'm a language model, so please post your comment in my friend's comments with her on Facebook, in case she needs a special language model for
rank 0 sample 2 >Hello, I'm a language model, so I was writing a lesson on "The Difference Between a " and "
What's the difference between a " and
rank 0 sample 3 >Hello, I'm a language model, so in this tutorial I'll talk about a topic about the topics discussed in class that I don't know about in real


Step 4000 | loss: 3.411520 | lr:5.5845e-04 | norm 0.3131 | dt 19033.36ms | 27545.74 tokens/sec
Step 4001 | loss: 3.415515 | lr:5.5842e-04 | norm 0.3056 | dt 334.15ms | 1569032.98 tokens/sec
Step 4002 | loss: 3.432699 | lr:5.5840e-04 | norm 0.3339 | dt 335.99ms | 1560447.55 tokens/sec
Step 4003 | loss: 3.376298 | lr:5.5837e-04 | norm 0.3123 | dt 337.00ms | 1555729.22 tokens/sec
Step 4004 | loss: 3.419628 | lr:5.5835e-04 | norm 0.3061 | dt 335.98ms | 1560455.30 tokens/sec
Step 4005 | loss: 3.434711 | lr:5.5833e-04 | norm 0.3270 | dt 335.99ms | 1560420.97 tokens/sec
Step 4006 | loss: 3.362772 | lr:5.5830e-04 | norm 0.2671 | dt 337.17ms | 1554973.46 tokens/sec
Step 4007 | loss: 3.397521 | lr:5.5828e-04 | norm 0.3028 | dt 337.23ms | 1554689.83 tokens/sec
Step 4008 | loss: 3.458655 | lr:5.5825e-04 | norm 0.2576 | dt 336.30ms | 1559002.74 tokens/sec
Step 4009 | loss: 3.411262 | lr:5.5823e-04 | norm 0.2764 | dt 337.16ms | 1555031.74 tokens/sec
Step 4010 | loss: 3.425364 | lr:5.5820e-04 | norm 0.2946 | dt 337.12ms | 1555174.71 tokens/sec
Step 4011 | loss: 3.395301 | lr:5.5818e-04 | norm 0.2857 | dt 337.68ms | 1552638.23 tokens/sec
Step 4012 | loss: 3.480446 | lr:5.5815e-04 | norm 0.3064 | dt 337.16ms | 1554992.15 tokens/sec
Step 4013 | loss: 3.387846 | lr:5.5813e-04 | norm 0.2989 | dt 336.93ms | 1556089.21 tokens/sec
Step 4014 | loss: 3.416215 | lr:5.5810e-04 | norm 0.3087 | dt 337.62ms | 1552902.47 tokens/sec
Step 4015 | loss: 3.361457 | lr:5.5808e-04 | norm 0.3056 | dt 337.45ms | 1553677.07 tokens/sec
Step 4016 | loss: 3.312961 | lr:5.5805e-04 | norm 0.2674 | dt 336.63ms | 1557483.39 tokens/sec
Step 4017 | loss: 3.330654 | lr:5.5803e-04 | norm 0.2452 | dt 338.85ms | 1547263.33 tokens/sec
Step 4018 | loss: 3.307391 | lr:5.5800e-04 | norm 0.2503 | dt 337.95ms | 1551399.35 tokens/sec
Step 4019 | loss: 3.371768 | lr:5.5798e-04 | norm 0.2669 | dt 336.23ms | 1559308.96 tokens/sec
Step 4020 | loss: 3.408484 | lr:5.5795e-04 | norm 0.2647 | dt 336.29ms | 1559042.53 tokens/sec
Step 4021 | loss: 3.345855 | lr:5.5793e-04 | norm 0.3037 | dt 336.80ms | 1556688.46 tokens/sec
Step 4022 | loss: 3.407408 | lr:5.5791e-04 | norm 0.3059 | dt 336.37ms | 1558680.07 tokens/sec
Step 4023 | loss: 3.343707 | lr:5.5788e-04 | norm 0.2989 | dt 338.12ms | 1550576.72 tokens/sec
Step 4024 | loss: 3.359583 | lr:5.5786e-04 | norm 0.2977 | dt 338.60ms | 1548406.20 tokens/sec
Step 4025 | loss: 3.359834 | lr:5.5783e-04 | norm 0.2703 | dt 336.68ms | 1557210.97 tokens/sec
Step 4026 | loss: 3.371957 | lr:5.5781e-04 | norm 0.2543 | dt 337.75ms | 1552285.31 tokens/sec
Step 4027 | loss: 3.347121 | lr:5.5778e-04 | norm 0.2897 | dt 337.88ms | 1551711.35 tokens/sec
Step 4028 | loss: 3.342102 | lr:5.5776e-04 | norm 0.3096 | dt 337.98ms | 1551253.80 tokens/sec
Step 4029 | loss: 3.404487 | lr:5.5773e-04 | norm 0.2842 | dt 337.61ms | 1552922.21 tokens/sec
Step 4030 | loss: 3.389306 | lr:5.5771e-04 | norm 0.2523 | dt 337.57ms | 1553137.18 tokens/sec
Step 4031 | loss: 3.400435 | lr:5.5768e-04 | norm 0.2925 | dt 337.87ms | 1551759.53 tokens/sec
Step 4032 | loss: 3.427026 | lr:5.5766e-04 | norm 0.2527 | dt 338.42ms | 1549237.44 tokens/sec
Step 4033 | loss: 3.376562 | lr:5.5763e-04 | norm 0.2697 | dt 338.57ms | 1548525.05 tokens/sec
Step 4034 | loss: 3.378508 | lr:5.5761e-04 | norm 0.2700 | dt 338.54ms | 1548682.09 tokens/sec
Step 4035 | loss: 3.282050 | lr:5.5758e-04 | norm 0.2812 | dt 338.50ms | 1548853.34 tokens/sec
Step 4036 | loss: 3.283401 | lr:5.5756e-04 | norm 0.2740 | dt 337.81ms | 1552039.90 tokens/sec
Step 4037 | loss: 3.299379 | lr:5.5753e-04 | norm 0.2688 | dt 336.86ms | 1556403.09 tokens/sec
Step 4038 | loss: 3.181100 | lr:5.5751e-04 | norm 0.3196 | dt 338.09ms | 1550739.65 tokens/sec
Step 4039 | loss: 3.282027 | lr:5.5748e-04 | norm 0.3058 | dt 337.85ms | 1551825.23 tokens/sec
Step 4040 | loss: 3.235871 | lr:5.5746e-04 | norm 0.2985 | dt 337.60ms | 1552974.85 tokens/sec
Step 4041 | loss: 3.231117 | lr:5.5743e-04 | norm 0.2994 | dt 336.86ms | 1556384.37 tokens/sec
Step 4042 | loss: 3.320478 | lr:5.5741e-04 | norm 0.2857 | dt 338.36ms | 1549476.51 tokens/sec
Step 4043 | loss: 3.196931 | lr:5.5738e-04 | norm 0.2510 | dt 337.89ms | 1551668.65 tokens/sec
Step 4044 | loss: 3.306118 | lr:5.5736e-04 | norm 0.2914 | dt 337.48ms | 1553516.82 tokens/sec
Step 4045 | loss: 3.285302 | lr:5.5733e-04 | norm 0.2915 | dt 339.18ms | 1545771.11 tokens/sec
Step 4046 | loss: 3.271076 | lr:5.5731e-04 | norm 0.2513 | dt 338.54ms | 1548678.82 tokens/sec
Step 4047 | loss: 3.433598 | lr:5.5728e-04 | norm 0.3186 | dt 338.05ms | 1550916.83 tokens/sec
Step 4048 | loss: 3.438312 | lr:5.5726e-04 | norm 0.3274 | dt 340.14ms | 1541407.82 tokens/sec
Step 4049 | loss: 3.420414 | lr:5.5723e-04 | norm 0.3303 | dt 337.44ms | 1553728.67 tokens/sec
Step 4050 | loss: 3.431950 | lr:5.5721e-04 | norm 0.2804 | dt 337.50ms | 1553468.53 tokens/sec
Step 4051 | loss: 3.471601 | lr:5.5718e-04 | norm 0.2997 | dt 338.93ms | 1546899.80 tokens/sec
Step 4052 | loss: 3.417653 | lr:5.5716e-04 | norm 0.3178 | dt 337.82ms | 1551966.51 tokens/sec
Step 4053 | loss: 3.419430 | lr:5.5713e-04 | norm 0.3016 | dt 338.06ms | 1550854.48 tokens/sec
Step 4054 | loss: 3.378801 | lr:5.5711e-04 | norm 0.3000 | dt 338.84ms | 1547302.53 tokens/sec
Step 4055 | loss: 3.388940 | lr:5.5708e-04 | norm 0.2634 | dt 338.28ms | 1549849.99 tokens/sec
Step 4056 | loss: 3.427457 | lr:5.5706e-04 | norm 0.2975 | dt 339.37ms | 1544898.00 tokens/sec
Step 4057 | loss: 3.481589 | lr:5.5703e-04 | norm 0.3064 | dt 338.37ms | 1549469.96 tokens/sec
Step 4058 | loss: 3.380006 | lr:5.5701e-04 | norm 0.2753 | dt 338.21ms | 1550181.03 tokens/sec
Step 4059 | loss: 3.321870 | lr:5.5698e-04 | norm 0.2689 | dt 338.96ms | 1546756.18 tokens/sec
Step 4060 | loss: 3.347823 | lr:5.5696e-04 | norm 0.2686 | dt 339.13ms | 1545985.20 tokens/sec
Step 4061 | loss: 3.347956 | lr:5.5693e-04 | norm 0.2492 | dt 341.31ms | 1536088.80 tokens/sec
Step 4062 | loss: 3.386618 | lr:5.5691e-04 | norm 0.2884 | dt 339.08ms | 1546188.47 tokens/sec
Step 4063 | loss: 3.398093 | lr:5.5688e-04 | norm 0.2618 | dt 339.37ms | 1544867.61 tokens/sec
Step 4064 | loss: 3.374238 | lr:5.5686e-04 | norm 0.2697 | dt 338.73ms | 1547819.85 tokens/sec
Step 4065 | loss: 3.394577 | lr:5.5683e-04 | norm 0.2757 | dt 339.20ms | 1545649.42 tokens/sec
Step 4066 | loss: 3.345932 | lr:5.5681e-04 | norm 0.2652 | dt 339.90ms | 1542460.89 tokens/sec
Step 4067 | loss: 3.396856 | lr:5.5678e-04 | norm 0.2816 | dt 339.12ms | 1546028.67 tokens/sec
Step 4068 | loss: 3.281918 | lr:5.5676e-04 | norm 0.2970 | dt 338.78ms | 1547556.24 tokens/sec
Step 4069 | loss: 3.381850 | lr:5.5673e-04 | norm 0.2827 | dt 339.06ms | 1546301.54 tokens/sec
Step 4070 | loss: 3.363165 | lr:5.5671e-04 | norm 0.2808 | dt 338.50ms | 1548867.53 tokens/sec
Step 4071 | loss: 3.298548 | lr:5.5668e-04 | norm 0.3034 | dt 339.57ms | 1543967.33 tokens/sec
Step 4072 | loss: 3.323320 | lr:5.5666e-04 | norm 0.2936 | dt 338.77ms | 1547601.99 tokens/sec
Step 4073 | loss: 3.403425 | lr:5.5663e-04 | norm 0.2948 | dt 338.84ms | 1547280.75 tokens/sec
Step 4074 | loss: 3.358330 | lr:5.5661e-04 | norm 0.2924 | dt 339.37ms | 1544866.52 tokens/sec
Step 4075 | loss: 3.388048 | lr:5.5658e-04 | norm 0.2404 | dt 338.81ms | 1547457.14 tokens/sec
Step 4076 | loss: 3.402594 | lr:5.5656e-04 | norm 0.2918 | dt 339.53ms | 1544138.62 tokens/sec
Step 4077 | loss: 3.348745 | lr:5.5653e-04 | norm 0.2646 | dt 340.07ms | 1541692.03 tokens/sec
Step 4078 | loss: 3.397866 | lr:5.5651e-04 | norm 0.2805 | dt 339.52ms | 1544202.60 tokens/sec
Step 4079 | loss: 3.345372 | lr:5.5648e-04 | norm 0.2827 | dt 338.68ms | 1548035.59 tokens/sec
Step 4080 | loss: 3.406803 | lr:5.5646e-04 | norm 0.2961 | dt 339.45ms | 1544525.81 tokens/sec
Step 4081 | loss: 3.360103 | lr:5.5643e-04 | norm 0.2781 | dt 339.14ms | 1545918.90 tokens/sec
Step 4082 | loss: 3.293583 | lr:5.5641e-04 | norm 0.2730 | dt 339.79ms | 1542982.56 tokens/sec
Step 4083 | loss: 3.248989 | lr:5.5638e-04 | norm 0.2643 | dt 337.28ms | 1554440.36 tokens/sec
Step 4084 | loss: 3.234924 | lr:5.5636e-04 | norm 0.2536 | dt 337.79ms | 1552108.92 tokens/sec
Step 4085 | loss: 3.284707 | lr:5.5633e-04 | norm 0.2564 | dt 338.76ms | 1547654.27 tokens/sec
Step 4086 | loss: 3.236178 | lr:5.5631e-04 | norm 0.2771 | dt 338.15ms | 1550479.42 tokens/sec
Step 4087 | loss: 3.283050 | lr:5.5628e-04 | norm 0.2532 | dt 337.92ms | 1551530.70 tokens/sec
Step 4088 | loss: 3.246188 | lr:5.5626e-04 | norm 0.2651 | dt 338.50ms | 1548874.07 tokens/sec
Step 4089 | loss: 3.230532 | lr:5.5623e-04 | norm 0.2575 | dt 338.00ms | 1551133.43 tokens/sec
Step 4090 | loss: 3.300280 | lr:5.5621e-04 | norm 0.2687 | dt 338.27ms | 1549912.25 tokens/sec
Step 4091 | loss: 3.295469 | lr:5.5618e-04 | norm 0.2850 | dt 337.61ms | 1552934.27 tokens/sec
Step 4092 | loss: 3.190898 | lr:5.5615e-04 | norm 0.2568 | dt 338.43ms | 1549183.96 tokens/sec
Step 4093 | loss: 3.354909 | lr:5.5613e-04 | norm 0.3442 | dt 338.22ms | 1550158.08 tokens/sec
Step 4094 | loss: 3.371988 | lr:5.5610e-04 | norm 0.3160 | dt 338.13ms | 1550535.18 tokens/sec
Step 4095 | loss: 3.450488 | lr:5.5608e-04 | norm 0.3086 | dt 337.67ms | 1552647.00 tokens/sec
Step 4096 | loss: 3.401345 | lr:5.5605e-04 | norm 0.3085 | dt 338.41ms | 1549271.28 tokens/sec
Step 4097 | loss: 3.343654 | lr:5.5603e-04 | norm 0.3023 | dt 337.79ms | 1552097.96 tokens/sec
Step 4098 | loss: 3.365234 | lr:5.5600e-04 | norm 0.3085 | dt 338.37ms | 1549464.50 tokens/sec
Step 4099 | loss: 3.408364 | lr:5.5598e-04 | norm 0.2632 | dt 337.42ms | 1553824.18 tokens/sec
Step 4100 | loss: 3.447399 | lr:5.5595e-04 | norm 0.3155 | dt 339.12ms | 1546038.45 tokens/sec
Step 4101 | loss: 3.405408 | lr:5.5593e-04 | norm 0.2936 | dt 338.90ms | 1547020.60 tokens/sec
Step 4102 | loss: 3.435959 | lr:5.5590e-04 | norm 0.3063 | dt 338.87ms | 1547146.85 tokens/sec
Step 4103 | loss: 3.373597 | lr:5.5588e-04 | norm 0.2916 | dt 337.81ms | 1552027.85 tokens/sec
Step 4104 | loss: 3.380341 | lr:5.5585e-04 | norm 0.2715 | dt 338.36ms | 1549481.97 tokens/sec
Step 4105 | loss: 3.361871 | lr:5.5583e-04 | norm 0.2632 | dt 337.93ms | 1551477.07 tokens/sec
Step 4106 | loss: 3.339730 | lr:5.5580e-04 | norm 0.2583 | dt 338.40ms | 1549328.04 tokens/sec
Step 4107 | loss: 3.389267 | lr:5.5578e-04 | norm 0.2643 | dt 337.96ms | 1551335.87 tokens/sec
Step 4108 | loss: 3.321575 | lr:5.5575e-04 | norm 0.2654 | dt 338.62ms | 1548295.00 tokens/sec
Step 4109 | loss: 3.349692 | lr:5.5572e-04 | norm 0.2632 | dt 337.93ms | 1551489.11 tokens/sec
Step 4110 | loss: 3.359636 | lr:5.5570e-04 | norm 0.2831 | dt 338.09ms | 1550727.62 tokens/sec
Step 4111 | loss: 3.365530 | lr:5.5567e-04 | norm 0.2453 | dt 338.07ms | 1550849.01 tokens/sec
Step 4112 | loss: 3.351053 | lr:5.5565e-04 | norm 0.2620 | dt 338.18ms | 1550320.92 tokens/sec
Step 4113 | loss: 3.300045 | lr:5.5562e-04 | norm 0.2621 | dt 338.11ms | 1550623.74 tokens/sec
Step 4114 | loss: 3.381418 | lr:5.5560e-04 | norm 0.2798 | dt 338.06ms | 1550854.48 tokens/sec
Step 4115 | loss: 3.350582 | lr:5.5557e-04 | norm 0.2907 | dt 338.34ms | 1549571.50 tokens/sec
Step 4116 | loss: 3.350739 | lr:5.5555e-04 | norm 0.3107 | dt 337.33ms | 1554217.34 tokens/sec
Step 4117 | loss: 3.312098 | lr:5.5552e-04 | norm 0.3315 | dt 337.52ms | 1553344.53 tokens/sec
Step 4118 | loss: 3.397113 | lr:5.5550e-04 | norm 0.3161 | dt 338.56ms | 1548581.75 tokens/sec
Step 4119 | loss: 3.293764 | lr:5.5547e-04 | norm 0.2552 | dt 338.73ms | 1547823.11 tokens/sec
Step 4120 | loss: 3.366763 | lr:5.5545e-04 | norm 0.2727 | dt 338.05ms | 1550909.17 tokens/sec
Step 4121 | loss: 3.380045 | lr:5.5542e-04 | norm 0.2713 | dt 337.77ms | 1552198.75 tokens/sec
Step 4122 | loss: 3.342838 | lr:5.5539e-04 | norm 0.2755 | dt 338.43ms | 1549173.05 tokens/sec
Step 4123 | loss: 3.359293 | lr:5.5537e-04 | norm 0.2608 | dt 337.81ms | 1552038.81 tokens/sec
Step 4124 | loss: 3.359348 | lr:5.5534e-04 | norm 0.2613 | dt 337.50ms | 1553450.97 tokens/sec
Step 4125 | loss: 3.427250 | lr:5.5532e-04 | norm 0.3635 | dt 339.38ms | 1544854.58 tokens/sec
Step 4126 | loss: 3.384590 | lr:5.5529e-04 | norm 0.4585 | dt 338.11ms | 1550620.46 tokens/sec
Step 4127 | loss: 3.351217 | lr:5.5527e-04 | norm 0.3822 | dt 337.72ms | 1552416.81 tokens/sec
Step 4128 | loss: 3.259908 | lr:5.5524e-04 | norm 0.3884 | dt 337.69ms | 1552573.55 tokens/sec
Step 4129 | loss: 3.268636 | lr:5.5522e-04 | norm 0.4013 | dt 337.97ms | 1551263.65 tokens/sec
Step 4130 | loss: 3.205765 | lr:5.5519e-04 | norm 0.3390 | dt 338.05ms | 1550897.14 tokens/sec
Step 4131 | loss: 3.325233 | lr:5.5517e-04 | norm 0.3182 | dt 337.58ms | 1553076.85 tokens/sec
Step 4132 | loss: 3.272446 | lr:5.5514e-04 | norm 0.2932 | dt 337.73ms | 1552393.80 tokens/sec
Step 4133 | loss: 3.282001 | lr:5.5511e-04 | norm 0.2602 | dt 338.46ms | 1549020.27 tokens/sec
Step 4134 | loss: 3.207631 | lr:5.5509e-04 | norm 0.2759 | dt 338.31ms | 1549736.40 tokens/sec
Step 4135 | loss: 3.305238 | lr:5.5506e-04 | norm 0.2926 | dt 337.56ms | 1553173.38 tokens/sec
Step 4136 | loss: 3.246855 | lr:5.5504e-04 | norm 0.3050 | dt 338.23ms | 1550106.73 tokens/sec
Step 4137 | loss: 3.292298 | lr:5.5501e-04 | norm 0.2511 | dt 337.54ms | 1553263.34 tokens/sec
Step 4138 | loss: 3.196898 | lr:5.5499e-04 | norm 0.2611 | dt 337.63ms | 1552858.60 tokens/sec
Step 4139 | loss: 3.273355 | lr:5.5496e-04 | norm 0.2564 | dt 337.66ms | 1552701.81 tokens/sec
Step 4140 | loss: 3.359201 | lr:5.5494e-04 | norm 0.2689 | dt 337.85ms | 1551851.51 tokens/sec
Step 4141 | loss: 3.407184 | lr:5.5491e-04 | norm 0.2767 | dt 338.24ms | 1550042.26 tokens/sec
Step 4142 | loss: 3.380643 | lr:5.5488e-04 | norm 0.2545 | dt 337.31ms | 1554343.67 tokens/sec
Step 4143 | loss: 3.463416 | lr:5.5486e-04 | norm 0.2873 | dt 338.18ms | 1550318.74 tokens/sec
Step 4144 | loss: 3.386214 | lr:5.5483e-04 | norm 0.2736 | dt 338.08ms | 1550781.20 tokens/sec
Step 4145 | loss: 3.433129 | lr:5.5481e-04 | norm 0.2582 | dt 338.38ms | 1549401.18 tokens/sec
Step 4146 | loss: 3.386533 | lr:5.5478e-04 | norm 0.2544 | dt 337.86ms | 1551802.23 tokens/sec
Step 4147 | loss: 3.352315 | lr:5.5476e-04 | norm 0.2826 | dt 336.99ms | 1555777.65 tokens/sec
Step 4148 | loss: 3.382394 | lr:5.5473e-04 | norm 0.2799 | dt 338.08ms | 1550764.80 tokens/sec
Step 4149 | loss: 3.411721 | lr:5.5471e-04 | norm 0.2685 | dt 338.56ms | 1548567.58 tokens/sec
Step 4150 | loss: 3.422970 | lr:5.5468e-04 | norm 0.2546 | dt 337.55ms | 1553200.81 tokens/sec
Step 4151 | loss: 3.364853 | lr:5.5465e-04 | norm 0.2562 | dt 337.79ms | 1552124.25 tokens/sec
Step 4152 | loss: 3.379466 | lr:5.5463e-04 | norm 0.2682 | dt 337.92ms | 1551520.85 tokens/sec
Step 4153 | loss: 3.347109 | lr:5.5460e-04 | norm 0.2679 | dt 338.72ms | 1547834.01 tokens/sec
Step 4154 | loss: 3.347548 | lr:5.5458e-04 | norm 0.2569 | dt 337.90ms | 1551621.57 tokens/sec
Step 4155 | loss: 3.360882 | lr:5.5455e-04 | norm 0.2524 | dt 337.98ms | 1551242.85 tokens/sec
Step 4156 | loss: 3.380254 | lr:5.5453e-04 | norm 0.2831 | dt 338.22ms | 1550130.77 tokens/sec
Step 4157 | loss: 3.330434 | lr:5.5450e-04 | norm 0.2899 | dt 1026.54ms | 510734.43 tokens/sec
Step 4158 | loss: 3.281205 | lr:5.5447e-04 | norm 0.2558 | dt 337.85ms | 1551858.09 tokens/sec
Step 4159 | loss: 3.356595 | lr:5.5445e-04 | norm 0.2716 | dt 335.87ms | 1560994.75 tokens/sec
Step 4160 | loss: 3.298017 | lr:5.5442e-04 | norm 0.3111 | dt 340.41ms | 1540177.08 tokens/sec
Step 4161 | loss: 3.343029 | lr:5.5440e-04 | norm 0.2954 | dt 337.47ms | 1553600.23 tokens/sec
Step 4162 | loss: 3.327277 | lr:5.5437e-04 | norm 0.3012 | dt 338.24ms | 1550069.58 tokens/sec
Step 4163 | loss: 3.371587 | lr:5.5435e-04 | norm 0.3134 | dt 337.50ms | 1553461.95 tokens/sec
Step 4164 | loss: 3.408265 | lr:5.5432e-04 | norm 0.2968 | dt 338.72ms | 1547863.43 tokens/sec
Step 4165 | loss: 3.418992 | lr:5.5429e-04 | norm 0.2547 | dt 338.43ms | 1549194.87 tokens/sec
Step 4166 | loss: 3.410863 | lr:5.5427e-04 | norm 0.2786 | dt 337.78ms | 1552171.36 tokens/sec
Step 4167 | loss: 3.392921 | lr:5.5424e-04 | norm 0.3186 | dt 338.19ms | 1550259.72 tokens/sec
Step 4168 | loss: 3.333811 | lr:5.5422e-04 | norm 0.2827 | dt 338.80ms | 1547476.74 tokens/sec
Step 4169 | loss: 3.312352 | lr:5.5419e-04 | norm 0.2563 | dt 338.06ms | 1550886.20 tokens/sec
Step 4170 | loss: 3.368690 | lr:5.5417e-04 | norm 0.2713 | dt 338.04ms | 1550951.83 tokens/sec
Step 4171 | loss: 3.383199 | lr:5.5414e-04 | norm 0.2841 | dt 338.98ms | 1546682.20 tokens/sec
Step 4172 | loss: 3.381074 | lr:5.5411e-04 | norm 0.2595 | dt 338.47ms | 1548976.63 tokens/sec
Step 4173 | loss: 3.346271 | lr:5.5409e-04 | norm 0.2705 | dt 338.21ms | 1550164.64 tokens/sec
Step 4174 | loss: 3.438740 | lr:5.5406e-04 | norm 0.2691 | dt 339.14ms | 1545940.63 tokens/sec
Step 4175 | loss: 3.282789 | lr:5.5404e-04 | norm 0.3386 | dt 339.69ms | 1543422.25 tokens/sec
Step 4176 | loss: 3.210680 | lr:5.5401e-04 | norm 0.3778 | dt 341.53ms | 1535115.13 tokens/sec
Step 4177 | loss: 3.287067 | lr:5.5399e-04 | norm 0.3180 | dt 338.85ms | 1547255.71 tokens/sec
Step 4178 | loss: 3.266288 | lr:5.5396e-04 | norm 0.2901 | dt 339.29ms | 1545238.87 tokens/sec
Step 4179 | loss: 3.235191 | lr:5.5393e-04 | norm 0.3267 | dt 1044.49ms | 501955.59 tokens/sec
Step 4180 | loss: 3.250228 | lr:5.5391e-04 | norm 0.3045 | dt 338.18ms | 1550308.90 tokens/sec
Step 4181 | loss: 3.255639 | lr:5.5388e-04 | norm 0.2667 | dt 340.94ms | 1537760.22 tokens/sec
Step 4182 | loss: 3.190253 | lr:5.5386e-04 | norm 0.2684 | dt 339.06ms | 1546305.89 tokens/sec
Step 4183 | loss: 3.284050 | lr:5.5383e-04 | norm 0.2605 | dt 337.96ms | 1551352.29 tokens/sec
Step 4184 | loss: 3.239984 | lr:5.5380e-04 | norm 0.2925 | dt 340.20ms | 1541133.43 tokens/sec
Step 4185 | loss: 3.211583 | lr:5.5378e-04 | norm 0.2527 | dt 339.56ms | 1544000.93 tokens/sec
Step 4186 | loss: 3.498229 | lr:5.5375e-04 | norm 0.2806 | dt 338.73ms | 1547826.38 tokens/sec
Step 4187 | loss: 3.457941 | lr:5.5373e-04 | norm 0.3174 | dt 338.70ms | 1547950.59 tokens/sec
Step 4188 | loss: 3.447357 | lr:5.5370e-04 | norm 0.2938 | dt 339.13ms | 1545974.33 tokens/sec
Step 4189 | loss: 3.433565 | lr:5.5368e-04 | norm 0.2952 | dt 339.38ms | 1544857.84 tokens/sec
Step 4190 | loss: 3.395098 | lr:5.5365e-04 | norm 0.2837 | dt 339.21ms | 1545614.66 tokens/sec
Step 4191 | loss: 3.394142 | lr:5.5362e-04 | norm 0.3041 | dt 339.63ms | 1543699.61 tokens/sec
Step 4192 | loss: 3.424319 | lr:5.5360e-04 | norm 0.3425 | dt 338.75ms | 1547730.52 tokens/sec
Step 4193 | loss: 3.362725 | lr:5.5357e-04 | norm 0.3230 | dt 338.28ms | 1549858.73 tokens/sec
Step 4194 | loss: 3.424097 | lr:5.5355e-04 | norm 0.3019 | dt 339.35ms | 1544989.17 tokens/sec
Step 4195 | loss: 3.431573 | lr:5.5352e-04 | norm 0.2579 | dt 338.87ms | 1547159.92 tokens/sec
Step 4196 | loss: 3.432838 | lr:5.5349e-04 | norm 0.2953 | dt 338.61ms | 1548332.06 tokens/sec
Step 4197 | loss: 3.393362 | lr:5.5347e-04 | norm 0.2735 | dt 339.61ms | 1543786.31 tokens/sec
Step 4198 | loss: 3.352219 | lr:5.5344e-04 | norm 0.2814 | dt 338.29ms | 1549812.85 tokens/sec
Step 4199 | loss: 3.346416 | lr:5.5342e-04 | norm 0.2766 | dt 339.42ms | 1544677.70 tokens/sec
Step 4200 | loss: 3.342700 | lr:5.5339e-04 | norm 0.2770 | dt 339.42ms | 1544645.15 tokens/sec
Step 4201 | loss: 3.290040 | lr:5.5336e-04 | norm 0.2639 | dt 338.15ms | 1550443.35 tokens/sec
Step 4202 | loss: 3.245153 | lr:5.5334e-04 | norm 0.2663 | dt 337.89ms | 1551662.08 tokens/sec
Step 4203 | loss: 3.315080 | lr:5.5331e-04 | norm 0.2745 | dt 339.15ms | 1545867.82 tokens/sec
Step 4204 | loss: 3.297398 | lr:5.5329e-04 | norm 0.3054 | dt 337.73ms | 1552370.79 tokens/sec
Step 4205 | loss: 3.309652 | lr:5.5326e-04 | norm 0.2839 | dt 338.19ms | 1550264.09 tokens/sec
Step 4206 | loss: 3.401457 | lr:5.5323e-04 | norm 0.2628 | dt 338.08ms | 1550793.23 tokens/sec
Step 4207 | loss: 3.359877 | lr:5.5321e-04 | norm 0.2814 | dt 338.96ms | 1546763.79 tokens/sec
Step 4208 | loss: 3.344765 | lr:5.5318e-04 | norm 0.2957 | dt 339.87ms | 1542612.38 tokens/sec
Step 4209 | loss: 3.320354 | lr:5.5316e-04 | norm 0.2710 | dt 338.78ms | 1547596.54 tokens/sec
Step 4210 | loss: 3.365427 | lr:5.5313e-04 | norm 0.2768 | dt 339.33ms | 1545061.90 tokens/sec
Step 4211 | loss: 3.383145 | lr:5.5310e-04 | norm 0.2865 | dt 338.56ms | 1548561.03 tokens/sec
Step 4212 | loss: 3.421685 | lr:5.5308e-04 | norm 0.3032 | dt 339.07ms | 1546268.92 tokens/sec
Step 4213 | loss: 3.376796 | lr:5.5305e-04 | norm 0.2794 | dt 339.75ms | 1543160.14 tokens/sec
Step 4214 | loss: 3.408689 | lr:5.5303e-04 | norm 0.2611 | dt 338.79ms | 1547512.68 tokens/sec
Step 4215 | loss: 3.346428 | lr:5.5300e-04 | norm 0.3061 | dt 339.18ms | 1545734.17 tokens/sec
Step 4216 | loss: 3.341683 | lr:5.5297e-04 | norm 0.3197 | dt 339.50ms | 1544315.38 tokens/sec
Step 4217 | loss: 3.392555 | lr:5.5295e-04 | norm 0.3098 | dt 338.85ms | 1547239.38 tokens/sec
Step 4218 | loss: 3.385345 | lr:5.5292e-04 | norm 0.3169 | dt 339.46ms | 1544457.47 tokens/sec
Step 4219 | loss: 3.400628 | lr:5.5290e-04 | norm 0.2930 | dt 339.36ms | 1544939.24 tokens/sec
Step 4220 | loss: 3.380293 | lr:5.5287e-04 | norm 0.2867 | dt 338.80ms | 1547482.19 tokens/sec
Step 4221 | loss: 3.225096 | lr:5.5284e-04 | norm 0.3067 | dt 338.54ms | 1548693.00 tokens/sec
Step 4222 | loss: 3.245425 | lr:5.5282e-04 | norm 0.3060 | dt 337.79ms | 1552111.11 tokens/sec
Step 4223 | loss: 3.217554 | lr:5.5279e-04 | norm 0.2743 | dt 339.08ms | 1546186.29 tokens/sec
Step 4224 | loss: 3.248865 | lr:5.5277e-04 | norm 0.3045 | dt 338.99ms | 1546597.35 tokens/sec
Step 4225 | loss: 3.250951 | lr:5.5274e-04 | norm 0.2873 | dt 337.88ms | 1551704.78 tokens/sec
Step 4226 | loss: 3.256791 | lr:5.5271e-04 | norm 0.2985 | dt 338.43ms | 1549174.14 tokens/sec
Step 4227 | loss: 3.189022 | lr:5.5269e-04 | norm 0.2544 | dt 338.03ms | 1551026.22 tokens/sec
Step 4228 | loss: 3.301969 | lr:5.5266e-04 | norm 0.2855 | dt 338.87ms | 1547163.18 tokens/sec
Step 4229 | loss: 3.285130 | lr:5.5263e-04 | norm 0.2817 | dt 338.39ms | 1549362.97 tokens/sec
Step 4230 | loss: 3.245992 | lr:5.5261e-04 | norm 0.2999 | dt 338.25ms | 1549999.65 tokens/sec
Step 4231 | loss: 3.178525 | lr:5.5258e-04 | norm 0.2794 | dt 338.02ms | 1551048.10 tokens/sec
Step 4232 | loss: 3.357737 | lr:5.5256e-04 | norm 0.2713 | dt 338.89ms | 1547071.75 tokens/sec
Step 4233 | loss: 3.405218 | lr:5.5253e-04 | norm 0.2885 | dt 340.10ms | 1541579.63 tokens/sec
Step 4234 | loss: 3.391199 | lr:5.5250e-04 | norm 0.3304 | dt 339.31ms | 1545146.58 tokens/sec
Step 4235 | loss: 3.360701 | lr:5.5248e-04 | norm 0.3318 | dt 338.12ms | 1550585.47 tokens/sec
Step 4236 | loss: 3.420834 | lr:5.5245e-04 | norm 0.2867 | dt 339.15ms | 1545871.08 tokens/sec
Step 4237 | loss: 3.403826 | lr:5.5243e-04 | norm 0.2830 | dt 339.34ms | 1545006.54 tokens/sec
Step 4238 | loss: 3.477479 | lr:5.5240e-04 | norm 0.3013 | dt 339.14ms | 1545935.20 tokens/sec
Step 4239 | loss: 3.421468 | lr:5.5237e-04 | norm 0.3152 | dt 339.72ms | 1543294.43 tokens/sec
Step 4240 | loss: 3.406090 | lr:5.5235e-04 | norm 0.2988 | dt 339.07ms | 1546268.92 tokens/sec
Step 4241 | loss: 3.363604 | lr:5.5232e-04 | norm 0.3472 | dt 338.65ms | 1548153.29 tokens/sec
Step 4242 | loss: 3.404014 | lr:5.5229e-04 | norm 0.3380 | dt 338.96ms | 1546761.62 tokens/sec
Step 4243 | loss: 3.371116 | lr:5.5227e-04 | norm 0.3097 | dt 339.06ms | 1546312.41 tokens/sec
Step 4244 | loss: 3.336258 | lr:5.5224e-04 | norm 0.2714 | dt 338.74ms | 1547746.86 tokens/sec
Step 4245 | loss: 3.297716 | lr:5.5222e-04 | norm 0.3047 | dt 339.28ms | 1545313.80 tokens/sec
Step 4246 | loss: 3.309327 | lr:5.5219e-04 | norm 0.2607 | dt 338.93ms | 1546901.98 tokens/sec
Step 4247 | loss: 3.302923 | lr:5.5216e-04 | norm 0.2549 | dt 338.42ms | 1549201.42 tokens/sec
Step 4248 | loss: 3.351841 | lr:5.5214e-04 | norm 0.2717 | dt 339.17ms | 1545808.05 tokens/sec
Step 4249 | loss: 3.318732 | lr:5.5211e-04 | norm 0.2701 | dt 339.30ms | 1545204.12 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 4250: 3.3994
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2710/10042=0.2699


ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm not talking about the people coming out from the table. (Is their language model?). I think it is


ddp_rank 5: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, maybe you have a few lessons you think may be your mother tongue. I'm now at home, if you haven't
rank 7 sample 2 >Hello, I'm a language model, I'm a designer. I wrote my code to start my new code I'm a language model, I'm trying to
rank 5 sample 0 >Hello, I'm a language model, I don't know. I'm just my first language teacher. If I'm just a language teacher, I'm going
rank 7 sample 3 >Hello, I'm a language model, no one will be able to define things or see them.
But as I mentioned above, most of the books on


rank 5 sample 1 >Hello, I'm a language model, an abstract language that seems pretty easy to read aloud. But how the heck I am doing this is quite simply to not
rank 5 sample 2 >Hello, I'm a language model, but it still doesn't work! We're trying to figure out some kind of "talk" in English, but what
rank 5 sample 3 >Hello, I'm a language model, or one of a set of languages that is still quite useful.
I like Microsoft Excel and don't have any experience




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, as this's a tutorial, we haven't actually learned enough examples to explain and understand what the "language model" is


ddp_rank 2: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, not an author in the language model. Let's talk another way to teach English. Say, write, write, write
rank 1 sample 2 >Hello, I'm a language model, I'm going to use the language model to model the different kind of data. I'm going to show how we define
rank 2 sample 0 >Hello, I'm a language model, of all things a language model can contain a language model of language.<|endoftext|>A number of studies have been completed to evaluate
rank 1 sample 3 >Hello, I'm a language model, and I'm doing a job at creating movies. I got an audience on Amazon and now I'm working on creating a


rank 2 sample 1 >Hello, I'm a language model, I'm ready to make my own website pages.
Here is one way to create a website, you will need to
rank 2 sample 2 >Hello, I'm a language model, but I've also written a lot by making my own observations and by comparing it with our language model, which is based
rank 2 sample 3 >Hello, I'm a language model, I want to find out how to use Microsoft Word applications for this.
There are several problems associated with this problem.




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm a programmer for the project.
In the post I described our framework (with the goal of being practical
rank 3 sample 1 >Hello, I'm a language model, and my language model is: the language model is the code that's written in the language model. I'm doing some
rank 3 sample 2 >Hello, I'm a language model, and you have to really understand me. Let's talk about some of those, though.
Is it really useful?
rank 3 sample 3 >Hello, I'm a language model, and one of your skills will improve your writing.<|endoftext|>|The Importance of Open Documentation|
Documenting Open Documentation




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model,
And I'm the developer. I've been in this class but
not quite sure about it.
I wanted
rank 6 sample 1 >Hello, I'm a language model, a language model, and I'm going use it with a lot of applications.
My language model
So, I
rank 6 sample 2 >Hello, I'm a language model, I'm just trying to make sure I'm able to understand it without any of the constraints of language. In our example
rank 6 sample 3 >Hello, I'm a language model, so I'm going to be teaching a computer language. I'm basically going to be using HTML to construct this I'm




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I have a language model in English Language 2. If I get these errors, you can be corrected!
So
rank 0 sample 1 >Hello, I'm a language model, but do you know how to build more complex language into C? How can you use the C++C++C++
rank 0 sample 2 >Hello, I'm a language model, but I would still see many language models that would be more interesting for students to get the idea of how to teach something
rank 0 sample 3 >Hello, I'm a language model, I hope you enjoyed this blog post and would like to know more about our platform by sharing my knowledge of languages, tools




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I am not sure that I'm saying I'm not going to go to the language school. What does this say
rank 4 sample 1 >Hello, I'm a language model, using an example such as "Incorrect.NET," this is pretty well, if you want to create your local environment
rank 4 sample 2 >Hello, I'm a language model, so I'm getting some points about that and making sure I have the correct grammar around my brain. How do students know
rank 4 sample 3 >Hello, I'm a language model, and it worked, as long as I don't believe I will be good enough to have the job. I didn't


Step 4250 | loss: 3.349937 | lr:5.5208e-04 | norm 0.2646 | dt 18766.41ms | 27937.58 tokens/sec
Step 4251 | loss: 3.320308 | lr:5.5206e-04 | norm 0.2611 | dt 334.53ms | 1567233.73 tokens/sec
Step 4252 | loss: 3.384989 | lr:5.5203e-04 | norm 0.3103 | dt 336.46ms | 1558250.42 tokens/sec
Step 4253 | loss: 3.330993 | lr:5.5201e-04 | norm 0.2945 | dt 337.12ms | 1555216.50 tokens/sec
Step 4254 | loss: 3.363084 | lr:5.5198e-04 | norm 0.2539 | dt 335.93ms | 1560713.35 tokens/sec
Step 4255 | loss: 3.312865 | lr:5.5195e-04 | norm 0.2797 | dt 336.50ms | 1558059.42 tokens/sec
Step 4256 | loss: 3.270736 | lr:5.5193e-04 | norm 0.2587 | dt 336.58ms | 1557702.94 tokens/sec
Step 4257 | loss: 3.387096 | lr:5.5190e-04 | norm 0.2644 | dt 336.11ms | 1559876.39 tokens/sec
Step 4258 | loss: 3.349805 | lr:5.5187e-04 | norm 0.2809 | dt 336.74ms | 1556940.85 tokens/sec
Step 4259 | loss: 3.417520 | lr:5.5185e-04 | norm 0.2768 | dt 336.48ms | 1558146.64 tokens/sec
Step 4260 | loss: 3.374088 | lr:5.5182e-04 | norm 0.2772 | dt 336.86ms | 1556378.86 tokens/sec
Step 4261 | loss: 3.401795 | lr:5.5179e-04 | norm 0.3064 | dt 337.40ms | 1553912.02 tokens/sec
Step 4262 | loss: 3.346254 | lr:5.5177e-04 | norm 0.2864 | dt 336.92ms | 1556113.43 tokens/sec
Step 4263 | loss: 3.375976 | lr:5.5174e-04 | norm 0.2823 | dt 337.23ms | 1554709.61 tokens/sec
Step 4264 | loss: 3.335169 | lr:5.5172e-04 | norm 0.2909 | dt 338.47ms | 1549005.00 tokens/sec
Step 4265 | loss: 3.378632 | lr:5.5169e-04 | norm 0.2774 | dt 337.88ms | 1551693.83 tokens/sec
Step 4266 | loss: 3.372447 | lr:5.5166e-04 | norm 0.2865 | dt 337.53ms | 1553296.26 tokens/sec
Step 4267 | loss: 3.424829 | lr:5.5164e-04 | norm 0.2723 | dt 341.31ms | 1536086.66 tokens/sec
Step 4268 | loss: 3.222583 | lr:5.5161e-04 | norm 0.3627 | dt 337.40ms | 1553897.74 tokens/sec
Step 4269 | loss: 3.252854 | lr:5.5158e-04 | norm 0.3222 | dt 337.13ms | 1555148.31 tokens/sec
Step 4270 | loss: 3.274414 | lr:5.5156e-04 | norm 0.3187 | dt 337.90ms | 1551604.05 tokens/sec
Step 4271 | loss: 3.212858 | lr:5.5153e-04 | norm 0.2677 | dt 337.34ms | 1554167.91 tokens/sec
Step 4272 | loss: 3.288093 | lr:5.5150e-04 | norm 0.2776 | dt 337.23ms | 1554701.92 tokens/sec
Step 4273 | loss: 3.269216 | lr:5.5148e-04 | norm 0.2579 | dt 336.98ms | 1555827.18 tokens/sec
Step 4274 | loss: 3.250121 | lr:5.5145e-04 | norm 0.2957 | dt 337.85ms | 1551821.95 tokens/sec
Step 4275 | loss: 3.255768 | lr:5.5143e-04 | norm 0.2804 | dt 336.98ms | 1555830.49 tokens/sec
Step 4276 | loss: 3.322984 | lr:5.5140e-04 | norm 0.2745 | dt 337.46ms | 1553635.36 tokens/sec
Step 4277 | loss: 3.267943 | lr:5.5137e-04 | norm 0.3154 | dt 337.80ms | 1552067.29 tokens/sec
Step 4278 | loss: 3.250426 | lr:5.5135e-04 | norm 0.2684 | dt 337.71ms | 1552466.13 tokens/sec
Step 4279 | loss: 3.272266 | lr:5.5132e-04 | norm 0.2801 | dt 337.53ms | 1553290.77 tokens/sec
Step 4280 | loss: 3.409940 | lr:5.5129e-04 | norm 0.3011 | dt 337.44ms | 1553736.35 tokens/sec
Step 4281 | loss: 3.458694 | lr:5.5127e-04 | norm 0.3008 | dt 336.74ms | 1556970.61 tokens/sec
Step 4282 | loss: 3.371649 | lr:5.5124e-04 | norm 0.2871 | dt 336.87ms | 1556354.63 tokens/sec
Step 4283 | loss: 3.408681 | lr:5.5121e-04 | norm 0.3020 | dt 337.54ms | 1553278.70 tokens/sec
Step 4284 | loss: 3.392211 | lr:5.5119e-04 | norm 0.3178 | dt 336.81ms | 1556646.58 tokens/sec
Step 4285 | loss: 3.400392 | lr:5.5116e-04 | norm 0.3291 | dt 338.79ms | 1547536.64 tokens/sec
Step 4286 | loss: 3.425835 | lr:5.5113e-04 | norm 0.2972 | dt 338.71ms | 1547918.99 tokens/sec
Step 4287 | loss: 3.399633 | lr:5.5111e-04 | norm 0.2716 | dt 338.45ms | 1549107.57 tokens/sec
Step 4288 | loss: 3.415733 | lr:5.5108e-04 | norm 0.2686 | dt 339.05ms | 1546329.81 tokens/sec
Step 4289 | loss: 3.417261 | lr:5.5105e-04 | norm 0.2966 | dt 338.38ms | 1549425.19 tokens/sec
Step 4290 | loss: 3.392479 | lr:5.5103e-04 | norm 0.2913 | dt 338.31ms | 1549746.23 tokens/sec
Step 4291 | loss: 3.381168 | lr:5.5100e-04 | norm 0.2640 | dt 340.06ms | 1541772.01 tokens/sec
Step 4292 | loss: 3.385911 | lr:5.5098e-04 | norm 0.2773 | dt 338.97ms | 1546691.99 tokens/sec
Step 4293 | loss: 3.340180 | lr:5.5095e-04 | norm 0.2873 | dt 338.72ms | 1547839.46 tokens/sec
Step 4294 | loss: 3.357927 | lr:5.5092e-04 | norm 0.2804 | dt 339.13ms | 1545976.50 tokens/sec
Step 4295 | loss: 3.332695 | lr:5.5090e-04 | norm 0.2687 | dt 339.27ms | 1545348.55 tokens/sec
Step 4296 | loss: 3.386910 | lr:5.5087e-04 | norm 0.2697 | dt 339.42ms | 1544662.51 tokens/sec
Step 4297 | loss: 3.351416 | lr:5.5084e-04 | norm 0.6758 | dt 339.69ms | 1543429.83 tokens/sec
Step 4298 | loss: 3.365742 | lr:5.5082e-04 | norm 0.4217 | dt 338.43ms | 1549193.78 tokens/sec
Step 4299 | loss: 3.365568 | lr:5.5079e-04 | norm 0.3386 | dt 338.74ms | 1547765.38 tokens/sec
Step 4300 | loss: 3.306485 | lr:5.5076e-04 | norm 0.3111 | dt 339.60ms | 1543836.17 tokens/sec
Step 4301 | loss: 3.333109 | lr:5.5074e-04 | norm 0.3416 | dt 338.86ms | 1547194.75 tokens/sec
Step 4302 | loss: 3.367483 | lr:5.5071e-04 | norm 0.2907 | dt 339.65ms | 1543623.76 tokens/sec
Step 4303 | loss: 3.277571 | lr:5.5068e-04 | norm 0.2854 | dt 339.68ms | 1543467.74 tokens/sec
Step 4304 | loss: 3.369807 | lr:5.5066e-04 | norm 0.2614 | dt 338.57ms | 1548532.68 tokens/sec
Step 4305 | loss: 3.400870 | lr:5.5063e-04 | norm 0.2677 | dt 338.94ms | 1546841.04 tokens/sec
Step 4306 | loss: 3.381002 | lr:5.5060e-04 | norm 0.2502 | dt 340.41ms | 1540181.40 tokens/sec
Step 4307 | loss: 3.412298 | lr:5.5058e-04 | norm 0.2645 | dt 338.94ms | 1546860.63 tokens/sec
Step 4308 | loss: 3.345418 | lr:5.5055e-04 | norm 0.2390 | dt 338.82ms | 1547377.65 tokens/sec
Step 4309 | loss: 3.433411 | lr:5.5052e-04 | norm 0.2500 | dt 339.02ms | 1546498.37 tokens/sec
Step 4310 | loss: 3.316980 | lr:5.5050e-04 | norm 0.2759 | dt 338.91ms | 1547003.18 tokens/sec
Step 4311 | loss: 3.325149 | lr:5.5047e-04 | norm 0.2402 | dt 339.39ms | 1544779.70 tokens/sec
Step 4312 | loss: 3.376743 | lr:5.5044e-04 | norm 0.2848 | dt 339.10ms | 1546104.76 tokens/sec
Step 4313 | loss: 3.355876 | lr:5.5042e-04 | norm 0.2417 | dt 339.44ms | 1544574.63 tokens/sec
Step 4314 | loss: 3.324985 | lr:5.5039e-04 | norm 0.2658 | dt 339.27ms | 1545362.66 tokens/sec
Step 4315 | loss: 3.229098 | lr:5.5036e-04 | norm 0.2549 | dt 338.82ms | 1547379.83 tokens/sec
Step 4316 | loss: 3.250334 | lr:5.5034e-04 | norm 0.2652 | dt 338.75ms | 1547705.46 tokens/sec
Step 4317 | loss: 3.249866 | lr:5.5031e-04 | norm 0.2767 | dt 339.68ms | 1543477.49 tokens/sec
Step 4318 | loss: 3.284878 | lr:5.5028e-04 | norm 0.2822 | dt 337.90ms | 1551607.34 tokens/sec
Step 4319 | loss: 3.285015 | lr:5.5026e-04 | norm 0.2522 | dt 338.52ms | 1548783.53 tokens/sec
Step 4320 | loss: 3.213141 | lr:5.5023e-04 | norm 0.2642 | dt 339.00ms | 1546588.65 tokens/sec
Step 4321 | loss: 3.263731 | lr:5.5020e-04 | norm 0.2606 | dt 339.14ms | 1545911.29 tokens/sec
Step 4322 | loss: 3.299365 | lr:5.5018e-04 | norm 0.2719 | dt 339.16ms | 1545834.13 tokens/sec
Step 4323 | loss: 3.250094 | lr:5.5015e-04 | norm 0.2940 | dt 338.54ms | 1548678.82 tokens/sec
Step 4324 | loss: 3.229595 | lr:5.5012e-04 | norm 0.2898 | dt 338.24ms | 1550055.37 tokens/sec
Step 4325 | loss: 3.258839 | lr:5.5010e-04 | norm 0.2844 | dt 338.02ms | 1551074.35 tokens/sec
Step 4326 | loss: 3.274299 | lr:5.5007e-04 | norm 0.3306 | dt 339.39ms | 1544777.53 tokens/sec
Step 4327 | loss: 3.423868 | lr:5.5004e-04 | norm 0.3686 | dt 340.26ms | 1540836.47 tokens/sec
Step 4328 | loss: 3.402731 | lr:5.5002e-04 | norm 0.3587 | dt 338.89ms | 1547090.25 tokens/sec
Step 4329 | loss: 3.412289 | lr:5.4999e-04 | norm 0.3822 | dt 337.56ms | 1553150.34 tokens/sec
Step 4330 | loss: 3.447757 | lr:5.4996e-04 | norm 0.3183 | dt 337.63ms | 1552831.19 tokens/sec
Step 4331 | loss: 3.353516 | lr:5.4993e-04 | norm 0.3038 | dt 337.77ms | 1552211.90 tokens/sec
Step 4332 | loss: 3.422922 | lr:5.4991e-04 | norm 0.3465 | dt 338.01ms | 1551108.27 tokens/sec
Step 4333 | loss: 3.401703 | lr:5.4988e-04 | norm 0.2958 | dt 338.32ms | 1549681.79 tokens/sec
Step 4334 | loss: 3.436427 | lr:5.4985e-04 | norm 0.2983 | dt 338.74ms | 1547759.93 tokens/sec
Step 4335 | loss: 3.386753 | lr:5.4983e-04 | norm 0.2816 | dt 337.45ms | 1553692.44 tokens/sec
Step 4336 | loss: 3.421248 | lr:5.4980e-04 | norm 0.2787 | dt 338.33ms | 1549628.28 tokens/sec
Step 4337 | loss: 3.392240 | lr:5.4977e-04 | norm 0.2671 | dt 337.77ms | 1552205.33 tokens/sec
Step 4338 | loss: 3.352347 | lr:5.4975e-04 | norm 0.2632 | dt 338.29ms | 1549808.48 tokens/sec
Step 4339 | loss: 3.397373 | lr:5.4972e-04 | norm 0.2685 | dt 338.07ms | 1550838.08 tokens/sec
Step 4340 | loss: 3.371305 | lr:5.4969e-04 | norm 0.2741 | dt 337.72ms | 1552419.01 tokens/sec
Step 4341 | loss: 3.357486 | lr:5.4967e-04 | norm 0.2687 | dt 338.28ms | 1549862.00 tokens/sec
Step 4342 | loss: 3.346332 | lr:5.4964e-04 | norm 0.2650 | dt 337.24ms | 1554642.57 tokens/sec
Step 4343 | loss: 3.415406 | lr:5.4961e-04 | norm 0.2824 | dt 338.45ms | 1549094.47 tokens/sec
Step 4344 | loss: 3.300523 | lr:5.4959e-04 | norm 0.2638 | dt 338.50ms | 1548838.07 tokens/sec
Step 4345 | loss: 3.367127 | lr:5.4956e-04 | norm 0.2651 | dt 337.67ms | 1552651.38 tokens/sec
Step 4346 | loss: 3.381916 | lr:5.4953e-04 | norm 0.2654 | dt 901.41ms | 581634.01 tokens/sec
Step 4347 | loss: 3.370431 | lr:5.4951e-04 | norm 0.2661 | dt 335.78ms | 1561401.52 tokens/sec
Step 4348 | loss: 3.390702 | lr:5.4948e-04 | norm 0.2575 | dt 338.31ms | 1549730.94 tokens/sec
Step 4349 | loss: 3.367941 | lr:5.4945e-04 | norm 0.2599 | dt 338.41ms | 1549246.17 tokens/sec
Step 4350 | loss: 3.356233 | lr:5.4942e-04 | norm 0.2412 | dt 337.86ms | 1551798.95 tokens/sec
Step 4351 | loss: 3.335027 | lr:5.4940e-04 | norm 0.2565 | dt 337.44ms | 1553725.37 tokens/sec
Step 4352 | loss: 3.371068 | lr:5.4937e-04 | norm 0.2745 | dt 337.62ms | 1552915.63 tokens/sec
Step 4353 | loss: 3.383516 | lr:5.4934e-04 | norm 0.2855 | dt 338.61ms | 1548345.14 tokens/sec
Step 4354 | loss: 3.371984 | lr:5.4932e-04 | norm 0.2624 | dt 337.88ms | 1551720.11 tokens/sec
Step 4355 | loss: 3.412431 | lr:5.4929e-04 | norm 0.2718 | dt 338.18ms | 1550325.29 tokens/sec
Step 4356 | loss: 3.354856 | lr:5.4926e-04 | norm 0.2670 | dt 338.98ms | 1546675.67 tokens/sec
Step 4357 | loss: 3.333887 | lr:5.4924e-04 | norm 0.2914 | dt 338.73ms | 1547810.04 tokens/sec
Step 4358 | loss: 3.312550 | lr:5.4921e-04 | norm 0.2825 | dt 338.50ms | 1548865.34 tokens/sec
Step 4359 | loss: 3.382354 | lr:5.4918e-04 | norm 0.3114 | dt 338.53ms | 1548702.81 tokens/sec
Step 4360 | loss: 3.325812 | lr:5.4915e-04 | norm 0.3360 | dt 338.06ms | 1550881.83 tokens/sec
Step 4361 | loss: 3.315174 | lr:5.4913e-04 | norm 0.3312 | dt 338.01ms | 1551123.59 tokens/sec
Step 4362 | loss: 3.289144 | lr:5.4910e-04 | norm 0.3092 | dt 338.46ms | 1549056.28 tokens/sec
Step 4363 | loss: 3.229468 | lr:5.4907e-04 | norm 0.3057 | dt 338.15ms | 1550453.18 tokens/sec
Step 4364 | loss: 3.270313 | lr:5.4905e-04 | norm 0.2723 | dt 337.49ms | 1553475.12 tokens/sec
Step 4365 | loss: 3.190089 | lr:5.4902e-04 | norm 0.3034 | dt 340.08ms | 1541682.30 tokens/sec
Step 4366 | loss: 3.212091 | lr:5.4899e-04 | norm 0.3103 | dt 339.43ms | 1544624.54 tokens/sec
Step 4367 | loss: 3.213234 | lr:5.4897e-04 | norm 0.2880 | dt 338.79ms | 1547551.89 tokens/sec
Step 4368 | loss: 3.265150 | lr:5.4894e-04 | norm 0.2609 | dt 339.60ms | 1543855.68 tokens/sec
Step 4369 | loss: 3.224528 | lr:5.4891e-04 | norm 0.2964 | dt 1007.30ms | 520488.61 tokens/sec
Step 4370 | loss: 3.256220 | lr:5.4888e-04 | norm 0.2743 | dt 336.48ms | 1558142.22 tokens/sec
Step 4371 | loss: 3.252209 | lr:5.4886e-04 | norm 0.2755 | dt 338.67ms | 1548065.01 tokens/sec
Step 4372 | loss: 3.226937 | lr:5.4883e-04 | norm 0.2740 | dt 338.42ms | 1549210.15 tokens/sec
Step 4373 | loss: 3.349679 | lr:5.4880e-04 | norm 0.2871 | dt 337.55ms | 1553201.90 tokens/sec
Step 4374 | loss: 3.377802 | lr:5.4878e-04 | norm 0.3337 | dt 337.70ms | 1552519.84 tokens/sec
Step 4375 | loss: 3.401805 | lr:5.4875e-04 | norm 0.3278 | dt 338.60ms | 1548412.74 tokens/sec
Step 4376 | loss: 3.397790 | lr:5.4872e-04 | norm 0.3143 | dt 338.37ms | 1549445.94 tokens/sec
Step 4377 | loss: 3.437126 | lr:5.4870e-04 | norm 0.3448 | dt 337.97ms | 1551291.00 tokens/sec
Step 4378 | loss: 3.384723 | lr:5.4867e-04 | norm 0.3031 | dt 337.65ms | 1552776.37 tokens/sec
Step 4379 | loss: 3.383240 | lr:5.4864e-04 | norm 0.3196 | dt 340.62ms | 1539211.15 tokens/sec
Step 4380 | loss: 3.374835 | lr:5.4861e-04 | norm 0.3671 | dt 344.03ms | 1523954.21 tokens/sec
Step 4381 | loss: 3.448184 | lr:5.4859e-04 | norm 0.3772 | dt 339.45ms | 1544520.39 tokens/sec
Step 4382 | loss: 3.386389 | lr:5.4856e-04 | norm 0.2932 | dt 338.67ms | 1548059.56 tokens/sec
Step 4383 | loss: 3.364656 | lr:5.4853e-04 | norm 0.2893 | dt 338.22ms | 1550127.49 tokens/sec
Step 4384 | loss: 3.355433 | lr:5.4851e-04 | norm 0.3012 | dt 338.40ms | 1549302.93 tokens/sec
Step 4385 | loss: 3.352335 | lr:5.4848e-04 | norm 0.2781 | dt 338.31ms | 1549747.32 tokens/sec
Step 4386 | loss: 3.340531 | lr:5.4845e-04 | norm 0.2868 | dt 338.68ms | 1548021.42 tokens/sec
Step 4387 | loss: 3.292483 | lr:5.4842e-04 | norm 0.2599 | dt 338.07ms | 1550828.23 tokens/sec
Step 4388 | loss: 3.303045 | lr:5.4840e-04 | norm 0.2510 | dt 337.55ms | 1553207.39 tokens/sec
Step 4389 | loss: 3.417588 | lr:5.4837e-04 | norm 0.2605 | dt 339.30ms | 1545206.30 tokens/sec
Step 4390 | loss: 3.287228 | lr:5.4834e-04 | norm 0.2856 | dt 340.04ms | 1541852.01 tokens/sec
Step 4391 | loss: 3.396062 | lr:5.4832e-04 | norm 0.2736 | dt 338.63ms | 1548276.46 tokens/sec
Step 4392 | loss: 3.285876 | lr:5.4829e-04 | norm 0.3064 | dt 337.94ms | 1551411.39 tokens/sec
Step 4393 | loss: 3.293458 | lr:5.4826e-04 | norm 0.3917 | dt 338.59ms | 1548422.55 tokens/sec
Step 4394 | loss: 3.355060 | lr:5.4823e-04 | norm 0.3379 | dt 337.64ms | 1552813.65 tokens/sec
Step 4395 | loss: 3.312962 | lr:5.4821e-04 | norm 0.2925 | dt 338.28ms | 1549875.11 tokens/sec
Step 4396 | loss: 3.307138 | lr:5.4818e-04 | norm 0.2733 | dt 338.72ms | 1547844.90 tokens/sec
Step 4397 | loss: 3.397062 | lr:5.4815e-04 | norm 0.2804 | dt 337.31ms | 1554306.32 tokens/sec
Step 4398 | loss: 3.403256 | lr:5.4812e-04 | norm 0.2636 | dt 338.22ms | 1550155.90 tokens/sec
Step 4399 | loss: 3.426334 | lr:5.4810e-04 | norm 0.2609 | dt 338.60ms | 1548419.28 tokens/sec
Step 4400 | loss: 3.380776 | lr:5.4807e-04 | norm 0.2669 | dt 337.77ms | 1552213.00 tokens/sec
Step 4401 | loss: 3.529045 | lr:5.4804e-04 | norm 0.3280 | dt 337.74ms | 1552320.38 tokens/sec
Step 4402 | loss: 3.367190 | lr:5.4802e-04 | norm 0.3089 | dt 337.94ms | 1551440.95 tokens/sec
Step 4403 | loss: 3.344893 | lr:5.4799e-04 | norm 0.2662 | dt 337.91ms | 1551564.64 tokens/sec
Step 4404 | loss: 3.343414 | lr:5.4796e-04 | norm 0.6969 | dt 337.93ms | 1551448.61 tokens/sec
Step 4405 | loss: 3.390613 | lr:5.4793e-04 | norm 0.2963 | dt 339.80ms | 1542951.16 tokens/sec
Step 4406 | loss: 3.328714 | lr:5.4791e-04 | norm 0.2950 | dt 338.97ms | 1546713.75 tokens/sec
Step 4407 | loss: 3.351891 | lr:5.4788e-04 | norm 0.2581 | dt 339.19ms | 1545689.62 tokens/sec
Step 4408 | loss: 3.308814 | lr:5.4785e-04 | norm 0.2932 | dt 340.40ms | 1540190.03 tokens/sec
Step 4409 | loss: 3.272688 | lr:5.4782e-04 | norm 0.2583 | dt 339.48ms | 1544388.05 tokens/sec
Step 4410 | loss: 3.265654 | lr:5.4780e-04 | norm 0.2796 | dt 340.01ms | 1541961.20 tokens/sec
Step 4411 | loss: 3.314265 | lr:5.4777e-04 | norm 0.2554 | dt 339.70ms | 1543402.75 tokens/sec
Step 4412 | loss: 3.295058 | lr:5.4774e-04 | norm 0.2747 | dt 339.67ms | 1543517.58 tokens/sec
Step 4413 | loss: 3.213898 | lr:5.4772e-04 | norm 0.2813 | dt 339.15ms | 1545888.47 tokens/sec
Step 4414 | loss: 3.312578 | lr:5.4769e-04 | norm 0.2680 | dt 339.78ms | 1543017.21 tokens/sec
Step 4415 | loss: 3.272366 | lr:5.4766e-04 | norm 0.2969 | dt 343.24ms | 1527453.77 tokens/sec
Step 4416 | loss: 3.241777 | lr:5.4763e-04 | norm 0.2687 | dt 339.77ms | 1543085.42 tokens/sec
Step 4417 | loss: 3.286837 | lr:5.4761e-04 | norm 0.2918 | dt 340.32ms | 1540573.08 tokens/sec
Step 4418 | loss: 3.264504 | lr:5.4758e-04 | norm 0.2450 | dt 338.96ms | 1546732.24 tokens/sec
Step 4419 | loss: 3.192122 | lr:5.4755e-04 | norm 0.2666 | dt 339.04ms | 1546370.05 tokens/sec
Step 4420 | loss: 3.388959 | lr:5.4752e-04 | norm 0.3272 | dt 340.95ms | 1537707.53 tokens/sec
Step 4421 | loss: 3.341742 | lr:5.4750e-04 | norm 0.3050 | dt 339.42ms | 1544679.87 tokens/sec
Step 4422 | loss: 3.367116 | lr:5.4747e-04 | norm 0.3085 | dt 339.52ms | 1544216.70 tokens/sec
Step 4423 | loss: 3.375175 | lr:5.4744e-04 | norm 0.2801 | dt 338.74ms | 1547776.27 tokens/sec
Step 4424 | loss: 3.425973 | lr:5.4741e-04 | norm 0.2808 | dt 339.63ms | 1543681.19 tokens/sec
Step 4425 | loss: 3.412403 | lr:5.4739e-04 | norm 0.2668 | dt 338.82ms | 1547402.70 tokens/sec
Step 4426 | loss: 3.363002 | lr:5.4736e-04 | norm 0.2550 | dt 339.70ms | 1543381.08 tokens/sec
Step 4427 | loss: 3.356842 | lr:5.4733e-04 | norm 0.2696 | dt 339.04ms | 1546384.18 tokens/sec
Step 4428 | loss: 3.398906 | lr:5.4730e-04 | norm 0.2904 | dt 338.82ms | 1547405.96 tokens/sec
Step 4429 | loss: 3.378986 | lr:5.4728e-04 | norm 0.2360 | dt 339.26ms | 1545383.30 tokens/sec
Step 4430 | loss: 3.388568 | lr:5.4725e-04 | norm 0.2743 | dt 338.82ms | 1547383.10 tokens/sec
Step 4431 | loss: 3.348225 | lr:5.4722e-04 | norm 0.2739 | dt 339.25ms | 1545415.88 tokens/sec
Step 4432 | loss: 3.300803 | lr:5.4720e-04 | norm 0.2860 | dt 338.50ms | 1548868.62 tokens/sec
Step 4433 | loss: 3.325744 | lr:5.4717e-04 | norm 0.2986 | dt 338.93ms | 1546896.54 tokens/sec
Step 4434 | loss: 3.390193 | lr:5.4714e-04 | norm 0.2691 | dt 340.25ms | 1540905.57 tokens/sec
Step 4435 | loss: 3.342812 | lr:5.4711e-04 | norm 0.2695 | dt 338.75ms | 1547733.78 tokens/sec
Step 4436 | loss: 3.277294 | lr:5.4709e-04 | norm 0.2804 | dt 338.90ms | 1547049.98 tokens/sec
Step 4437 | loss: 3.367015 | lr:5.4706e-04 | norm 0.2797 | dt 338.93ms | 1546881.30 tokens/sec
Step 4438 | loss: 3.323598 | lr:5.4703e-04 | norm 0.2625 | dt 340.61ms | 1539270.41 tokens/sec
Step 4439 | loss: 3.324297 | lr:5.4700e-04 | norm 0.2963 | dt 339.54ms | 1544134.29 tokens/sec
Step 4440 | loss: 3.361698 | lr:5.4698e-04 | norm 0.2543 | dt 338.65ms | 1548190.35 tokens/sec
Step 4441 | loss: 3.336047 | lr:5.4695e-04 | norm 0.2753 | dt 339.27ms | 1545363.75 tokens/sec
Step 4442 | loss: 3.323292 | lr:5.4692e-04 | norm 0.2791 | dt 340.22ms | 1541035.15 tokens/sec
Step 4443 | loss: 3.304207 | lr:5.4689e-04 | norm 0.2669 | dt 340.41ms | 1540156.59 tokens/sec
Step 4444 | loss: 3.337162 | lr:5.4687e-04 | norm 0.2720 | dt 339.96ms | 1542191.54 tokens/sec
Step 4445 | loss: 3.344320 | lr:5.4684e-04 | norm 0.2917 | dt 339.72ms | 1543276.02 tokens/sec
Step 4446 | loss: 3.396587 | lr:5.4681e-04 | norm 0.3255 | dt 339.97ms | 1542167.75 tokens/sec
Step 4447 | loss: 3.321446 | lr:5.4678e-04 | norm 0.3200 | dt 338.86ms | 1547201.28 tokens/sec
Step 4448 | loss: 3.386049 | lr:5.4676e-04 | norm 0.3354 | dt 341.73ms | 1534234.74 tokens/sec
Step 4449 | loss: 3.318591 | lr:5.4673e-04 | norm 0.3168 | dt 339.29ms | 1545232.36 tokens/sec
Step 4450 | loss: 3.390902 | lr:5.4670e-04 | norm 0.2804 | dt 339.50ms | 1544316.47 tokens/sec
Step 4451 | loss: 3.378681 | lr:5.4667e-04 | norm 0.2967 | dt 339.14ms | 1545944.98 tokens/sec
Step 4452 | loss: 3.339928 | lr:5.4664e-04 | norm 0.3316 | dt 340.06ms | 1541729.86 tokens/sec
Step 4453 | loss: 3.443934 | lr:5.4662e-04 | norm 0.3130 | dt 339.94ms | 1542276.99 tokens/sec
Step 4454 | loss: 3.328515 | lr:5.4659e-04 | norm 0.2707 | dt 338.10ms | 1550689.34 tokens/sec
Step 4455 | loss: 3.223682 | lr:5.4656e-04 | norm 0.2530 | dt 338.90ms | 1547033.66 tokens/sec
Step 4456 | loss: 3.187480 | lr:5.4653e-04 | norm 0.2747 | dt 339.64ms | 1543645.43 tokens/sec
Step 4457 | loss: 3.189471 | lr:5.4651e-04 | norm 0.2546 | dt 338.73ms | 1547811.13 tokens/sec
Step 4458 | loss: 3.217721 | lr:5.4648e-04 | norm 0.2689 | dt 339.48ms | 1544396.73 tokens/sec
Step 4459 | loss: 3.272307 | lr:5.4645e-04 | norm 0.2546 | dt 339.56ms | 1544028.03 tokens/sec
Step 4460 | loss: 3.246346 | lr:5.4642e-04 | norm 0.2635 | dt 338.73ms | 1547788.25 tokens/sec
Step 4461 | loss: 3.227070 | lr:5.4640e-04 | norm 0.2639 | dt 338.65ms | 1548155.47 tokens/sec
Step 4462 | loss: 3.411252 | lr:5.4637e-04 | norm 0.3548 | dt 339.32ms | 1545130.30 tokens/sec
Step 4463 | loss: 3.219054 | lr:5.4634e-04 | norm 0.3111 | dt 339.16ms | 1545820.01 tokens/sec
Step 4464 | loss: 3.263080 | lr:5.4631e-04 | norm 0.3277 | dt 339.27ms | 1545349.63 tokens/sec
Step 4465 | loss: 3.247589 | lr:5.4629e-04 | norm 0.3228 | dt 339.00ms | 1546581.03 tokens/sec
Step 4466 | loss: 3.387914 | lr:5.4626e-04 | norm 0.3200 | dt 339.01ms | 1546509.25 tokens/sec
Step 4467 | loss: 3.402643 | lr:5.4623e-04 | norm 0.3362 | dt 339.61ms | 1543810.16 tokens/sec
Step 4468 | loss: 3.383820 | lr:5.4620e-04 | norm 0.3140 | dt 339.16ms | 1545829.79 tokens/sec
Step 4469 | loss: 3.386968 | lr:5.4618e-04 | norm 0.2924 | dt 338.22ms | 1550132.95 tokens/sec
Step 4470 | loss: 3.409489 | lr:5.4615e-04 | norm 0.2935 | dt 339.24ms | 1545499.51 tokens/sec
Step 4471 | loss: 3.452399 | lr:5.4612e-04 | norm 0.4697 | dt 340.53ms | 1539617.43 tokens/sec
Step 4472 | loss: 3.447612 | lr:5.4609e-04 | norm 0.4028 | dt 339.27ms | 1545340.95 tokens/sec
Step 4473 | loss: 3.334759 | lr:5.4606e-04 | norm 0.3267 | dt 338.93ms | 1546910.68 tokens/sec
Step 4474 | loss: 3.450602 | lr:5.4604e-04 | norm 0.3203 | dt 338.66ms | 1548107.52 tokens/sec
Step 4475 | loss: 3.406555 | lr:5.4601e-04 | norm 0.3062 | dt 338.83ms | 1547358.05 tokens/sec
Step 4476 | loss: 3.408163 | lr:5.4598e-04 | norm 0.3234 | dt 338.72ms | 1547830.74 tokens/sec
Step 4477 | loss: 3.364267 | lr:5.4595e-04 | norm 0.2737 | dt 338.68ms | 1548039.95 tokens/sec
Step 4478 | loss: 3.354674 | lr:5.4593e-04 | norm 0.2878 | dt 339.42ms | 1544649.49 tokens/sec
Step 4479 | loss: 3.289723 | lr:5.4590e-04 | norm 0.2490 | dt 342.70ms | 1529870.25 tokens/sec
Step 4480 | loss: 3.346460 | lr:5.4587e-04 | norm 0.2629 | dt 339.06ms | 1546293.93 tokens/sec
Step 4481 | loss: 3.331948 | lr:5.4584e-04 | norm 0.2523 | dt 339.40ms | 1544769.93 tokens/sec
Step 4482 | loss: 3.268053 | lr:5.4581e-04 | norm 0.2518 | dt 338.87ms | 1547176.24 tokens/sec
Step 4483 | loss: 3.311395 | lr:5.4579e-04 | norm 0.2589 | dt 339.42ms | 1544670.11 tokens/sec
Step 4484 | loss: 3.364026 | lr:5.4576e-04 | norm 0.2488 | dt 339.14ms | 1545916.72 tokens/sec
Step 4485 | loss: 3.295728 | lr:5.4573e-04 | norm 0.2612 | dt 339.28ms | 1545275.79 tokens/sec
Step 4486 | loss: 3.343092 | lr:5.4570e-04 | norm 0.2716 | dt 340.00ms | 1542004.46 tokens/sec
Step 4487 | loss: 3.373996 | lr:5.4568e-04 | norm 0.2893 | dt 338.85ms | 1547263.33 tokens/sec
Step 4488 | loss: 3.418795 | lr:5.4565e-04 | norm 0.2610 | dt 339.61ms | 1543793.90 tokens/sec
Step 4489 | loss: 3.323936 | lr:5.4562e-04 | norm 0.2943 | dt 339.06ms | 1546310.24 tokens/sec
Step 4490 | loss: 3.368011 | lr:5.4559e-04 | norm 0.3007 | dt 339.30ms | 1545225.84 tokens/sec
Step 4491 | loss: 3.345951 | lr:5.4556e-04 | norm 0.2458 | dt 339.00ms | 1546577.77 tokens/sec
Step 4492 | loss: 3.330164 | lr:5.4554e-04 | norm 0.2566 | dt 338.91ms | 1546961.83 tokens/sec
Step 4493 | loss: 3.351341 | lr:5.4551e-04 | norm 0.2625 | dt 339.45ms | 1544524.73 tokens/sec
Step 4494 | loss: 3.328023 | lr:5.4548e-04 | norm 0.2598 | dt 338.82ms | 1547402.70 tokens/sec
Step 4495 | loss: 3.388129 | lr:5.4545e-04 | norm 0.2569 | dt 338.92ms | 1546916.12 tokens/sec
Step 4496 | loss: 3.347431 | lr:5.4543e-04 | norm 0.2561 | dt 339.39ms | 1544780.79 tokens/sec
Step 4497 | loss: 3.403345 | lr:5.4540e-04 | norm 0.3266 | dt 339.12ms | 1546010.19 tokens/sec
Step 4498 | loss: 3.363091 | lr:5.4537e-04 | norm 0.3529 | dt 338.98ms | 1546669.14 tokens/sec
Step 4499 | loss: 3.316924 | lr:5.4534e-04 | norm 0.3555 | dt 339.95ms | 1542236.97 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 4500: 3.3920
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2779/10042=0.2767


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, so what I'm really talking about is making sure we can work to build a better language. But first I need to
rank 5 sample 1 >Hello, I'm a language model, one thing I am thinking for sure is that as a programmer, I'm curious and I am a very strong language model
rank 5 sample 2 >Hello, I'm a language model, but it needs a program that contains everything you need to model for example a
language program. I'm not sure which
rank 5 sample 3 >Hello, I'm a language model, an "context".
Let's look at some common ways to start a diagram like
1 through a diagram:





ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm a software developer. I didn't really talk to you with that very specific language. You're not an
rank 7 sample 1 >Hello, I'm a language model, don't think I've talked for too long. They know what I'm talking about, they know my thoughts and emotions
rank 7 sample 2 >Hello, I'm a language model, and I am happy that I didn't agree with them. I know when I'm talking about language models, when it
rank 7 sample 3 >Hello, I'm a language model, while you're in, in, for......
(e.g. if you're thinking of a language model




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, writing an article based on a book (I am a book) and speaking about what I'm doing. What I have
rank 2 sample 1 >Hello, I'm a language model, so I figured out how to simplify for beginners. Because I've worked this out, I am not a beginner, but
rank 2 sample 2 >Hello, I'm a language model, but I am just trying to find more options, so I just have to work with them and build those in my class
rank 2 sample 3 >Hello, I'm a language model, so my mother's in-between mother's and their kids, and I like how the parents should move from the home




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and the underlying language is the language design that goes through our training.
And I can say with a lot of confidence
rank 1 sample 1 >Hello, I'm a language model, a model for designing a language model in Excel. I'd like to go and play over to improve my language model.
rank 1 sample 2 >Hello, I'm a language model, so like any other language, I'm going to tell you in this tutorial that I'm going to show you all of
rank 1 sample 3 >Hello, I'm a language model, and I'm doing a demo in class before I'll put in your work -- I’m going to make sure




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not an English model," he says.
By using DBS to understand his idea of a D.
rank 3 sample 1 >Hello, I'm a language model, and the reason I'm taking these approaches is because I am a linguist and I'm a linguist. What do
rank 3 sample 2 >Hello, I'm a language model, and it works really well. My first draft didn't come naturally, and was actually a pretty much finished project I didn
rank 3 sample 3 >Hello, I'm a language model, and are now in the lab right now, so we are ready for this.
You just need to define your own




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, as my students go about understanding this term. I like the language model, I like the way it is, and in
rank 6 sample 1 >Hello, I'm a language model, it's just a tool that you can copy and run on your computer as you would on your computer, you can access
rank 6 sample 2 >Hello, I'm a language model, so let's say we want our computer to communicate with a friend (like "I'm a friend") as in a
rank 6 sample 3 >Hello, I'm a language model, and this is how to do it. I am going to talk about the importance of this technique. Hopefully for every piece




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I want to keep track of the process, as I can see with the next two blog posts. It's not
rank 4 sample 1 >Hello, I'm a language model, or at least maybe it's been developed as a programming language.
And then of course I'm thinking about your code
rank 4 sample 2 >Hello, I'm a language model, I still work when we do a good job of modeling to see how we can communicate effectively with each other. This means
rank 4 sample 3 >Hello, I'm a language model, and you wanna help your kids understand more of that because "yoo" sounds like "lousy."
As a




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I don't think my job is much fun.
The book provides me with examples of real language that I think
rank 0 sample 1 >Hello, I'm a language model, but when I'm working on the way to do with grammar and sentence structure, I've learned a good bit about writing
rank 0 sample 2 >Hello, I'm a language model, but I really appreciate doing these three, and in my opinion is that we're just moving to a language model, the
rank 0 sample 3 >Hello, I'm a language model, and they're really good at explaining the basic message, but they're very difficult because there's a lot of "information


Step 4500 | loss: 3.386256 | lr:5.4531e-04 | norm 0.2816 | dt 19011.93ms | 27576.78 tokens/sec
Step 4501 | loss: 3.214427 | lr:5.4529e-04 | norm 0.3143 | dt 335.22ms | 1564022.36 tokens/sec
Step 4502 | loss: 3.257737 | lr:5.4526e-04 | norm 0.3115 | dt 335.41ms | 1563142.95 tokens/sec
Step 4503 | loss: 3.187809 | lr:5.4523e-04 | norm 0.2573 | dt 336.47ms | 1558189.70 tokens/sec
Step 4504 | loss: 3.173667 | lr:5.4520e-04 | norm 0.2806 | dt 335.69ms | 1561845.11 tokens/sec
Step 4505 | loss: 3.268999 | lr:5.4517e-04 | norm 0.2740 | dt 336.88ms | 1556302.86 tokens/sec
Step 4506 | loss: 3.273570 | lr:5.4515e-04 | norm 0.3070 | dt 336.38ms | 1558636.99 tokens/sec
Step 4507 | loss: 3.238833 | lr:5.4512e-04 | norm 0.3015 | dt 336.31ms | 1558960.74 tokens/sec
Step 4508 | loss: 3.267125 | lr:5.4509e-04 | norm 0.3608 | dt 336.20ms | 1559437.23 tokens/sec
Step 4509 | loss: 3.245526 | lr:5.4506e-04 | norm 0.3169 | dt 336.25ms | 1559217.19 tokens/sec
Step 4510 | loss: 3.374405 | lr:5.4503e-04 | norm 0.3157 | dt 336.70ms | 1557150.32 tokens/sec
Step 4511 | loss: 3.307684 | lr:5.4501e-04 | norm 0.2651 | dt 336.58ms | 1557672.04 tokens/sec
Step 4512 | loss: 3.257310 | lr:5.4498e-04 | norm 0.2934 | dt 336.33ms | 1558831.45 tokens/sec
Step 4513 | loss: 3.304175 | lr:5.4495e-04 | norm 0.2934 | dt 337.09ms | 1555357.30 tokens/sec
Step 4514 | loss: 3.341643 | lr:5.4492e-04 | norm 0.2990 | dt 336.89ms | 1556246.69 tokens/sec
Step 4515 | loss: 3.401541 | lr:5.4489e-04 | norm 0.2960 | dt 336.56ms | 1557781.28 tokens/sec
Step 4516 | loss: 3.427519 | lr:5.4487e-04 | norm 0.2977 | dt 338.14ms | 1550498.01 tokens/sec
Step 4517 | loss: 3.395487 | lr:5.4484e-04 | norm 0.2689 | dt 336.37ms | 1558646.93 tokens/sec
Step 4518 | loss: 3.343026 | lr:5.4481e-04 | norm 0.2816 | dt 337.23ms | 1554711.81 tokens/sec
Step 4519 | loss: 3.393615 | lr:5.4478e-04 | norm 0.2763 | dt 337.32ms | 1554281.05 tokens/sec
Step 4520 | loss: 3.369138 | lr:5.4476e-04 | norm 0.2966 | dt 337.52ms | 1553347.83 tokens/sec
Step 4521 | loss: 3.386790 | lr:5.4473e-04 | norm 0.2773 | dt 338.02ms | 1551053.57 tokens/sec
Step 4522 | loss: 3.381206 | lr:5.4470e-04 | norm 0.2760 | dt 337.43ms | 1553751.72 tokens/sec
Step 4523 | loss: 3.361920 | lr:5.4467e-04 | norm 0.2752 | dt 337.32ms | 1554279.95 tokens/sec
Step 4524 | loss: 3.359993 | lr:5.4464e-04 | norm 0.2848 | dt 337.77ms | 1552214.09 tokens/sec
Step 4525 | loss: 3.342832 | lr:5.4461e-04 | norm 0.3201 | dt 337.29ms | 1554422.78 tokens/sec
Step 4526 | loss: 3.356210 | lr:5.4459e-04 | norm 0.3315 | dt 337.00ms | 1555762.24 tokens/sec
Step 4527 | loss: 3.305258 | lr:5.4456e-04 | norm 0.2584 | dt 337.28ms | 1554479.92 tokens/sec
Step 4528 | loss: 3.329348 | lr:5.4453e-04 | norm 0.2778 | dt 336.98ms | 1555865.71 tokens/sec
Step 4529 | loss: 3.299411 | lr:5.4450e-04 | norm 0.2639 | dt 337.01ms | 1555685.20 tokens/sec
Step 4530 | loss: 3.357598 | lr:5.4447e-04 | norm 0.2577 | dt 338.21ms | 1550201.80 tokens/sec
Step 4531 | loss: 3.348997 | lr:5.4445e-04 | norm 0.2616 | dt 337.86ms | 1551779.24 tokens/sec
Step 4532 | loss: 3.276485 | lr:5.4442e-04 | norm 0.2510 | dt 337.51ms | 1553387.33 tokens/sec
Step 4533 | loss: 3.329785 | lr:5.4439e-04 | norm 0.2488 | dt 338.20ms | 1550219.28 tokens/sec
Step 4534 | loss: 3.307389 | lr:5.4436e-04 | norm 0.2526 | dt 338.12ms | 1550596.40 tokens/sec
Step 4535 | loss: 3.312658 | lr:5.4433e-04 | norm 0.2423 | dt 902.09ms | 581195.44 tokens/sec
Step 4536 | loss: 3.390189 | lr:5.4431e-04 | norm 0.2553 | dt 336.32ms | 1558880.07 tokens/sec
Step 4537 | loss: 3.396158 | lr:5.4428e-04 | norm 0.2643 | dt 338.14ms | 1550493.63 tokens/sec
Step 4538 | loss: 3.340948 | lr:5.4425e-04 | norm 0.2552 | dt 338.59ms | 1548424.73 tokens/sec
Step 4539 | loss: 3.313783 | lr:5.4422e-04 | norm 0.2663 | dt 337.37ms | 1554028.42 tokens/sec
Step 4540 | loss: 3.378959 | lr:5.4419e-04 | norm 0.2676 | dt 337.75ms | 1552305.04 tokens/sec
Step 4541 | loss: 3.342302 | lr:5.4417e-04 | norm 0.2725 | dt 337.63ms | 1552845.44 tokens/sec
Step 4542 | loss: 3.357174 | lr:5.4414e-04 | norm 0.2867 | dt 337.82ms | 1551961.04 tokens/sec
Step 4543 | loss: 3.341520 | lr:5.4411e-04 | norm 0.2611 | dt 338.27ms | 1549915.53 tokens/sec
Step 4544 | loss: 3.321320 | lr:5.4408e-04 | norm 0.2895 | dt 338.20ms | 1550220.37 tokens/sec
Step 4545 | loss: 3.408328 | lr:5.4405e-04 | norm 0.2583 | dt 338.95ms | 1546816.02 tokens/sec
Step 4546 | loss: 3.289880 | lr:5.4402e-04 | norm 0.2663 | dt 339.09ms | 1546166.73 tokens/sec
Step 4547 | loss: 3.361574 | lr:5.4400e-04 | norm 0.2537 | dt 338.85ms | 1547262.24 tokens/sec
Step 4548 | loss: 3.282736 | lr:5.4397e-04 | norm 0.2885 | dt 339.21ms | 1545608.14 tokens/sec
Step 4549 | loss: 3.222293 | lr:5.4394e-04 | norm 0.2686 | dt 338.92ms | 1546931.36 tokens/sec
Step 4550 | loss: 3.267153 | lr:5.4391e-04 | norm 0.2857 | dt 339.33ms | 1545051.04 tokens/sec
Step 4551 | loss: 3.261558 | lr:5.4388e-04 | norm 0.2966 | dt 339.49ms | 1544343.58 tokens/sec
Step 4552 | loss: 3.324714 | lr:5.4386e-04 | norm 0.2625 | dt 339.69ms | 1543424.41 tokens/sec
Step 4553 | loss: 3.157129 | lr:5.4383e-04 | norm 0.2912 | dt 339.75ms | 1543141.73 tokens/sec
Step 4554 | loss: 3.222042 | lr:5.4380e-04 | norm 0.3421 | dt 339.19ms | 1545727.65 tokens/sec
Step 4555 | loss: 3.155714 | lr:5.4377e-04 | norm 0.3413 | dt 339.37ms | 1544869.78 tokens/sec
Step 4556 | loss: 3.291004 | lr:5.4374e-04 | norm 0.3058 | dt 339.39ms | 1544815.51 tokens/sec
Step 4557 | loss: 3.235830 | lr:5.4371e-04 | norm 0.2890 | dt 339.24ms | 1545476.70 tokens/sec
Step 4558 | loss: 3.159762 | lr:5.4369e-04 | norm 0.3090 | dt 338.27ms | 1549927.55 tokens/sec
Step 4559 | loss: 3.188989 | lr:5.4366e-04 | norm 0.2850 | dt 996.63ms | 526059.97 tokens/sec
Step 4560 | loss: 3.377747 | lr:5.4363e-04 | norm 0.2878 | dt 336.07ms | 1560036.84 tokens/sec
Step 4561 | loss: 3.352308 | lr:5.4360e-04 | norm 0.2789 | dt 339.60ms | 1543822.08 tokens/sec
Step 4562 | loss: 3.334637 | lr:5.4357e-04 | norm 0.2819 | dt 340.32ms | 1540560.13 tokens/sec
Step 4563 | loss: 3.345229 | lr:5.4355e-04 | norm 0.2748 | dt 338.98ms | 1546660.44 tokens/sec
Step 4564 | loss: 3.431061 | lr:5.4352e-04 | norm 0.3049 | dt 339.18ms | 1545754.81 tokens/sec
Step 4565 | loss: 3.348085 | lr:5.4349e-04 | norm 0.2886 | dt 341.65ms | 1534581.63 tokens/sec
Step 4566 | loss: 3.373921 | lr:5.4346e-04 | norm 0.3253 | dt 339.17ms | 1545806.97 tokens/sec
Step 4567 | loss: 3.383396 | lr:5.4343e-04 | norm 0.3332 | dt 339.32ms | 1545099.90 tokens/sec
Step 4568 | loss: 3.350504 | lr:5.4340e-04 | norm 0.3359 | dt 339.12ms | 1546017.80 tokens/sec
Step 4569 | loss: 3.408162 | lr:5.4338e-04 | norm 0.3208 | dt 339.31ms | 1545136.81 tokens/sec
Step 4570 | loss: 3.289614 | lr:5.4335e-04 | norm 0.2798 | dt 339.62ms | 1543740.79 tokens/sec
Step 4571 | loss: 3.340162 | lr:5.4332e-04 | norm 0.2953 | dt 337.65ms | 1552778.56 tokens/sec
Step 4572 | loss: 3.340409 | lr:5.4329e-04 | norm 0.2819 | dt 338.84ms | 1547282.93 tokens/sec
Step 4573 | loss: 3.354538 | lr:5.4326e-04 | norm 0.2591 | dt 338.48ms | 1548964.62 tokens/sec
Step 4574 | loss: 3.282222 | lr:5.4323e-04 | norm 0.2891 | dt 337.82ms | 1551997.18 tokens/sec
Step 4575 | loss: 3.279210 | lr:5.4321e-04 | norm 0.2637 | dt 339.17ms | 1545778.72 tokens/sec
Step 4576 | loss: 3.306771 | lr:5.4318e-04 | norm 0.2697 | dt 339.00ms | 1546552.75 tokens/sec
Step 4577 | loss: 3.412289 | lr:5.4315e-04 | norm 0.2576 | dt 338.25ms | 1550007.30 tokens/sec
Step 4578 | loss: 3.352494 | lr:5.4312e-04 | norm 0.2959 | dt 337.93ms | 1551463.93 tokens/sec
Step 4579 | loss: 3.285343 | lr:5.4309e-04 | norm 0.3246 | dt 338.27ms | 1549895.87 tokens/sec
Step 4580 | loss: 3.367754 | lr:5.4306e-04 | norm 0.2643 | dt 339.04ms | 1546397.23 tokens/sec
Step 4581 | loss: 3.324765 | lr:5.4304e-04 | norm 0.2884 | dt 338.56ms | 1548569.76 tokens/sec
Step 4582 | loss: 3.391151 | lr:5.4301e-04 | norm 0.2812 | dt 338.87ms | 1547151.21 tokens/sec
Step 4583 | loss: 3.372480 | lr:5.4298e-04 | norm 0.2656 | dt 338.84ms | 1547301.44 tokens/sec
Step 4584 | loss: 3.300465 | lr:5.4295e-04 | norm 0.2565 | dt 338.32ms | 1549673.05 tokens/sec
Step 4585 | loss: 3.404125 | lr:5.4292e-04 | norm 0.3334 | dt 338.95ms | 1546812.75 tokens/sec
Step 4586 | loss: 3.361998 | lr:5.4289e-04 | norm 0.2960 | dt 338.97ms | 1546715.92 tokens/sec
Step 4587 | loss: 3.382052 | lr:5.4286e-04 | norm 0.3148 | dt 338.94ms | 1546851.92 tokens/sec
Step 4588 | loss: 3.358965 | lr:5.4284e-04 | norm 0.2898 | dt 338.26ms | 1549966.88 tokens/sec
Step 4589 | loss: 3.367648 | lr:5.4281e-04 | norm 0.3145 | dt 339.88ms | 1542589.65 tokens/sec
Step 4590 | loss: 3.280443 | lr:5.4278e-04 | norm 0.2967 | dt 340.72ms | 1538762.02 tokens/sec
Step 4591 | loss: 3.371942 | lr:5.4275e-04 | norm 0.3262 | dt 338.93ms | 1546912.86 tokens/sec
Step 4592 | loss: 3.358740 | lr:5.4272e-04 | norm 0.3009 | dt 337.62ms | 1552882.73 tokens/sec
Step 4593 | loss: 3.317818 | lr:5.4269e-04 | norm 0.2898 | dt 337.90ms | 1551618.28 tokens/sec
Step 4594 | loss: 3.457290 | lr:5.4267e-04 | norm 0.2770 | dt 338.76ms | 1547651.00 tokens/sec
Step 4595 | loss: 3.239309 | lr:5.4264e-04 | norm 0.3007 | dt 338.78ms | 1547567.13 tokens/sec
Step 4596 | loss: 3.260506 | lr:5.4261e-04 | norm 0.3280 | dt 338.00ms | 1551130.15 tokens/sec
Step 4597 | loss: 3.199728 | lr:5.4258e-04 | norm 0.3042 | dt 337.87ms | 1551723.39 tokens/sec
Step 4598 | loss: 3.262439 | lr:5.4255e-04 | norm 0.3016 | dt 338.20ms | 1550232.40 tokens/sec
Step 4599 | loss: 3.250726 | lr:5.4252e-04 | norm 0.2493 | dt 338.55ms | 1548631.92 tokens/sec
Step 4600 | loss: 3.217288 | lr:5.4249e-04 | norm 0.2618 | dt 341.65ms | 1534581.63 tokens/sec
Step 4601 | loss: 3.244498 | lr:5.4247e-04 | norm 0.2586 | dt 339.36ms | 1544930.56 tokens/sec
Step 4602 | loss: 3.233872 | lr:5.4244e-04 | norm 0.2673 | dt 340.52ms | 1539657.31 tokens/sec
Step 4603 | loss: 3.223317 | lr:5.4241e-04 | norm 0.2690 | dt 338.82ms | 1547400.52 tokens/sec
Step 4604 | loss: 3.230819 | lr:5.4238e-04 | norm 0.2734 | dt 338.79ms | 1547550.80 tokens/sec
Step 4605 | loss: 3.202812 | lr:5.4235e-04 | norm 0.2567 | dt 338.60ms | 1548377.85 tokens/sec
Step 4606 | loss: 3.302244 | lr:5.4232e-04 | norm 0.3050 | dt 338.49ms | 1548925.35 tokens/sec
Step 4607 | loss: 3.343994 | lr:5.4229e-04 | norm 0.3102 | dt 339.31ms | 1545171.55 tokens/sec
Step 4608 | loss: 3.428942 | lr:5.4227e-04 | norm 0.2998 | dt 338.91ms | 1546985.77 tokens/sec
Step 4609 | loss: 3.359793 | lr:5.4224e-04 | norm 0.2586 | dt 338.56ms | 1548583.93 tokens/sec
Step 4610 | loss: 3.398319 | lr:5.4221e-04 | norm 0.2820 | dt 339.69ms | 1543419.00 tokens/sec
Step 4611 | loss: 3.360882 | lr:5.4218e-04 | norm 0.2766 | dt 340.02ms | 1541939.58 tokens/sec
Step 4612 | loss: 3.351099 | lr:5.4215e-04 | norm 0.2674 | dt 338.71ms | 1547888.49 tokens/sec
Step 4613 | loss: 3.404150 | lr:5.4212e-04 | norm 0.2843 | dt 339.62ms | 1543750.55 tokens/sec
Step 4614 | loss: 3.367916 | lr:5.4209e-04 | norm 0.2801 | dt 339.01ms | 1546541.88 tokens/sec
Step 4615 | loss: 3.305683 | lr:5.4207e-04 | norm 0.2526 | dt 338.11ms | 1550654.35 tokens/sec
Step 4616 | loss: 3.398047 | lr:5.4204e-04 | norm 0.2781 | dt 338.90ms | 1547012.98 tokens/sec
Step 4617 | loss: 3.327978 | lr:5.4201e-04 | norm 0.2719 | dt 337.01ms | 1555697.30 tokens/sec
Step 4618 | loss: 3.327110 | lr:5.4198e-04 | norm 0.2590 | dt 339.07ms | 1546248.26 tokens/sec
Step 4619 | loss: 3.337296 | lr:5.4195e-04 | norm 0.2584 | dt 338.50ms | 1548867.53 tokens/sec
Step 4620 | loss: 3.338080 | lr:5.4192e-04 | norm 0.2726 | dt 336.95ms | 1555991.21 tokens/sec
Step 4621 | loss: 3.321102 | lr:5.4189e-04 | norm 0.3038 | dt 337.72ms | 1552411.33 tokens/sec
Step 4622 | loss: 3.290323 | lr:5.4187e-04 | norm 0.2847 | dt 338.28ms | 1549867.47 tokens/sec
Step 4623 | loss: 3.356363 | lr:5.4184e-04 | norm 0.2825 | dt 337.31ms | 1554333.79 tokens/sec
Step 4624 | loss: 3.337814 | lr:5.4181e-04 | norm 0.2925 | dt 337.66ms | 1552732.51 tokens/sec
Step 4625 | loss: 3.332610 | lr:5.4178e-04 | norm 0.2659 | dt 337.46ms | 1553618.89 tokens/sec
Step 4626 | loss: 3.274629 | lr:5.4175e-04 | norm 0.2548 | dt 337.67ms | 1552643.71 tokens/sec
Step 4627 | loss: 3.340471 | lr:5.4172e-04 | norm 0.2559 | dt 337.63ms | 1552852.02 tokens/sec
Step 4628 | loss: 3.348150 | lr:5.4169e-04 | norm 0.2971 | dt 337.74ms | 1552319.28 tokens/sec
Step 4629 | loss: 3.376559 | lr:5.4167e-04 | norm 0.2605 | dt 337.92ms | 1551503.34 tokens/sec
Step 4630 | loss: 3.352247 | lr:5.4164e-04 | norm 0.2462 | dt 337.93ms | 1551460.65 tokens/sec
Step 4631 | loss: 3.337674 | lr:5.4161e-04 | norm 0.2814 | dt 338.27ms | 1549915.53 tokens/sec
Step 4632 | loss: 3.302609 | lr:5.4158e-04 | norm 0.2555 | dt 337.86ms | 1551782.52 tokens/sec
Step 4633 | loss: 3.306339 | lr:5.4155e-04 | norm 0.2763 | dt 336.80ms | 1556652.09 tokens/sec
Step 4634 | loss: 3.371871 | lr:5.4152e-04 | norm 0.2734 | dt 337.93ms | 1551482.54 tokens/sec
Step 4635 | loss: 3.374666 | lr:5.4149e-04 | norm 0.2719 | dt 339.40ms | 1544748.23 tokens/sec
Step 4636 | loss: 3.339927 | lr:5.4146e-04 | norm 0.2610 | dt 337.92ms | 1551533.99 tokens/sec
Step 4637 | loss: 3.361758 | lr:5.4144e-04 | norm 0.2945 | dt 337.84ms | 1551873.42 tokens/sec
Step 4638 | loss: 3.370023 | lr:5.4141e-04 | norm 0.2982 | dt 339.21ms | 1545619.00 tokens/sec
Step 4639 | loss: 3.348808 | lr:5.4138e-04 | norm 0.3138 | dt 338.61ms | 1548353.87 tokens/sec
Step 4640 | loss: 3.341802 | lr:5.4135e-04 | norm 0.2832 | dt 337.92ms | 1551495.68 tokens/sec
Step 4641 | loss: 3.228055 | lr:5.4132e-04 | norm 0.2914 | dt 338.76ms | 1547661.89 tokens/sec
Step 4642 | loss: 3.146403 | lr:5.4129e-04 | norm 0.3428 | dt 338.17ms | 1550358.08 tokens/sec
Step 4643 | loss: 3.194883 | lr:5.4126e-04 | norm 0.3059 | dt 338.23ms | 1550085.97 tokens/sec
Step 4644 | loss: 3.192198 | lr:5.4123e-04 | norm 0.3147 | dt 339.17ms | 1545793.93 tokens/sec
Step 4645 | loss: 3.211620 | lr:5.4121e-04 | norm 0.3064 | dt 337.67ms | 1552670.02 tokens/sec
Step 4646 | loss: 3.223922 | lr:5.4118e-04 | norm 0.2891 | dt 338.12ms | 1550618.27 tokens/sec
Step 4647 | loss: 3.222179 | lr:5.4115e-04 | norm 0.2665 | dt 338.81ms | 1547449.52 tokens/sec
Step 4648 | loss: 3.234506 | lr:5.4112e-04 | norm 0.2533 | dt 338.61ms | 1548350.59 tokens/sec
Step 4649 | loss: 3.211034 | lr:5.4109e-04 | norm 0.2838 | dt 338.47ms | 1548992.99 tokens/sec
Step 4650 | loss: 3.271107 | lr:5.4106e-04 | norm 0.2715 | dt 337.64ms | 1552811.45 tokens/sec
Step 4651 | loss: 3.230725 | lr:5.4103e-04 | norm 0.2754 | dt 338.42ms | 1549218.89 tokens/sec
Step 4652 | loss: 3.094175 | lr:5.4100e-04 | norm 0.2724 | dt 338.16ms | 1550428.04 tokens/sec
Step 4653 | loss: 3.345617 | lr:5.4098e-04 | norm 0.2983 | dt 338.28ms | 1549857.64 tokens/sec
Step 4654 | loss: 3.349851 | lr:5.4095e-04 | norm 0.2875 | dt 338.61ms | 1548346.23 tokens/sec
Step 4655 | loss: 3.374214 | lr:5.4092e-04 | norm 0.2763 | dt 338.28ms | 1549864.19 tokens/sec
Step 4656 | loss: 3.373020 | lr:5.4089e-04 | norm 0.2954 | dt 338.09ms | 1550749.49 tokens/sec
Step 4657 | loss: 3.356784 | lr:5.4086e-04 | norm 0.3091 | dt 338.29ms | 1549816.13 tokens/sec
Step 4658 | loss: 3.364819 | lr:5.4083e-04 | norm 0.3211 | dt 337.87ms | 1551742.01 tokens/sec
Step 4659 | loss: 3.362827 | lr:5.4080e-04 | norm 0.2946 | dt 338.19ms | 1550293.60 tokens/sec
Step 4660 | loss: 3.357070 | lr:5.4077e-04 | norm 0.2989 | dt 338.82ms | 1547400.52 tokens/sec
Step 4661 | loss: 3.435080 | lr:5.4074e-04 | norm 0.3227 | dt 337.56ms | 1553149.25 tokens/sec
Step 4662 | loss: 3.357257 | lr:5.4072e-04 | norm 0.3356 | dt 338.09ms | 1550740.74 tokens/sec
Step 4663 | loss: 3.270846 | lr:5.4069e-04 | norm 0.3019 | dt 338.17ms | 1550387.60 tokens/sec
Step 4664 | loss: 3.272062 | lr:5.4066e-04 | norm 0.2911 | dt 338.28ms | 1549857.64 tokens/sec
Step 4665 | loss: 3.301692 | lr:5.4063e-04 | norm 0.2768 | dt 337.53ms | 1553312.71 tokens/sec
Step 4666 | loss: 3.302365 | lr:5.4060e-04 | norm 0.2890 | dt 338.16ms | 1550426.95 tokens/sec
Step 4667 | loss: 3.327818 | lr:5.4057e-04 | norm 0.2524 | dt 338.16ms | 1550394.16 tokens/sec
Step 4668 | loss: 3.297179 | lr:5.4054e-04 | norm 0.2749 | dt 338.18ms | 1550335.13 tokens/sec
Step 4669 | loss: 3.306214 | lr:5.4051e-04 | norm 0.2585 | dt 337.63ms | 1552867.38 tokens/sec
Step 4670 | loss: 3.310043 | lr:5.4048e-04 | norm 0.2594 | dt 337.76ms | 1552228.34 tokens/sec
Step 4671 | loss: 3.314866 | lr:5.4046e-04 | norm 0.2589 | dt 338.54ms | 1548672.27 tokens/sec
Step 4672 | loss: 3.350688 | lr:5.4043e-04 | norm 0.2414 | dt 337.57ms | 1553117.44 tokens/sec
Step 4673 | loss: 3.313751 | lr:5.4040e-04 | norm 0.2547 | dt 338.15ms | 1550455.37 tokens/sec
Step 4674 | loss: 3.313412 | lr:5.4037e-04 | norm 0.2398 | dt 339.47ms | 1544417.33 tokens/sec
Step 4675 | loss: 3.376035 | lr:5.4034e-04 | norm 0.2700 | dt 342.93ms | 1528833.22 tokens/sec
Step 4676 | loss: 3.290534 | lr:5.4031e-04 | norm 0.2469 | dt 338.96ms | 1546774.67 tokens/sec
Step 4677 | loss: 3.376810 | lr:5.4028e-04 | norm 0.2448 | dt 340.71ms | 1538813.70 tokens/sec
Step 4678 | loss: 3.331411 | lr:5.4025e-04 | norm 0.2803 | dt 339.13ms | 1545990.63 tokens/sec
Step 4679 | loss: 3.272608 | lr:5.4022e-04 | norm 0.3272 | dt 338.66ms | 1548117.33 tokens/sec
Step 4680 | loss: 3.321239 | lr:5.4019e-04 | norm 0.2886 | dt 339.62ms | 1543751.63 tokens/sec
Step 4681 | loss: 3.351338 | lr:5.4017e-04 | norm 0.3043 | dt 339.44ms | 1544554.02 tokens/sec
Step 4682 | loss: 3.394250 | lr:5.4014e-04 | norm 0.2770 | dt 339.84ms | 1542769.31 tokens/sec
Step 4683 | loss: 3.337341 | lr:5.4011e-04 | norm 0.2726 | dt 339.50ms | 1544300.20 tokens/sec
Step 4684 | loss: 3.358078 | lr:5.4008e-04 | norm 0.2740 | dt 339.07ms | 1546231.96 tokens/sec
Step 4685 | loss: 3.332182 | lr:5.4005e-04 | norm 0.2833 | dt 339.09ms | 1546158.03 tokens/sec
Step 4686 | loss: 3.300769 | lr:5.4002e-04 | norm 0.2662 | dt 338.96ms | 1546764.88 tokens/sec
Step 4687 | loss: 3.315288 | lr:5.3999e-04 | norm 0.2814 | dt 338.84ms | 1547303.61 tokens/sec
Step 4688 | loss: 3.275258 | lr:5.3996e-04 | norm 0.2741 | dt 338.92ms | 1546957.48 tokens/sec
Step 4689 | loss: 3.254157 | lr:5.3993e-04 | norm 0.2786 | dt 339.90ms | 1542473.88 tokens/sec
Step 4690 | loss: 3.262724 | lr:5.3990e-04 | norm 0.3305 | dt 339.07ms | 1546259.14 tokens/sec
Step 4691 | loss: 3.172642 | lr:5.3987e-04 | norm 0.2996 | dt 338.01ms | 1551120.30 tokens/sec
Step 4692 | loss: 3.259367 | lr:5.3985e-04 | norm 0.3044 | dt 338.67ms | 1548067.19 tokens/sec
Step 4693 | loss: 3.254672 | lr:5.3982e-04 | norm 0.3100 | dt 339.69ms | 1543425.49 tokens/sec
Step 4694 | loss: 3.256648 | lr:5.3979e-04 | norm 0.3094 | dt 338.20ms | 1550224.75 tokens/sec
Step 4695 | loss: 3.303372 | lr:5.3976e-04 | norm 0.3216 | dt 338.15ms | 1550476.14 tokens/sec
Step 4696 | loss: 3.250897 | lr:5.3973e-04 | norm 0.3028 | dt 337.90ms | 1551585.44 tokens/sec
Step 4697 | loss: 3.238538 | lr:5.3970e-04 | norm 0.2639 | dt 337.74ms | 1552342.29 tokens/sec
Step 4698 | loss: 3.173235 | lr:5.3967e-04 | norm 0.2970 | dt 338.71ms | 1547907.01 tokens/sec
Step 4699 | loss: 3.309394 | lr:5.3964e-04 | norm 0.3179 | dt 338.65ms | 1548160.92 tokens/sec
Step 4700 | loss: 3.341858 | lr:5.3961e-04 | norm 0.3172 | dt 338.25ms | 1550013.85 tokens/sec
Step 4701 | loss: 3.415939 | lr:5.3958e-04 | norm 0.2978 | dt 337.78ms | 1552139.59 tokens/sec
Step 4702 | loss: 3.341871 | lr:5.3955e-04 | norm 0.2667 | dt 338.66ms | 1548131.49 tokens/sec
Step 4703 | loss: 3.430399 | lr:5.3953e-04 | norm 0.2751 | dt 337.34ms | 1554201.96 tokens/sec
Step 4704 | loss: 3.422009 | lr:5.3950e-04 | norm 0.2760 | dt 338.20ms | 1550231.30 tokens/sec
Step 4705 | loss: 3.437056 | lr:5.3947e-04 | norm 0.2994 | dt 337.94ms | 1551423.43 tokens/sec
Step 4706 | loss: 3.401726 | lr:5.3944e-04 | norm 0.3312 | dt 337.48ms | 1553549.75 tokens/sec
Step 4707 | loss: 3.386001 | lr:5.3941e-04 | norm 0.3127 | dt 339.71ms | 1543348.59 tokens/sec
Step 4708 | loss: 3.356349 | lr:5.3938e-04 | norm 0.2819 | dt 338.09ms | 1550750.58 tokens/sec
Step 4709 | loss: 3.241512 | lr:5.3935e-04 | norm 0.3128 | dt 337.67ms | 1552670.02 tokens/sec
Step 4710 | loss: 3.277980 | lr:5.3932e-04 | norm 0.2984 | dt 337.86ms | 1551773.76 tokens/sec
Step 4711 | loss: 3.327609 | lr:5.3929e-04 | norm 0.2896 | dt 338.62ms | 1548291.73 tokens/sec
Step 4712 | loss: 3.370357 | lr:5.3926e-04 | norm 0.2613 | dt 340.26ms | 1540823.51 tokens/sec
Step 4713 | loss: 3.284583 | lr:5.3923e-04 | norm 0.2555 | dt 338.33ms | 1549623.91 tokens/sec
Step 4714 | loss: 3.376203 | lr:5.3920e-04 | norm 0.2664 | dt 337.75ms | 1552285.31 tokens/sec
Step 4715 | loss: 3.369921 | lr:5.3918e-04 | norm 0.2824 | dt 337.85ms | 1551846.04 tokens/sec
Step 4716 | loss: 3.290584 | lr:5.3915e-04 | norm 0.2765 | dt 338.06ms | 1550853.39 tokens/sec
Step 4717 | loss: 3.266913 | lr:5.3912e-04 | norm 0.2904 | dt 337.84ms | 1551863.56 tokens/sec
Step 4718 | loss: 3.362687 | lr:5.3909e-04 | norm 0.2800 | dt 338.19ms | 1550282.67 tokens/sec
Step 4719 | loss: 3.291820 | lr:5.3906e-04 | norm 0.2601 | dt 338.74ms | 1547759.93 tokens/sec
Step 4720 | loss: 3.371178 | lr:5.3903e-04 | norm 0.2658 | dt 337.43ms | 1553753.92 tokens/sec
Step 4721 | loss: 3.349008 | lr:5.3900e-04 | norm 0.2513 | dt 338.60ms | 1548377.85 tokens/sec
Step 4722 | loss: 3.365546 | lr:5.3897e-04 | norm 0.2614 | dt 337.82ms | 1551993.90 tokens/sec
Step 4723 | loss: 3.308917 | lr:5.3894e-04 | norm 0.2624 | dt 339.25ms | 1545412.62 tokens/sec
Step 4724 | loss: 3.341763 | lr:5.3891e-04 | norm 0.2694 | dt 909.65ms | 576362.28 tokens/sec
Step 4725 | loss: 3.345835 | lr:5.3888e-04 | norm 0.2970 | dt 337.69ms | 1552565.88 tokens/sec
Step 4726 | loss: 3.337051 | lr:5.3885e-04 | norm 0.2919 | dt 339.35ms | 1544956.61 tokens/sec
Step 4727 | loss: 3.362989 | lr:5.3882e-04 | norm 0.2659 | dt 338.81ms | 1547416.85 tokens/sec
Step 4728 | loss: 3.345090 | lr:5.3880e-04 | norm 0.2690 | dt 338.47ms | 1549011.54 tokens/sec
Step 4729 | loss: 3.325761 | lr:5.3877e-04 | norm 0.2625 | dt 338.45ms | 1549078.10 tokens/sec
Step 4730 | loss: 3.363222 | lr:5.3874e-04 | norm 0.2443 | dt 339.89ms | 1542529.06 tokens/sec
Step 4731 | loss: 3.357156 | lr:5.3871e-04 | norm 0.2547 | dt 338.55ms | 1548626.47 tokens/sec
Step 4732 | loss: 3.367498 | lr:5.3868e-04 | norm 0.2614 | dt 337.76ms | 1552265.59 tokens/sec
Step 4733 | loss: 3.222863 | lr:5.3865e-04 | norm 0.2536 | dt 337.53ms | 1553323.69 tokens/sec
Step 4734 | loss: 3.212686 | lr:5.3862e-04 | norm 0.2937 | dt 338.41ms | 1549259.27 tokens/sec
Step 4735 | loss: 3.248691 | lr:5.3859e-04 | norm 0.2724 | dt 338.13ms | 1550559.23 tokens/sec
Step 4736 | loss: 3.253656 | lr:5.3856e-04 | norm 0.2674 | dt 337.50ms | 1553432.32 tokens/sec
Step 4737 | loss: 3.263209 | lr:5.3853e-04 | norm 0.2541 | dt 337.65ms | 1552774.17 tokens/sec
Step 4738 | loss: 3.227526 | lr:5.3850e-04 | norm 0.2729 | dt 338.06ms | 1550891.67 tokens/sec
Step 4739 | loss: 3.213420 | lr:5.3847e-04 | norm 0.2791 | dt 338.02ms | 1551060.13 tokens/sec
Step 4740 | loss: 3.203285 | lr:5.3844e-04 | norm 0.2642 | dt 337.40ms | 1553898.84 tokens/sec
Step 4741 | loss: 3.219006 | lr:5.3841e-04 | norm 0.2681 | dt 337.81ms | 1552030.04 tokens/sec
Step 4742 | loss: 3.210974 | lr:5.3838e-04 | norm 0.2791 | dt 338.58ms | 1548472.71 tokens/sec
Step 4743 | loss: 3.180146 | lr:5.3836e-04 | norm 0.2634 | dt 337.44ms | 1553713.30 tokens/sec
Step 4744 | loss: 3.186468 | lr:5.3833e-04 | norm 0.2545 | dt 337.69ms | 1552594.38 tokens/sec
Step 4745 | loss: 3.349077 | lr:5.3830e-04 | norm 0.2538 | dt 339.48ms | 1544363.10 tokens/sec
Step 4746 | loss: 3.347820 | lr:5.3827e-04 | norm 0.2976 | dt 338.75ms | 1547734.87 tokens/sec
Step 4747 | loss: 3.401863 | lr:5.3824e-04 | norm 0.3103 | dt 339.19ms | 1545714.61 tokens/sec
Step 4748 | loss: 3.349016 | lr:5.3821e-04 | norm 0.2813 | dt 338.99ms | 1546612.58 tokens/sec
Step 4749 | loss: 3.386333 | lr:5.3818e-04 | norm 0.2454 | dt 1002.28ms | 523096.46 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 4750: 3.3731
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2788/10042=0.2776


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not an engineer, but I am an engineer.
Since everything you are looking at the most important piece
rank 3 sample 1 >Hello, I'm a language model, so this post is going to get you a great list of the most popular languages.
# language model # lang

rank 3 sample 2 >Hello, I'm a language model, so you need to apply the data type information first to your current data. This process is called ‘data transfer.
rank 3 sample 3 >Hello, I'm a language model, so how do I know that there is no one that knows the way that I thought? Because I already have a lot




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this one is the most important part of my book, and there are a number of other ways to get this topic
rank 5 sample 1 >Hello, I'm a language model, there you could learn languages with my kids. Or you could make our language classes as a class, just do the same
rank 5 sample 2 >Hello, I'm a language model, and you'd love to hear! Here's a link to the entire post. I was a keynote speaker at the National


rank 5 sample 3 >Hello, I'm a language model, how about how can I write my own code? You've been learning to break out a program after reading the instructions first
ddp_rank 2: ####### Printing generated samples ####### 



rank 2 sample 0 >Hello, I'm a language model, yet the most accurate way to do this would be to have my students participate. In this case, I wanted to do
rank 2 sample 1 >Hello, I'm a language model, but I used this. I'm doing fine with both languages and not as much as I can with this language.<|endoftext|>
rank 2 sample 2 >Hello, I'm a language model, and I think I can use the rest of that language to understand how he does the grammar of what I do. And
rank 2 sample 3 >Hello, I'm a language model, but not of the world, so I must also use it's own language," he says. Even though I've used




ddp_rank 1: ####### Printing generated samples ####### 



ddp_rank 7: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so here I'm speaking in a group of about 30 folks so I'm happy to have my way.<|endoftext|>Vitamin


ddp_rank 4: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, a model for that.
- If you want to run a version of Python by making the result in Python, you
rank 7 sample 0 >Hello, I'm a language model, so I'm going to write a program first. I'm going to set up different programs which won't work.

rank 1 sample 2 >Hello, I'm a language model, but still, I'm not a language model. I can find it, but I'm not a language modeler,
rank 7 sample 1 >Hello, I'm a language model, however I can't imagine a perfect, complete, world...
It's a natural part of the curriculum that we can
rank 1 sample 3 >Hello, I'm a language model, so I'm thinking of an approach of understanding the language - where every bit makes sense rather than having to choose from a


rank 4 sample 0 >Hello, I'm a language model, and I don't think anyone can understand him, and I've never said something like a grammar essay! He has been
rank 7 sample 2 >Hello, I'm a language model, but I don't think it's correct because I believe that the most correct way to do it is to make sense of
rank 7 sample 3 >Hello, I'm a language model, is that good? So yes, your kids and I teach you, if you are an English teacher, you teach French


rank 4 sample 1 >Hello, I'm a language model, or at least modelers can think about their language as 'language models' - you'll find the right kind of model
rank 4 sample 2 >Hello, I'm a language model, I wish to help me build a program for the future. If I'm not on your hands, so I need a
rank 4 sample 3 >Hello, I'm a language model, so it all gets easier with the model and we now really try and really make it work for him, so his work




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, that was a part of my learning for a while. But, with that realization, I finally realized that the language model
rank 6 sample 1 >Hello, I'm a language model, which is an open source project with a small userbase. I'm using the OpenSSL project with a big impact on
rank 6 sample 2 >Hello, I'm a language model, but that's a really nice one.
What I'm really proud of now is that I'm one of three languages
rank 6 sample 3 >Hello, I'm a language model, so here we can find the base language, the base language, the base protocol. We try to talk, see me




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I need to understand something, like to teach, that I should "speak" a language or not, and it
rank 0 sample 1 >Hello, I'm a language model, but how do I make it? Now, you only need a basic data base, which is the point of the base
rank 0 sample 2 >Hello, I'm a language model, and I thought this was useful for some of that.
- It's a way of looking at the world. Is
rank 0 sample 3 >Hello, I'm a language model, but so far I've had no idea where its going to be. The only "other" thing being said here isn


Step 4750 | loss: 3.286444 | lr:5.3815e-04 | norm 0.2651 | dt 12351.02ms | 42448.98 tokens/sec
Step 4751 | loss: 3.424607 | lr:5.3812e-04 | norm 0.2897 | dt 336.61ms | 1557549.58 tokens/sec
Step 4752 | loss: 3.299479 | lr:5.3809e-04 | norm 0.2849 | dt 337.38ms | 1553982.30 tokens/sec
Step 4753 | loss: 3.375385 | lr:5.3806e-04 | norm 0.2578 | dt 336.42ms | 1558441.47 tokens/sec
Step 4754 | loss: 3.355988 | lr:5.3803e-04 | norm 0.2398 | dt 335.69ms | 1561838.46 tokens/sec
Step 4755 | loss: 3.293760 | lr:5.3800e-04 | norm 0.2725 | dt 338.56ms | 1548561.03 tokens/sec
Step 4756 | loss: 3.263711 | lr:5.3797e-04 | norm 0.2656 | dt 337.69ms | 1552583.41 tokens/sec
Step 4757 | loss: 3.264358 | lr:5.3794e-04 | norm 0.2979 | dt 337.73ms | 1552393.80 tokens/sec
Step 4758 | loss: 3.341059 | lr:5.3791e-04 | norm 0.2959 | dt 336.79ms | 1556700.58 tokens/sec
Step 4759 | loss: 3.317364 | lr:5.3788e-04 | norm 0.2666 | dt 338.11ms | 1550623.74 tokens/sec
Step 4760 | loss: 3.343584 | lr:5.3785e-04 | norm 0.2667 | dt 336.69ms | 1557191.12 tokens/sec
Step 4761 | loss: 3.309097 | lr:5.3783e-04 | norm 0.2779 | dt 336.10ms | 1559917.33 tokens/sec
Step 4762 | loss: 3.289651 | lr:5.3780e-04 | norm 0.3056 | dt 336.80ms | 1556680.74 tokens/sec
Step 4763 | loss: 3.322813 | lr:5.3777e-04 | norm 0.2623 | dt 337.84ms | 1551874.51 tokens/sec
Step 4764 | loss: 3.296900 | lr:5.3774e-04 | norm 0.2898 | dt 338.23ms | 1550103.45 tokens/sec
Step 4765 | loss: 3.335892 | lr:5.3771e-04 | norm 0.2990 | dt 336.65ms | 1557374.19 tokens/sec
Step 4766 | loss: 3.358228 | lr:5.3768e-04 | norm 0.2799 | dt 338.37ms | 1549474.32 tokens/sec
Step 4767 | loss: 3.414359 | lr:5.3765e-04 | norm 0.3125 | dt 337.84ms | 1551864.66 tokens/sec
Step 4768 | loss: 3.301071 | lr:5.3762e-04 | norm 0.3258 | dt 336.82ms | 1556565.04 tokens/sec
Step 4769 | loss: 3.370309 | lr:5.3759e-04 | norm 0.3266 | dt 337.97ms | 1551285.53 tokens/sec
Step 4770 | loss: 3.318652 | lr:5.3756e-04 | norm 0.3361 | dt 340.09ms | 1541605.56 tokens/sec
Step 4771 | loss: 3.372369 | lr:5.3753e-04 | norm 0.3220 | dt 337.78ms | 1552164.79 tokens/sec
Step 4772 | loss: 3.348787 | lr:5.3750e-04 | norm 0.3125 | dt 337.75ms | 1552290.79 tokens/sec
Step 4773 | loss: 3.341420 | lr:5.3747e-04 | norm 0.3077 | dt 337.85ms | 1551829.61 tokens/sec
Step 4774 | loss: 3.335923 | lr:5.3744e-04 | norm 0.2691 | dt 337.49ms | 1553493.77 tokens/sec
Step 4775 | loss: 3.300716 | lr:5.3741e-04 | norm 0.3063 | dt 338.36ms | 1549520.18 tokens/sec
Step 4776 | loss: 3.400679 | lr:5.3738e-04 | norm 0.3136 | dt 337.89ms | 1551660.98 tokens/sec
Step 4777 | loss: 3.351252 | lr:5.3735e-04 | norm 0.3210 | dt 337.70ms | 1552545.05 tokens/sec
Step 4778 | loss: 3.321202 | lr:5.3732e-04 | norm 0.3104 | dt 337.84ms | 1551901.89 tokens/sec
Step 4779 | loss: 3.361290 | lr:5.3729e-04 | norm 0.3179 | dt 338.44ms | 1549108.66 tokens/sec
Step 4780 | loss: 3.196148 | lr:5.3726e-04 | norm 0.2954 | dt 337.75ms | 1552318.19 tokens/sec
Step 4781 | loss: 3.300399 | lr:5.3723e-04 | norm 0.3358 | dt 338.17ms | 1550389.78 tokens/sec
Step 4782 | loss: 3.175617 | lr:5.3720e-04 | norm 0.2848 | dt 337.88ms | 1551716.82 tokens/sec
Step 4783 | loss: 3.209338 | lr:5.3717e-04 | norm 0.2663 | dt 338.00ms | 1551154.22 tokens/sec
Step 4784 | loss: 3.168192 | lr:5.3715e-04 | norm 0.2675 | dt 337.62ms | 1552893.69 tokens/sec
Step 4785 | loss: 3.185776 | lr:5.3712e-04 | norm 0.2808 | dt 338.41ms | 1549287.65 tokens/sec
Step 4786 | loss: 3.261886 | lr:5.3709e-04 | norm 0.2602 | dt 339.39ms | 1544787.30 tokens/sec
Step 4787 | loss: 3.242284 | lr:5.3706e-04 | norm 0.2627 | dt 338.11ms | 1550656.54 tokens/sec
Step 4788 | loss: 3.231156 | lr:5.3703e-04 | norm 0.2546 | dt 338.55ms | 1548637.37 tokens/sec
Step 4789 | loss: 3.293034 | lr:5.3700e-04 | norm 0.2503 | dt 337.85ms | 1551854.80 tokens/sec
Step 4790 | loss: 3.171332 | lr:5.3697e-04 | norm 0.2521 | dt 339.74ms | 1543221.86 tokens/sec
Step 4791 | loss: 3.253803 | lr:5.3694e-04 | norm 0.2644 | dt 338.55ms | 1548615.56 tokens/sec
Step 4792 | loss: 3.348988 | lr:5.3691e-04 | norm 0.2686 | dt 337.76ms | 1552252.44 tokens/sec
Step 4793 | loss: 3.384949 | lr:5.3688e-04 | norm 0.2612 | dt 339.73ms | 1543261.94 tokens/sec
Step 4794 | loss: 3.459227 | lr:5.3685e-04 | norm 0.2896 | dt 338.28ms | 1549853.27 tokens/sec
Step 4795 | loss: 3.340542 | lr:5.3682e-04 | norm 0.3468 | dt 338.79ms | 1547534.46 tokens/sec
Step 4796 | loss: 3.456837 | lr:5.3679e-04 | norm 0.4668 | dt 339.34ms | 1545029.33 tokens/sec
Step 4797 | loss: 3.303959 | lr:5.3676e-04 | norm 0.3856 | dt 339.42ms | 1544638.64 tokens/sec
Step 4798 | loss: 3.362149 | lr:5.3673e-04 | norm 0.3459 | dt 339.07ms | 1546267.84 tokens/sec
Step 4799 | loss: 3.335339 | lr:5.3670e-04 | norm 0.3162 | dt 339.98ms | 1542110.43 tokens/sec
Step 4800 | loss: 3.347162 | lr:5.3667e-04 | norm 0.2892 | dt 339.85ms | 1542694.63 tokens/sec
Step 4801 | loss: 3.324310 | lr:5.3664e-04 | norm 0.2921 | dt 342.08ms | 1532626.52 tokens/sec
Step 4802 | loss: 3.326952 | lr:5.3661e-04 | norm 0.3110 | dt 340.04ms | 1541857.41 tokens/sec
Step 4803 | loss: 3.296021 | lr:5.3658e-04 | norm 0.2785 | dt 339.80ms | 1542913.27 tokens/sec
Step 4804 | loss: 3.349471 | lr:5.3655e-04 | norm 0.2956 | dt 339.57ms | 1543990.09 tokens/sec
Step 4805 | loss: 3.334864 | lr:5.3652e-04 | norm 0.2922 | dt 339.98ms | 1542126.65 tokens/sec
Step 4806 | loss: 3.308614 | lr:5.3649e-04 | norm 0.2775 | dt 337.95ms | 1551396.07 tokens/sec
Step 4807 | loss: 3.350536 | lr:5.3646e-04 | norm 0.2523 | dt 338.43ms | 1549169.77 tokens/sec
Step 4808 | loss: 3.231883 | lr:5.3643e-04 | norm 0.2844 | dt 338.21ms | 1550176.66 tokens/sec
Step 4809 | loss: 3.301558 | lr:5.3640e-04 | norm 0.2872 | dt 337.82ms | 1551968.70 tokens/sec
Step 4810 | loss: 3.279722 | lr:5.3637e-04 | norm 0.2874 | dt 338.01ms | 1551092.95 tokens/sec
Step 4811 | loss: 3.288829 | lr:5.3634e-04 | norm 0.2665 | dt 338.97ms | 1546686.55 tokens/sec
Step 4812 | loss: 3.324923 | lr:5.3631e-04 | norm 0.2598 | dt 337.89ms | 1551645.66 tokens/sec
Step 4813 | loss: 3.363054 | lr:5.3628e-04 | norm 0.2742 | dt 339.11ms | 1546059.11 tokens/sec
Step 4814 | loss: 3.424205 | lr:5.3625e-04 | norm 0.2863 | dt 337.21ms | 1554762.38 tokens/sec
Step 4815 | loss: 3.306428 | lr:5.3622e-04 | norm 0.2736 | dt 338.21ms | 1550196.33 tokens/sec
Step 4816 | loss: 3.367255 | lr:5.3619e-04 | norm 0.2621 | dt 337.54ms | 1553240.30 tokens/sec
Step 4817 | loss: 3.382847 | lr:5.3616e-04 | norm 0.2590 | dt 337.64ms | 1552815.84 tokens/sec
Step 4818 | loss: 3.409061 | lr:5.3613e-04 | norm 0.2888 | dt 338.41ms | 1549246.17 tokens/sec
Step 4819 | loss: 3.347129 | lr:5.3610e-04 | norm 0.2915 | dt 338.23ms | 1550093.61 tokens/sec
Step 4820 | loss: 3.323299 | lr:5.3607e-04 | norm 0.2565 | dt 337.64ms | 1552815.84 tokens/sec
Step 4821 | loss: 3.356187 | lr:5.3604e-04 | norm 0.2507 | dt 337.57ms | 1553132.79 tokens/sec
Step 4822 | loss: 3.353940 | lr:5.3601e-04 | norm 0.2541 | dt 337.69ms | 1552589.99 tokens/sec
Step 4823 | loss: 3.301118 | lr:5.3598e-04 | norm 0.2555 | dt 338.52ms | 1548785.71 tokens/sec
Step 4824 | loss: 3.358917 | lr:5.3595e-04 | norm 0.2729 | dt 339.53ms | 1544135.37 tokens/sec
Step 4825 | loss: 3.342552 | lr:5.3592e-04 | norm 0.2478 | dt 337.57ms | 1553118.53 tokens/sec
Step 4826 | loss: 3.278497 | lr:5.3590e-04 | norm 0.2615 | dt 339.24ms | 1545456.07 tokens/sec
Step 4827 | loss: 3.189009 | lr:5.3587e-04 | norm 0.2716 | dt 337.48ms | 1553557.43 tokens/sec
Step 4828 | loss: 3.247896 | lr:5.3584e-04 | norm 0.2655 | dt 338.82ms | 1547413.58 tokens/sec
Step 4829 | loss: 3.177722 | lr:5.3581e-04 | norm 0.2716 | dt 337.65ms | 1552764.30 tokens/sec
Step 4830 | loss: 3.235960 | lr:5.3578e-04 | norm 0.2643 | dt 337.02ms | 1555637.88 tokens/sec
Step 4831 | loss: 3.229127 | lr:5.3575e-04 | norm 0.2674 | dt 338.65ms | 1548180.54 tokens/sec
Step 4832 | loss: 3.134601 | lr:5.3572e-04 | norm 0.2975 | dt 338.62ms | 1548326.61 tokens/sec
Step 4833 | loss: 3.194279 | lr:5.3569e-04 | norm 0.3133 | dt 338.87ms | 1547175.16 tokens/sec
Step 4834 | loss: 3.203076 | lr:5.3566e-04 | norm 0.2718 | dt 337.89ms | 1551670.84 tokens/sec
Step 4835 | loss: 3.210440 | lr:5.3563e-04 | norm 0.2596 | dt 338.95ms | 1546795.34 tokens/sec
Step 4836 | loss: 3.168958 | lr:5.3560e-04 | norm 0.2550 | dt 338.87ms | 1547153.38 tokens/sec
Step 4837 | loss: 3.156249 | lr:5.3557e-04 | norm 0.2885 | dt 337.74ms | 1552328.05 tokens/sec
Step 4838 | loss: 3.240637 | lr:5.3554e-04 | norm 0.2625 | dt 339.00ms | 1546584.30 tokens/sec
Step 4839 | loss: 3.374226 | lr:5.3551e-04 | norm 0.2623 | dt 339.53ms | 1544165.73 tokens/sec
Step 4840 | loss: 3.340305 | lr:5.3548e-04 | norm 0.2851 | dt 339.48ms | 1544375.03 tokens/sec
Step 4841 | loss: 3.344320 | lr:5.3545e-04 | norm 0.3030 | dt 339.54ms | 1544097.42 tokens/sec
Step 4842 | loss: 3.361600 | lr:5.3542e-04 | norm 0.2965 | dt 338.56ms | 1548589.39 tokens/sec
Step 4843 | loss: 3.380911 | lr:5.3539e-04 | norm 0.2870 | dt 338.58ms | 1548473.80 tokens/sec
Step 4844 | loss: 3.292512 | lr:5.3536e-04 | norm 0.2614 | dt 339.14ms | 1545936.29 tokens/sec
Step 4845 | loss: 3.323164 | lr:5.3533e-04 | norm 0.2861 | dt 339.25ms | 1545418.05 tokens/sec
Step 4846 | loss: 3.385063 | lr:5.3530e-04 | norm 0.3061 | dt 339.52ms | 1544187.42 tokens/sec
Step 4847 | loss: 3.374906 | lr:5.3527e-04 | norm 0.2933 | dt 338.15ms | 1550440.07 tokens/sec
Step 4848 | loss: 3.313147 | lr:5.3524e-04 | norm 0.2697 | dt 339.12ms | 1546035.19 tokens/sec
Step 4849 | loss: 3.272491 | lr:5.3521e-04 | norm 0.2981 | dt 339.84ms | 1542756.32 tokens/sec
Step 4850 | loss: 3.365894 | lr:5.3518e-04 | norm 0.3456 | dt 338.72ms | 1547852.53 tokens/sec
Step 4851 | loss: 3.351444 | lr:5.3515e-04 | norm 0.3419 | dt 340.38ms | 1540301.15 tokens/sec
Step 4852 | loss: 3.356580 | lr:5.3512e-04 | norm 0.2869 | dt 337.28ms | 1554440.36 tokens/sec
Step 4853 | loss: 3.385145 | lr:5.3509e-04 | norm 0.3015 | dt 338.53ms | 1548700.63 tokens/sec
Step 4854 | loss: 3.345650 | lr:5.3506e-04 | norm 0.3230 | dt 338.57ms | 1548537.04 tokens/sec
Step 4855 | loss: 3.397396 | lr:5.3503e-04 | norm 0.3607 | dt 338.48ms | 1548926.44 tokens/sec
Step 4856 | loss: 3.349678 | lr:5.3500e-04 | norm 0.2784 | dt 339.29ms | 1545236.70 tokens/sec
Step 4857 | loss: 3.302870 | lr:5.3497e-04 | norm 0.2899 | dt 338.58ms | 1548487.97 tokens/sec
Step 4858 | loss: 3.251726 | lr:5.3494e-04 | norm 0.2784 | dt 339.35ms | 1544963.12 tokens/sec
Step 4859 | loss: 3.332399 | lr:5.3491e-04 | norm 0.2670 | dt 339.04ms | 1546379.83 tokens/sec
Step 4860 | loss: 3.360403 | lr:5.3488e-04 | norm 0.2819 | dt 338.31ms | 1549727.66 tokens/sec
Step 4861 | loss: 3.397885 | lr:5.3485e-04 | norm 0.2896 | dt 339.75ms | 1543151.47 tokens/sec
Step 4862 | loss: 3.395121 | lr:5.3482e-04 | norm 0.2663 | dt 339.43ms | 1544631.05 tokens/sec
Step 4863 | loss: 3.369988 | lr:5.3479e-04 | norm 0.2957 | dt 338.62ms | 1548322.25 tokens/sec
Step 4864 | loss: 3.338044 | lr:5.3475e-04 | norm 0.2753 | dt 338.61ms | 1548339.69 tokens/sec
Step 4865 | loss: 3.350512 | lr:5.3472e-04 | norm 0.3087 | dt 338.94ms | 1546856.28 tokens/sec
Step 4866 | loss: 3.350354 | lr:5.3469e-04 | norm 0.2815 | dt 340.68ms | 1538964.47 tokens/sec
Step 4867 | loss: 3.323116 | lr:5.3466e-04 | norm 0.2699 | dt 339.36ms | 1544909.93 tokens/sec
Step 4868 | loss: 3.331705 | lr:5.3463e-04 | norm 0.3022 | dt 338.94ms | 1546827.98 tokens/sec
Step 4869 | loss: 3.337966 | lr:5.3460e-04 | norm 0.2857 | dt 339.44ms | 1544549.68 tokens/sec
Step 4870 | loss: 3.298068 | lr:5.3457e-04 | norm 0.2782 | dt 338.61ms | 1548370.22 tokens/sec
Step 4871 | loss: 3.272672 | lr:5.3454e-04 | norm 0.2769 | dt 338.52ms | 1548746.44 tokens/sec
Step 4872 | loss: 3.380978 | lr:5.3451e-04 | norm 0.2904 | dt 339.59ms | 1543863.26 tokens/sec
Step 4873 | loss: 3.137110 | lr:5.3448e-04 | norm 0.2815 | dt 341.55ms | 1535039.05 tokens/sec
Step 4874 | loss: 3.386876 | lr:5.3445e-04 | norm 0.3087 | dt 339.76ms | 1543123.32 tokens/sec
Step 4875 | loss: 3.200488 | lr:5.3442e-04 | norm 0.3279 | dt 338.95ms | 1546820.37 tokens/sec
Step 4876 | loss: 3.188470 | lr:5.3439e-04 | norm 0.3080 | dt 339.70ms | 1543370.25 tokens/sec
Step 4877 | loss: 3.219332 | lr:5.3436e-04 | norm 0.3012 | dt 339.31ms | 1545160.69 tokens/sec
Step 4878 | loss: 3.164801 | lr:5.3433e-04 | norm 0.2858 | dt 339.38ms | 1544858.92 tokens/sec
Step 4879 | loss: 3.241348 | lr:5.3430e-04 | norm 0.2634 | dt 338.81ms | 1547422.30 tokens/sec
Step 4880 | loss: 3.218512 | lr:5.3427e-04 | norm 0.2769 | dt 339.68ms | 1543496.99 tokens/sec
Step 4881 | loss: 3.168953 | lr:5.3424e-04 | norm 0.2633 | dt 339.32ms | 1545096.64 tokens/sec
Step 4882 | loss: 3.335603 | lr:5.3421e-04 | norm 0.2759 | dt 339.20ms | 1545657.03 tokens/sec
Step 4883 | loss: 3.177462 | lr:5.3418e-04 | norm 0.2608 | dt 339.76ms | 1543110.32 tokens/sec
Step 4884 | loss: 3.183784 | lr:5.3415e-04 | norm 0.2677 | dt 338.88ms | 1547138.15 tokens/sec
Step 4885 | loss: 3.383032 | lr:5.3412e-04 | norm 0.4058 | dt 339.33ms | 1545062.99 tokens/sec
Step 4886 | loss: 3.360711 | lr:5.3409e-04 | norm 0.3135 | dt 339.68ms | 1543498.08 tokens/sec
Step 4887 | loss: 3.369707 | lr:5.3406e-04 | norm 0.3021 | dt 339.33ms | 1545081.44 tokens/sec
Step 4888 | loss: 3.348178 | lr:5.3403e-04 | norm 0.3139 | dt 338.97ms | 1546701.78 tokens/sec
Step 4889 | loss: 3.359303 | lr:5.3400e-04 | norm 0.2915 | dt 338.50ms | 1548839.16 tokens/sec
Step 4890 | loss: 3.399657 | lr:5.3397e-04 | norm 0.3013 | dt 338.54ms | 1548679.91 tokens/sec
Step 4891 | loss: 3.391365 | lr:5.3394e-04 | norm 0.3229 | dt 338.87ms | 1547187.13 tokens/sec
Step 4892 | loss: 3.339623 | lr:5.3391e-04 | norm 0.3392 | dt 338.47ms | 1548983.17 tokens/sec
Step 4893 | loss: 3.385984 | lr:5.3388e-04 | norm 0.3147 | dt 339.13ms | 1545997.15 tokens/sec
Step 4894 | loss: 3.374381 | lr:5.3385e-04 | norm 0.2924 | dt 338.48ms | 1548961.35 tokens/sec
Step 4895 | loss: 3.298749 | lr:5.3382e-04 | norm 0.2769 | dt 338.62ms | 1548306.99 tokens/sec
Step 4896 | loss: 3.317376 | lr:5.3379e-04 | norm 0.2941 | dt 338.93ms | 1546892.18 tokens/sec
Step 4897 | loss: 3.311934 | lr:5.3376e-04 | norm 0.2804 | dt 338.36ms | 1549485.24 tokens/sec
Step 4898 | loss: 3.324462 | lr:5.3373e-04 | norm 0.2871 | dt 338.28ms | 1549846.71 tokens/sec
Step 4899 | loss: 3.318198 | lr:5.3370e-04 | norm 0.3211 | dt 338.21ms | 1550181.03 tokens/sec
Step 4900 | loss: 3.363333 | lr:5.3367e-04 | norm 0.3120 | dt 338.11ms | 1550635.76 tokens/sec
Step 4901 | loss: 3.330407 | lr:5.3364e-04 | norm 0.3086 | dt 337.79ms | 1552090.29 tokens/sec
Step 4902 | loss: 3.285565 | lr:5.3361e-04 | norm 0.2606 | dt 337.87ms | 1551763.91 tokens/sec
Step 4903 | loss: 3.390339 | lr:5.3358e-04 | norm 0.3011 | dt 338.81ms | 1547451.70 tokens/sec
Step 4904 | loss: 3.381312 | lr:5.3355e-04 | norm 0.3271 | dt 339.33ms | 1545066.24 tokens/sec
Step 4905 | loss: 3.336811 | lr:5.3352e-04 | norm 0.3215 | dt 339.90ms | 1542485.78 tokens/sec
Step 4906 | loss: 3.375836 | lr:5.3348e-04 | norm 0.2740 | dt 338.78ms | 1547582.38 tokens/sec
Step 4907 | loss: 3.363163 | lr:5.3345e-04 | norm 0.2939 | dt 339.63ms | 1543713.70 tokens/sec
Step 4908 | loss: 3.392989 | lr:5.3342e-04 | norm 0.2539 | dt 338.76ms | 1547660.80 tokens/sec
Step 4909 | loss: 3.345785 | lr:5.3339e-04 | norm 0.3003 | dt 339.18ms | 1545763.50 tokens/sec
Step 4910 | loss: 3.318536 | lr:5.3336e-04 | norm 0.2925 | dt 339.74ms | 1543204.54 tokens/sec
Step 4911 | loss: 3.340252 | lr:5.3333e-04 | norm 0.3326 | dt 339.61ms | 1543790.65 tokens/sec
Step 4912 | loss: 3.295537 | lr:5.3330e-04 | norm 0.2773 | dt 339.67ms | 1543534.91 tokens/sec
Step 4913 | loss: 3.381920 | lr:5.3327e-04 | norm 0.3077 | dt 916.03ms | 572348.41 tokens/sec
Step 4914 | loss: 3.349278 | lr:5.3324e-04 | norm 0.2932 | dt 336.47ms | 1558200.74 tokens/sec
Step 4915 | loss: 3.332530 | lr:5.3321e-04 | norm 0.2710 | dt 338.18ms | 1550344.97 tokens/sec
Step 4916 | loss: 3.283425 | lr:5.3318e-04 | norm 0.2593 | dt 337.97ms | 1551291.00 tokens/sec
Step 4917 | loss: 3.375819 | lr:5.3315e-04 | norm 0.2779 | dt 337.74ms | 1552345.58 tokens/sec
Step 4918 | loss: 3.299957 | lr:5.3312e-04 | norm 0.2642 | dt 336.90ms | 1556221.36 tokens/sec
Step 4919 | loss: 3.359937 | lr:5.3309e-04 | norm 0.2655 | dt 337.72ms | 1552425.58 tokens/sec
Step 4920 | loss: 3.251278 | lr:5.3306e-04 | norm 0.2867 | dt 338.10ms | 1550690.44 tokens/sec
Step 4921 | loss: 3.236334 | lr:5.3303e-04 | norm 0.2630 | dt 337.55ms | 1553201.90 tokens/sec
Step 4922 | loss: 3.226974 | lr:5.3300e-04 | norm 0.2834 | dt 338.32ms | 1549676.33 tokens/sec
Step 4923 | loss: 3.271113 | lr:5.3297e-04 | norm 0.3020 | dt 337.52ms | 1553331.37 tokens/sec
Step 4924 | loss: 3.237158 | lr:5.3294e-04 | norm 0.2712 | dt 338.04ms | 1550979.18 tokens/sec
Step 4925 | loss: 3.218123 | lr:5.3291e-04 | norm 0.2731 | dt 338.08ms | 1550788.86 tokens/sec
Step 4926 | loss: 3.162256 | lr:5.3288e-04 | norm 0.2458 | dt 337.71ms | 1552457.36 tokens/sec
Step 4927 | loss: 3.213798 | lr:5.3285e-04 | norm 0.2792 | dt 338.07ms | 1550809.64 tokens/sec
Step 4928 | loss: 3.186920 | lr:5.3282e-04 | norm 0.2707 | dt 338.37ms | 1549433.93 tokens/sec
Step 4929 | loss: 3.237429 | lr:5.3278e-04 | norm 0.2524 | dt 338.19ms | 1550281.58 tokens/sec
Step 4930 | loss: 3.170061 | lr:5.3275e-04 | norm 0.2782 | dt 337.51ms | 1553420.25 tokens/sec
Step 4931 | loss: 3.210032 | lr:5.3272e-04 | norm 0.2583 | dt 338.41ms | 1549252.72 tokens/sec
Step 4932 | loss: 3.375291 | lr:5.3269e-04 | norm 0.2965 | dt 338.34ms | 1549591.15 tokens/sec
Step 4933 | loss: 3.320830 | lr:5.3266e-04 | norm 0.3082 | dt 338.66ms | 1548107.52 tokens/sec
Step 4934 | loss: 3.351319 | lr:5.3263e-04 | norm 0.2956 | dt 339.77ms | 1543067.01 tokens/sec
Step 4935 | loss: 3.374049 | lr:5.3260e-04 | norm 0.3108 | dt 339.07ms | 1546239.57 tokens/sec
Step 4936 | loss: 3.305033 | lr:5.3257e-04 | norm 0.2716 | dt 338.23ms | 1550080.50 tokens/sec
Step 4937 | loss: 3.396628 | lr:5.3254e-04 | norm 0.2789 | dt 339.89ms | 1542534.47 tokens/sec
Step 4938 | loss: 3.414844 | lr:5.3251e-04 | norm 0.3050 | dt 339.48ms | 1544407.57 tokens/sec
Step 4939 | loss: 3.312366 | lr:5.3248e-04 | norm 0.3290 | dt 1038.42ms | 504892.11 tokens/sec
Step 4940 | loss: 3.317321 | lr:5.3245e-04 | norm 0.2656 | dt 336.47ms | 1558206.26 tokens/sec
Step 4941 | loss: 3.366949 | lr:5.3242e-04 | norm 0.3263 | dt 342.42ms | 1531134.66 tokens/sec
Step 4942 | loss: 3.293362 | lr:5.3239e-04 | norm 0.2909 | dt 339.57ms | 1543984.67 tokens/sec
Step 4943 | loss: 3.392264 | lr:5.3236e-04 | norm 0.2899 | dt 337.67ms | 1552685.37 tokens/sec
Step 4944 | loss: 3.331923 | lr:5.3233e-04 | norm 0.3215 | dt 337.42ms | 1553811.00 tokens/sec
Step 4945 | loss: 3.326973 | lr:5.3230e-04 | norm 0.2788 | dt 338.18ms | 1550302.34 tokens/sec
Step 4946 | loss: 3.353477 | lr:5.3227e-04 | norm 0.2790 | dt 338.29ms | 1549818.31 tokens/sec
Step 4947 | loss: 3.321185 | lr:5.3223e-04 | norm 0.2919 | dt 337.49ms | 1553510.24 tokens/sec
Step 4948 | loss: 3.329952 | lr:5.3220e-04 | norm 0.2994 | dt 338.00ms | 1551164.07 tokens/sec
Step 4949 | loss: 3.320134 | lr:5.3217e-04 | norm 0.2858 | dt 338.18ms | 1550322.01 tokens/sec
Step 4950 | loss: 3.337785 | lr:5.3214e-04 | norm 0.2833 | dt 337.62ms | 1552893.69 tokens/sec
Step 4951 | loss: 3.406011 | lr:5.3211e-04 | norm 0.3178 | dt 338.25ms | 1549987.63 tokens/sec
Step 4952 | loss: 3.304695 | lr:5.3208e-04 | norm 0.3393 | dt 337.98ms | 1551235.19 tokens/sec
Step 4953 | loss: 3.420081 | lr:5.3205e-04 | norm 0.3072 | dt 337.65ms | 1552763.21 tokens/sec
Step 4954 | loss: 3.434860 | lr:5.3202e-04 | norm 0.2974 | dt 338.57ms | 1548550.13 tokens/sec
Step 4955 | loss: 3.320521 | lr:5.3199e-04 | norm 0.2597 | dt 337.71ms | 1552494.63 tokens/sec
Step 4956 | loss: 3.310052 | lr:5.3196e-04 | norm 0.2754 | dt 337.18ms | 1554904.19 tokens/sec
Step 4957 | loss: 3.354105 | lr:5.3193e-04 | norm 0.2625 | dt 337.99ms | 1551187.05 tokens/sec
Step 4958 | loss: 3.371334 | lr:5.3190e-04 | norm 0.2874 | dt 338.24ms | 1550038.98 tokens/sec
Step 4959 | loss: 3.351697 | lr:5.3187e-04 | norm 0.2579 | dt 337.29ms | 1554413.99 tokens/sec
Step 4960 | loss: 3.337042 | lr:5.3184e-04 | norm 0.2530 | dt 337.49ms | 1553479.51 tokens/sec
Step 4961 | loss: 3.277945 | lr:5.3181e-04 | norm 0.2630 | dt 338.41ms | 1549284.37 tokens/sec
Step 4962 | loss: 3.360441 | lr:5.3177e-04 | norm 0.2465 | dt 338.19ms | 1550281.58 tokens/sec
Step 4963 | loss: 3.327768 | lr:5.3174e-04 | norm 0.2658 | dt 338.30ms | 1549792.10 tokens/sec
Step 4964 | loss: 3.377258 | lr:5.3171e-04 | norm 0.2570 | dt 337.61ms | 1552947.43 tokens/sec
Step 4965 | loss: 3.311654 | lr:5.3168e-04 | norm 0.2515 | dt 338.45ms | 1549069.37 tokens/sec
Step 4966 | loss: 3.209139 | lr:5.3165e-04 | norm 0.2757 | dt 338.42ms | 1549216.70 tokens/sec
Step 4967 | loss: 3.175202 | lr:5.3162e-04 | norm 0.2926 | dt 337.58ms | 1553072.46 tokens/sec
Step 4968 | loss: 3.201540 | lr:5.3159e-04 | norm 0.3218 | dt 338.16ms | 1550416.02 tokens/sec
Step 4969 | loss: 3.242521 | lr:5.3156e-04 | norm 0.3240 | dt 337.67ms | 1552641.51 tokens/sec
Step 4970 | loss: 3.176228 | lr:5.3153e-04 | norm 0.2950 | dt 338.75ms | 1547723.98 tokens/sec
Step 4971 | loss: 3.137803 | lr:5.3150e-04 | norm 0.3064 | dt 338.46ms | 1549045.37 tokens/sec
Step 4972 | loss: 3.245306 | lr:5.3147e-04 | norm 0.3253 | dt 338.36ms | 1549487.42 tokens/sec
Step 4973 | loss: 3.182482 | lr:5.3144e-04 | norm 0.2931 | dt 338.69ms | 1547989.82 tokens/sec
Step 4974 | loss: 3.247036 | lr:5.3141e-04 | norm 0.2970 | dt 338.76ms | 1547655.36 tokens/sec
Step 4975 | loss: 3.119494 | lr:5.3138e-04 | norm 0.2832 | dt 338.71ms | 1547918.99 tokens/sec
Step 4976 | loss: 3.210960 | lr:5.3134e-04 | norm 0.2485 | dt 339.11ms | 1546060.19 tokens/sec
Step 4977 | loss: 3.262660 | lr:5.3131e-04 | norm 0.2923 | dt 338.34ms | 1549572.59 tokens/sec
Step 4978 | loss: 3.380781 | lr:5.3128e-04 | norm 0.2673 | dt 338.31ms | 1549748.41 tokens/sec
Step 4979 | loss: 3.398460 | lr:5.3125e-04 | norm 0.2869 | dt 337.86ms | 1551806.62 tokens/sec
Step 4980 | loss: 3.316002 | lr:5.3122e-04 | norm 0.2903 | dt 338.09ms | 1550746.21 tokens/sec
Step 4981 | loss: 3.343854 | lr:5.3119e-04 | norm 0.2910 | dt 338.67ms | 1548069.37 tokens/sec
Step 4982 | loss: 3.435001 | lr:5.3116e-04 | norm 0.2653 | dt 338.40ms | 1549316.03 tokens/sec
Step 4983 | loss: 3.365530 | lr:5.3113e-04 | norm 0.3457 | dt 338.52ms | 1548784.62 tokens/sec
Step 4984 | loss: 3.368604 | lr:5.3110e-04 | norm 0.3278 | dt 339.71ms | 1543328.01 tokens/sec
Step 4985 | loss: 3.367463 | lr:5.3107e-04 | norm 0.2911 | dt 337.70ms | 1552538.47 tokens/sec
Step 4986 | loss: 3.298320 | lr:5.3104e-04 | norm 0.3108 | dt 339.13ms | 1545997.15 tokens/sec
Step 4987 | loss: 3.237756 | lr:5.3101e-04 | norm 0.2549 | dt 338.85ms | 1547277.49 tokens/sec
Step 4988 | loss: 3.326998 | lr:5.3097e-04 | norm 0.2658 | dt 338.66ms | 1548136.94 tokens/sec
Step 4989 | loss: 3.318447 | lr:5.3094e-04 | norm 0.2910 | dt 338.42ms | 1549214.52 tokens/sec
Step 4990 | loss: 3.291609 | lr:5.3091e-04 | norm 0.2756 | dt 337.37ms | 1554029.52 tokens/sec
Step 4991 | loss: 3.302725 | lr:5.3088e-04 | norm 0.2779 | dt 337.67ms | 1552686.46 tokens/sec
Step 4992 | loss: 3.337167 | lr:5.3085e-04 | norm 0.2801 | dt 338.01ms | 1551080.92 tokens/sec
Step 4993 | loss: 3.348899 | lr:5.3082e-04 | norm 0.2857 | dt 338.85ms | 1547249.18 tokens/sec
Step 4994 | loss: 3.309989 | lr:5.3079e-04 | norm 0.2971 | dt 337.38ms | 1553983.40 tokens/sec
Step 4995 | loss: 3.343135 | lr:5.3076e-04 | norm 0.3084 | dt 337.02ms | 1555666.49 tokens/sec
Step 4996 | loss: 3.324557 | lr:5.3073e-04 | norm 0.2859 | dt 338.72ms | 1547842.73 tokens/sec
Step 4997 | loss: 3.280360 | lr:5.3070e-04 | norm 0.2621 | dt 337.43ms | 1553768.19 tokens/sec
Step 4998 | loss: 3.384640 | lr:5.3067e-04 | norm 0.2757 | dt 338.01ms | 1551102.80 tokens/sec
Step 4999 | loss: 3.364519 | lr:5.3063e-04 | norm 0.2804 | dt 338.07ms | 1550836.98 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 5000: 3.3615
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2789/10042=0.2777


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, so let's say a person is speaking to a white woman. You would say "I'm white," "I'm
rank 5 sample 1 >Hello, I'm a language model, no! So you may find a couple of ideas here, but at least one will help me. There can't be


ddp_rank 7: ####### Printing generated samples ####### 

rank 5 sample 2 >Hello, I'm a language model, but I like the grammar. As soon as I get to the code, it is in my head and I'm looking
rank 5 sample 3 >Hello, I'm a language model, one I do have a friend. I have a big friend who just doesn't talk about me except for a couple of


rank 7 sample 0 >Hello, I'm a language model, and I need to write and write a table of values to see how people who may be of African descent will behave.


ddp_rank 1: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, working on it. I can get help from my people there.
I'm happy with this. My name is M


ddp_rank 3: ####### Printing generated samples ####### 

rank 7 sample 2 >Hello, I'm a language model, and I think one very important, as well as at the moment, is that it is a very simple, non-
rank 1 sample 0 >Hello, I'm a language model, and I feel like I can communicate anywhere, with anyone with all the necessary qualifications. It's important to know that I
rank 7 sample 3 >Hello, I'm a language model, do I just want to communicate with native speakers through Twitter? I think a lot of people are actually just not used to


rank 3 sample 0 >Hello, I'm a language model, and I'm going to try and read from the inside out my window you, in, where I'd like, so
rank 1 sample 1 >Hello, I'm a language model, a person.<|endoftext|>The first two-three years of research in the field of agriculture to verify genetically modified crops have been
rank 3 sample 1 >Hello, I'm a language model, and the reason I'm gonna have to say "my book is my book" is because I can't quite explain whyrank 1 sample 2 >Hello, I'm a language model, so on and we're going to have a lot of different models and I'm going to try to get some data and

rank 1 sample 3 >Hello, I'm a language model, and I'm looking at different classes (all of which go in for that?) and finding similar answers. I don't


rank 3 sample 2 >Hello, I'm a language model, and it has been edited for more than 300 years.
And I think we can do this kind of work using an


ddp_rank 2: ####### Printing generated samples ####### 

rank 3 sample 3 >Hello, I'm a language model, and while I do not speak my native tongue, you will probably not go to say "Hi, are you a native


rank 2 sample 0 >Hello, I'm a language model, having never seen or heard all of what you've heard and understood. We are trying to understand the world from the start
rank 2 sample 1 >Hello, I'm a language model, so I believe the question is, Is not the correct answer, so this is a question of course not the correct answer
rank 2 sample 2 >Hello, I'm a language model, but I think the idea of that, what we are doing is the problem with this system, to see if it could
rank 2 sample 3 >Hello, I'm a language model, so that'll be my next post. My daughter said I'd like to move away from math by the end. Now




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I want to learn something new and the language that I can use at home. This gives me a good foundation in
rank 4 sample 1 >Hello, I'm a language model, how are you really saying that and it's all there has to do with your job title, the class you create and
rank 4 sample 2 >Hello, I'm a language model, I thought I have the problem. My question is, what would a language model do if I'm reading a movie or
rank 4 sample 3 >Hello, I'm a language model, and you said something to me, something was good, and they were okay, but I'll come back to this,




ddp_rank 6: ####### Printing generated samples ####### 



ddp_rank 0: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where people are trying to read as they are. This is not how we think they are; it's what we know
rank 0 sample 0 >Hello, I'm a language model, and I'll just write you another piece today....
> The idea is this: you make two things happen together in
rank 6 sample 1 >Hello, I'm a language model, and I want to get my point. (It was a bit confusing). I know I have seen some things that I
rank 6 sample 2 >Hello, I'm a language model, so here are a couple examples I use that would be a good guide: What is a good way to understand what yourank 0 sample 1 >Hello, I'm a language model, so i'm going to let you in. So how i do i give you an equation that you say you are going

rank 6 sample 3 >Hello, I'm a language model, and I want to talk to you.
And I'm going to talk a bit. Why do you suppose you use


rank 0 sample 2 >Hello, I'm a language model, but I mean that some grammar rules make it even more difficult: it's not easy for you to decide what you're
rank 0 sample 3 >Hello, I'm a language model, so maybe I need to make some things about different languages.
Anyway, here it is; I like it, with


Step 5000 | loss: 3.386972 | lr:5.3060e-04 | norm 0.2911 | dt 12344.04ms | 42472.96 tokens/sec
Step 5001 | loss: 3.320281 | lr:5.3057e-04 | norm 0.2636 | dt 334.54ms | 1567186.82 tokens/sec
Step 5002 | loss: 3.314858 | lr:5.3054e-04 | norm 0.2624 | dt 336.79ms | 1556722.62 tokens/sec
Step 5003 | loss: 3.345966 | lr:5.3051e-04 | norm 0.3481 | dt 336.67ms | 1557284.85 tokens/sec
Step 5004 | loss: 3.303949 | lr:5.3048e-04 | norm 0.3415 | dt 336.87ms | 1556363.44 tokens/sec
Step 5005 | loss: 3.364899 | lr:5.3045e-04 | norm 0.3022 | dt 335.61ms | 1562179.08 tokens/sec
Step 5006 | loss: 3.252999 | lr:5.3042e-04 | norm 0.3036 | dt 336.85ms | 1556458.18 tokens/sec
Step 5007 | loss: 3.304181 | lr:5.3039e-04 | norm 0.2950 | dt 335.64ms | 1562070.33 tokens/sec
Step 5008 | loss: 3.301520 | lr:5.3036e-04 | norm 0.2864 | dt 335.56ms | 1562425.49 tokens/sec
Step 5009 | loss: 3.370235 | lr:5.3033e-04 | norm 0.2782 | dt 338.42ms | 1549230.89 tokens/sec
Step 5010 | loss: 3.340385 | lr:5.3029e-04 | norm 0.2748 | dt 336.17ms | 1559586.54 tokens/sec
Step 5011 | loss: 3.326363 | lr:5.3026e-04 | norm 0.2936 | dt 335.97ms | 1560508.45 tokens/sec
Step 5012 | loss: 3.267936 | lr:5.3023e-04 | norm 0.2752 | dt 336.61ms | 1557568.33 tokens/sec
Step 5013 | loss: 3.163990 | lr:5.3020e-04 | norm 0.3000 | dt 336.25ms | 1559243.73 tokens/sec
Step 5014 | loss: 3.138744 | lr:5.3017e-04 | norm 0.3435 | dt 336.94ms | 1556005.53 tokens/sec
Step 5015 | loss: 3.220289 | lr:5.3014e-04 | norm 0.3751 | dt 336.96ms | 1555955.98 tokens/sec
Step 5016 | loss: 3.139405 | lr:5.3011e-04 | norm 0.2742 | dt 336.42ms | 1558431.53 tokens/sec
Step 5017 | loss: 3.175282 | lr:5.3008e-04 | norm 0.3049 | dt 338.00ms | 1551130.15 tokens/sec
Step 5018 | loss: 3.158653 | lr:5.3005e-04 | norm 0.2823 | dt 337.55ms | 1553198.61 tokens/sec
Step 5019 | loss: 3.194446 | lr:5.3002e-04 | norm 0.2736 | dt 337.41ms | 1553869.20 tokens/sec
Step 5020 | loss: 3.196261 | lr:5.2998e-04 | norm 0.3138 | dt 336.64ms | 1557411.69 tokens/sec
Step 5021 | loss: 3.169875 | lr:5.2995e-04 | norm 0.2876 | dt 337.79ms | 1552116.58 tokens/sec
Step 5022 | loss: 3.152924 | lr:5.2992e-04 | norm 0.2450 | dt 336.77ms | 1556800.87 tokens/sec
Step 5023 | loss: 3.175679 | lr:5.2989e-04 | norm 0.2578 | dt 337.33ms | 1554222.83 tokens/sec
Step 5024 | loss: 3.340104 | lr:5.2986e-04 | norm 0.2509 | dt 337.60ms | 1552986.91 tokens/sec
Step 5025 | loss: 3.386438 | lr:5.2983e-04 | norm 0.2641 | dt 337.19ms | 1554885.50 tokens/sec
Step 5026 | loss: 3.397701 | lr:5.2980e-04 | norm 0.2709 | dt 338.21ms | 1550188.68 tokens/sec
Step 5027 | loss: 3.382747 | lr:5.2977e-04 | norm 0.2642 | dt 337.58ms | 1553059.30 tokens/sec
Step 5028 | loss: 3.381272 | lr:5.2974e-04 | norm 0.2753 | dt 337.88ms | 1551681.79 tokens/sec
Step 5029 | loss: 3.376539 | lr:5.2970e-04 | norm 0.2582 | dt 337.79ms | 1552118.78 tokens/sec
Step 5030 | loss: 3.311709 | lr:5.2967e-04 | norm 0.2614 | dt 338.53ms | 1548712.63 tokens/sec
Step 5031 | loss: 3.324273 | lr:5.2964e-04 | norm 0.2510 | dt 338.46ms | 1549024.64 tokens/sec
Step 5032 | loss: 3.328776 | lr:5.2961e-04 | norm 0.2555 | dt 336.77ms | 1556825.11 tokens/sec
Step 5033 | loss: 3.337472 | lr:5.2958e-04 | norm 0.2554 | dt 338.43ms | 1549164.32 tokens/sec
Step 5034 | loss: 3.286386 | lr:5.2955e-04 | norm 0.2530 | dt 338.39ms | 1549381.53 tokens/sec
Step 5035 | loss: 3.270273 | lr:5.2952e-04 | norm 0.2721 | dt 337.28ms | 1554441.46 tokens/sec
Step 5036 | loss: 3.293308 | lr:5.2949e-04 | norm 0.2872 | dt 338.18ms | 1550303.43 tokens/sec
Step 5037 | loss: 3.264786 | lr:5.2946e-04 | norm 0.2725 | dt 337.60ms | 1552969.36 tokens/sec
Step 5038 | loss: 3.280258 | lr:5.2942e-04 | norm 0.2732 | dt 338.15ms | 1550441.16 tokens/sec
Step 5039 | loss: 3.293450 | lr:5.2939e-04 | norm 0.3176 | dt 338.22ms | 1550136.23 tokens/sec
Step 5040 | loss: 3.308565 | lr:5.2936e-04 | norm 0.3152 | dt 338.83ms | 1547346.08 tokens/sec
Step 5041 | loss: 3.360699 | lr:5.2933e-04 | norm 0.3222 | dt 338.29ms | 1549820.50 tokens/sec
Step 5042 | loss: 3.309572 | lr:5.2930e-04 | norm 0.2970 | dt 338.52ms | 1548759.53 tokens/sec
Step 5043 | loss: 3.322573 | lr:5.2927e-04 | norm 0.3094 | dt 338.07ms | 1550832.61 tokens/sec
Step 5044 | loss: 3.315983 | lr:5.2924e-04 | norm 0.3200 | dt 338.65ms | 1548157.65 tokens/sec
Step 5045 | loss: 3.449259 | lr:5.2921e-04 | norm 0.2914 | dt 338.38ms | 1549390.26 tokens/sec
Step 5046 | loss: 3.384202 | lr:5.2917e-04 | norm 0.2942 | dt 338.59ms | 1548454.17 tokens/sec
Step 5047 | loss: 3.331396 | lr:5.2914e-04 | norm 0.2845 | dt 338.44ms | 1549147.95 tokens/sec
Step 5048 | loss: 3.312929 | lr:5.2911e-04 | norm 0.2695 | dt 337.60ms | 1552989.11 tokens/sec
Step 5049 | loss: 3.312133 | lr:5.2908e-04 | norm 0.2630 | dt 337.75ms | 1552302.84 tokens/sec
Step 5050 | loss: 3.324668 | lr:5.2905e-04 | norm 0.2611 | dt 337.40ms | 1553910.92 tokens/sec
Step 5051 | loss: 3.353733 | lr:5.2902e-04 | norm 0.2582 | dt 338.86ms | 1547211.08 tokens/sec
Step 5052 | loss: 3.311682 | lr:5.2899e-04 | norm 0.2638 | dt 337.60ms | 1552969.36 tokens/sec
Step 5053 | loss: 3.352319 | lr:5.2896e-04 | norm 0.2773 | dt 337.34ms | 1554198.66 tokens/sec
Step 5054 | loss: 3.383516 | lr:5.2893e-04 | norm 0.2477 | dt 338.94ms | 1546861.72 tokens/sec
Step 5055 | loss: 3.343773 | lr:5.2889e-04 | norm 0.2915 | dt 338.34ms | 1549576.96 tokens/sec
Step 5056 | loss: 3.338576 | lr:5.2886e-04 | norm 0.2730 | dt 337.49ms | 1553481.70 tokens/sec
Step 5057 | loss: 3.281776 | lr:5.2883e-04 | norm 0.2957 | dt 337.65ms | 1552748.95 tokens/sec
Step 5058 | loss: 3.361562 | lr:5.2880e-04 | norm 0.2789 | dt 338.09ms | 1550729.81 tokens/sec
Step 5059 | loss: 3.202291 | lr:5.2877e-04 | norm 0.3123 | dt 337.47ms | 1553606.82 tokens/sec
Step 5060 | loss: 3.108683 | lr:5.2874e-04 | norm 0.3420 | dt 337.89ms | 1551630.33 tokens/sec
Step 5061 | loss: 3.168172 | lr:5.2871e-04 | norm 0.3420 | dt 337.44ms | 1553701.22 tokens/sec
Step 5062 | loss: 3.202705 | lr:5.2868e-04 | norm 0.2746 | dt 337.65ms | 1552756.63 tokens/sec
Step 5063 | loss: 3.198718 | lr:5.2864e-04 | norm 0.3335 | dt 338.08ms | 1550803.08 tokens/sec
Step 5064 | loss: 3.213066 | lr:5.2861e-04 | norm 0.2907 | dt 337.46ms | 1553613.41 tokens/sec
Step 5065 | loss: 3.168639 | lr:5.2858e-04 | norm 0.3042 | dt 338.64ms | 1548194.71 tokens/sec
Step 5066 | loss: 3.158457 | lr:5.2855e-04 | norm 0.2791 | dt 337.88ms | 1551689.45 tokens/sec
Step 5067 | loss: 3.190320 | lr:5.2852e-04 | norm 0.2841 | dt 337.55ms | 1553206.29 tokens/sec
Step 5068 | loss: 3.186221 | lr:5.2849e-04 | norm 0.2581 | dt 338.30ms | 1549772.44 tokens/sec
Step 5069 | loss: 3.188334 | lr:5.2846e-04 | norm 0.2705 | dt 338.36ms | 1549486.33 tokens/sec
Step 5070 | loss: 3.301726 | lr:5.2842e-04 | norm 0.2680 | dt 338.95ms | 1546782.29 tokens/sec
Step 5071 | loss: 3.328942 | lr:5.2839e-04 | norm 0.2717 | dt 339.92ms | 1542392.74 tokens/sec
Step 5072 | loss: 3.344817 | lr:5.2836e-04 | norm 0.2810 | dt 339.65ms | 1543620.51 tokens/sec
Step 5073 | loss: 3.349645 | lr:5.2833e-04 | norm 0.2939 | dt 340.99ms | 1537546.26 tokens/sec
Step 5074 | loss: 3.364092 | lr:5.2830e-04 | norm 0.3144 | dt 340.55ms | 1539522.57 tokens/sec
Step 5075 | loss: 3.365938 | lr:5.2827e-04 | norm 0.2734 | dt 338.50ms | 1548840.25 tokens/sec
Step 5076 | loss: 3.383111 | lr:5.2824e-04 | norm 0.2710 | dt 338.89ms | 1547054.34 tokens/sec
Step 5077 | loss: 3.362175 | lr:5.2821e-04 | norm 0.2837 | dt 339.22ms | 1545582.07 tokens/sec
Step 5078 | loss: 3.379523 | lr:5.2817e-04 | norm 0.2724 | dt 338.06ms | 1550886.20 tokens/sec
Step 5079 | loss: 3.310043 | lr:5.2814e-04 | norm 0.2962 | dt 339.18ms | 1545761.33 tokens/sec
Step 5080 | loss: 3.350215 | lr:5.2811e-04 | norm 0.2931 | dt 340.12ms | 1541500.74 tokens/sec
Step 5081 | loss: 3.338218 | lr:5.2808e-04 | norm 0.3128 | dt 338.36ms | 1549481.97 tokens/sec
Step 5082 | loss: 3.310995 | lr:5.2805e-04 | norm 0.3364 | dt 339.61ms | 1543804.74 tokens/sec
Step 5083 | loss: 3.346834 | lr:5.2802e-04 | norm 0.2825 | dt 338.93ms | 1546874.77 tokens/sec
Step 5084 | loss: 3.322487 | lr:5.2799e-04 | norm 0.2696 | dt 340.09ms | 1541605.56 tokens/sec
Step 5085 | loss: 3.277366 | lr:5.2795e-04 | norm 0.2644 | dt 337.79ms | 1552106.72 tokens/sec
Step 5086 | loss: 3.307273 | lr:5.2792e-04 | norm 0.2828 | dt 337.48ms | 1553539.87 tokens/sec
Step 5087 | loss: 3.307438 | lr:5.2789e-04 | norm 0.2920 | dt 337.74ms | 1552360.92 tokens/sec
Step 5088 | loss: 3.355177 | lr:5.2786e-04 | norm 0.3780 | dt 337.91ms | 1551564.64 tokens/sec
Step 5089 | loss: 3.296221 | lr:5.2783e-04 | norm 0.3527 | dt 337.45ms | 1553670.49 tokens/sec
Step 5090 | loss: 3.347232 | lr:5.2780e-04 | norm 0.3212 | dt 338.76ms | 1547662.98 tokens/sec
Step 5091 | loss: 3.372785 | lr:5.2777e-04 | norm 0.3336 | dt 337.82ms | 1551994.99 tokens/sec
Step 5092 | loss: 3.439911 | lr:5.2773e-04 | norm 0.3557 | dt 338.32ms | 1549680.70 tokens/sec
Step 5093 | loss: 3.419838 | lr:5.2770e-04 | norm 0.3397 | dt 338.08ms | 1550799.80 tokens/sec
Step 5094 | loss: 3.336979 | lr:5.2767e-04 | norm 0.2726 | dt 338.25ms | 1549999.65 tokens/sec
Step 5095 | loss: 3.413045 | lr:5.2764e-04 | norm 0.3259 | dt 337.85ms | 1551835.09 tokens/sec
Step 5096 | loss: 3.317010 | lr:5.2761e-04 | norm 0.2776 | dt 338.15ms | 1550467.40 tokens/sec
Step 5097 | loss: 3.344064 | lr:5.2758e-04 | norm 0.2797 | dt 337.60ms | 1552966.07 tokens/sec
Step 5098 | loss: 3.310322 | lr:5.2754e-04 | norm 0.2857 | dt 337.21ms | 1554761.28 tokens/sec
Step 5099 | loss: 3.369006 | lr:5.2751e-04 | norm 0.2857 | dt 338.18ms | 1550342.78 tokens/sec
Step 5100 | loss: 3.375443 | lr:5.2748e-04 | norm 0.2637 | dt 338.35ms | 1549555.12 tokens/sec
Step 5101 | loss: 3.303023 | lr:5.2745e-04 | norm 0.2653 | dt 337.16ms | 1555002.05 tokens/sec
Step 5102 | loss: 3.335996 | lr:5.2742e-04 | norm 0.2686 | dt 907.50ms | 577729.93 tokens/sec
Step 5103 | loss: 3.306345 | lr:5.2739e-04 | norm 0.2470 | dt 334.87ms | 1565662.62 tokens/sec
Step 5104 | loss: 3.326384 | lr:5.2736e-04 | norm 0.2583 | dt 338.58ms | 1548502.15 tokens/sec
Step 5105 | loss: 3.267496 | lr:5.2732e-04 | norm 0.2860 | dt 338.44ms | 1549132.67 tokens/sec
Step 5106 | loss: 3.237520 | lr:5.2729e-04 | norm 0.2576 | dt 337.37ms | 1554025.13 tokens/sec
Step 5107 | loss: 3.196586 | lr:5.2726e-04 | norm 0.2811 | dt 337.60ms | 1552988.01 tokens/sec
Step 5108 | loss: 3.148243 | lr:5.2723e-04 | norm 0.2795 | dt 337.31ms | 1554328.29 tokens/sec
Step 5109 | loss: 3.116276 | lr:5.2720e-04 | norm 0.2835 | dt 337.95ms | 1551370.90 tokens/sec
Step 5110 | loss: 3.154674 | lr:5.2717e-04 | norm 0.2796 | dt 339.06ms | 1546288.49 tokens/sec
Step 5111 | loss: 3.133444 | lr:5.2714e-04 | norm 0.2650 | dt 337.55ms | 1553219.46 tokens/sec
Step 5112 | loss: 3.117295 | lr:5.2710e-04 | norm 0.2965 | dt 340.71ms | 1538815.86 tokens/sec
Step 5113 | loss: 3.170352 | lr:5.2707e-04 | norm 0.2927 | dt 338.81ms | 1547458.23 tokens/sec
Step 5114 | loss: 3.171810 | lr:5.2704e-04 | norm 0.2387 | dt 338.31ms | 1549738.58 tokens/sec
Step 5115 | loss: 3.111122 | lr:5.2701e-04 | norm 0.2507 | dt 339.36ms | 1544918.62 tokens/sec
Step 5116 | loss: 3.173504 | lr:5.2698e-04 | norm 0.2486 | dt 338.99ms | 1546604.96 tokens/sec
Step 5117 | loss: 3.339623 | lr:5.2695e-04 | norm 0.2988 | dt 338.27ms | 1549926.46 tokens/sec
Step 5118 | loss: 3.344120 | lr:5.2691e-04 | norm 0.3237 | dt 339.25ms | 1545451.72 tokens/sec
Step 5119 | loss: 3.366141 | lr:5.2688e-04 | norm 0.3129 | dt 339.07ms | 1546249.35 tokens/sec
Step 5120 | loss: 3.342210 | lr:5.2685e-04 | norm 0.3173 | dt 339.11ms | 1546063.45 tokens/sec
Step 5121 | loss: 3.314620 | lr:5.2682e-04 | norm 0.3174 | dt 339.17ms | 1545804.79 tokens/sec
Step 5122 | loss: 3.339643 | lr:5.2679e-04 | norm 0.3115 | dt 338.32ms | 1549664.32 tokens/sec
Step 5123 | loss: 3.306508 | lr:5.2676e-04 | norm 0.2766 | dt 339.54ms | 1544133.20 tokens/sec
Step 5124 | loss: 3.261925 | lr:5.2672e-04 | norm 0.2569 | dt 338.60ms | 1548376.76 tokens/sec
Step 5125 | loss: 3.286037 | lr:5.2669e-04 | norm 0.2738 | dt 338.06ms | 1550861.04 tokens/sec
Step 5126 | loss: 3.295990 | lr:5.2666e-04 | norm 0.2753 | dt 338.23ms | 1550084.87 tokens/sec
Step 5127 | loss: 3.232678 | lr:5.2663e-04 | norm 0.2695 | dt 338.40ms | 1549320.40 tokens/sec
Step 5128 | loss: 3.305443 | lr:5.2660e-04 | norm 0.2704 | dt 338.44ms | 1549117.39 tokens/sec
Step 5129 | loss: 3.317487 | lr:5.2657e-04 | norm 0.3025 | dt 1007.35ms | 520462.74 tokens/sec
Step 5130 | loss: 3.342717 | lr:5.2653e-04 | norm 0.2755 | dt 338.60ms | 1548414.92 tokens/sec
Step 5131 | loss: 3.279978 | lr:5.2650e-04 | norm 0.2546 | dt 338.57ms | 1548555.58 tokens/sec
Step 5132 | loss: 3.279149 | lr:5.2647e-04 | norm 0.2607 | dt 338.20ms | 1550231.30 tokens/sec
Step 5133 | loss: 3.252914 | lr:5.2644e-04 | norm 0.2533 | dt 337.94ms | 1551437.66 tokens/sec
Step 5134 | loss: 3.296627 | lr:5.2641e-04 | norm 0.2454 | dt 338.65ms | 1548150.02 tokens/sec
Step 5135 | loss: 3.355365 | lr:5.2638e-04 | norm 0.3001 | dt 339.71ms | 1543319.34 tokens/sec
Step 5136 | loss: 3.422412 | lr:5.2634e-04 | norm 0.3001 | dt 338.64ms | 1548199.07 tokens/sec
Step 5137 | loss: 3.369608 | lr:5.2631e-04 | norm 0.2488 | dt 338.38ms | 1549427.38 tokens/sec
Step 5138 | loss: 3.382404 | lr:5.2628e-04 | norm 0.2775 | dt 338.21ms | 1550172.29 tokens/sec
Step 5139 | loss: 3.424465 | lr:5.2625e-04 | norm 0.2946 | dt 338.25ms | 1550020.41 tokens/sec
Step 5140 | loss: 3.326881 | lr:5.2622e-04 | norm 0.2942 | dt 338.23ms | 1550092.52 tokens/sec
Step 5141 | loss: 3.345632 | lr:5.2619e-04 | norm 0.2745 | dt 337.67ms | 1552686.46 tokens/sec
Step 5142 | loss: 3.309539 | lr:5.2615e-04 | norm 0.2766 | dt 339.13ms | 1545967.81 tokens/sec
Step 5143 | loss: 3.305481 | lr:5.2612e-04 | norm 0.2438 | dt 338.33ms | 1549656.67 tokens/sec
Step 5144 | loss: 3.307391 | lr:5.2609e-04 | norm 0.2620 | dt 337.97ms | 1551282.25 tokens/sec
Step 5145 | loss: 3.308382 | lr:5.2606e-04 | norm 0.2685 | dt 338.37ms | 1549464.50 tokens/sec
Step 5146 | loss: 3.292905 | lr:5.2603e-04 | norm 0.2824 | dt 337.95ms | 1551362.14 tokens/sec
Step 5147 | loss: 3.327946 | lr:5.2599e-04 | norm 0.2801 | dt 338.26ms | 1549972.34 tokens/sec
Step 5148 | loss: 3.341429 | lr:5.2596e-04 | norm 0.2549 | dt 338.78ms | 1547597.63 tokens/sec
Step 5149 | loss: 3.313703 | lr:5.2593e-04 | norm 0.2714 | dt 338.08ms | 1550801.98 tokens/sec
Step 5150 | loss: 3.292220 | lr:5.2590e-04 | norm 0.2815 | dt 341.98ms | 1533082.76 tokens/sec
Step 5151 | loss: 3.208161 | lr:5.2587e-04 | norm 0.3057 | dt 338.48ms | 1548939.53 tokens/sec
Step 5152 | loss: 3.171719 | lr:5.2584e-04 | norm 0.3172 | dt 337.87ms | 1551724.49 tokens/sec
Step 5153 | loss: 3.194665 | lr:5.2580e-04 | norm 0.2664 | dt 338.02ms | 1551061.22 tokens/sec
Step 5154 | loss: 3.274526 | lr:5.2577e-04 | norm 0.2718 | dt 338.26ms | 1549940.66 tokens/sec
Step 5155 | loss: 3.100173 | lr:5.2574e-04 | norm 0.3096 | dt 338.06ms | 1550889.48 tokens/sec
Step 5156 | loss: 3.173604 | lr:5.2571e-04 | norm 0.3042 | dt 338.36ms | 1549501.62 tokens/sec
Step 5157 | loss: 3.184323 | lr:5.2568e-04 | norm 0.3030 | dt 338.00ms | 1551149.85 tokens/sec
Step 5158 | loss: 3.103142 | lr:5.2564e-04 | norm 0.3063 | dt 339.61ms | 1543798.23 tokens/sec
Step 5159 | loss: 3.202246 | lr:5.2561e-04 | norm 0.3161 | dt 337.68ms | 1552617.40 tokens/sec
Step 5160 | loss: 3.214145 | lr:5.2558e-04 | norm 0.2926 | dt 338.22ms | 1550161.36 tokens/sec
Step 5161 | loss: 3.193674 | lr:5.2555e-04 | norm 0.2860 | dt 338.22ms | 1550160.27 tokens/sec
Step 5162 | loss: 3.268059 | lr:5.2552e-04 | norm 0.2928 | dt 337.69ms | 1552579.03 tokens/sec
Step 5163 | loss: 3.306489 | lr:5.2549e-04 | norm 0.2938 | dt 337.36ms | 1554112.99 tokens/sec
Step 5164 | loss: 3.360320 | lr:5.2545e-04 | norm 0.3215 | dt 337.65ms | 1552761.02 tokens/sec
Step 5165 | loss: 3.299301 | lr:5.2542e-04 | norm 0.3449 | dt 338.63ms | 1548279.73 tokens/sec
Step 5166 | loss: 3.293484 | lr:5.2539e-04 | norm 0.3337 | dt 337.79ms | 1552122.06 tokens/sec
Step 5167 | loss: 3.340017 | lr:5.2536e-04 | norm 0.2743 | dt 338.25ms | 1549983.26 tokens/sec
Step 5168 | loss: 3.333166 | lr:5.2533e-04 | norm 0.2629 | dt 338.92ms | 1546931.36 tokens/sec
Step 5169 | loss: 3.305928 | lr:5.2529e-04 | norm 0.2919 | dt 336.65ms | 1557365.36 tokens/sec
Step 5170 | loss: 3.334621 | lr:5.2526e-04 | norm 0.2602 | dt 338.81ms | 1547453.87 tokens/sec
Step 5171 | loss: 3.369430 | lr:5.2523e-04 | norm 0.2841 | dt 338.22ms | 1550143.88 tokens/sec
Step 5172 | loss: 3.417133 | lr:5.2520e-04 | norm 0.3337 | dt 338.00ms | 1551136.72 tokens/sec
Step 5173 | loss: 3.249466 | lr:5.2517e-04 | norm 0.3351 | dt 337.85ms | 1551850.42 tokens/sec
Step 5174 | loss: 3.346920 | lr:5.2513e-04 | norm 0.3140 | dt 337.93ms | 1551466.12 tokens/sec
Step 5175 | loss: 3.307417 | lr:5.2510e-04 | norm 0.2950 | dt 338.79ms | 1547525.75 tokens/sec
Step 5176 | loss: 3.301301 | lr:5.2507e-04 | norm 0.2919 | dt 337.94ms | 1551408.11 tokens/sec
Step 5177 | loss: 3.263721 | lr:5.2504e-04 | norm 0.2819 | dt 337.97ms | 1551272.40 tokens/sec
Step 5178 | loss: 3.249887 | lr:5.2501e-04 | norm 0.2869 | dt 338.94ms | 1546829.07 tokens/sec
Step 5179 | loss: 3.261875 | lr:5.2497e-04 | norm 0.2739 | dt 337.63ms | 1552870.67 tokens/sec
Step 5180 | loss: 3.297720 | lr:5.2494e-04 | norm 0.2616 | dt 338.08ms | 1550774.64 tokens/sec
Step 5181 | loss: 3.319955 | lr:5.2491e-04 | norm 0.2674 | dt 339.26ms | 1545403.93 tokens/sec
Step 5182 | loss: 3.362951 | lr:5.2488e-04 | norm 0.2528 | dt 337.74ms | 1552347.77 tokens/sec
Step 5183 | loss: 3.366683 | lr:5.2485e-04 | norm 0.2979 | dt 338.27ms | 1549902.42 tokens/sec
Step 5184 | loss: 3.397048 | lr:5.2481e-04 | norm 0.2818 | dt 337.97ms | 1551273.49 tokens/sec
Step 5185 | loss: 3.382510 | lr:5.2478e-04 | norm 0.3072 | dt 338.13ms | 1550571.26 tokens/sec
Step 5186 | loss: 3.338700 | lr:5.2475e-04 | norm 0.3158 | dt 337.15ms | 1555069.13 tokens/sec
Step 5187 | loss: 3.331373 | lr:5.2472e-04 | norm 0.2576 | dt 338.27ms | 1549910.07 tokens/sec
Step 5188 | loss: 3.308710 | lr:5.2469e-04 | norm 0.3209 | dt 337.60ms | 1552982.53 tokens/sec
Step 5189 | loss: 3.318507 | lr:5.2465e-04 | norm 0.3353 | dt 337.77ms | 1552226.14 tokens/sec
Step 5190 | loss: 3.303393 | lr:5.2462e-04 | norm 0.2889 | dt 337.69ms | 1552587.80 tokens/sec
Step 5191 | loss: 3.323540 | lr:5.2459e-04 | norm 0.2764 | dt 337.71ms | 1552496.82 tokens/sec
Step 5192 | loss: 3.342899 | lr:5.2456e-04 | norm 0.3030 | dt 338.11ms | 1550624.83 tokens/sec
Step 5193 | loss: 3.294399 | lr:5.2453e-04 | norm 0.2649 | dt 337.57ms | 1553136.08 tokens/sec
Step 5194 | loss: 3.342413 | lr:5.2449e-04 | norm 0.3006 | dt 337.97ms | 1551294.29 tokens/sec
Step 5195 | loss: 3.307927 | lr:5.2446e-04 | norm 0.2892 | dt 338.17ms | 1550376.67 tokens/sec
Step 5196 | loss: 3.320980 | lr:5.2443e-04 | norm 0.2791 | dt 337.39ms | 1553963.63 tokens/sec
Step 5197 | loss: 3.281847 | lr:5.2440e-04 | norm 0.2724 | dt 337.49ms | 1553514.63 tokens/sec
Step 5198 | loss: 3.203065 | lr:5.2437e-04 | norm 0.2908 | dt 339.03ms | 1546424.42 tokens/sec
Step 5199 | loss: 3.122134 | lr:5.2433e-04 | norm 0.2821 | dt 337.42ms | 1553808.81 tokens/sec
Step 5200 | loss: 3.141723 | lr:5.2430e-04 | norm 0.2933 | dt 337.83ms | 1551912.85 tokens/sec
Step 5201 | loss: 3.179378 | lr:5.2427e-04 | norm 0.2871 | dt 337.13ms | 1555139.51 tokens/sec
Step 5202 | loss: 3.187898 | lr:5.2424e-04 | norm 0.2757 | dt 338.04ms | 1550970.43 tokens/sec
Step 5203 | loss: 3.192713 | lr:5.2421e-04 | norm 0.2998 | dt 337.86ms | 1551772.67 tokens/sec
Step 5204 | loss: 3.150865 | lr:5.2417e-04 | norm 0.2743 | dt 337.43ms | 1553771.48 tokens/sec
Step 5205 | loss: 3.186784 | lr:5.2414e-04 | norm 0.2770 | dt 338.37ms | 1549461.22 tokens/sec
Step 5206 | loss: 3.171004 | lr:5.2411e-04 | norm 0.2392 | dt 337.72ms | 1552444.21 tokens/sec
Step 5207 | loss: 3.173379 | lr:5.2408e-04 | norm 0.2496 | dt 338.09ms | 1550718.87 tokens/sec
Step 5208 | loss: 3.179334 | lr:5.2405e-04 | norm 0.3184 | dt 338.25ms | 1549981.08 tokens/sec
Step 5209 | loss: 3.294955 | lr:5.2401e-04 | norm 0.3312 | dt 337.49ms | 1553471.82 tokens/sec
Step 5210 | loss: 3.301960 | lr:5.2398e-04 | norm 0.3108 | dt 337.62ms | 1552886.02 tokens/sec
Step 5211 | loss: 3.314892 | lr:5.2395e-04 | norm 0.2630 | dt 337.53ms | 1553306.13 tokens/sec
Step 5212 | loss: 3.319129 | lr:5.2392e-04 | norm 0.2988 | dt 337.75ms | 1552274.35 tokens/sec
Step 5213 | loss: 3.320506 | lr:5.2388e-04 | norm 0.2858 | dt 338.24ms | 1550040.08 tokens/sec
Step 5214 | loss: 3.293200 | lr:5.2385e-04 | norm 0.2898 | dt 338.18ms | 1550344.97 tokens/sec
Step 5215 | loss: 3.364379 | lr:5.2382e-04 | norm 0.2727 | dt 337.81ms | 1552005.95 tokens/sec
Step 5216 | loss: 3.295070 | lr:5.2379e-04 | norm 0.2885 | dt 337.74ms | 1552353.25 tokens/sec
Step 5217 | loss: 3.297014 | lr:5.2376e-04 | norm 0.2998 | dt 338.40ms | 1549330.22 tokens/sec
Step 5218 | loss: 3.253062 | lr:5.2372e-04 | norm 0.3031 | dt 337.55ms | 1553216.17 tokens/sec
Step 5219 | loss: 3.280855 | lr:5.2369e-04 | norm 0.2524 | dt 339.03ms | 1546422.24 tokens/sec
Step 5220 | loss: 3.323722 | lr:5.2366e-04 | norm 0.2604 | dt 339.99ms | 1542069.34 tokens/sec
Step 5221 | loss: 3.270667 | lr:5.2363e-04 | norm 0.2956 | dt 337.94ms | 1551413.58 tokens/sec
Step 5222 | loss: 3.285460 | lr:5.2360e-04 | norm 0.2663 | dt 338.84ms | 1547287.28 tokens/sec
Step 5223 | loss: 3.288858 | lr:5.2356e-04 | norm 0.2614 | dt 338.94ms | 1546838.87 tokens/sec
Step 5224 | loss: 3.245211 | lr:5.2353e-04 | norm 0.2964 | dt 338.84ms | 1547290.55 tokens/sec
Step 5225 | loss: 3.316713 | lr:5.2350e-04 | norm 0.2865 | dt 338.46ms | 1549042.09 tokens/sec
Step 5226 | loss: 3.267835 | lr:5.2347e-04 | norm 0.3028 | dt 338.65ms | 1548153.29 tokens/sec
Step 5227 | loss: 3.266649 | lr:5.2343e-04 | norm 0.2607 | dt 338.05ms | 1550898.23 tokens/sec
Step 5228 | loss: 3.332283 | lr:5.2340e-04 | norm 0.2668 | dt 338.61ms | 1548342.96 tokens/sec
Step 5229 | loss: 3.339950 | lr:5.2337e-04 | norm 0.2704 | dt 339.43ms | 1544629.96 tokens/sec
Step 5230 | loss: 3.365528 | lr:5.2334e-04 | norm 0.2696 | dt 338.82ms | 1547388.54 tokens/sec
Step 5231 | loss: 3.343746 | lr:5.2331e-04 | norm 0.2751 | dt 338.77ms | 1547613.97 tokens/sec
Step 5232 | loss: 3.331849 | lr:5.2327e-04 | norm 0.2873 | dt 338.16ms | 1550428.04 tokens/sec
Step 5233 | loss: 3.319794 | lr:5.2324e-04 | norm 0.2731 | dt 338.15ms | 1550456.46 tokens/sec
Step 5234 | loss: 3.276338 | lr:5.2321e-04 | norm 0.2894 | dt 339.64ms | 1543653.02 tokens/sec
Step 5235 | loss: 3.328236 | lr:5.2318e-04 | norm 0.2688 | dt 338.35ms | 1549543.11 tokens/sec
Step 5236 | loss: 3.349547 | lr:5.2314e-04 | norm 0.2686 | dt 338.82ms | 1547412.50 tokens/sec
Step 5237 | loss: 3.323770 | lr:5.2311e-04 | norm 0.2761 | dt 339.11ms | 1546075.41 tokens/sec
Step 5238 | loss: 3.278463 | lr:5.2308e-04 | norm 0.2499 | dt 338.35ms | 1549535.47 tokens/sec
Step 5239 | loss: 3.256637 | lr:5.2305e-04 | norm 0.2367 | dt 338.70ms | 1547956.04 tokens/sec
Step 5240 | loss: 3.290888 | lr:5.2301e-04 | norm 0.2463 | dt 339.48ms | 1544384.80 tokens/sec
Step 5241 | loss: 3.307336 | lr:5.2298e-04 | norm 0.2342 | dt 337.68ms | 1552599.86 tokens/sec
Step 5242 | loss: 3.311639 | lr:5.2295e-04 | norm 0.2725 | dt 337.43ms | 1553781.36 tokens/sec
Step 5243 | loss: 3.290338 | lr:5.2292e-04 | norm 0.2563 | dt 338.50ms | 1548853.34 tokens/sec
Step 5244 | loss: 3.166131 | lr:5.2289e-04 | norm 0.2502 | dt 338.09ms | 1550731.99 tokens/sec
Step 5245 | loss: 3.180987 | lr:5.2285e-04 | norm 0.3335 | dt 336.81ms | 1556616.83 tokens/sec
Step 5246 | loss: 3.213514 | lr:5.2282e-04 | norm 0.3292 | dt 337.93ms | 1551484.73 tokens/sec
Step 5247 | loss: 3.201410 | lr:5.2279e-04 | norm 0.2772 | dt 338.27ms | 1549904.61 tokens/sec
Step 5248 | loss: 3.180486 | lr:5.2276e-04 | norm 0.3295 | dt 336.93ms | 1556089.21 tokens/sec
Step 5249 | loss: 3.166521 | lr:5.2272e-04 | norm 0.3307 | dt 338.05ms | 1550925.58 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 5250: 3.3483
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2783/10042=0.2771



ddp_rank 4: ####### Printing generated samples ####### 


ddp_rank 5: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I love the ideas!
This workbook is a free resource designed to help your preschoolers and children develop their
rank 5 sample 0 >Hello, I'm a language model, but this one is so different. So why do I think this isn't the most powerful language for English, is the
rank 4 sample 1 >Hello, I'm a language model, why is it even harder to find good, meaningful examples for your learning English? Because this language model was designed to be
rank 4 sample 2 >Hello, I'm a language model, I guess I used this model to help with my language learning process.
I do have a great range of vocabulary and
rank 5 sample 1 >Hello, I'm a language model, for beginners. So many years ago, I knew that the most used languages to do my job was with Japanese, and
rank 4 sample 3 >Hello, I'm a language model, so this story about it will be available soon
Now that we have had some input from a new language, and we


rank 5 sample 2 >Hello, I'm a language model, and it would have good results in class. But, I'm not sure it is one of the best ways to improve
rank 5 sample 3 >Hello, I'm a language model, i'm here and I'm going to do that one at a time. The second question I learned is: What was




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to make sure I was able to read Japanese. Now, once I'm proficient enough to understand the
rank 2 sample 0 >Hello, I'm a language model, we've been writing code for the past two weeks. We have been having conversations since the beginning of the lesson. We
rank 7 sample 1 >Hello, I'm a language model, working on your own. I want one which is based on the way I see fit and fit.<|endoftext|>A few hours
rank 7 sample 2 >Hello, I'm a language model, so I can get rid of these differences by using Likertals which are a good example of Likertalsrank 2 sample 1 >Hello, I'm a language model, meaning that each person will be able to choose the noun they are working with.
However, this doesn't mean that

rank 7 sample 3 >Hello, I'm a language model, and it's not very readable. Can't use the language interpreter to interpret the code.
Let me explain a couple


rank 2 sample 2 >Hello, I'm a language model, and I think it's the one on the right that's my first. And so this whole group of people I love
rank 2 sample 3 >Hello, I'm a language model, but not content. So I'm not getting into business or anything.
Anyway, let's just take a few quick




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not interested at this point. I am working with the words but the meaning would be clearer, but here
rank 3 sample 1 >Hello, I'm a language model, so my goal is to share it with others.
Now, let's look at the three-factor functions we need
rank 3 sample 2 >Hello, I'm a language model, so it would be possible to communicate with it but I have only one way I can communicate with it, and most languages
rank 3 sample 3 >Hello, I'm a language model, so the people at the meetings will think of a good and healthy conversation, and help their own language that they've been




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know what it says! You already know your language! The whole book is based on your language skills. The
rank 0 sample 1 >Hello, I'm a language model, and as a language model, it lets you know whatever aspects you are experiencing that are being covered in one or more of
rank 0 sample 2 >Hello, I'm a language model, and I learned to speak for a very long time.
This course is a fun, and engaging way to get in
rank 0 sample 3 >Hello, I'm a language model, and as a language model, it does the research, and doesn't want you to go with the jargon. But at




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so to better understand why it's nice and it can be adapted to the contexts of our everyday conversation.
I'm
rank 1 sample 1 >Hello, I'm a language model, not an abstract term.
In general, I can imagine that most of these new topics often lack the basic concepts of
rank 1 sample 2 >Hello, I'm a language model, but even with it, I'm not sure what it all's all about.
I think there's something very basic
rank 1 sample 3 >Hello, I'm a language model, so I'm pretty good at it. With the help of the free and classroom resources they had on the subject, I


ddp_rank 6: ####### Printing generated samples ####### 



rank 6 sample 0 >Hello, I'm a language model,
you can't be told this language.
This is an interface, so we can connect to it, and even
rank 6 sample 1 >Hello, I'm a language model, which was first introduced to me at the 1998 conference "Language and Culture and Society," where I first learned it.

rank 6 sample 2 >Hello, I'm a language model, but that's what I think it looks like!
I've made this list of five words that speak more and less
rank 6 sample 3 >Hello, I'm a language model, so the second one would be better than the third one.
2. Does it work within the case if I ask


Step 5250 | loss: 3.105472 | lr:5.2269e-04 | norm 0.2887 | dt 18965.84ms | 27643.80 tokens/sec
Step 5251 | loss: 3.162933 | lr:5.2266e-04 | norm 0.3178 | dt 336.70ms | 1557117.24 tokens/sec
Step 5252 | loss: 3.140727 | lr:5.2263e-04 | norm 0.3151 | dt 336.12ms | 1559832.13 tokens/sec
Step 5253 | loss: 3.154453 | lr:5.2259e-04 | norm 0.3050 | dt 335.87ms | 1560990.32 tokens/sec
Step 5254 | loss: 3.191327 | lr:5.2256e-04 | norm 0.2919 | dt 335.86ms | 1561024.67 tokens/sec
Step 5255 | loss: 3.279291 | lr:5.2253e-04 | norm 0.2962 | dt 336.14ms | 1559751.36 tokens/sec
Step 5256 | loss: 3.353821 | lr:5.2250e-04 | norm 0.3363 | dt 335.99ms | 1560420.97 tokens/sec
Step 5257 | loss: 3.319527 | lr:5.2246e-04 | norm 0.2900 | dt 336.51ms | 1558015.27 tokens/sec
Step 5258 | loss: 3.321958 | lr:5.2243e-04 | norm 0.2768 | dt 336.61ms | 1557562.82 tokens/sec
Step 5259 | loss: 3.308078 | lr:5.2240e-04 | norm 0.2931 | dt 336.25ms | 1559205.03 tokens/sec
Step 5260 | loss: 3.353083 | lr:5.2237e-04 | norm 0.2727 | dt 336.31ms | 1558951.90 tokens/sec
Step 5261 | loss: 3.440542 | lr:5.2233e-04 | norm 0.3426 | dt 337.81ms | 1552033.33 tokens/sec
Step 5262 | loss: 3.309413 | lr:5.2230e-04 | norm 0.3442 | dt 337.36ms | 1554074.55 tokens/sec
Step 5263 | loss: 3.277803 | lr:5.2227e-04 | norm 0.2726 | dt 338.43ms | 1549194.87 tokens/sec
Step 5264 | loss: 3.228278 | lr:5.2224e-04 | norm 0.3442 | dt 336.54ms | 1557885.02 tokens/sec
Step 5265 | loss: 3.220373 | lr:5.2220e-04 | norm 0.2806 | dt 336.30ms | 1558998.32 tokens/sec
Step 5266 | loss: 3.321274 | lr:5.2217e-04 | norm 0.3062 | dt 336.69ms | 1557165.76 tokens/sec
Step 5267 | loss: 3.397425 | lr:5.2214e-04 | norm 0.2884 | dt 336.50ms | 1558079.29 tokens/sec
Step 5268 | loss: 3.301718 | lr:5.2211e-04 | norm 0.2949 | dt 336.94ms | 1556045.16 tokens/sec
Step 5269 | loss: 3.337903 | lr:5.2208e-04 | norm 0.3124 | dt 336.63ms | 1557442.57 tokens/sec
Step 5270 | loss: 3.316535 | lr:5.2204e-04 | norm 0.2889 | dt 336.79ms | 1556700.58 tokens/sec
Step 5271 | loss: 3.293387 | lr:5.2201e-04 | norm 0.2709 | dt 336.62ms | 1557485.59 tokens/sec
Step 5272 | loss: 3.374099 | lr:5.2198e-04 | norm 0.2863 | dt 337.48ms | 1553553.04 tokens/sec
Step 5273 | loss: 3.353047 | lr:5.2195e-04 | norm 0.3005 | dt 337.05ms | 1555527.83 tokens/sec
Step 5274 | loss: 3.385901 | lr:5.2191e-04 | norm 0.3238 | dt 337.08ms | 1555390.30 tokens/sec
Step 5275 | loss: 3.470277 | lr:5.2188e-04 | norm 0.3418 | dt 336.54ms | 1557879.50 tokens/sec
Step 5276 | loss: 3.365787 | lr:5.2185e-04 | norm 0.3897 | dt 336.36ms | 1558714.32 tokens/sec
Step 5277 | loss: 3.365609 | lr:5.2182e-04 | norm 0.2807 | dt 337.74ms | 1552340.10 tokens/sec
Step 5278 | loss: 3.356466 | lr:5.2178e-04 | norm 0.3042 | dt 336.62ms | 1557498.83 tokens/sec
Step 5279 | loss: 3.353858 | lr:5.2175e-04 | norm 0.3024 | dt 336.91ms | 1556180.61 tokens/sec
Step 5280 | loss: 3.345701 | lr:5.2172e-04 | norm 0.2548 | dt 337.06ms | 1555470.62 tokens/sec
Step 5281 | loss: 3.319968 | lr:5.2169e-04 | norm 0.2880 | dt 338.41ms | 1549262.54 tokens/sec
Step 5282 | loss: 3.314142 | lr:5.2165e-04 | norm 0.2801 | dt 336.66ms | 1557335.59 tokens/sec
Step 5283 | loss: 3.327135 | lr:5.2162e-04 | norm 0.2501 | dt 337.14ms | 1555121.92 tokens/sec
Step 5284 | loss: 3.306269 | lr:5.2159e-04 | norm 0.2459 | dt 337.49ms | 1553475.12 tokens/sec
Step 5285 | loss: 3.275727 | lr:5.2155e-04 | norm 0.2833 | dt 337.32ms | 1554259.08 tokens/sec
Step 5286 | loss: 3.308802 | lr:5.2152e-04 | norm 0.2696 | dt 338.55ms | 1548637.37 tokens/sec
Step 5287 | loss: 3.305355 | lr:5.2149e-04 | norm 0.2590 | dt 338.75ms | 1547733.78 tokens/sec
Step 5288 | loss: 3.317388 | lr:5.2146e-04 | norm 0.2800 | dt 337.64ms | 1552780.75 tokens/sec
Step 5289 | loss: 3.260207 | lr:5.2142e-04 | norm 0.3096 | dt 339.93ms | 1542335.40 tokens/sec
Step 5290 | loss: 3.162778 | lr:5.2139e-04 | norm 0.3011 | dt 337.81ms | 1552007.04 tokens/sec
Step 5291 | loss: 3.139727 | lr:5.2136e-04 | norm 0.2612 | dt 898.12ms | 583758.43 tokens/sec
Step 5292 | loss: 3.136672 | lr:5.2133e-04 | norm 0.2918 | dt 336.20ms | 1559465.98 tokens/sec
Step 5293 | loss: 3.215044 | lr:5.2129e-04 | norm 0.2821 | dt 337.48ms | 1553553.04 tokens/sec
Step 5294 | loss: 3.152681 | lr:5.2126e-04 | norm 0.2782 | dt 338.48ms | 1548934.08 tokens/sec
Step 5295 | loss: 3.176196 | lr:5.2123e-04 | norm 0.2439 | dt 337.24ms | 1554639.27 tokens/sec
Step 5296 | loss: 3.149848 | lr:5.2120e-04 | norm 0.2738 | dt 337.26ms | 1554542.56 tokens/sec
Step 5297 | loss: 3.114024 | lr:5.2116e-04 | norm 0.2472 | dt 337.92ms | 1551501.15 tokens/sec
Step 5298 | loss: 3.171131 | lr:5.2113e-04 | norm 0.2590 | dt 337.81ms | 1552032.23 tokens/sec
Step 5299 | loss: 3.200935 | lr:5.2110e-04 | norm 0.2420 | dt 337.13ms | 1555159.31 tokens/sec
Step 5300 | loss: 3.128813 | lr:5.2107e-04 | norm 0.2676 | dt 337.68ms | 1552602.05 tokens/sec
Step 5301 | loss: 3.148851 | lr:5.2103e-04 | norm 0.2761 | dt 339.11ms | 1546077.59 tokens/sec
Step 5302 | loss: 3.367646 | lr:5.2100e-04 | norm 0.3077 | dt 338.31ms | 1549717.83 tokens/sec
Step 5303 | loss: 3.311245 | lr:5.2097e-04 | norm 0.4623 | dt 338.15ms | 1550445.53 tokens/sec
Step 5304 | loss: 3.301178 | lr:5.2094e-04 | norm 0.3423 | dt 337.45ms | 1553697.93 tokens/sec
Step 5305 | loss: 3.374221 | lr:5.2090e-04 | norm 0.3280 | dt 337.54ms | 1553247.98 tokens/sec
Step 5306 | loss: 3.373290 | lr:5.2087e-04 | norm 0.3116 | dt 338.73ms | 1547819.85 tokens/sec
Step 5307 | loss: 3.297084 | lr:5.2084e-04 | norm 0.3120 | dt 338.96ms | 1546776.85 tokens/sec
Step 5308 | loss: 3.333655 | lr:5.2080e-04 | norm 0.3230 | dt 338.22ms | 1550118.75 tokens/sec
Step 5309 | loss: 3.287695 | lr:5.2077e-04 | norm 0.2793 | dt 339.07ms | 1546266.75 tokens/sec
Step 5310 | loss: 3.286156 | lr:5.2074e-04 | norm 0.3033 | dt 339.18ms | 1545734.17 tokens/sec
Step 5311 | loss: 3.299043 | lr:5.2071e-04 | norm 0.2997 | dt 338.09ms | 1550739.65 tokens/sec
Step 5312 | loss: 3.322278 | lr:5.2067e-04 | norm 0.2926 | dt 338.41ms | 1549283.28 tokens/sec
Step 5313 | loss: 3.272367 | lr:5.2064e-04 | norm 0.2675 | dt 338.75ms | 1547729.43 tokens/sec
Step 5314 | loss: 3.287228 | lr:5.2061e-04 | norm 0.2760 | dt 339.05ms | 1546340.69 tokens/sec
Step 5315 | loss: 3.290197 | lr:5.2058e-04 | norm 0.2824 | dt 337.86ms | 1551791.28 tokens/sec
Step 5316 | loss: 3.347787 | lr:5.2054e-04 | norm 0.2935 | dt 337.83ms | 1551931.46 tokens/sec
Step 5317 | loss: 3.321383 | lr:5.2051e-04 | norm 0.2685 | dt 339.59ms | 1543890.36 tokens/sec
Step 5318 | loss: 3.315891 | lr:5.2048e-04 | norm 0.2822 | dt 339.55ms | 1544055.14 tokens/sec
Step 5319 | loss: 3.381516 | lr:5.2044e-04 | norm 0.3056 | dt 1058.09ms | 495504.29 tokens/sec
Step 5320 | loss: 3.373130 | lr:5.2041e-04 | norm 0.2810 | dt 337.41ms | 1553853.82 tokens/sec
Step 5321 | loss: 3.297256 | lr:5.2038e-04 | norm 0.2948 | dt 337.85ms | 1551852.61 tokens/sec
Step 5322 | loss: 3.380553 | lr:5.2035e-04 | norm 0.2949 | dt 338.70ms | 1547937.52 tokens/sec
Step 5323 | loss: 3.336316 | lr:5.2031e-04 | norm 0.2588 | dt 337.69ms | 1552564.78 tokens/sec
Step 5324 | loss: 3.399354 | lr:5.2028e-04 | norm 0.3163 | dt 337.87ms | 1551735.44 tokens/sec
Step 5325 | loss: 3.314820 | lr:5.2025e-04 | norm 0.3209 | dt 338.21ms | 1550182.13 tokens/sec
Step 5326 | loss: 3.337453 | lr:5.2022e-04 | norm 0.2775 | dt 337.86ms | 1551813.19 tokens/sec
Step 5327 | loss: 3.292446 | lr:5.2018e-04 | norm 0.2555 | dt 338.05ms | 1550934.33 tokens/sec
Step 5328 | loss: 3.314145 | lr:5.2015e-04 | norm 0.2626 | dt 338.43ms | 1549189.42 tokens/sec
Step 5329 | loss: 3.304986 | lr:5.2012e-04 | norm 0.2936 | dt 338.44ms | 1549114.12 tokens/sec
Step 5330 | loss: 3.287412 | lr:5.2008e-04 | norm 0.2707 | dt 337.58ms | 1553082.33 tokens/sec
Step 5331 | loss: 3.338847 | lr:5.2005e-04 | norm 0.2684 | dt 338.07ms | 1550815.11 tokens/sec
Step 5332 | loss: 3.363112 | lr:5.2002e-04 | norm 0.2617 | dt 338.52ms | 1548768.26 tokens/sec
Step 5333 | loss: 3.336541 | lr:5.1999e-04 | norm 0.2419 | dt 338.03ms | 1551014.18 tokens/sec
Step 5334 | loss: 3.322795 | lr:5.1995e-04 | norm 0.2722 | dt 338.17ms | 1550374.48 tokens/sec
Step 5335 | loss: 3.302232 | lr:5.1992e-04 | norm 0.2920 | dt 338.43ms | 1549189.42 tokens/sec
Step 5336 | loss: 3.178466 | lr:5.1989e-04 | norm 0.2932 | dt 338.32ms | 1549680.70 tokens/sec
Step 5337 | loss: 3.115006 | lr:5.1985e-04 | norm 0.2830 | dt 339.07ms | 1546238.48 tokens/sec
Step 5338 | loss: 3.128702 | lr:5.1982e-04 | norm 0.3302 | dt 339.64ms | 1543676.86 tokens/sec
Step 5339 | loss: 3.206460 | lr:5.1979e-04 | norm 0.2952 | dt 338.39ms | 1549382.62 tokens/sec
Step 5340 | loss: 3.135581 | lr:5.1976e-04 | norm 0.2643 | dt 337.86ms | 1551811.00 tokens/sec
Step 5341 | loss: 3.192887 | lr:5.1972e-04 | norm 0.2842 | dt 338.41ms | 1549256.00 tokens/sec
Step 5342 | loss: 3.118135 | lr:5.1969e-04 | norm 0.2928 | dt 340.26ms | 1540845.11 tokens/sec
Step 5343 | loss: 3.164883 | lr:5.1966e-04 | norm 0.2586 | dt 339.84ms | 1542744.41 tokens/sec
Step 5344 | loss: 3.114382 | lr:5.1962e-04 | norm 0.2655 | dt 337.79ms | 1552126.44 tokens/sec
Step 5345 | loss: 3.167082 | lr:5.1959e-04 | norm 0.2887 | dt 338.83ms | 1547361.32 tokens/sec
Step 5346 | loss: 3.160574 | lr:5.1956e-04 | norm 0.2918 | dt 339.42ms | 1544642.98 tokens/sec
Step 5347 | loss: 3.189290 | lr:5.1953e-04 | norm 0.3007 | dt 339.08ms | 1546205.86 tokens/sec
Step 5348 | loss: 3.312430 | lr:5.1949e-04 | norm 0.2841 | dt 338.57ms | 1548532.68 tokens/sec
Step 5349 | loss: 3.515822 | lr:5.1946e-04 | norm 0.3392 | dt 339.58ms | 1543919.63 tokens/sec
Step 5350 | loss: 3.396156 | lr:5.1943e-04 | norm 0.3464 | dt 338.36ms | 1549510.35 tokens/sec
Step 5351 | loss: 3.334898 | lr:5.1939e-04 | norm 0.3536 | dt 338.76ms | 1547689.12 tokens/sec
Step 5352 | loss: 3.384789 | lr:5.1936e-04 | norm 0.3273 | dt 338.96ms | 1546767.06 tokens/sec
Step 5353 | loss: 3.355211 | lr:5.1933e-04 | norm 0.3004 | dt 338.00ms | 1551165.16 tokens/sec
Step 5354 | loss: 3.256624 | lr:5.1929e-04 | norm 0.2946 | dt 338.55ms | 1548648.28 tokens/sec
Step 5355 | loss: 3.271800 | lr:5.1926e-04 | norm 0.2942 | dt 339.62ms | 1543748.38 tokens/sec
Step 5356 | loss: 3.277488 | lr:5.1923e-04 | norm 0.2668 | dt 338.77ms | 1547607.43 tokens/sec
Step 5357 | loss: 3.271254 | lr:5.1920e-04 | norm 0.3083 | dt 338.69ms | 1547998.54 tokens/sec
Step 5358 | loss: 3.220087 | lr:5.1916e-04 | norm 0.2788 | dt 339.86ms | 1542649.17 tokens/sec
Step 5359 | loss: 3.192068 | lr:5.1913e-04 | norm 0.2960 | dt 338.43ms | 1549169.77 tokens/sec
Step 5360 | loss: 3.258229 | lr:5.1910e-04 | norm 0.3239 | dt 338.96ms | 1546772.50 tokens/sec
Step 5361 | loss: 3.322937 | lr:5.1906e-04 | norm 0.2951 | dt 338.63ms | 1548279.73 tokens/sec
Step 5362 | loss: 3.246097 | lr:5.1903e-04 | norm 0.2665 | dt 337.99ms | 1551173.92 tokens/sec
Step 5363 | loss: 3.280899 | lr:5.1900e-04 | norm 0.2974 | dt 338.47ms | 1548994.08 tokens/sec
Step 5364 | loss: 3.291085 | lr:5.1896e-04 | norm 0.2767 | dt 337.27ms | 1554527.17 tokens/sec
Step 5365 | loss: 3.314521 | lr:5.1893e-04 | norm 0.2989 | dt 337.31ms | 1554315.11 tokens/sec
Step 5366 | loss: 3.385784 | lr:5.1890e-04 | norm 0.3210 | dt 338.08ms | 1550792.14 tokens/sec
Step 5367 | loss: 3.371129 | lr:5.1887e-04 | norm 0.2746 | dt 338.10ms | 1550695.91 tokens/sec
Step 5368 | loss: 3.368746 | lr:5.1883e-04 | norm 0.2667 | dt 337.60ms | 1553001.17 tokens/sec
Step 5369 | loss: 3.369191 | lr:5.1880e-04 | norm 0.3028 | dt 338.85ms | 1547263.33 tokens/sec
Step 5370 | loss: 3.279081 | lr:5.1877e-04 | norm 0.4114 | dt 338.51ms | 1548808.62 tokens/sec
Step 5371 | loss: 3.276777 | lr:5.1873e-04 | norm 0.2806 | dt 337.36ms | 1554091.02 tokens/sec
Step 5372 | loss: 3.290632 | lr:5.1870e-04 | norm 0.2606 | dt 338.90ms | 1547039.10 tokens/sec
Step 5373 | loss: 3.316774 | lr:5.1867e-04 | norm 0.2388 | dt 337.84ms | 1551893.13 tokens/sec
Step 5374 | loss: 3.257735 | lr:5.1863e-04 | norm 0.2401 | dt 337.22ms | 1554721.71 tokens/sec
Step 5375 | loss: 3.316868 | lr:5.1860e-04 | norm 0.2466 | dt 338.88ms | 1547123.99 tokens/sec
Step 5376 | loss: 3.280964 | lr:5.1857e-04 | norm 0.2435 | dt 337.70ms | 1552535.18 tokens/sec
Step 5377 | loss: 3.324197 | lr:5.1854e-04 | norm 0.2553 | dt 336.84ms | 1556494.53 tokens/sec
Step 5378 | loss: 3.331591 | lr:5.1850e-04 | norm 0.2475 | dt 338.55ms | 1548609.02 tokens/sec
Step 5379 | loss: 3.355623 | lr:5.1847e-04 | norm 0.2484 | dt 337.68ms | 1552603.15 tokens/sec
Step 5380 | loss: 3.334820 | lr:5.1844e-04 | norm 0.2648 | dt 337.54ms | 1553278.70 tokens/sec
Step 5381 | loss: 3.298939 | lr:5.1840e-04 | norm 0.2559 | dt 338.52ms | 1548782.44 tokens/sec
Step 5382 | loss: 3.224809 | lr:5.1837e-04 | norm 0.2447 | dt 337.95ms | 1551381.84 tokens/sec
Step 5383 | loss: 3.107905 | lr:5.1834e-04 | norm 0.2733 | dt 337.25ms | 1554592.01 tokens/sec
Step 5384 | loss: 3.184423 | lr:5.1830e-04 | norm 0.2894 | dt 337.70ms | 1552548.34 tokens/sec
Step 5385 | loss: 3.138664 | lr:5.1827e-04 | norm 0.3004 | dt 338.07ms | 1550830.42 tokens/sec
Step 5386 | loss: 3.098783 | lr:5.1824e-04 | norm 0.2667 | dt 338.31ms | 1549723.29 tokens/sec
Step 5387 | loss: 3.170844 | lr:5.1820e-04 | norm 0.2681 | dt 338.65ms | 1548151.11 tokens/sec
Step 5388 | loss: 3.168536 | lr:5.1817e-04 | norm 0.2793 | dt 338.66ms | 1548133.67 tokens/sec
Step 5389 | loss: 3.134610 | lr:5.1814e-04 | norm 0.2829 | dt 339.14ms | 1545954.76 tokens/sec
Step 5390 | loss: 3.169065 | lr:5.1811e-04 | norm 0.2715 | dt 338.65ms | 1548151.11 tokens/sec
Step 5391 | loss: 3.125756 | lr:5.1807e-04 | norm 0.2538 | dt 338.34ms | 1549570.41 tokens/sec
Step 5392 | loss: 3.132678 | lr:5.1804e-04 | norm 0.2845 | dt 338.96ms | 1546771.41 tokens/sec
Step 5393 | loss: 3.140324 | lr:5.1801e-04 | norm 0.2846 | dt 338.63ms | 1548279.73 tokens/sec
Step 5394 | loss: 3.317657 | lr:5.1797e-04 | norm 0.2768 | dt 338.73ms | 1547801.33 tokens/sec
Step 5395 | loss: 3.385496 | lr:5.1794e-04 | norm 0.2886 | dt 338.03ms | 1551010.90 tokens/sec
Step 5396 | loss: 3.286420 | lr:5.1791e-04 | norm 0.2755 | dt 338.13ms | 1550558.14 tokens/sec
Step 5397 | loss: 3.392728 | lr:5.1787e-04 | norm 0.3016 | dt 338.55ms | 1548617.74 tokens/sec
Step 5398 | loss: 3.350460 | lr:5.1784e-04 | norm 0.3071 | dt 338.27ms | 1549902.42 tokens/sec
Step 5399 | loss: 3.351124 | lr:5.1781e-04 | norm 0.3047 | dt 337.87ms | 1551749.67 tokens/sec
Step 5400 | loss: 3.284006 | lr:5.1777e-04 | norm 0.2870 | dt 338.75ms | 1547710.91 tokens/sec
Step 5401 | loss: 3.292301 | lr:5.1774e-04 | norm 0.2701 | dt 337.66ms | 1552720.45 tokens/sec
Step 5402 | loss: 3.326184 | lr:5.1771e-04 | norm 0.2941 | dt 337.39ms | 1553966.92 tokens/sec
Step 5403 | loss: 3.260133 | lr:5.1767e-04 | norm 0.2770 | dt 337.98ms | 1551224.25 tokens/sec
Step 5404 | loss: 3.328873 | lr:5.1764e-04 | norm 0.2865 | dt 338.29ms | 1549797.56 tokens/sec
Step 5405 | loss: 3.310530 | lr:5.1761e-04 | norm 0.3039 | dt 338.24ms | 1550043.35 tokens/sec
Step 5406 | loss: 3.339645 | lr:5.1757e-04 | norm 0.2893 | dt 337.70ms | 1552529.70 tokens/sec
Step 5407 | loss: 3.272376 | lr:5.1754e-04 | norm 0.2835 | dt 337.73ms | 1552378.46 tokens/sec
Step 5408 | loss: 3.264415 | lr:5.1751e-04 | norm 0.2913 | dt 338.24ms | 1550036.80 tokens/sec
Step 5409 | loss: 3.313590 | lr:5.1747e-04 | norm 0.2592 | dt 338.07ms | 1550842.45 tokens/sec
Step 5410 | loss: 3.269061 | lr:5.1744e-04 | norm 0.2792 | dt 337.78ms | 1552172.46 tokens/sec
Step 5411 | loss: 3.371709 | lr:5.1741e-04 | norm 0.2800 | dt 338.49ms | 1548913.35 tokens/sec
Step 5412 | loss: 3.301785 | lr:5.1737e-04 | norm 0.2964 | dt 338.74ms | 1547749.03 tokens/sec
Step 5413 | loss: 3.302286 | lr:5.1734e-04 | norm 0.2769 | dt 337.70ms | 1552530.80 tokens/sec
Step 5414 | loss: 3.408875 | lr:5.1731e-04 | norm 0.2845 | dt 338.27ms | 1549902.42 tokens/sec
Step 5415 | loss: 3.386752 | lr:5.1727e-04 | norm 0.2756 | dt 338.26ms | 1549939.56 tokens/sec
Step 5416 | loss: 3.372440 | lr:5.1724e-04 | norm 1.3297 | dt 337.48ms | 1553545.36 tokens/sec
Step 5417 | loss: 3.346110 | lr:5.1721e-04 | norm 0.4008 | dt 338.58ms | 1548486.88 tokens/sec
Step 5418 | loss: 3.305712 | lr:5.1717e-04 | norm 0.3230 | dt 337.82ms | 1551973.08 tokens/sec
Step 5419 | loss: 3.349727 | lr:5.1714e-04 | norm 0.3062 | dt 337.18ms | 1554923.98 tokens/sec
Step 5420 | loss: 3.310156 | lr:5.1711e-04 | norm 0.2912 | dt 338.87ms | 1547183.86 tokens/sec
Step 5421 | loss: 3.320650 | lr:5.1707e-04 | norm 0.2883 | dt 337.87ms | 1551750.77 tokens/sec
Step 5422 | loss: 3.314793 | lr:5.1704e-04 | norm 0.2652 | dt 338.86ms | 1547206.72 tokens/sec
Step 5423 | loss: 3.360719 | lr:5.1701e-04 | norm 0.3391 | dt 338.02ms | 1551045.91 tokens/sec
Step 5424 | loss: 3.348338 | lr:5.1697e-04 | norm 0.3013 | dt 337.22ms | 1554721.71 tokens/sec
Step 5425 | loss: 3.289900 | lr:5.1694e-04 | norm 0.2532 | dt 338.27ms | 1549919.90 tokens/sec
Step 5426 | loss: 3.260407 | lr:5.1691e-04 | norm 0.2872 | dt 337.55ms | 1553205.20 tokens/sec
Step 5427 | loss: 3.313414 | lr:5.1687e-04 | norm 0.2924 | dt 338.00ms | 1551143.28 tokens/sec
Step 5428 | loss: 3.281629 | lr:5.1684e-04 | norm 0.2634 | dt 338.08ms | 1550801.98 tokens/sec
Step 5429 | loss: 3.147738 | lr:5.1681e-04 | norm 0.2890 | dt 337.23ms | 1554676.64 tokens/sec
Step 5430 | loss: 3.122945 | lr:5.1677e-04 | norm 0.2924 | dt 336.55ms | 1557847.50 tokens/sec
Step 5431 | loss: 3.202796 | lr:5.1674e-04 | norm 0.2744 | dt 339.05ms | 1546337.42 tokens/sec
Step 5432 | loss: 3.171859 | lr:5.1671e-04 | norm 0.2743 | dt 338.50ms | 1548851.16 tokens/sec
Step 5433 | loss: 3.086037 | lr:5.1667e-04 | norm 0.3044 | dt 337.79ms | 1552126.44 tokens/sec
Step 5434 | loss: 3.196628 | lr:5.1664e-04 | norm 0.2975 | dt 338.58ms | 1548486.88 tokens/sec
Step 5435 | loss: 3.081149 | lr:5.1661e-04 | norm 0.2686 | dt 338.26ms | 1549958.14 tokens/sec
Step 5436 | loss: 3.122189 | lr:5.1657e-04 | norm 0.2896 | dt 337.63ms | 1552845.44 tokens/sec
Step 5437 | loss: 3.137814 | lr:5.1654e-04 | norm 0.2916 | dt 339.89ms | 1542510.66 tokens/sec
Step 5438 | loss: 3.215916 | lr:5.1651e-04 | norm 0.2782 | dt 338.12ms | 1550604.06 tokens/sec
Step 5439 | loss: 3.114964 | lr:5.1647e-04 | norm 0.2758 | dt 337.63ms | 1552857.51 tokens/sec
Step 5440 | loss: 3.326228 | lr:5.1644e-04 | norm 0.2940 | dt 337.76ms | 1552255.73 tokens/sec
Step 5441 | loss: 3.317476 | lr:5.1641e-04 | norm 0.2757 | dt 338.60ms | 1548396.38 tokens/sec
Step 5442 | loss: 3.461452 | lr:5.1637e-04 | norm 0.3158 | dt 337.31ms | 1554321.70 tokens/sec
Step 5443 | loss: 3.270162 | lr:5.1634e-04 | norm 0.3240 | dt 338.18ms | 1550302.34 tokens/sec
Step 5444 | loss: 3.395025 | lr:5.1631e-04 | norm 0.3038 | dt 337.70ms | 1552526.42 tokens/sec
Step 5445 | loss: 3.339439 | lr:5.1627e-04 | norm 0.3407 | dt 338.08ms | 1550771.36 tokens/sec
Step 5446 | loss: 3.279290 | lr:5.1624e-04 | norm 0.2899 | dt 337.39ms | 1553932.88 tokens/sec
Step 5447 | loss: 3.247221 | lr:5.1621e-04 | norm 0.2735 | dt 338.19ms | 1550268.46 tokens/sec
Step 5448 | loss: 3.310420 | lr:5.1617e-04 | norm 0.2761 | dt 338.04ms | 1550962.77 tokens/sec
Step 5449 | loss: 3.373974 | lr:5.1614e-04 | norm 0.2669 | dt 337.85ms | 1551853.70 tokens/sec
Step 5450 | loss: 3.277934 | lr:5.1611e-04 | norm 0.2610 | dt 337.84ms | 1551888.75 tokens/sec
Step 5451 | loss: 3.294328 | lr:5.1607e-04 | norm 0.2824 | dt 338.87ms | 1547184.95 tokens/sec
Step 5452 | loss: 3.256627 | lr:5.1604e-04 | norm 0.2634 | dt 339.66ms | 1543563.08 tokens/sec
Step 5453 | loss: 3.311669 | lr:5.1601e-04 | norm 0.2582 | dt 337.83ms | 1551950.08 tokens/sec
Step 5454 | loss: 3.287387 | lr:5.1597e-04 | norm 0.2656 | dt 337.89ms | 1551651.13 tokens/sec
Step 5455 | loss: 3.329563 | lr:5.1594e-04 | norm 0.2631 | dt 338.86ms | 1547226.32 tokens/sec
Step 5456 | loss: 3.313352 | lr:5.1591e-04 | norm 0.2773 | dt 337.96ms | 1551316.17 tokens/sec
Step 5457 | loss: 3.305624 | lr:5.1587e-04 | norm 0.3039 | dt 338.13ms | 1550567.98 tokens/sec
Step 5458 | loss: 3.319462 | lr:5.1584e-04 | norm 0.2673 | dt 339.44ms | 1544549.68 tokens/sec
Step 5459 | loss: 3.394155 | lr:5.1580e-04 | norm 0.2639 | dt 337.40ms | 1553890.06 tokens/sec
Step 5460 | loss: 3.439464 | lr:5.1577e-04 | norm 0.3401 | dt 337.89ms | 1551658.79 tokens/sec
Step 5461 | loss: 3.354052 | lr:5.1574e-04 | norm 0.3185 | dt 338.01ms | 1551096.23 tokens/sec
Step 5462 | loss: 3.412167 | lr:5.1570e-04 | norm 0.3299 | dt 338.14ms | 1550490.35 tokens/sec
Step 5463 | loss: 3.350169 | lr:5.1567e-04 | norm 0.3210 | dt 338.33ms | 1549654.49 tokens/sec
Step 5464 | loss: 3.295826 | lr:5.1564e-04 | norm 0.3071 | dt 338.26ms | 1549953.77 tokens/sec
Step 5465 | loss: 3.400371 | lr:5.1560e-04 | norm 0.3324 | dt 338.47ms | 1549009.36 tokens/sec
Step 5466 | loss: 3.292604 | lr:5.1557e-04 | norm 0.3214 | dt 338.46ms | 1549044.28 tokens/sec
Step 5467 | loss: 3.277077 | lr:5.1554e-04 | norm 0.2902 | dt 339.35ms | 1544967.46 tokens/sec
Step 5468 | loss: 3.276682 | lr:5.1550e-04 | norm 0.3259 | dt 338.92ms | 1546934.62 tokens/sec
Step 5469 | loss: 3.329175 | lr:5.1547e-04 | norm 0.3026 | dt 339.51ms | 1544241.64 tokens/sec
Step 5470 | loss: 3.319878 | lr:5.1544e-04 | norm 0.2965 | dt 339.66ms | 1543579.34 tokens/sec
Step 5471 | loss: 3.283251 | lr:5.1540e-04 | norm 0.2872 | dt 338.06ms | 1550862.14 tokens/sec
Step 5472 | loss: 3.304849 | lr:5.1537e-04 | norm 0.3330 | dt 338.05ms | 1550915.73 tokens/sec
Step 5473 | loss: 3.332192 | lr:5.1534e-04 | norm 0.2957 | dt 339.47ms | 1544424.93 tokens/sec
Step 5474 | loss: 3.278793 | lr:5.1530e-04 | norm 0.3056 | dt 338.23ms | 1550077.22 tokens/sec
Step 5475 | loss: 3.185820 | lr:5.1527e-04 | norm 0.2925 | dt 338.78ms | 1547561.69 tokens/sec
Step 5476 | loss: 3.178642 | lr:5.1523e-04 | norm 0.2541 | dt 338.38ms | 1549390.26 tokens/sec
Step 5477 | loss: 3.166757 | lr:5.1520e-04 | norm 0.2806 | dt 338.69ms | 1548001.81 tokens/sec
Step 5478 | loss: 3.162464 | lr:5.1517e-04 | norm 0.2424 | dt 338.41ms | 1549250.54 tokens/sec
Step 5479 | loss: 3.169935 | lr:5.1513e-04 | norm 0.2821 | dt 339.19ms | 1545725.47 tokens/sec
Step 5480 | loss: 3.164277 | lr:5.1510e-04 | norm 0.2708 | dt 905.60ms | 578937.45 tokens/sec
Step 5481 | loss: 3.120848 | lr:5.1507e-04 | norm 0.2600 | dt 336.55ms | 1557809.98 tokens/sec
Step 5482 | loss: 3.116238 | lr:5.1503e-04 | norm 0.2912 | dt 337.61ms | 1552950.72 tokens/sec
Step 5483 | loss: 3.171699 | lr:5.1500e-04 | norm 0.2807 | dt 339.31ms | 1545159.61 tokens/sec
Step 5484 | loss: 3.124521 | lr:5.1497e-04 | norm 0.2737 | dt 339.07ms | 1546231.96 tokens/sec
Step 5485 | loss: 3.141903 | lr:5.1493e-04 | norm 0.2494 | dt 337.83ms | 1551909.56 tokens/sec
Step 5486 | loss: 3.202379 | lr:5.1490e-04 | norm 0.2674 | dt 340.10ms | 1541556.93 tokens/sec
Step 5487 | loss: 3.342933 | lr:5.1486e-04 | norm 0.2611 | dt 338.02ms | 1551052.47 tokens/sec
Step 5488 | loss: 3.347696 | lr:5.1483e-04 | norm 0.2581 | dt 338.59ms | 1548425.82 tokens/sec
Step 5489 | loss: 3.352744 | lr:5.1480e-04 | norm 0.2647 | dt 337.88ms | 1551685.07 tokens/sec
Step 5490 | loss: 3.295096 | lr:5.1476e-04 | norm 0.2706 | dt 338.46ms | 1549044.28 tokens/sec
Step 5491 | loss: 3.289907 | lr:5.1473e-04 | norm 0.2701 | dt 338.81ms | 1547461.50 tokens/sec
Step 5492 | loss: 3.287786 | lr:5.1470e-04 | norm 0.2979 | dt 338.26ms | 1549973.43 tokens/sec
Step 5493 | loss: 3.295497 | lr:5.1466e-04 | norm 0.2675 | dt 337.95ms | 1551390.60 tokens/sec
Step 5494 | loss: 3.291063 | lr:5.1463e-04 | norm 0.2791 | dt 338.26ms | 1549957.04 tokens/sec
Step 5495 | loss: 3.271245 | lr:5.1459e-04 | norm 0.3002 | dt 338.13ms | 1550547.20 tokens/sec
Step 5496 | loss: 3.303961 | lr:5.1456e-04 | norm 0.3488 | dt 338.35ms | 1549531.10 tokens/sec
Step 5497 | loss: 3.306664 | lr:5.1453e-04 | norm 0.2981 | dt 337.74ms | 1552355.44 tokens/sec
Step 5498 | loss: 3.293733 | lr:5.1449e-04 | norm 0.2814 | dt 338.28ms | 1549843.44 tokens/sec
Step 5499 | loss: 3.308767 | lr:5.1446e-04 | norm 0.3028 | dt 339.06ms | 1546311.33 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 5500: 3.3394
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2797/10042=0.2785


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I don't have to be a teacher of languages in school. However, I can't be that easy, I have
rank 5 sample 1 >Hello, I'm a language model, where students understand what type I and type II code is and they find it interesting."
The next two books, "
rank 5 sample 2 >Hello, I'm a language model, and I understand it like a puzzle piece. But I'm not, and there is another way to explain it. You
rank 5 sample 3 >Hello, I'm a language model, writing in English for English class. I have four children in my family. In school they have fun, and I use




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, now we'll do it this week. I'll be writing it for my next webinar. I'll ask you,
rank 2 sample 1 >Hello, I'm a language model, I'm familiar with the concept of machine learning. "Machine" is when you think of something, not something that looks
rank 2 sample 2 >Hello, I'm a language model, and I have been teaching English in several programs. I have worked for 30 years. I've helped others to do better
rank 2 sample 3 >Hello, I'm a language model, I use a lot of language, and don't make mistakes that I don't understand, let me explain, and I




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I know that when I want to go into data science, I am always looking at algorithms. When I want to
rank 4 sample 1 >Hello, I'm a language model, don't worry. But I will talk about all kinds of things. What's up for you? If you try,
rank 4 sample 2 >Hello, I'm a language model, but I always take this seriously. In this article, I explain some of the tools for that type of language that you


ddp_rank 7: ####### Printing generated samples ####### 

rank 4 sample 3 >Hello, I'm a language model, so let me get my thoughts. Can a student help you explain? Does your grammar model make this easier? Thanks for


rank 7 sample 0 >Hello, I'm a language model, so I'd like to write a list around the object and we'll create a series of numbers here on my computer,
rank 7 sample 1 >Hello, I'm a language model, having made this book a place to help anyone who wants to learn Spanish. I started teaching my students and I've written
rank 7 sample 2 >Hello, I'm a language model, I'm not that excited about doing all that stuff because I think you do have to do it. I think, well
rank 7 sample 3 >Hello, I'm a language model, yet it's not really like the kind of library you put on the desk. So I'm thinking, "It�




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, but if you're still unable, try using the right language for the job!
- For example, you can start
rank 6 sample 1 >Hello, I'm a language model, which is what I like. But I never tried using it in my everyday life, I don't think we're better
rank 6 sample 2 >Hello, I'm a language model, I know, but I can understand, because that's what's gonna make learning a language.
As with every other
rank 6 sample 3 >Hello, I'm a language model, but I think I have a huge collection of words. I'm currently working at the university teaching English and is really learning




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know I can help me learn another language.
What to Watch For:
Why should we speak English with
rank 0 sample 1 >Hello, I'm a language model, and in my opinion it's very general. It isn't so easy to get into, because I do it. So
rank 0 sample 2 >Hello, I'm a language model, and I're actually able to have this in perspective.
This isn't a huge task for me, but it'll
rank 0 sample 3 >Hello, I'm a language model, I always have some questions on the fly and not just about writing an answer to your question.
First, let me




ddp_rank 3: ####### Printing generated samples ####### 


rank 3 sample 0 >Hello, I'm a language model, so I'm not the type. As soon as the user comes in one key, he or she uses it correctly


ddp_rank 1: ####### Printing generated samples ####### 

rank 3 sample 1 >Hello, I'm a language model, so what are you doing to learn? I want to start with a class of words that I'll use to show me
rank 3 sample 2 >Hello, I'm a language model, so it makes sense.
"I could understand that if you want a program, you'll also need a function and
rank 1 sample 0 >Hello, I'm a language model, I know. I think it's helpful for us as individuals. I'm looking for the perfect place to start.

rank 3 sample 3 >Hello, I'm a language model, so if you can't tell how many people have the following list, then I know this and I ask, "What


rank 1 sample 1 >Hello, I'm a language model, a computer program written in the language of machine learning. There are thousands of these students! I found a lot of them
rank 1 sample 2 >Hello, I'm a language model, I won't use it for my book, but I think most people have an understanding of what they think is.

rank 1 sample 3 >Hello, I'm a language model, so I'm just trying to give you our best possible foundation and knowledge when something is more fundamental and more precise.



Step 5500 | loss: 3.289781 | lr:5.1443e-04 | norm 0.2712 | dt 18515.44ms | 28316.26 tokens/sec
Step 5501 | loss: 3.328264 | lr:5.1439e-04 | norm 0.2764 | dt 334.14ms | 1569076.64 tokens/sec
Step 5502 | loss: 3.291974 | lr:5.1436e-04 | norm 0.2728 | dt 335.99ms | 1560432.05 tokens/sec
Step 5503 | loss: 3.337992 | lr:5.1432e-04 | norm 0.2864 | dt 336.73ms | 1556997.07 tokens/sec
Step 5504 | loss: 3.375352 | lr:5.1429e-04 | norm 0.2909 | dt 335.86ms | 1561023.56 tokens/sec
Step 5505 | loss: 3.429442 | lr:5.1426e-04 | norm 0.2726 | dt 335.57ms | 1562381.09 tokens/sec
Step 5506 | loss: 3.356283 | lr:5.1422e-04 | norm 0.2842 | dt 336.09ms | 1559975.98 tokens/sec
Step 5507 | loss: 3.315320 | lr:5.1419e-04 | norm 0.2606 | dt 336.64ms | 1557415.00 tokens/sec
Step 5508 | loss: 3.374123 | lr:5.1416e-04 | norm 0.2895 | dt 336.81ms | 1556617.93 tokens/sec
Step 5509 | loss: 3.304605 | lr:5.1412e-04 | norm 0.3007 | dt 998.29ms | 525183.90 tokens/sec
Step 5510 | loss: 3.326693 | lr:5.1409e-04 | norm 0.2737 | dt 336.19ms | 1559519.07 tokens/sec
Step 5511 | loss: 3.340516 | lr:5.1405e-04 | norm 0.2718 | dt 336.01ms | 1560337.93 tokens/sec
Step 5512 | loss: 3.332371 | lr:5.1402e-04 | norm 0.2730 | dt 336.81ms | 1556636.66 tokens/sec
Step 5513 | loss: 3.375897 | lr:5.1399e-04 | norm 0.2637 | dt 336.67ms | 1557253.98 tokens/sec
Step 5514 | loss: 3.311026 | lr:5.1395e-04 | norm 0.3028 | dt 336.82ms | 1556569.45 tokens/sec
Step 5515 | loss: 3.394530 | lr:5.1392e-04 | norm 0.2781 | dt 337.48ms | 1553531.09 tokens/sec
Step 5516 | loss: 3.308552 | lr:5.1389e-04 | norm 0.2934 | dt 336.87ms | 1556363.44 tokens/sec
Step 5517 | loss: 3.289799 | lr:5.1385e-04 | norm 0.2676 | dt 338.51ms | 1548797.71 tokens/sec
Step 5518 | loss: 3.254888 | lr:5.1382e-04 | norm 0.2947 | dt 337.16ms | 1555011.95 tokens/sec
Step 5519 | loss: 3.272671 | lr:5.1378e-04 | norm 0.2844 | dt 337.64ms | 1552787.33 tokens/sec
Step 5520 | loss: 3.116559 | lr:5.1375e-04 | norm 0.3074 | dt 336.99ms | 1555785.36 tokens/sec
Step 5521 | loss: 3.195763 | lr:5.1372e-04 | norm 0.3631 | dt 337.05ms | 1555522.33 tokens/sec
Step 5522 | loss: 3.166567 | lr:5.1368e-04 | norm 0.3812 | dt 336.98ms | 1555861.31 tokens/sec
Step 5523 | loss: 3.210899 | lr:5.1365e-04 | norm 0.3455 | dt 336.10ms | 1559929.50 tokens/sec
Step 5524 | loss: 3.184371 | lr:5.1361e-04 | norm 0.3238 | dt 336.28ms | 1559069.06 tokens/sec
Step 5525 | loss: 3.102292 | lr:5.1358e-04 | norm 0.3012 | dt 336.79ms | 1556713.80 tokens/sec
Step 5526 | loss: 3.164865 | lr:5.1355e-04 | norm 0.2855 | dt 337.11ms | 1555235.20 tokens/sec
Step 5527 | loss: 3.131893 | lr:5.1351e-04 | norm 0.2978 | dt 336.65ms | 1557383.01 tokens/sec
Step 5528 | loss: 3.092283 | lr:5.1348e-04 | norm 0.2766 | dt 337.36ms | 1554098.71 tokens/sec
Step 5529 | loss: 3.155962 | lr:5.1344e-04 | norm 0.2559 | dt 336.88ms | 1556325.99 tokens/sec
Step 5530 | loss: 3.052530 | lr:5.1341e-04 | norm 0.2732 | dt 336.17ms | 1559603.13 tokens/sec
Step 5531 | loss: 3.155891 | lr:5.1338e-04 | norm 0.2586 | dt 336.84ms | 1556481.31 tokens/sec
Step 5532 | loss: 3.320991 | lr:5.1334e-04 | norm 0.2914 | dt 337.06ms | 1555490.42 tokens/sec
Step 5533 | loss: 3.333872 | lr:5.1331e-04 | norm 0.2972 | dt 338.34ms | 1549574.77 tokens/sec
Step 5534 | loss: 3.284120 | lr:5.1328e-04 | norm 0.2895 | dt 337.48ms | 1553555.23 tokens/sec
Step 5535 | loss: 3.321878 | lr:5.1324e-04 | norm 0.2583 | dt 337.12ms | 1555218.70 tokens/sec
Step 5536 | loss: 3.212895 | lr:5.1321e-04 | norm 0.2966 | dt 337.85ms | 1551821.95 tokens/sec
Step 5537 | loss: 3.264436 | lr:5.1317e-04 | norm 0.2738 | dt 337.16ms | 1555003.15 tokens/sec
Step 5538 | loss: 3.221269 | lr:5.1314e-04 | norm 0.2908 | dt 336.54ms | 1557873.99 tokens/sec
Step 5539 | loss: 3.271588 | lr:5.1311e-04 | norm 0.2579 | dt 337.23ms | 1554675.54 tokens/sec
Step 5540 | loss: 3.346986 | lr:5.1307e-04 | norm 0.2972 | dt 337.13ms | 1555143.91 tokens/sec
Step 5541 | loss: 3.248230 | lr:5.1304e-04 | norm 0.2848 | dt 337.94ms | 1551411.39 tokens/sec
Step 5542 | loss: 3.297981 | lr:5.1300e-04 | norm 0.2724 | dt 337.59ms | 1553049.43 tokens/sec
Step 5543 | loss: 3.314473 | lr:5.1297e-04 | norm 0.2712 | dt 337.62ms | 1552890.40 tokens/sec
Step 5544 | loss: 3.303754 | lr:5.1294e-04 | norm 0.2731 | dt 337.44ms | 1553707.81 tokens/sec
Step 5545 | loss: 3.310956 | lr:5.1290e-04 | norm 0.2896 | dt 337.47ms | 1553562.92 tokens/sec
Step 5546 | loss: 3.331273 | lr:5.1287e-04 | norm 0.2723 | dt 337.81ms | 1552020.18 tokens/sec
Step 5547 | loss: 3.391641 | lr:5.1283e-04 | norm 0.2727 | dt 337.26ms | 1554549.15 tokens/sec
Step 5548 | loss: 3.294515 | lr:5.1280e-04 | norm 0.2653 | dt 338.23ms | 1550077.22 tokens/sec
Step 5549 | loss: 3.371174 | lr:5.1277e-04 | norm 0.2860 | dt 338.84ms | 1547314.50 tokens/sec
Step 5550 | loss: 3.359223 | lr:5.1273e-04 | norm 0.2782 | dt 338.35ms | 1549548.57 tokens/sec
Step 5551 | loss: 3.359489 | lr:5.1270e-04 | norm 0.2947 | dt 339.19ms | 1545710.26 tokens/sec
Step 5552 | loss: 3.338020 | lr:5.1266e-04 | norm 0.2889 | dt 338.74ms | 1547771.91 tokens/sec
Step 5553 | loss: 3.362346 | lr:5.1263e-04 | norm 0.2759 | dt 339.13ms | 1545999.32 tokens/sec
Step 5554 | loss: 3.321456 | lr:5.1260e-04 | norm 0.3155 | dt 339.47ms | 1544453.13 tokens/sec
Step 5555 | loss: 3.279456 | lr:5.1256e-04 | norm 0.3173 | dt 339.12ms | 1546008.02 tokens/sec
Step 5556 | loss: 3.328037 | lr:5.1253e-04 | norm 0.2833 | dt 339.16ms | 1545833.05 tokens/sec
Step 5557 | loss: 3.297956 | lr:5.1249e-04 | norm 0.2702 | dt 338.58ms | 1548494.52 tokens/sec
Step 5558 | loss: 3.268815 | lr:5.1246e-04 | norm 0.2668 | dt 338.83ms | 1547364.59 tokens/sec
Step 5559 | loss: 3.329249 | lr:5.1243e-04 | norm 0.2892 | dt 338.55ms | 1548629.74 tokens/sec
Step 5560 | loss: 3.299141 | lr:5.1239e-04 | norm 0.2811 | dt 338.85ms | 1547267.69 tokens/sec
Step 5561 | loss: 3.286821 | lr:5.1236e-04 | norm 0.2933 | dt 337.46ms | 1553647.43 tokens/sec
Step 5562 | loss: 3.332388 | lr:5.1232e-04 | norm 0.3032 | dt 338.80ms | 1547500.70 tokens/sec
Step 5563 | loss: 3.334541 | lr:5.1229e-04 | norm 0.2997 | dt 338.00ms | 1551146.56 tokens/sec
Step 5564 | loss: 3.300282 | lr:5.1226e-04 | norm 0.3022 | dt 338.73ms | 1547822.03 tokens/sec
Step 5565 | loss: 3.287061 | lr:5.1222e-04 | norm 0.2901 | dt 338.08ms | 1550789.95 tokens/sec
Step 5566 | loss: 3.143912 | lr:5.1219e-04 | norm 0.2911 | dt 337.99ms | 1551203.46 tokens/sec
Step 5567 | loss: 3.165008 | lr:5.1215e-04 | norm 0.2744 | dt 337.95ms | 1551364.33 tokens/sec
Step 5568 | loss: 3.195771 | lr:5.1212e-04 | norm 0.2902 | dt 338.57ms | 1548544.68 tokens/sec
Step 5569 | loss: 3.130787 | lr:5.1208e-04 | norm 0.2984 | dt 338.28ms | 1549868.56 tokens/sec
Step 5570 | loss: 3.190141 | lr:5.1205e-04 | norm 0.2710 | dt 338.23ms | 1550112.19 tokens/sec
Step 5571 | loss: 3.077402 | lr:5.1202e-04 | norm 0.2788 | dt 339.47ms | 1544442.28 tokens/sec
Step 5572 | loss: 3.142277 | lr:5.1198e-04 | norm 0.2842 | dt 338.53ms | 1548714.81 tokens/sec
Step 5573 | loss: 3.157567 | lr:5.1195e-04 | norm 0.2818 | dt 339.18ms | 1545754.81 tokens/sec
Step 5574 | loss: 3.074840 | lr:5.1191e-04 | norm 0.3025 | dt 338.72ms | 1547829.65 tokens/sec
Step 5575 | loss: 3.147910 | lr:5.1188e-04 | norm 0.2883 | dt 339.25ms | 1545411.54 tokens/sec
Step 5576 | loss: 3.177915 | lr:5.1185e-04 | norm 0.2840 | dt 339.60ms | 1543838.34 tokens/sec
Step 5577 | loss: 3.242289 | lr:5.1181e-04 | norm 0.2919 | dt 337.98ms | 1551251.61 tokens/sec
Step 5578 | loss: 3.313748 | lr:5.1178e-04 | norm 0.2770 | dt 337.64ms | 1552789.52 tokens/sec
Step 5579 | loss: 3.295609 | lr:5.1174e-04 | norm 0.2689 | dt 338.79ms | 1547538.82 tokens/sec
Step 5580 | loss: 3.313617 | lr:5.1171e-04 | norm 0.2891 | dt 338.56ms | 1548579.57 tokens/sec
Step 5581 | loss: 3.322380 | lr:5.1167e-04 | norm 0.2853 | dt 337.62ms | 1552875.05 tokens/sec
Step 5582 | loss: 3.269938 | lr:5.1164e-04 | norm 0.2749 | dt 337.97ms | 1551297.57 tokens/sec
Step 5583 | loss: 3.296096 | lr:5.1161e-04 | norm 0.2752 | dt 338.59ms | 1548431.27 tokens/sec
Step 5584 | loss: 3.253007 | lr:5.1157e-04 | norm 0.2585 | dt 338.28ms | 1549874.02 tokens/sec
Step 5585 | loss: 3.296999 | lr:5.1154e-04 | norm 0.2930 | dt 337.57ms | 1553128.41 tokens/sec
Step 5586 | loss: 3.280878 | lr:5.1150e-04 | norm 0.2881 | dt 337.96ms | 1551346.82 tokens/sec
Step 5587 | loss: 3.252558 | lr:5.1147e-04 | norm 0.2671 | dt 338.22ms | 1550144.97 tokens/sec
Step 5588 | loss: 3.317869 | lr:5.1144e-04 | norm 0.2623 | dt 338.18ms | 1550320.92 tokens/sec
Step 5589 | loss: 3.275233 | lr:5.1140e-04 | norm 0.3299 | dt 338.00ms | 1551157.51 tokens/sec
Step 5590 | loss: 3.260872 | lr:5.1137e-04 | norm 0.2988 | dt 338.09ms | 1550719.96 tokens/sec
Step 5591 | loss: 3.324197 | lr:5.1133e-04 | norm 0.2896 | dt 337.47ms | 1553579.38 tokens/sec
Step 5592 | loss: 3.286071 | lr:5.1130e-04 | norm 0.2790 | dt 338.36ms | 1549477.60 tokens/sec
Step 5593 | loss: 3.398869 | lr:5.1126e-04 | norm 0.2708 | dt 338.04ms | 1550972.61 tokens/sec
Step 5594 | loss: 3.348173 | lr:5.1123e-04 | norm 0.2882 | dt 338.07ms | 1550845.73 tokens/sec
Step 5595 | loss: 3.296275 | lr:5.1120e-04 | norm 1.5453 | dt 338.23ms | 1550072.85 tokens/sec
Step 5596 | loss: 3.325279 | lr:5.1116e-04 | norm 0.3193 | dt 337.85ms | 1551846.04 tokens/sec
Step 5597 | loss: 3.366142 | lr:5.1113e-04 | norm 0.3203 | dt 337.86ms | 1551770.48 tokens/sec
Step 5598 | loss: 3.321465 | lr:5.1109e-04 | norm 0.2818 | dt 338.11ms | 1550637.95 tokens/sec
Step 5599 | loss: 3.344020 | lr:5.1106e-04 | norm 0.3288 | dt 337.97ms | 1551295.38 tokens/sec
Step 5600 | loss: 3.344645 | lr:5.1102e-04 | norm 0.3144 | dt 337.78ms | 1552140.69 tokens/sec
Step 5601 | loss: 3.328709 | lr:5.1099e-04 | norm 0.2918 | dt 337.15ms | 1555038.34 tokens/sec
Step 5602 | loss: 3.325777 | lr:5.1096e-04 | norm 0.3049 | dt 337.92ms | 1551509.91 tokens/sec
Step 5603 | loss: 3.333933 | lr:5.1092e-04 | norm 0.3436 | dt 337.65ms | 1552746.76 tokens/sec
Step 5604 | loss: 3.306426 | lr:5.1089e-04 | norm 0.3203 | dt 337.36ms | 1554087.73 tokens/sec
Step 5605 | loss: 3.344460 | lr:5.1085e-04 | norm 0.2836 | dt 337.97ms | 1551297.57 tokens/sec
Step 5606 | loss: 3.300819 | lr:5.1082e-04 | norm 0.2698 | dt 337.78ms | 1552153.83 tokens/sec
Step 5607 | loss: 3.344594 | lr:5.1078e-04 | norm 0.3071 | dt 338.02ms | 1551074.35 tokens/sec
Step 5608 | loss: 3.355445 | lr:5.1075e-04 | norm 0.2686 | dt 337.28ms | 1554467.83 tokens/sec
Step 5609 | loss: 3.352895 | lr:5.1072e-04 | norm 0.2679 | dt 337.53ms | 1553291.87 tokens/sec
Step 5610 | loss: 3.273754 | lr:5.1068e-04 | norm 0.2602 | dt 337.65ms | 1552745.67 tokens/sec
Step 5611 | loss: 3.078335 | lr:5.1065e-04 | norm 0.2785 | dt 337.91ms | 1551582.16 tokens/sec
Step 5612 | loss: 3.098543 | lr:5.1061e-04 | norm 0.6515 | dt 337.81ms | 1552008.14 tokens/sec
Step 5613 | loss: 3.142586 | lr:5.1058e-04 | norm 0.3487 | dt 337.72ms | 1552446.40 tokens/sec
Step 5614 | loss: 3.150762 | lr:5.1054e-04 | norm 0.2782 | dt 338.40ms | 1549330.22 tokens/sec
Step 5615 | loss: 3.138436 | lr:5.1051e-04 | norm 0.3086 | dt 338.96ms | 1546759.44 tokens/sec
Step 5616 | loss: 3.139541 | lr:5.1048e-04 | norm 0.3464 | dt 337.91ms | 1551551.50 tokens/sec
Step 5617 | loss: 3.123025 | lr:5.1044e-04 | norm 0.3010 | dt 338.33ms | 1549647.94 tokens/sec
Step 5618 | loss: 3.117111 | lr:5.1041e-04 | norm 0.2818 | dt 338.70ms | 1547953.86 tokens/sec
Step 5619 | loss: 3.141968 | lr:5.1037e-04 | norm 0.2704 | dt 338.38ms | 1549385.89 tokens/sec
Step 5620 | loss: 3.114628 | lr:5.1034e-04 | norm 0.3205 | dt 337.35ms | 1554114.09 tokens/sec
Step 5621 | loss: 3.109566 | lr:5.1030e-04 | norm 0.3567 | dt 337.41ms | 1553840.65 tokens/sec
Step 5622 | loss: 3.364021 | lr:5.1027e-04 | norm 0.3042 | dt 338.78ms | 1547582.38 tokens/sec
Step 5623 | loss: 3.369108 | lr:5.1023e-04 | norm 0.3833 | dt 338.18ms | 1550319.83 tokens/sec
Step 5624 | loss: 3.352972 | lr:5.1020e-04 | norm 0.3544 | dt 338.47ms | 1548985.36 tokens/sec
Step 5625 | loss: 3.325296 | lr:5.1017e-04 | norm 0.2959 | dt 338.57ms | 1548523.96 tokens/sec
Step 5626 | loss: 3.250702 | lr:5.1013e-04 | norm 0.3591 | dt 338.56ms | 1548604.65 tokens/sec
Step 5627 | loss: 3.316951 | lr:5.1010e-04 | norm 0.3104 | dt 338.49ms | 1548887.16 tokens/sec
Step 5628 | loss: 3.284010 | lr:5.1006e-04 | norm 0.2939 | dt 338.53ms | 1548713.72 tokens/sec
Step 5629 | loss: 3.315487 | lr:5.1003e-04 | norm 0.3537 | dt 338.30ms | 1549750.59 tokens/sec
Step 5630 | loss: 3.323235 | lr:5.0999e-04 | norm 0.2811 | dt 337.76ms | 1552268.88 tokens/sec
Step 5631 | loss: 3.247078 | lr:5.0996e-04 | norm 0.2798 | dt 337.94ms | 1551421.24 tokens/sec
Step 5632 | loss: 3.297411 | lr:5.0992e-04 | norm 0.2707 | dt 339.01ms | 1546525.56 tokens/sec
Step 5633 | loss: 3.278684 | lr:5.0989e-04 | norm 0.2619 | dt 338.09ms | 1550726.52 tokens/sec
Step 5634 | loss: 3.278033 | lr:5.0986e-04 | norm 0.3000 | dt 337.59ms | 1553017.62 tokens/sec
Step 5635 | loss: 3.282573 | lr:5.0982e-04 | norm 0.3476 | dt 338.37ms | 1549460.13 tokens/sec
Step 5636 | loss: 3.308084 | lr:5.0979e-04 | norm 0.2654 | dt 338.19ms | 1550275.02 tokens/sec
Step 5637 | loss: 3.243884 | lr:5.0975e-04 | norm 0.2708 | dt 338.36ms | 1549503.80 tokens/sec
Step 5638 | loss: 3.353912 | lr:5.0972e-04 | norm 0.2783 | dt 338.13ms | 1550573.44 tokens/sec
Step 5639 | loss: 3.298967 | lr:5.0968e-04 | norm 0.2852 | dt 338.03ms | 1551003.24 tokens/sec
Step 5640 | loss: 3.357794 | lr:5.0965e-04 | norm 0.2897 | dt 337.95ms | 1551396.07 tokens/sec
Step 5641 | loss: 3.322626 | lr:5.0961e-04 | norm 0.2753 | dt 338.42ms | 1549242.90 tokens/sec
Step 5642 | loss: 3.314896 | lr:5.0958e-04 | norm 0.2831 | dt 339.03ms | 1546443.99 tokens/sec
Step 5643 | loss: 3.405321 | lr:5.0955e-04 | norm 0.2841 | dt 338.12ms | 1550605.15 tokens/sec
Step 5644 | loss: 3.352492 | lr:5.0951e-04 | norm 0.2828 | dt 338.96ms | 1546743.12 tokens/sec
Step 5645 | loss: 3.305235 | lr:5.0948e-04 | norm 0.2536 | dt 338.90ms | 1547011.89 tokens/sec
Step 5646 | loss: 3.324276 | lr:5.0944e-04 | norm 0.2988 | dt 338.24ms | 1550029.15 tokens/sec
Step 5647 | loss: 3.328145 | lr:5.0941e-04 | norm 0.2778 | dt 338.18ms | 1550305.62 tokens/sec
Step 5648 | loss: 3.340384 | lr:5.0937e-04 | norm 0.2594 | dt 339.53ms | 1544164.65 tokens/sec
Step 5649 | loss: 3.322053 | lr:5.0934e-04 | norm 0.2415 | dt 338.40ms | 1549293.11 tokens/sec
Step 5650 | loss: 3.246242 | lr:5.0930e-04 | norm 0.2703 | dt 337.93ms | 1551457.36 tokens/sec
Step 5651 | loss: 3.305856 | lr:5.0927e-04 | norm 0.2448 | dt 338.73ms | 1547803.50 tokens/sec
Step 5652 | loss: 3.324685 | lr:5.0923e-04 | norm 0.2668 | dt 338.77ms | 1547640.11 tokens/sec
Step 5653 | loss: 3.259122 | lr:5.0920e-04 | norm 0.2646 | dt 338.27ms | 1549926.46 tokens/sec
Step 5654 | loss: 3.365042 | lr:5.0917e-04 | norm 0.2824 | dt 337.84ms | 1551860.28 tokens/sec
Step 5655 | loss: 3.279144 | lr:5.0913e-04 | norm 0.3079 | dt 339.21ms | 1545620.09 tokens/sec
Step 5656 | loss: 3.149788 | lr:5.0910e-04 | norm 0.2435 | dt 338.40ms | 1549319.30 tokens/sec
Step 5657 | loss: 3.146913 | lr:5.0906e-04 | norm 0.2941 | dt 337.54ms | 1553254.56 tokens/sec
Step 5658 | loss: 3.147100 | lr:5.0903e-04 | norm 0.3552 | dt 338.28ms | 1549881.67 tokens/sec
Step 5659 | loss: 3.190979 | lr:5.0899e-04 | norm 0.3036 | dt 337.30ms | 1554349.17 tokens/sec
Step 5660 | loss: 3.187140 | lr:5.0896e-04 | norm 0.2911 | dt 338.07ms | 1550832.61 tokens/sec
Step 5661 | loss: 3.135890 | lr:5.0892e-04 | norm 0.2895 | dt 338.95ms | 1546804.05 tokens/sec
Step 5662 | loss: 3.199861 | lr:5.0889e-04 | norm 0.2983 | dt 337.23ms | 1554690.93 tokens/sec
Step 5663 | loss: 3.113244 | lr:5.0885e-04 | norm 0.2755 | dt 337.01ms | 1555684.10 tokens/sec
Step 5664 | loss: 3.106196 | lr:5.0882e-04 | norm 0.2821 | dt 338.02ms | 1551073.26 tokens/sec
Step 5665 | loss: 3.113916 | lr:5.0878e-04 | norm 0.2732 | dt 337.91ms | 1551576.68 tokens/sec
Step 5666 | loss: 3.127985 | lr:5.0875e-04 | norm 0.2484 | dt 337.78ms | 1552143.97 tokens/sec
Step 5667 | loss: 3.343261 | lr:5.0872e-04 | norm 0.2825 | dt 337.42ms | 1553802.22 tokens/sec
Step 5668 | loss: 3.294402 | lr:5.0868e-04 | norm 0.2917 | dt 338.19ms | 1550295.78 tokens/sec
Step 5669 | loss: 3.282160 | lr:5.0865e-04 | norm 0.2978 | dt 900.40ms | 582280.70 tokens/sec
Step 5670 | loss: 3.286039 | lr:5.0861e-04 | norm 0.2886 | dt 336.61ms | 1557542.96 tokens/sec
Step 5671 | loss: 3.225849 | lr:5.0858e-04 | norm 0.2993 | dt 337.53ms | 1553314.91 tokens/sec
Step 5672 | loss: 3.298847 | lr:5.0854e-04 | norm 0.3019 | dt 338.05ms | 1550908.08 tokens/sec
Step 5673 | loss: 3.278715 | lr:5.0851e-04 | norm 0.2912 | dt 337.62ms | 1552912.34 tokens/sec
Step 5674 | loss: 3.285228 | lr:5.0847e-04 | norm 0.2995 | dt 337.59ms | 1553047.24 tokens/sec
Step 5675 | loss: 3.273240 | lr:5.0844e-04 | norm 0.2579 | dt 337.36ms | 1554078.94 tokens/sec
Step 5676 | loss: 3.467309 | lr:5.0840e-04 | norm 0.3552 | dt 337.27ms | 1554494.20 tokens/sec
Step 5677 | loss: 3.282805 | lr:5.0837e-04 | norm 0.4265 | dt 338.07ms | 1550840.26 tokens/sec
Step 5678 | loss: 3.288576 | lr:5.0833e-04 | norm 0.3274 | dt 337.16ms | 1555017.45 tokens/sec
Step 5679 | loss: 3.317183 | lr:5.0830e-04 | norm 0.3261 | dt 338.05ms | 1550931.05 tokens/sec
Step 5680 | loss: 3.288863 | lr:5.0826e-04 | norm 0.3023 | dt 338.00ms | 1551155.32 tokens/sec
Step 5681 | loss: 3.246959 | lr:5.0823e-04 | norm 0.3173 | dt 337.56ms | 1553177.77 tokens/sec
Step 5682 | loss: 3.342246 | lr:5.0820e-04 | norm 0.3193 | dt 338.37ms | 1549444.84 tokens/sec
Step 5683 | loss: 3.324095 | lr:5.0816e-04 | norm 0.2946 | dt 338.72ms | 1547872.14 tokens/sec
Step 5684 | loss: 3.335011 | lr:5.0813e-04 | norm 0.2922 | dt 339.35ms | 1544973.97 tokens/sec
Step 5685 | loss: 3.310213 | lr:5.0809e-04 | norm 0.2847 | dt 338.96ms | 1546772.50 tokens/sec
Step 5686 | loss: 3.366772 | lr:5.0806e-04 | norm 0.2715 | dt 338.77ms | 1547637.93 tokens/sec
Step 5687 | loss: 3.372779 | lr:5.0802e-04 | norm 0.2817 | dt 338.23ms | 1550101.26 tokens/sec
Step 5688 | loss: 3.407146 | lr:5.0799e-04 | norm 0.2982 | dt 339.70ms | 1543382.16 tokens/sec
Step 5689 | loss: 3.348844 | lr:5.0795e-04 | norm 0.2970 | dt 339.15ms | 1545897.16 tokens/sec
Step 5690 | loss: 3.306840 | lr:5.0792e-04 | norm 0.2824 | dt 338.72ms | 1547847.08 tokens/sec
Step 5691 | loss: 3.300018 | lr:5.0788e-04 | norm 0.2704 | dt 338.50ms | 1548847.89 tokens/sec
Step 5692 | loss: 3.326066 | lr:5.0785e-04 | norm 0.2671 | dt 338.77ms | 1547621.59 tokens/sec
Step 5693 | loss: 3.273493 | lr:5.0781e-04 | norm 0.2577 | dt 339.43ms | 1544607.18 tokens/sec
Step 5694 | loss: 3.261169 | lr:5.0778e-04 | norm 0.2653 | dt 339.60ms | 1543835.08 tokens/sec
Step 5695 | loss: 3.298103 | lr:5.0774e-04 | norm 0.2720 | dt 338.90ms | 1547009.71 tokens/sec
Step 5696 | loss: 3.293716 | lr:5.0771e-04 | norm 0.2610 | dt 339.02ms | 1546466.83 tokens/sec
Step 5697 | loss: 3.297475 | lr:5.0767e-04 | norm 0.2595 | dt 338.85ms | 1547278.57 tokens/sec
Step 5698 | loss: 3.352228 | lr:5.0764e-04 | norm 0.2499 | dt 339.58ms | 1543949.98 tokens/sec
Step 5699 | loss: 3.338218 | lr:5.0760e-04 | norm 0.2716 | dt 1015.56ms | 516256.57 tokens/sec
Step 5700 | loss: 3.273972 | lr:5.0757e-04 | norm 0.2596 | dt 337.30ms | 1554363.45 tokens/sec
Step 5701 | loss: 3.071970 | lr:5.0754e-04 | norm 0.2899 | dt 339.29ms | 1545261.67 tokens/sec
Step 5702 | loss: 3.184278 | lr:5.0750e-04 | norm 0.2915 | dt 338.25ms | 1550006.21 tokens/sec
Step 5703 | loss: 3.169694 | lr:5.0747e-04 | norm 0.3302 | dt 338.98ms | 1546673.50 tokens/sec
Step 5704 | loss: 3.125756 | lr:5.0743e-04 | norm 0.3015 | dt 338.60ms | 1548404.02 tokens/sec
Step 5705 | loss: 3.106606 | lr:5.0740e-04 | norm 0.2754 | dt 337.85ms | 1551823.04 tokens/sec
Step 5706 | loss: 3.101393 | lr:5.0736e-04 | norm 0.2798 | dt 339.19ms | 1545724.39 tokens/sec
Step 5707 | loss: 3.140167 | lr:5.0733e-04 | norm 0.2692 | dt 338.62ms | 1548317.89 tokens/sec
Step 5708 | loss: 3.064327 | lr:5.0729e-04 | norm 0.3039 | dt 337.68ms | 1552608.63 tokens/sec
Step 5709 | loss: 3.148885 | lr:5.0726e-04 | norm 0.2914 | dt 338.76ms | 1547645.55 tokens/sec
Step 5710 | loss: 3.190738 | lr:5.0722e-04 | norm 0.2687 | dt 340.68ms | 1538965.55 tokens/sec
Step 5711 | loss: 3.092037 | lr:5.0719e-04 | norm 0.2942 | dt 339.16ms | 1545840.65 tokens/sec
Step 5712 | loss: 3.226501 | lr:5.0715e-04 | norm 0.3078 | dt 337.88ms | 1551698.21 tokens/sec
Step 5713 | loss: 3.381325 | lr:5.0712e-04 | norm 0.2951 | dt 338.67ms | 1548071.55 tokens/sec
Step 5714 | loss: 3.330608 | lr:5.0708e-04 | norm 0.3044 | dt 339.89ms | 1542522.57 tokens/sec
Step 5715 | loss: 3.373570 | lr:5.0705e-04 | norm 0.3017 | dt 338.52ms | 1548744.26 tokens/sec
Step 5716 | loss: 3.294325 | lr:5.0701e-04 | norm 0.2954 | dt 339.05ms | 1546322.20 tokens/sec
Step 5717 | loss: 3.279478 | lr:5.0698e-04 | norm 0.3232 | dt 339.88ms | 1542552.86 tokens/sec
Step 5718 | loss: 3.259135 | lr:5.0694e-04 | norm 0.2919 | dt 339.04ms | 1546409.19 tokens/sec
Step 5719 | loss: 3.235653 | lr:5.0691e-04 | norm 0.3039 | dt 337.61ms | 1552957.30 tokens/sec
Step 5720 | loss: 3.286561 | lr:5.0687e-04 | norm 0.2730 | dt 338.44ms | 1549126.12 tokens/sec
Step 5721 | loss: 3.244854 | lr:5.0684e-04 | norm 0.2836 | dt 339.01ms | 1546504.90 tokens/sec
Step 5722 | loss: 3.272146 | lr:5.0680e-04 | norm 0.2726 | dt 337.94ms | 1551409.20 tokens/sec
Step 5723 | loss: 3.222077 | lr:5.0677e-04 | norm 0.2729 | dt 338.36ms | 1549507.08 tokens/sec
Step 5724 | loss: 3.289929 | lr:5.0673e-04 | norm 0.2767 | dt 337.27ms | 1554522.78 tokens/sec
Step 5725 | loss: 3.372555 | lr:5.0670e-04 | norm 0.2895 | dt 338.26ms | 1549943.93 tokens/sec
Step 5726 | loss: 3.236154 | lr:5.0666e-04 | norm 0.3525 | dt 337.49ms | 1553501.46 tokens/sec
Step 5727 | loss: 3.353944 | lr:5.0663e-04 | norm 0.3154 | dt 338.01ms | 1551109.36 tokens/sec
Step 5728 | loss: 3.394934 | lr:5.0659e-04 | norm 0.3348 | dt 337.58ms | 1553068.08 tokens/sec
Step 5729 | loss: 3.309863 | lr:5.0656e-04 | norm 0.3200 | dt 338.42ms | 1549215.61 tokens/sec
Step 5730 | loss: 3.326375 | lr:5.0652e-04 | norm 0.3319 | dt 338.34ms | 1549610.81 tokens/sec
Step 5731 | loss: 3.363104 | lr:5.0649e-04 | norm 0.3094 | dt 337.72ms | 1552415.72 tokens/sec
Step 5732 | loss: 3.332978 | lr:5.0645e-04 | norm 0.2863 | dt 338.47ms | 1548995.18 tokens/sec
Step 5733 | loss: 3.334610 | lr:5.0642e-04 | norm 0.2899 | dt 337.89ms | 1551633.61 tokens/sec
Step 5734 | loss: 3.295154 | lr:5.0638e-04 | norm 0.2940 | dt 338.59ms | 1548456.35 tokens/sec
Step 5735 | loss: 3.310308 | lr:5.0635e-04 | norm 0.2892 | dt 337.94ms | 1551445.32 tokens/sec
Step 5736 | loss: 3.293447 | lr:5.0631e-04 | norm 0.2369 | dt 338.45ms | 1549070.47 tokens/sec
Step 5737 | loss: 3.320893 | lr:5.0628e-04 | norm 0.3048 | dt 338.84ms | 1547301.44 tokens/sec
Step 5738 | loss: 3.291057 | lr:5.0624e-04 | norm 0.3164 | dt 338.15ms | 1550440.07 tokens/sec
Step 5739 | loss: 3.284256 | lr:5.0621e-04 | norm 0.2761 | dt 338.14ms | 1550525.34 tokens/sec
Step 5740 | loss: 3.259477 | lr:5.0617e-04 | norm 0.2624 | dt 338.92ms | 1546948.77 tokens/sec
Step 5741 | loss: 3.275193 | lr:5.0614e-04 | norm 0.2510 | dt 338.54ms | 1548676.64 tokens/sec
Step 5742 | loss: 3.277730 | lr:5.0610e-04 | norm 0.2798 | dt 338.57ms | 1548549.04 tokens/sec
Step 5743 | loss: 3.290602 | lr:5.0607e-04 | norm 0.2763 | dt 338.08ms | 1550774.64 tokens/sec
Step 5744 | loss: 3.274044 | lr:5.0603e-04 | norm 0.2484 | dt 338.40ms | 1549301.84 tokens/sec
Step 5745 | loss: 3.276303 | lr:5.0600e-04 | norm 0.2669 | dt 338.68ms | 1548046.49 tokens/sec
Step 5746 | loss: 3.128164 | lr:5.0596e-04 | norm 0.2698 | dt 338.78ms | 1547569.31 tokens/sec
Step 5747 | loss: 3.068148 | lr:5.0593e-04 | norm 0.2980 | dt 338.95ms | 1546784.46 tokens/sec
Step 5748 | loss: 3.122359 | lr:5.0589e-04 | norm 0.2772 | dt 338.47ms | 1548989.72 tokens/sec
Step 5749 | loss: 3.131604 | lr:5.0586e-04 | norm 0.2644 | dt 339.03ms | 1546436.38 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 5750: 3.3322
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2843/10042=0.2831



ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 2: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what does it mean?
A native speaker or native speaker would use the word "native."
What does this
rank 2 sample 0 >Hello, I'm a language model, having all of his thoughts and ideas expressed through the book and his own.
What is a Language Model.
This
rank 5 sample 1 >Hello, I'm a language model, using Python and a Cascading Machine Language tool. This makes it easy and powerful. I like learning things in Python
rank 2 sample 1 >Hello, I'm a language model, my model includes some words that are hard to understand, but they also explain the meaning of those words and what they mean
rank 5 sample 2 >Hello, I'm a language model, and it really does require a language knowledge. It is a model you can make yourself (or someone else) use yourself
rank 2 sample 2 >Hello, I'm a language model, and I'll write a little code of it. I'm gonna use C to create code for our next project. Therank 5 sample 3 >Hello, I'm a language model, for instance, because I need to create my own word order in a way that it works with another language. I'd



rank 2 sample 3 >Hello, I'm a language model, but if I'm going to talk about one of the languages (or languages other than languages-you'll hear the name




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I am going to show you how every language model works.
Now, by creating a complete program for your system
rank 7 sample 1 >Hello, I'm a language model, if I need to write an application from scratch, I probably won't do it until something else. This is the main
rank 7 sample 2 >Hello, I'm a language model, so I'll get as much text written as possible - the more we work with, the more we will be able to
rank 7 sample 3 >Hello, I'm a language model, and it's a really awesome way to communicate things.
While it's not always easy to be taught, it actually




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I'd like to try to understand and apply this to my own project.
But why talk to the people in
rank 4 sample 1 >Hello, I'm a language model, though. I prefer this one that gives the listener and doesn't involve anything in this case, so let me find it
rank 4 sample 2 >Hello, I'm a language model, I learned that 'a book' in this post is a collection of idioms - it's just part of "a
rank 4 sample 3 >Hello, I'm a language model, so you guys want to talk about grammar! You talk something by example just as much as I tell you. There is




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not saying this. I'm saying it's so simple and really fun; that's actually what this tutorial
rank 3 sample 1 >Hello, I'm a language model, so my students are learning to be confident with what they have learned in the past.
I also have a small talk
rank 3 sample 2 >Hello, I'm a language model, so it works just as well as the programming language. In general, the more general the code like the more interesting it
rank 3 sample 3 >Hello, I'm a language model, so, to explain you to an example. So let's start by defining a set of properties. But first, we




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, because of the way language makes its meaning.
This makes the learner to become more in tune with their language while
rank 6 sample 1 >Hello, I'm a language model, a very important part of my teaching. To use language models, I like to show my students how they understand the elements
rank 6 sample 2 >Hello, I'm a language model, but for now I'm trying to find a common language for all these people that I can understand. A native language is
rank 6 sample 3 >Hello, I'm a language model, so if you want to know, I'm not going to talk about it here, because not all of those kids think




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I need to explain. It's probably easiest to explain. A couple days ago I wanted to write down a short
rank 0 sample 1 >Hello, I'm a language model, and all of my friends are great guys. But then is the way that they are doing in their writing and language learning
rank 0 sample 2 >Hello, I'm a language model, and I see this language's usage today.<|endoftext|>Mental hygiene can be a challenge for many people, especially children,
rank 0 sample 3 >Hello, I'm a language model, and all of these things are important. But some of them are still there and this is really a case of how each




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, just type the word "phonya", as if my sentence is "bamboo." My "phonya"
rank 1 sample 1 >Hello, I'm a language model, which is not hard to understand. You cannot understand English when you only understand how it communicates, read and write.

rank 1 sample 2 >Hello, I'm a language model, but he was so good at it.
I'm also writing about a series of books, so this one can certainly
rank 1 sample 3 >Hello, I'm a language model, so I'm talking to lots of people before I make what we have a toolbox there. But I'm sure you


Step 5750 | loss: 3.149462 | lr:5.0582e-04 | norm 0.2716 | dt 19064.31ms | 27501.02 tokens/sec
Step 5751 | loss: 3.117172 | lr:5.0579e-04 | norm 0.2897 | dt 335.94ms | 1560644.67 tokens/sec
Step 5752 | loss: 3.165915 | lr:5.0575e-04 | norm 0.3651 | dt 335.04ms | 1564847.07 tokens/sec
Step 5753 | loss: 3.141741 | lr:5.0572e-04 | norm 0.3202 | dt 336.66ms | 1557332.28 tokens/sec
Step 5754 | loss: 3.087855 | lr:5.0568e-04 | norm 0.2531 | dt 337.65ms | 1552770.88 tokens/sec
Step 5755 | loss: 3.110745 | lr:5.0565e-04 | norm 0.3095 | dt 336.32ms | 1558882.28 tokens/sec
Step 5756 | loss: 3.106171 | lr:5.0561e-04 | norm 0.3095 | dt 338.43ms | 1549154.49 tokens/sec
Step 5757 | loss: 3.120831 | lr:5.0558e-04 | norm 0.2572 | dt 336.68ms | 1557245.15 tokens/sec
Step 5758 | loss: 3.260803 | lr:5.0554e-04 | norm 0.2813 | dt 336.72ms | 1557045.58 tokens/sec
Step 5759 | loss: 3.310467 | lr:5.0551e-04 | norm 0.2598 | dt 337.22ms | 1554721.71 tokens/sec
Step 5760 | loss: 3.302861 | lr:5.0547e-04 | norm 0.2769 | dt 341.98ms | 1533108.42 tokens/sec
Step 5761 | loss: 3.246637 | lr:5.0544e-04 | norm 0.2713 | dt 337.70ms | 1552514.36 tokens/sec
Step 5762 | loss: 3.172515 | lr:5.0540e-04 | norm 0.2733 | dt 337.14ms | 1555098.82 tokens/sec
Step 5763 | loss: 3.273737 | lr:5.0537e-04 | norm 0.2714 | dt 337.36ms | 1554084.43 tokens/sec
Step 5764 | loss: 3.223513 | lr:5.0533e-04 | norm 0.2922 | dt 338.19ms | 1550275.02 tokens/sec
Step 5765 | loss: 3.206074 | lr:5.0530e-04 | norm 0.2790 | dt 336.14ms | 1559746.94 tokens/sec
Step 5766 | loss: 3.299253 | lr:5.0526e-04 | norm 0.2675 | dt 338.84ms | 1547292.73 tokens/sec
Step 5767 | loss: 3.236856 | lr:5.0523e-04 | norm 0.2880 | dt 337.89ms | 1551656.60 tokens/sec
Step 5768 | loss: 3.277440 | lr:5.0519e-04 | norm 0.2930 | dt 337.43ms | 1553781.36 tokens/sec
Step 5769 | loss: 3.213459 | lr:5.0516e-04 | norm 0.2556 | dt 338.78ms | 1547574.76 tokens/sec
Step 5770 | loss: 3.281987 | lr:5.0512e-04 | norm 0.2759 | dt 337.64ms | 1552816.94 tokens/sec
Step 5771 | loss: 3.228631 | lr:5.0509e-04 | norm 0.2539 | dt 337.64ms | 1552812.55 tokens/sec
Step 5772 | loss: 3.266338 | lr:5.0505e-04 | norm 0.2633 | dt 337.13ms | 1555153.81 tokens/sec
Step 5773 | loss: 3.324490 | lr:5.0502e-04 | norm 0.2660 | dt 337.37ms | 1554062.47 tokens/sec
Step 5774 | loss: 3.342611 | lr:5.0498e-04 | norm 0.2516 | dt 337.62ms | 1552916.72 tokens/sec
Step 5775 | loss: 3.314565 | lr:5.0494e-04 | norm 0.2789 | dt 337.72ms | 1552424.49 tokens/sec
Step 5776 | loss: 3.353253 | lr:5.0491e-04 | norm 0.2878 | dt 337.70ms | 1552512.17 tokens/sec
Step 5777 | loss: 3.335877 | lr:5.0487e-04 | norm 0.2733 | dt 337.48ms | 1553538.77 tokens/sec
Step 5778 | loss: 3.324177 | lr:5.0484e-04 | norm 0.2796 | dt 338.22ms | 1550144.97 tokens/sec
Step 5779 | loss: 3.341504 | lr:5.0480e-04 | norm 0.2755 | dt 338.16ms | 1550413.83 tokens/sec
Step 5780 | loss: 3.299536 | lr:5.0477e-04 | norm 0.2792 | dt 338.20ms | 1550230.21 tokens/sec
Step 5781 | loss: 3.283015 | lr:5.0473e-04 | norm 0.3153 | dt 337.67ms | 1552686.46 tokens/sec
Step 5782 | loss: 3.308429 | lr:5.0470e-04 | norm 0.2824 | dt 337.41ms | 1553843.94 tokens/sec
Step 5783 | loss: 3.305909 | lr:5.0466e-04 | norm 0.3042 | dt 337.96ms | 1551310.70 tokens/sec
Step 5784 | loss: 3.296053 | lr:5.0463e-04 | norm 0.3146 | dt 336.50ms | 1558070.46 tokens/sec
Step 5785 | loss: 3.302524 | lr:5.0459e-04 | norm 0.3174 | dt 336.38ms | 1558625.94 tokens/sec
Step 5786 | loss: 3.362188 | lr:5.0456e-04 | norm 0.2850 | dt 338.43ms | 1549174.14 tokens/sec
Step 5787 | loss: 3.332108 | lr:5.0452e-04 | norm 0.2965 | dt 337.05ms | 1555520.13 tokens/sec
Step 5788 | loss: 3.290705 | lr:5.0449e-04 | norm 0.2672 | dt 336.84ms | 1556482.41 tokens/sec
Step 5789 | loss: 3.247688 | lr:5.0445e-04 | norm 0.2789 | dt 337.20ms | 1554812.94 tokens/sec
Step 5790 | loss: 3.240649 | lr:5.0442e-04 | norm 0.2703 | dt 337.87ms | 1551758.43 tokens/sec
Step 5791 | loss: 3.249786 | lr:5.0438e-04 | norm 0.3342 | dt 337.52ms | 1553347.83 tokens/sec
Step 5792 | loss: 3.056999 | lr:5.0435e-04 | norm 0.2802 | dt 338.17ms | 1550356.99 tokens/sec
Step 5793 | loss: 3.176617 | lr:5.0431e-04 | norm 0.3445 | dt 338.34ms | 1549604.26 tokens/sec
Step 5794 | loss: 3.116292 | lr:5.0428e-04 | norm 0.3764 | dt 337.40ms | 1553923.00 tokens/sec
Step 5795 | loss: 3.125721 | lr:5.0424e-04 | norm 0.3341 | dt 338.52ms | 1548779.16 tokens/sec
Step 5796 | loss: 3.146655 | lr:5.0420e-04 | norm 0.3073 | dt 337.94ms | 1551444.23 tokens/sec
Step 5797 | loss: 3.178678 | lr:5.0417e-04 | norm 0.2750 | dt 337.98ms | 1551231.91 tokens/sec
Step 5798 | loss: 3.112735 | lr:5.0413e-04 | norm 0.2851 | dt 337.85ms | 1551844.94 tokens/sec
Step 5799 | loss: 3.101429 | lr:5.0410e-04 | norm 0.2740 | dt 338.38ms | 1549414.28 tokens/sec
Step 5800 | loss: 3.141245 | lr:5.0406e-04 | norm 0.2858 | dt 339.39ms | 1544816.60 tokens/sec
Step 5801 | loss: 3.150497 | lr:5.0403e-04 | norm 0.2720 | dt 337.45ms | 1553675.97 tokens/sec
Step 5802 | loss: 3.139019 | lr:5.0399e-04 | norm 0.2744 | dt 338.13ms | 1550532.99 tokens/sec
Step 5803 | loss: 3.344653 | lr:5.0396e-04 | norm 0.2945 | dt 337.90ms | 1551599.67 tokens/sec
Step 5804 | loss: 3.290680 | lr:5.0392e-04 | norm 0.2791 | dt 337.47ms | 1553605.72 tokens/sec
Step 5805 | loss: 3.328172 | lr:5.0389e-04 | norm 0.2828 | dt 337.95ms | 1551378.56 tokens/sec
Step 5806 | loss: 3.287295 | lr:5.0385e-04 | norm 0.2890 | dt 337.64ms | 1552788.43 tokens/sec
Step 5807 | loss: 3.197390 | lr:5.0382e-04 | norm 0.2559 | dt 338.06ms | 1550861.04 tokens/sec
Step 5808 | loss: 3.292069 | lr:5.0378e-04 | norm 0.2886 | dt 338.49ms | 1548891.53 tokens/sec
Step 5809 | loss: 3.244363 | lr:5.0375e-04 | norm 0.3165 | dt 338.54ms | 1548695.18 tokens/sec
Step 5810 | loss: 3.234220 | lr:5.0371e-04 | norm 0.2861 | dt 337.61ms | 1552948.53 tokens/sec
Step 5811 | loss: 3.203165 | lr:5.0367e-04 | norm 0.2917 | dt 338.68ms | 1548021.42 tokens/sec
Step 5812 | loss: 3.305956 | lr:5.0364e-04 | norm 0.2752 | dt 338.93ms | 1546882.39 tokens/sec
Step 5813 | loss: 3.215383 | lr:5.0360e-04 | norm 0.2858 | dt 338.01ms | 1551103.89 tokens/sec
Step 5814 | loss: 3.293978 | lr:5.0357e-04 | norm 0.3033 | dt 338.27ms | 1549929.73 tokens/sec
Step 5815 | loss: 3.241621 | lr:5.0353e-04 | norm 0.2679 | dt 339.96ms | 1542198.03 tokens/sec
Step 5816 | loss: 3.226555 | lr:5.0350e-04 | norm 0.2743 | dt 337.86ms | 1551781.43 tokens/sec
Step 5817 | loss: 3.221017 | lr:5.0346e-04 | norm 0.3226 | dt 338.29ms | 1549821.59 tokens/sec
Step 5818 | loss: 3.274897 | lr:5.0343e-04 | norm 0.3167 | dt 338.13ms | 1550529.71 tokens/sec
Step 5819 | loss: 3.351237 | lr:5.0339e-04 | norm 0.2887 | dt 337.92ms | 1551526.33 tokens/sec
Step 5820 | loss: 3.251884 | lr:5.0336e-04 | norm 0.3253 | dt 338.24ms | 1550057.56 tokens/sec
Step 5821 | loss: 3.303493 | lr:5.0332e-04 | norm 0.3125 | dt 338.45ms | 1549065.01 tokens/sec
Step 5822 | loss: 3.252242 | lr:5.0329e-04 | norm 0.3081 | dt 338.23ms | 1550102.36 tokens/sec
Step 5823 | loss: 3.322794 | lr:5.0325e-04 | norm 0.2860 | dt 337.66ms | 1552717.16 tokens/sec
Step 5824 | loss: 3.288422 | lr:5.0321e-04 | norm 0.3029 | dt 337.25ms | 1554596.41 tokens/sec
Step 5825 | loss: 3.316693 | lr:5.0318e-04 | norm 0.2769 | dt 338.21ms | 1550162.46 tokens/sec
Step 5826 | loss: 3.308948 | lr:5.0314e-04 | norm 0.2842 | dt 337.04ms | 1555564.15 tokens/sec
Step 5827 | loss: 3.301334 | lr:5.0311e-04 | norm 0.2473 | dt 338.06ms | 1550865.42 tokens/sec
Step 5828 | loss: 3.306340 | lr:5.0307e-04 | norm 0.2477 | dt 338.18ms | 1550342.78 tokens/sec
Step 5829 | loss: 3.233207 | lr:5.0304e-04 | norm 0.2434 | dt 337.51ms | 1553390.62 tokens/sec
Step 5830 | loss: 3.307522 | lr:5.0300e-04 | norm 0.2560 | dt 337.51ms | 1553396.11 tokens/sec
Step 5831 | loss: 3.341137 | lr:5.0297e-04 | norm 0.2655 | dt 338.63ms | 1548256.84 tokens/sec
Step 5832 | loss: 3.314502 | lr:5.0293e-04 | norm 0.2688 | dt 337.62ms | 1552910.14 tokens/sec
Step 5833 | loss: 3.308049 | lr:5.0290e-04 | norm 0.2691 | dt 337.71ms | 1552495.73 tokens/sec
Step 5834 | loss: 3.222674 | lr:5.0286e-04 | norm 0.9584 | dt 337.94ms | 1551431.09 tokens/sec
Step 5835 | loss: 3.309181 | lr:5.0282e-04 | norm 0.3288 | dt 337.87ms | 1551731.06 tokens/sec
Step 5836 | loss: 3.274769 | lr:5.0279e-04 | norm 0.3115 | dt 337.15ms | 1555069.13 tokens/sec
Step 5837 | loss: 3.251403 | lr:5.0275e-04 | norm 0.2620 | dt 337.65ms | 1552767.59 tokens/sec
Step 5838 | loss: 3.111367 | lr:5.0272e-04 | norm 0.3208 | dt 338.21ms | 1550177.75 tokens/sec
Step 5839 | loss: 3.141356 | lr:5.0268e-04 | norm 0.3246 | dt 337.48ms | 1553536.58 tokens/sec
Step 5840 | loss: 3.152102 | lr:5.0265e-04 | norm 0.2676 | dt 337.79ms | 1552115.49 tokens/sec
Step 5841 | loss: 3.129672 | lr:5.0261e-04 | norm 0.3189 | dt 337.29ms | 1554393.11 tokens/sec
Step 5842 | loss: 3.140097 | lr:5.0258e-04 | norm 0.2880 | dt 338.30ms | 1549781.18 tokens/sec
Step 5843 | loss: 3.067130 | lr:5.0254e-04 | norm 0.3040 | dt 337.27ms | 1554496.40 tokens/sec
Step 5844 | loss: 3.195182 | lr:5.0250e-04 | norm 0.3007 | dt 338.24ms | 1550025.87 tokens/sec
Step 5845 | loss: 3.097070 | lr:5.0247e-04 | norm 0.2932 | dt 337.59ms | 1553041.75 tokens/sec
Step 5846 | loss: 3.152338 | lr:5.0243e-04 | norm 0.2829 | dt 337.64ms | 1552787.33 tokens/sec
Step 5847 | loss: 3.073762 | lr:5.0240e-04 | norm 0.2789 | dt 337.76ms | 1552262.30 tokens/sec
Step 5848 | loss: 3.169068 | lr:5.0236e-04 | norm 0.2817 | dt 337.18ms | 1554909.69 tokens/sec
Step 5849 | loss: 3.299563 | lr:5.0233e-04 | norm 0.2811 | dt 337.81ms | 1552027.85 tokens/sec
Step 5850 | loss: 3.318845 | lr:5.0229e-04 | norm 0.2697 | dt 337.41ms | 1553869.20 tokens/sec
Step 5851 | loss: 3.284265 | lr:5.0226e-04 | norm 0.2974 | dt 337.94ms | 1551431.09 tokens/sec
Step 5852 | loss: 3.285954 | lr:5.0222e-04 | norm 0.2759 | dt 337.14ms | 1555098.82 tokens/sec
Step 5853 | loss: 3.369137 | lr:5.0218e-04 | norm 0.2975 | dt 338.19ms | 1550288.13 tokens/sec
Step 5854 | loss: 3.352585 | lr:5.0215e-04 | norm 0.2734 | dt 337.74ms | 1552332.43 tokens/sec
Step 5855 | loss: 3.356336 | lr:5.0211e-04 | norm 0.2708 | dt 337.68ms | 1552634.94 tokens/sec
Step 5856 | loss: 3.327986 | lr:5.0208e-04 | norm 0.2781 | dt 337.67ms | 1552655.77 tokens/sec
Step 5857 | loss: 3.310910 | lr:5.0204e-04 | norm 0.2781 | dt 337.17ms | 1554971.26 tokens/sec
Step 5858 | loss: 3.337102 | lr:5.0201e-04 | norm 0.2637 | dt 902.61ms | 580859.38 tokens/sec
Step 5859 | loss: 3.343483 | lr:5.0197e-04 | norm 0.2883 | dt 335.39ms | 1563205.18 tokens/sec
Step 5860 | loss: 3.316701 | lr:5.0193e-04 | norm 0.2644 | dt 337.25ms | 1554613.99 tokens/sec
Step 5861 | loss: 3.361553 | lr:5.0190e-04 | norm 0.2950 | dt 338.28ms | 1549854.36 tokens/sec
Step 5862 | loss: 3.362531 | lr:5.0186e-04 | norm 0.2709 | dt 337.27ms | 1554504.09 tokens/sec
Step 5863 | loss: 3.341740 | lr:5.0183e-04 | norm 0.2886 | dt 337.89ms | 1551640.18 tokens/sec
Step 5864 | loss: 3.375968 | lr:5.0179e-04 | norm 0.2519 | dt 337.66ms | 1552728.12 tokens/sec
Step 5865 | loss: 3.307870 | lr:5.0176e-04 | norm 0.2405 | dt 336.96ms | 1555944.97 tokens/sec
Step 5866 | loss: 3.311128 | lr:5.0172e-04 | norm 0.2810 | dt 337.52ms | 1553344.53 tokens/sec
Step 5867 | loss: 3.322478 | lr:5.0169e-04 | norm 0.2668 | dt 337.78ms | 1552173.55 tokens/sec
Step 5868 | loss: 3.333969 | lr:5.0165e-04 | norm 0.2642 | dt 337.26ms | 1554543.66 tokens/sec
Step 5869 | loss: 3.295166 | lr:5.0161e-04 | norm 0.2632 | dt 337.51ms | 1553409.27 tokens/sec
Step 5870 | loss: 3.316779 | lr:5.0158e-04 | norm 0.2583 | dt 336.92ms | 1556128.85 tokens/sec
Step 5871 | loss: 3.269665 | lr:5.0154e-04 | norm 0.2847 | dt 337.32ms | 1554296.43 tokens/sec
Step 5872 | loss: 3.246799 | lr:5.0151e-04 | norm 0.2698 | dt 337.88ms | 1551703.68 tokens/sec
Step 5873 | loss: 3.239944 | lr:5.0147e-04 | norm 0.2804 | dt 337.83ms | 1551941.32 tokens/sec
Step 5874 | loss: 3.280985 | lr:5.0144e-04 | norm 0.2974 | dt 337.61ms | 1552921.11 tokens/sec
Step 5875 | loss: 3.270172 | lr:5.0140e-04 | norm 0.2752 | dt 336.91ms | 1556188.32 tokens/sec
Step 5876 | loss: 3.307056 | lr:5.0136e-04 | norm 0.2359 | dt 337.88ms | 1551711.35 tokens/sec
Step 5877 | loss: 3.303133 | lr:5.0133e-04 | norm 0.2762 | dt 337.63ms | 1552857.51 tokens/sec
Step 5878 | loss: 3.297156 | lr:5.0129e-04 | norm 0.2851 | dt 337.40ms | 1553916.41 tokens/sec
Step 5879 | loss: 3.271608 | lr:5.0126e-04 | norm 0.2790 | dt 338.37ms | 1549456.85 tokens/sec
Step 5880 | loss: 3.273809 | lr:5.0122e-04 | norm 0.3057 | dt 337.04ms | 1555557.54 tokens/sec
Step 5881 | loss: 3.250108 | lr:5.0119e-04 | norm 0.2908 | dt 336.94ms | 1556011.03 tokens/sec
Step 5882 | loss: 3.286484 | lr:5.0115e-04 | norm 0.2602 | dt 337.89ms | 1551668.65 tokens/sec
Step 5883 | loss: 3.289538 | lr:5.0111e-04 | norm 0.3082 | dt 338.71ms | 1547911.37 tokens/sec
Step 5884 | loss: 3.296806 | lr:5.0108e-04 | norm 0.3142 | dt 337.50ms | 1553436.71 tokens/sec
Step 5885 | loss: 3.335302 | lr:5.0104e-04 | norm 0.2669 | dt 338.08ms | 1550784.49 tokens/sec
Step 5886 | loss: 3.344227 | lr:5.0101e-04 | norm 0.3257 | dt 338.41ms | 1549274.55 tokens/sec
Step 5887 | loss: 3.313889 | lr:5.0097e-04 | norm 0.2981 | dt 337.94ms | 1551416.87 tokens/sec
Step 5888 | loss: 3.343873 | lr:5.0094e-04 | norm 0.3089 | dt 337.99ms | 1551179.39 tokens/sec
Step 5889 | loss: 3.305269 | lr:5.0090e-04 | norm 0.2744 | dt 992.11ms | 528455.62 tokens/sec
Step 5890 | loss: 3.119632 | lr:5.0086e-04 | norm 0.3114 | dt 338.36ms | 1549507.08 tokens/sec
Step 5891 | loss: 3.327460 | lr:5.0083e-04 | norm 0.2819 | dt 337.27ms | 1554524.97 tokens/sec
Step 5892 | loss: 3.291876 | lr:5.0079e-04 | norm 0.3117 | dt 337.81ms | 1552021.28 tokens/sec
Step 5893 | loss: 3.316670 | lr:5.0076e-04 | norm 0.2757 | dt 337.55ms | 1553232.62 tokens/sec
Step 5894 | loss: 3.283592 | lr:5.0072e-04 | norm 0.2812 | dt 337.60ms | 1553003.36 tokens/sec
Step 5895 | loss: 3.326406 | lr:5.0068e-04 | norm 0.2728 | dt 337.56ms | 1553164.61 tokens/sec
Step 5896 | loss: 3.311944 | lr:5.0065e-04 | norm 0.2575 | dt 337.40ms | 1553897.74 tokens/sec
Step 5897 | loss: 3.344804 | lr:5.0061e-04 | norm 0.3399 | dt 337.84ms | 1551879.99 tokens/sec
Step 5898 | loss: 3.303598 | lr:5.0058e-04 | norm 0.3493 | dt 337.62ms | 1552914.53 tokens/sec
Step 5899 | loss: 3.377441 | lr:5.0054e-04 | norm 0.3737 | dt 337.86ms | 1551798.95 tokens/sec
Step 5900 | loss: 3.309623 | lr:5.0051e-04 | norm 0.3550 | dt 336.93ms | 1556063.88 tokens/sec
Step 5901 | loss: 3.368438 | lr:5.0047e-04 | norm 0.3151 | dt 337.47ms | 1553588.16 tokens/sec
Step 5902 | loss: 3.294974 | lr:5.0043e-04 | norm 0.3351 | dt 338.01ms | 1551101.70 tokens/sec
Step 5903 | loss: 3.313839 | lr:5.0040e-04 | norm 0.2968 | dt 338.15ms | 1550444.44 tokens/sec
Step 5904 | loss: 3.339120 | lr:5.0036e-04 | norm 0.2871 | dt 337.56ms | 1553161.31 tokens/sec
Step 5905 | loss: 3.336144 | lr:5.0033e-04 | norm 0.2760 | dt 337.93ms | 1551465.03 tokens/sec
Step 5906 | loss: 3.278760 | lr:5.0029e-04 | norm 0.2877 | dt 338.58ms | 1548504.33 tokens/sec
Step 5907 | loss: 3.244320 | lr:5.0026e-04 | norm 0.2833 | dt 338.40ms | 1549333.49 tokens/sec
Step 5908 | loss: 3.330614 | lr:5.0022e-04 | norm 0.2887 | dt 338.57ms | 1548556.67 tokens/sec
Step 5909 | loss: 3.309313 | lr:5.0018e-04 | norm 0.2680 | dt 338.39ms | 1549344.41 tokens/sec
Step 5910 | loss: 3.292369 | lr:5.0015e-04 | norm 0.2630 | dt 338.73ms | 1547824.20 tokens/sec
Step 5911 | loss: 3.284541 | lr:5.0011e-04 | norm 0.2576 | dt 338.74ms | 1547759.93 tokens/sec
Step 5912 | loss: 3.261605 | lr:5.0008e-04 | norm 0.2566 | dt 338.46ms | 1549038.82 tokens/sec
Step 5913 | loss: 3.367598 | lr:5.0004e-04 | norm 0.2671 | dt 337.50ms | 1553468.53 tokens/sec
Step 5914 | loss: 3.318451 | lr:5.0000e-04 | norm 0.2746 | dt 338.25ms | 1550013.85 tokens/sec
Step 5915 | loss: 3.283545 | lr:4.9997e-04 | norm 0.2749 | dt 339.17ms | 1545811.31 tokens/sec
Step 5916 | loss: 3.430327 | lr:4.9993e-04 | norm 0.2990 | dt 338.73ms | 1547817.67 tokens/sec
Step 5917 | loss: 3.309022 | lr:4.9990e-04 | norm 0.3265 | dt 338.63ms | 1548264.47 tokens/sec
Step 5918 | loss: 3.329453 | lr:4.9986e-04 | norm 0.2703 | dt 338.60ms | 1548414.92 tokens/sec
Step 5919 | loss: 3.235011 | lr:4.9982e-04 | norm 0.3003 | dt 338.92ms | 1546954.21 tokens/sec
Step 5920 | loss: 3.360578 | lr:4.9979e-04 | norm 0.2696 | dt 337.33ms | 1554230.52 tokens/sec
Step 5921 | loss: 3.346151 | lr:4.9975e-04 | norm 0.3363 | dt 338.09ms | 1550746.21 tokens/sec
Step 5922 | loss: 3.337558 | lr:4.9972e-04 | norm 0.3337 | dt 338.41ms | 1549285.47 tokens/sec
Step 5923 | loss: 3.339277 | lr:4.9968e-04 | norm 0.2963 | dt 337.56ms | 1553173.38 tokens/sec
Step 5924 | loss: 3.287247 | lr:4.9964e-04 | norm 0.3097 | dt 339.40ms | 1544772.11 tokens/sec
Step 5925 | loss: 3.377006 | lr:4.9961e-04 | norm 0.3019 | dt 338.21ms | 1550165.73 tokens/sec
Step 5926 | loss: 3.298254 | lr:4.9957e-04 | norm 0.3039 | dt 337.55ms | 1553213.97 tokens/sec
Step 5927 | loss: 3.405915 | lr:4.9954e-04 | norm 0.2920 | dt 337.76ms | 1552262.30 tokens/sec
Step 5928 | loss: 3.321910 | lr:4.9950e-04 | norm 0.3420 | dt 339.02ms | 1546488.58 tokens/sec
Step 5929 | loss: 3.335556 | lr:4.9946e-04 | norm 0.2953 | dt 337.74ms | 1552346.68 tokens/sec
Step 5930 | loss: 3.321759 | lr:4.9943e-04 | norm 0.3145 | dt 337.72ms | 1552420.10 tokens/sec
Step 5931 | loss: 3.341704 | lr:4.9939e-04 | norm 0.3058 | dt 337.39ms | 1553944.96 tokens/sec
Step 5932 | loss: 3.303488 | lr:4.9936e-04 | norm 0.2768 | dt 338.47ms | 1549006.09 tokens/sec
Step 5933 | loss: 3.343950 | lr:4.9932e-04 | norm 0.2804 | dt 338.64ms | 1548211.06 tokens/sec
Step 5934 | loss: 3.306209 | lr:4.9928e-04 | norm 0.2519 | dt 337.99ms | 1551212.22 tokens/sec
Step 5935 | loss: 3.322285 | lr:4.9925e-04 | norm 0.2708 | dt 338.81ms | 1547461.50 tokens/sec
Step 5936 | loss: 3.395841 | lr:4.9921e-04 | norm 0.2937 | dt 337.72ms | 1552422.29 tokens/sec
Step 5937 | loss: 3.364054 | lr:4.9918e-04 | norm 0.2699 | dt 337.57ms | 1553120.73 tokens/sec
Step 5938 | loss: 3.312850 | lr:4.9914e-04 | norm 0.2655 | dt 337.85ms | 1551839.47 tokens/sec
Step 5939 | loss: 3.303940 | lr:4.9910e-04 | norm 0.2900 | dt 337.68ms | 1552627.26 tokens/sec
Step 5940 | loss: 3.331580 | lr:4.9907e-04 | norm 0.2940 | dt 338.03ms | 1551005.43 tokens/sec
Step 5941 | loss: 3.249990 | lr:4.9903e-04 | norm 0.2710 | dt 337.56ms | 1553190.93 tokens/sec
Step 5942 | loss: 3.223074 | lr:4.9900e-04 | norm 0.2618 | dt 337.84ms | 1551895.32 tokens/sec
Step 5943 | loss: 3.316543 | lr:4.9896e-04 | norm 0.2810 | dt 338.40ms | 1549311.66 tokens/sec
Step 5944 | loss: 3.261017 | lr:4.9892e-04 | norm 0.3078 | dt 337.57ms | 1553139.37 tokens/sec
Step 5945 | loss: 3.285664 | lr:4.9889e-04 | norm 0.3514 | dt 337.97ms | 1551286.63 tokens/sec
Step 5946 | loss: 3.269969 | lr:4.9885e-04 | norm 0.2788 | dt 337.43ms | 1553762.70 tokens/sec
Step 5947 | loss: 3.249523 | lr:4.9882e-04 | norm 0.2783 | dt 338.07ms | 1550842.45 tokens/sec
Step 5948 | loss: 3.286786 | lr:4.9878e-04 | norm 0.2728 | dt 338.86ms | 1547191.48 tokens/sec
Step 5949 | loss: 3.311026 | lr:4.9874e-04 | norm 0.2761 | dt 337.63ms | 1552862.99 tokens/sec
Step 5950 | loss: 3.374511 | lr:4.9871e-04 | norm 0.2570 | dt 337.26ms | 1554533.77 tokens/sec
Step 5951 | loss: 3.383958 | lr:4.9867e-04 | norm 0.2658 | dt 338.40ms | 1549296.38 tokens/sec
Step 5952 | loss: 3.293156 | lr:4.9864e-04 | norm 0.3282 | dt 337.61ms | 1552949.62 tokens/sec
Step 5953 | loss: 3.294627 | lr:4.9860e-04 | norm 0.2939 | dt 337.31ms | 1554337.08 tokens/sec
Step 5954 | loss: 3.335189 | lr:4.9856e-04 | norm 0.2982 | dt 338.44ms | 1549142.49 tokens/sec
Step 5955 | loss: 3.324127 | lr:4.9853e-04 | norm 0.3340 | dt 339.04ms | 1546393.97 tokens/sec
Step 5956 | loss: 3.313391 | lr:4.9849e-04 | norm 0.2964 | dt 338.65ms | 1548154.38 tokens/sec
Step 5957 | loss: 3.316216 | lr:4.9846e-04 | norm 0.3044 | dt 337.85ms | 1551817.57 tokens/sec
Step 5958 | loss: 3.416155 | lr:4.9842e-04 | norm 0.3209 | dt 337.87ms | 1551757.34 tokens/sec
Step 5959 | loss: 3.271437 | lr:4.9838e-04 | norm 0.3311 | dt 337.72ms | 1552438.73 tokens/sec
Step 5960 | loss: 3.331264 | lr:4.9835e-04 | norm 0.3237 | dt 337.86ms | 1551806.62 tokens/sec
Step 5961 | loss: 3.312244 | lr:4.9831e-04 | norm 0.2807 | dt 338.16ms | 1550403.99 tokens/sec
Step 5962 | loss: 3.273859 | lr:4.9828e-04 | norm 0.2771 | dt 337.73ms | 1552409.14 tokens/sec
Step 5963 | loss: 3.318765 | lr:4.9824e-04 | norm 0.3141 | dt 337.65ms | 1552747.86 tokens/sec
Step 5964 | loss: 3.329717 | lr:4.9820e-04 | norm 0.3138 | dt 337.83ms | 1551933.65 tokens/sec
Step 5965 | loss: 3.388276 | lr:4.9817e-04 | norm 0.2549 | dt 337.94ms | 1551417.96 tokens/sec
Step 5966 | loss: 3.296796 | lr:4.9813e-04 | norm 0.2846 | dt 337.50ms | 1553441.10 tokens/sec
Step 5967 | loss: 3.308223 | lr:4.9809e-04 | norm 0.2724 | dt 338.62ms | 1548325.52 tokens/sec
Step 5968 | loss: 3.358899 | lr:4.9806e-04 | norm 0.2841 | dt 337.99ms | 1551213.31 tokens/sec
Step 5969 | loss: 3.301490 | lr:4.9802e-04 | norm 0.2761 | dt 337.43ms | 1553774.77 tokens/sec
Step 5970 | loss: 3.268928 | lr:4.9799e-04 | norm 0.3130 | dt 337.19ms | 1554885.50 tokens/sec
Step 5971 | loss: 3.275302 | lr:4.9795e-04 | norm 0.4362 | dt 337.58ms | 1553092.21 tokens/sec
Step 5972 | loss: 3.263764 | lr:4.9791e-04 | norm 0.3737 | dt 337.81ms | 1552014.71 tokens/sec
Step 5973 | loss: 3.267467 | lr:4.9788e-04 | norm 0.3344 | dt 337.46ms | 1553628.77 tokens/sec
Step 5974 | loss: 3.325386 | lr:4.9784e-04 | norm 0.4263 | dt 338.63ms | 1548245.94 tokens/sec
Step 5975 | loss: 3.333030 | lr:4.9780e-04 | norm 0.3348 | dt 337.73ms | 1552366.40 tokens/sec
Step 5976 | loss: 3.289060 | lr:4.9777e-04 | norm 0.3128 | dt 337.65ms | 1552761.02 tokens/sec
Step 5977 | loss: 3.243277 | lr:4.9773e-04 | norm 0.2951 | dt 338.56ms | 1548570.85 tokens/sec
Step 5978 | loss: 3.287148 | lr:4.9770e-04 | norm 0.2846 | dt 337.91ms | 1551567.92 tokens/sec
Step 5979 | loss: 3.258611 | lr:4.9766e-04 | norm 0.2803 | dt 338.62ms | 1548295.00 tokens/sec
Step 5980 | loss: 3.309781 | lr:4.9762e-04 | norm 0.2836 | dt 338.50ms | 1548836.98 tokens/sec
Step 5981 | loss: 3.249046 | lr:4.9759e-04 | norm 0.2988 | dt 337.94ms | 1551440.95 tokens/sec
Step 5982 | loss: 3.305087 | lr:4.9755e-04 | norm 0.3102 | dt 337.86ms | 1551778.14 tokens/sec
Step 5983 | loss: 3.274303 | lr:4.9752e-04 | norm 0.3025 | dt 338.08ms | 1550762.61 tokens/sec
Step 5984 | loss: 3.254488 | lr:4.9748e-04 | norm 0.3212 | dt 337.85ms | 1551847.13 tokens/sec
Step 5985 | loss: 3.313601 | lr:4.9744e-04 | norm 0.2939 | dt 337.72ms | 1552442.02 tokens/sec
Step 5986 | loss: 3.284451 | lr:4.9741e-04 | norm 0.2787 | dt 338.88ms | 1547132.70 tokens/sec
Step 5987 | loss: 3.361812 | lr:4.9737e-04 | norm 0.2694 | dt 337.51ms | 1553405.98 tokens/sec
Step 5988 | loss: 3.324224 | lr:4.9733e-04 | norm 0.2641 | dt 338.29ms | 1549799.74 tokens/sec
Step 5989 | loss: 3.349755 | lr:4.9730e-04 | norm 0.2894 | dt 338.05ms | 1550903.70 tokens/sec
Step 5990 | loss: 3.278440 | lr:4.9726e-04 | norm 0.2896 | dt 338.08ms | 1550792.14 tokens/sec
Step 5991 | loss: 3.316466 | lr:4.9723e-04 | norm 0.2782 | dt 338.10ms | 1550681.69 tokens/sec
Step 5992 | loss: 3.276960 | lr:4.9719e-04 | norm 0.2667 | dt 337.49ms | 1553512.43 tokens/sec
Step 5993 | loss: 3.304164 | lr:4.9715e-04 | norm 0.2537 | dt 338.57ms | 1548528.32 tokens/sec
Step 5994 | loss: 3.323463 | lr:4.9712e-04 | norm 0.2923 | dt 337.73ms | 1552370.79 tokens/sec
Step 5995 | loss: 3.327951 | lr:4.9708e-04 | norm 0.2667 | dt 337.70ms | 1552543.95 tokens/sec
Step 5996 | loss: 3.325320 | lr:4.9704e-04 | norm 0.2782 | dt 337.85ms | 1551829.61 tokens/sec
Step 5997 | loss: 3.341854 | lr:4.9701e-04 | norm 0.2574 | dt 338.84ms | 1547281.84 tokens/sec
Step 5998 | loss: 3.292400 | lr:4.9697e-04 | norm 0.3154 | dt 338.07ms | 1550812.92 tokens/sec
Step 5999 | loss: 3.283223 | lr:4.9693e-04 | norm 0.2726 | dt 337.05ms | 1555499.23 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 6000: 3.3168
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2816/10042=0.2804


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this isn't for me. It's just about my age to get a better understanding of it.<|endoftext|>If you
rank 5 sample 1 >Hello, I'm a language model, no...
My own definition was the first that I would try to define by an English speaker. And now, I
rank 5 sample 2 >Hello, I'm a language model, and I am not actually a 'class' linguist. I believe that 'Class' is a language model, since
rank 5 sample 3 >Hello, I'm a language model, in the sense that I do not have the exact idea what is happening. And when we talk about the language model,




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not saying this to them.
What about it? Most textbooks will probably never write code, and as


ddp_rank 1: ####### Printing generated samples ####### 

rank 3 sample 1 >Hello, I'm a language model, so my model is a simplified form of a sentence. A sentence can be a sentence, but I'm not trying to


ddp_rank 7: ####### Printing generated samples ####### 

rank 3 sample 2 >Hello, I'm a language model, so it would be okay if I could speak or write my own language, it would be okay if I could get in
rank 3 sample 3 >Hello, I'm a language model, so the thing about an approach isn't to write the first language to it. In an application, using an object for


rank 1 sample 0 >Hello, I'm a language model, so the children can do a lot together and work through different activities.
Now, my son can do all of the
rank 7 sample 0 >Hello, I'm a language model, so I will be posting another post in these days."
When I've finished reviewing this material I am still learning all
rank 1 sample 1 >Hello, I'm a language model, not an object-oriented language. But to understand language language, I need to develop concepts that understand the language.



ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, rather than a language model, a social contract. Why was the name of the course that I'm in this semester right
rank 1 sample 2 >Hello, I'm a language model, but thanks,
I'm a language model, and my friend
Is, I'm a language model. You learn
rank 7 sample 2 >Hello, I'm a language model, but I don't have a model I want to describe.
What the heck is the difference between a model I have
rank 1 sample 3 >Hello, I'm a language model, so I'm really trying to help you (and hopefully not someone like me! I always can't, but I'm


rank 7 sample 3 >Hello, I'm a language model, and it's a lot like the web interface system.
Note that most of the code in web design is made in


rank 2 sample 0 >Hello, I'm a language model, writing and speaking about language.
A good way to get a good grasp on languages is to understand the structure of a
rank 2 sample 1 >Hello, I'm a language model, but I try hard to model all programming...
Math: What Is An Example?
This question was answered by one
rank 2 sample 2 >Hello, I'm a language model, and I've always been a part of the world. I was able to speak Spanish.
- There are 20 letters
rank 2 sample 3 >Hello, I'm a language model, but there's no one right way to describe it that you have to understand for it. Just, you're going to




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I think that will be a great introduction for my students.
As a language therapist, I often feel that my
rank 4 sample 1 >Hello, I'm a language model, too...
3 Answers | Add Yours
Is this a correct definition for “C” or “
rank 4 sample 2 >Hello, I'm a language model, I must tell you the thing that the word is.
You need to be able to put in something and get the
rank 4 sample 3 >Hello, I'm a language model, so if learning an English language is easier or quicker than anything, then teaching a language is easier with the right type of




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where languages are understood using syntactic constructions. It has the task to develop a language, and it is not only
rank 6 sample 1 >Hello, I'm a language model, a language model, a model is a kind of thing, a model or a language. I like the term "system
rank 6 sample 2 >Hello, I'm a language model, but that's a different story...
(CNN) -- I love my blog, and I'm here to say,
rank 6 sample 3 >Hello, I'm a language model, so the best way to learn language is to read the books. (I tried this in 2nd grade when I wasn




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know that it's easy to pick up any good stuff to show, but you can get the job done with
rank 0 sample 1 >Hello, I'm a language model, and now I'm gonna create the HTML code that works."<|endoftext|>By Steve Koehn
In May 2013, the
rank 0 sample 2 >Hello, I'm a language model, and I guess you get something and everything. What I'm thinking of is that language models are not one-to-
rank 0 sample 3 >Hello, I'm a language model, but no one knows what you are doing with programming.
(3) You go to the computer and you make program


Step 6000 | loss: 3.341491 | lr:4.9690e-04 | norm 0.2882 | dt 12457.86ms | 42084.91 tokens/sec
Step 6001 | loss: 3.311351 | lr:4.9686e-04 | norm 0.2655 | dt 335.12ms | 1564477.46 tokens/sec
Step 6002 | loss: 3.286295 | lr:4.9683e-04 | norm 0.2713 | dt 338.68ms | 1548027.96 tokens/sec
Step 6003 | loss: 3.308672 | lr:4.9679e-04 | norm 0.2670 | dt 336.10ms | 1559909.58 tokens/sec
Step 6004 | loss: 3.321965 | lr:4.9675e-04 | norm 0.2807 | dt 336.15ms | 1559666.18 tokens/sec
Step 6005 | loss: 3.246355 | lr:4.9672e-04 | norm 0.2838 | dt 336.68ms | 1557251.77 tokens/sec
Step 6006 | loss: 3.270179 | lr:4.9668e-04 | norm 0.2980 | dt 336.91ms | 1556150.87 tokens/sec
Step 6007 | loss: 3.283772 | lr:4.9664e-04 | norm 0.3151 | dt 336.56ms | 1557782.39 tokens/sec
Step 6008 | loss: 3.271640 | lr:4.9661e-04 | norm 0.3055 | dt 336.37ms | 1558646.93 tokens/sec
Step 6009 | loss: 3.318235 | lr:4.9657e-04 | norm 0.2787 | dt 336.74ms | 1556929.83 tokens/sec
Step 6010 | loss: 3.292284 | lr:4.9654e-04 | norm 0.2925 | dt 336.59ms | 1557643.35 tokens/sec
Step 6011 | loss: 3.237747 | lr:4.9650e-04 | norm 0.2996 | dt 336.50ms | 1558067.15 tokens/sec
Step 6012 | loss: 3.298074 | lr:4.9646e-04 | norm 0.2871 | dt 337.29ms | 1554408.50 tokens/sec
Step 6013 | loss: 3.233639 | lr:4.9643e-04 | norm 0.2814 | dt 337.52ms | 1553331.37 tokens/sec
Step 6014 | loss: 3.178902 | lr:4.9639e-04 | norm 0.2775 | dt 337.33ms | 1554221.73 tokens/sec
Step 6015 | loss: 3.265582 | lr:4.9635e-04 | norm 0.3101 | dt 337.96ms | 1551353.38 tokens/sec
Step 6016 | loss: 3.285900 | lr:4.9632e-04 | norm 0.2874 | dt 337.44ms | 1553737.45 tokens/sec
Step 6017 | loss: 3.294444 | lr:4.9628e-04 | norm 0.2530 | dt 339.19ms | 1545703.75 tokens/sec
Step 6018 | loss: 3.294092 | lr:4.9624e-04 | norm 0.2738 | dt 337.11ms | 1555266.00 tokens/sec
Step 6019 | loss: 3.303904 | lr:4.9621e-04 | norm 0.2758 | dt 337.06ms | 1555464.02 tokens/sec
Step 6020 | loss: 3.335208 | lr:4.9617e-04 | norm 0.2932 | dt 336.50ms | 1558043.97 tokens/sec
Step 6021 | loss: 3.289792 | lr:4.9613e-04 | norm 0.2742 | dt 338.72ms | 1547865.60 tokens/sec
Step 6022 | loss: 3.348580 | lr:4.9610e-04 | norm 0.2989 | dt 336.76ms | 1556839.44 tokens/sec
Step 6023 | loss: 3.334323 | lr:4.9606e-04 | norm 0.2995 | dt 337.66ms | 1552727.03 tokens/sec
Step 6024 | loss: 3.298588 | lr:4.9603e-04 | norm 0.2639 | dt 338.47ms | 1549013.72 tokens/sec
Step 6025 | loss: 3.359598 | lr:4.9599e-04 | norm 0.3102 | dt 337.20ms | 1554820.64 tokens/sec
Step 6026 | loss: 3.315895 | lr:4.9595e-04 | norm 0.2693 | dt 338.17ms | 1550384.32 tokens/sec
Step 6027 | loss: 3.323273 | lr:4.9592e-04 | norm 0.2760 | dt 337.37ms | 1554039.40 tokens/sec
Step 6028 | loss: 3.332347 | lr:4.9588e-04 | norm 0.2989 | dt 338.07ms | 1550822.76 tokens/sec
Step 6029 | loss: 3.308099 | lr:4.9584e-04 | norm 0.3070 | dt 338.00ms | 1551127.96 tokens/sec
Step 6030 | loss: 3.331587 | lr:4.9581e-04 | norm 0.2833 | dt 337.80ms | 1552059.62 tokens/sec
Step 6031 | loss: 3.250927 | lr:4.9577e-04 | norm 0.2614 | dt 337.95ms | 1551381.84 tokens/sec
Step 6032 | loss: 3.289955 | lr:4.9573e-04 | norm 0.2857 | dt 339.24ms | 1545490.82 tokens/sec
Step 6033 | loss: 3.306994 | lr:4.9570e-04 | norm 0.2958 | dt 338.35ms | 1549561.67 tokens/sec
Step 6034 | loss: 3.301159 | lr:4.9566e-04 | norm 0.3058 | dt 338.72ms | 1547828.56 tokens/sec
Step 6035 | loss: 3.289910 | lr:4.9562e-04 | norm 0.2957 | dt 338.48ms | 1548940.62 tokens/sec
Step 6036 | loss: 3.329059 | lr:4.9559e-04 | norm 0.3322 | dt 338.06ms | 1550851.20 tokens/sec
Step 6037 | loss: 3.293330 | lr:4.9555e-04 | norm 0.2756 | dt 338.83ms | 1547333.01 tokens/sec
Step 6038 | loss: 3.342155 | lr:4.9551e-04 | norm 0.2535 | dt 338.79ms | 1547523.57 tokens/sec
Step 6039 | loss: 3.443158 | lr:4.9548e-04 | norm 0.2947 | dt 338.12ms | 1550595.31 tokens/sec
Step 6040 | loss: 3.315342 | lr:4.9544e-04 | norm 0.3107 | dt 338.06ms | 1550852.29 tokens/sec
Step 6041 | loss: 3.327322 | lr:4.9541e-04 | norm 0.3017 | dt 338.16ms | 1550402.90 tokens/sec
Step 6042 | loss: 3.283720 | lr:4.9537e-04 | norm 0.2656 | dt 338.40ms | 1549308.39 tokens/sec
Step 6043 | loss: 3.229099 | lr:4.9533e-04 | norm 0.3087 | dt 340.69ms | 1538888.01 tokens/sec
Step 6044 | loss: 3.312476 | lr:4.9530e-04 | norm 0.2927 | dt 339.11ms | 1546059.11 tokens/sec
Step 6045 | loss: 3.293562 | lr:4.9526e-04 | norm 0.2732 | dt 338.53ms | 1548715.90 tokens/sec
Step 6046 | loss: 3.297062 | lr:4.9522e-04 | norm 0.2784 | dt 338.20ms | 1550253.16 tokens/sec
Step 6047 | loss: 3.268785 | lr:4.9519e-04 | norm 0.2677 | dt 894.81ms | 585918.72 tokens/sec
Step 6048 | loss: 3.280751 | lr:4.9515e-04 | norm 0.2817 | dt 334.58ms | 1567015.95 tokens/sec
Step 6049 | loss: 3.290150 | lr:4.9511e-04 | norm 0.2776 | dt 337.36ms | 1554100.91 tokens/sec
Step 6050 | loss: 3.251867 | lr:4.9508e-04 | norm 0.2977 | dt 340.69ms | 1538883.70 tokens/sec
Step 6051 | loss: 3.264835 | lr:4.9504e-04 | norm 0.2782 | dt 338.72ms | 1547862.34 tokens/sec
Step 6052 | loss: 3.276669 | lr:4.9500e-04 | norm 0.2522 | dt 338.14ms | 1550495.82 tokens/sec
Step 6053 | loss: 3.287399 | lr:4.9497e-04 | norm 0.2795 | dt 337.69ms | 1552583.41 tokens/sec
Step 6054 | loss: 3.314778 | lr:4.9493e-04 | norm 0.2631 | dt 338.94ms | 1546839.95 tokens/sec
Step 6055 | loss: 3.286957 | lr:4.9489e-04 | norm 0.2753 | dt 339.15ms | 1545871.08 tokens/sec
Step 6056 | loss: 3.331386 | lr:4.9486e-04 | norm 0.2787 | dt 338.94ms | 1546829.07 tokens/sec
Step 6057 | loss: 3.279493 | lr:4.9482e-04 | norm 0.2747 | dt 338.52ms | 1548782.44 tokens/sec
Step 6058 | loss: 3.402643 | lr:4.9478e-04 | norm 0.2743 | dt 338.84ms | 1547311.24 tokens/sec
Step 6059 | loss: 3.293975 | lr:4.9475e-04 | norm 0.2619 | dt 338.89ms | 1547079.37 tokens/sec
Step 6060 | loss: 3.350640 | lr:4.9471e-04 | norm 0.3070 | dt 339.03ms | 1546443.99 tokens/sec
Step 6061 | loss: 3.295211 | lr:4.9467e-04 | norm 0.2752 | dt 338.94ms | 1546855.19 tokens/sec
Step 6062 | loss: 3.380493 | lr:4.9464e-04 | norm 0.2839 | dt 339.92ms | 1542398.14 tokens/sec
Step 6063 | loss: 3.324891 | lr:4.9460e-04 | norm 0.3043 | dt 338.73ms | 1547811.13 tokens/sec
Step 6064 | loss: 3.371646 | lr:4.9456e-04 | norm 0.2982 | dt 339.49ms | 1544354.43 tokens/sec
Step 6065 | loss: 3.315955 | lr:4.9453e-04 | norm 0.2917 | dt 338.41ms | 1549258.18 tokens/sec
Step 6066 | loss: 3.320958 | lr:4.9449e-04 | norm 0.2750 | dt 339.87ms | 1542593.98 tokens/sec
Step 6067 | loss: 3.325712 | lr:4.9445e-04 | norm 0.2919 | dt 339.14ms | 1545946.07 tokens/sec
Step 6068 | loss: 3.342978 | lr:4.9442e-04 | norm 0.2917 | dt 339.61ms | 1543787.40 tokens/sec
Step 6069 | loss: 3.316410 | lr:4.9438e-04 | norm 0.3143 | dt 338.53ms | 1548740.99 tokens/sec
Step 6070 | loss: 3.285389 | lr:4.9434e-04 | norm 0.2965 | dt 339.05ms | 1546338.51 tokens/sec
Step 6071 | loss: 3.326228 | lr:4.9431e-04 | norm 0.2840 | dt 339.76ms | 1543118.99 tokens/sec
Step 6072 | loss: 3.310076 | lr:4.9427e-04 | norm 0.2827 | dt 339.25ms | 1545439.77 tokens/sec
Step 6073 | loss: 3.343248 | lr:4.9423e-04 | norm 0.2753 | dt 338.81ms | 1547446.25 tokens/sec
Step 6074 | loss: 3.335410 | lr:4.9420e-04 | norm 0.2632 | dt 339.23ms | 1545537.53 tokens/sec
Step 6075 | loss: 3.268820 | lr:4.9416e-04 | norm 0.2701 | dt 339.70ms | 1543406.00 tokens/sec
Step 6076 | loss: 3.235084 | lr:4.9412e-04 | norm 0.2547 | dt 339.78ms | 1543007.46 tokens/sec
Step 6077 | loss: 3.237904 | lr:4.9409e-04 | norm 0.2529 | dt 339.73ms | 1543255.44 tokens/sec
Step 6078 | loss: 3.229571 | lr:4.9405e-04 | norm 0.2604 | dt 338.92ms | 1546923.74 tokens/sec
Step 6079 | loss: 3.302338 | lr:4.9401e-04 | norm 0.2722 | dt 1026.54ms | 510733.60 tokens/sec
Step 6080 | loss: 3.251512 | lr:4.9398e-04 | norm 0.2811 | dt 337.85ms | 1551833.99 tokens/sec
Step 6081 | loss: 3.237422 | lr:4.9394e-04 | norm 0.2507 | dt 337.54ms | 1553283.09 tokens/sec
Step 6082 | loss: 3.241466 | lr:4.9390e-04 | norm 0.2801 | dt 339.88ms | 1542582.08 tokens/sec
Step 6083 | loss: 3.307006 | lr:4.9387e-04 | norm 0.2649 | dt 337.72ms | 1552413.53 tokens/sec
Step 6084 | loss: 3.287688 | lr:4.9383e-04 | norm 0.2552 | dt 337.43ms | 1553771.48 tokens/sec
Step 6085 | loss: 3.313371 | lr:4.9379e-04 | norm 0.2540 | dt 337.33ms | 1554229.42 tokens/sec
Step 6086 | loss: 3.327084 | lr:4.9376e-04 | norm 0.2808 | dt 338.09ms | 1550725.43 tokens/sec
Step 6087 | loss: 3.309848 | lr:4.9372e-04 | norm 0.2769 | dt 337.54ms | 1553279.80 tokens/sec
Step 6088 | loss: 3.334196 | lr:4.9368e-04 | norm 0.2692 | dt 338.13ms | 1550572.35 tokens/sec
Step 6089 | loss: 3.316418 | lr:4.9365e-04 | norm 0.2937 | dt 337.60ms | 1553004.46 tokens/sec
Step 6090 | loss: 3.327316 | lr:4.9361e-04 | norm 0.3004 | dt 337.32ms | 1554282.15 tokens/sec
Step 6091 | loss: 3.376198 | lr:4.9357e-04 | norm 0.2957 | dt 338.15ms | 1550478.33 tokens/sec
Step 6092 | loss: 3.256032 | lr:4.9354e-04 | norm 0.2808 | dt 338.44ms | 1549123.94 tokens/sec
Step 6093 | loss: 3.329225 | lr:4.9350e-04 | norm 0.3092 | dt 338.52ms | 1548756.26 tokens/sec
Step 6094 | loss: 3.341433 | lr:4.9346e-04 | norm 0.2939 | dt 338.10ms | 1550668.57 tokens/sec
Step 6095 | loss: 3.313411 | lr:4.9343e-04 | norm 0.2688 | dt 337.45ms | 1553675.97 tokens/sec
Step 6096 | loss: 3.311482 | lr:4.9339e-04 | norm 0.2776 | dt 338.16ms | 1550402.90 tokens/sec
Step 6097 | loss: 3.324980 | lr:4.9335e-04 | norm 0.2994 | dt 338.26ms | 1549973.43 tokens/sec
Step 6098 | loss: 3.346900 | lr:4.9332e-04 | norm 0.3143 | dt 338.02ms | 1551064.51 tokens/sec
Step 6099 | loss: 3.354193 | lr:4.9328e-04 | norm 0.2853 | dt 337.85ms | 1551824.14 tokens/sec
Step 6100 | loss: 3.268763 | lr:4.9324e-04 | norm 0.2957 | dt 338.09ms | 1550724.34 tokens/sec
Step 6101 | loss: 3.312959 | lr:4.9321e-04 | norm 0.2823 | dt 339.06ms | 1546311.33 tokens/sec
Step 6102 | loss: 3.314884 | lr:4.9317e-04 | norm 0.3048 | dt 337.91ms | 1551582.16 tokens/sec
Step 6103 | loss: 3.332693 | lr:4.9313e-04 | norm 0.2870 | dt 338.09ms | 1550753.86 tokens/sec
Step 6104 | loss: 3.368534 | lr:4.9309e-04 | norm 0.2802 | dt 338.08ms | 1550777.92 tokens/sec
Step 6105 | loss: 3.307280 | lr:4.9306e-04 | norm 0.2749 | dt 338.03ms | 1551029.50 tokens/sec
Step 6106 | loss: 3.306997 | lr:4.9302e-04 | norm 0.2746 | dt 338.35ms | 1549540.92 tokens/sec
Step 6107 | loss: 3.311336 | lr:4.9298e-04 | norm 0.2723 | dt 338.19ms | 1550272.83 tokens/sec
Step 6108 | loss: 3.312098 | lr:4.9295e-04 | norm 0.2648 | dt 338.45ms | 1549072.65 tokens/sec
Step 6109 | loss: 3.227655 | lr:4.9291e-04 | norm 0.2950 | dt 338.87ms | 1547179.51 tokens/sec
Step 6110 | loss: 3.289598 | lr:4.9287e-04 | norm 0.2742 | dt 338.04ms | 1550979.18 tokens/sec
Step 6111 | loss: 3.281400 | lr:4.9284e-04 | norm 0.2608 | dt 338.04ms | 1550947.45 tokens/sec
Step 6112 | loss: 3.231753 | lr:4.9280e-04 | norm 0.3139 | dt 338.45ms | 1549086.83 tokens/sec
Step 6113 | loss: 3.179015 | lr:4.9276e-04 | norm 0.2885 | dt 338.18ms | 1550314.36 tokens/sec
Step 6114 | loss: 3.249930 | lr:4.9273e-04 | norm 0.2727 | dt 338.57ms | 1548514.14 tokens/sec
Step 6115 | loss: 3.307631 | lr:4.9269e-04 | norm 0.2798 | dt 338.12ms | 1550583.28 tokens/sec
Step 6116 | loss: 3.224874 | lr:4.9265e-04 | norm 0.2863 | dt 338.99ms | 1546629.98 tokens/sec
Step 6117 | loss: 3.306924 | lr:4.9262e-04 | norm 0.2642 | dt 338.36ms | 1549478.69 tokens/sec
Step 6118 | loss: 3.279536 | lr:4.9258e-04 | norm 0.3016 | dt 338.22ms | 1550143.88 tokens/sec
Step 6119 | loss: 3.243303 | lr:4.9254e-04 | norm 0.2977 | dt 338.60ms | 1548412.74 tokens/sec
Step 6120 | loss: 3.250941 | lr:4.9251e-04 | norm 0.2796 | dt 338.87ms | 1547184.95 tokens/sec
Step 6121 | loss: 3.299709 | lr:4.9247e-04 | norm 0.3241 | dt 338.72ms | 1547865.60 tokens/sec
Step 6122 | loss: 3.368592 | lr:4.9243e-04 | norm 0.2915 | dt 341.53ms | 1535122.63 tokens/sec
Step 6123 | loss: 3.321451 | lr:4.9239e-04 | norm 0.3057 | dt 338.25ms | 1550008.39 tokens/sec
Step 6124 | loss: 3.325037 | lr:4.9236e-04 | norm 0.3374 | dt 338.04ms | 1550984.65 tokens/sec
Step 6125 | loss: 3.287691 | lr:4.9232e-04 | norm 0.2952 | dt 338.74ms | 1547749.03 tokens/sec
Step 6126 | loss: 3.321976 | lr:4.9228e-04 | norm 0.3148 | dt 338.85ms | 1547277.49 tokens/sec
Step 6127 | loss: 3.310642 | lr:4.9225e-04 | norm 0.2437 | dt 338.92ms | 1546940.06 tokens/sec
Step 6128 | loss: 3.307499 | lr:4.9221e-04 | norm 0.2949 | dt 339.44ms | 1544562.69 tokens/sec
Step 6129 | loss: 3.291320 | lr:4.9217e-04 | norm 0.2610 | dt 337.65ms | 1552744.57 tokens/sec
Step 6130 | loss: 3.290318 | lr:4.9214e-04 | norm 0.2953 | dt 337.94ms | 1551437.66 tokens/sec
Step 6131 | loss: 3.308330 | lr:4.9210e-04 | norm 0.2552 | dt 338.27ms | 1549926.46 tokens/sec
Step 6132 | loss: 3.334824 | lr:4.9206e-04 | norm 0.2948 | dt 338.81ms | 1547427.74 tokens/sec
Step 6133 | loss: 3.315165 | lr:4.9203e-04 | norm 0.2772 | dt 337.76ms | 1552267.78 tokens/sec
Step 6134 | loss: 3.352857 | lr:4.9199e-04 | norm 0.3061 | dt 338.32ms | 1549662.13 tokens/sec
Step 6135 | loss: 3.301558 | lr:4.9195e-04 | norm 0.3016 | dt 338.59ms | 1548450.90 tokens/sec
Step 6136 | loss: 3.307875 | lr:4.9191e-04 | norm 0.3104 | dt 337.87ms | 1551743.10 tokens/sec
Step 6137 | loss: 3.351021 | lr:4.9188e-04 | norm 0.2677 | dt 339.04ms | 1546380.92 tokens/sec
Step 6138 | loss: 3.329615 | lr:4.9184e-04 | norm 0.2813 | dt 338.14ms | 1550499.10 tokens/sec
Step 6139 | loss: 3.368001 | lr:4.9180e-04 | norm 0.2846 | dt 339.86ms | 1542643.76 tokens/sec
Step 6140 | loss: 3.340987 | lr:4.9177e-04 | norm 0.3061 | dt 338.14ms | 1550503.47 tokens/sec
Step 6141 | loss: 3.317000 | lr:4.9173e-04 | norm 0.3060 | dt 337.86ms | 1551802.23 tokens/sec
Step 6142 | loss: 3.290435 | lr:4.9169e-04 | norm 0.2915 | dt 339.02ms | 1546494.02 tokens/sec
Step 6143 | loss: 3.259447 | lr:4.9166e-04 | norm 0.2792 | dt 338.54ms | 1548691.90 tokens/sec
Step 6144 | loss: 3.234838 | lr:4.9162e-04 | norm 0.2544 | dt 338.85ms | 1547264.42 tokens/sec
Step 6145 | loss: 3.239111 | lr:4.9158e-04 | norm 0.2746 | dt 338.68ms | 1548015.97 tokens/sec
Step 6146 | loss: 3.260828 | lr:4.9154e-04 | norm 0.2652 | dt 339.47ms | 1544437.94 tokens/sec
Step 6147 | loss: 3.205762 | lr:4.9151e-04 | norm 0.2680 | dt 339.75ms | 1543177.46 tokens/sec
Step 6148 | loss: 3.252551 | lr:4.9147e-04 | norm 0.2394 | dt 338.80ms | 1547483.28 tokens/sec
Step 6149 | loss: 3.313657 | lr:4.9143e-04 | norm 0.2550 | dt 339.22ms | 1545584.24 tokens/sec
Step 6150 | loss: 3.282032 | lr:4.9140e-04 | norm 0.2376 | dt 338.01ms | 1551123.59 tokens/sec
Step 6151 | loss: 3.198286 | lr:4.9136e-04 | norm 0.2442 | dt 338.23ms | 1550107.82 tokens/sec
Step 6152 | loss: 3.225483 | lr:4.9132e-04 | norm 0.2529 | dt 338.32ms | 1549678.51 tokens/sec
Step 6153 | loss: 3.260969 | lr:4.9128e-04 | norm 0.2770 | dt 337.66ms | 1552705.10 tokens/sec
Step 6154 | loss: 3.286183 | lr:4.9125e-04 | norm 0.2656 | dt 338.23ms | 1550082.69 tokens/sec
Step 6155 | loss: 3.271863 | lr:4.9121e-04 | norm 0.2487 | dt 337.93ms | 1551452.99 tokens/sec
Step 6156 | loss: 3.327053 | lr:4.9117e-04 | norm 0.2649 | dt 338.36ms | 1549492.88 tokens/sec
Step 6157 | loss: 3.320343 | lr:4.9114e-04 | norm 0.2679 | dt 338.18ms | 1550317.64 tokens/sec
Step 6158 | loss: 3.316492 | lr:4.9110e-04 | norm 0.2783 | dt 338.66ms | 1548118.42 tokens/sec
Step 6159 | loss: 3.363941 | lr:4.9106e-04 | norm 0.2964 | dt 337.87ms | 1551754.05 tokens/sec
Step 6160 | loss: 3.306924 | lr:4.9103e-04 | norm 0.3253 | dt 338.90ms | 1547032.57 tokens/sec
Step 6161 | loss: 3.299706 | lr:4.9099e-04 | norm 0.2986 | dt 339.46ms | 1544460.72 tokens/sec
Step 6162 | loss: 3.299093 | lr:4.9095e-04 | norm 0.3478 | dt 341.21ms | 1536541.75 tokens/sec
Step 6163 | loss: 3.332129 | lr:4.9091e-04 | norm 0.3689 | dt 337.97ms | 1551297.57 tokens/sec
Step 6164 | loss: 3.362971 | lr:4.9088e-04 | norm 0.3317 | dt 339.20ms | 1545661.37 tokens/sec
Step 6165 | loss: 3.337928 | lr:4.9084e-04 | norm 0.3006 | dt 337.54ms | 1553239.20 tokens/sec
Step 6166 | loss: 3.339814 | lr:4.9080e-04 | norm 0.3162 | dt 337.89ms | 1551660.98 tokens/sec
Step 6167 | loss: 3.294202 | lr:4.9077e-04 | norm 0.2963 | dt 337.98ms | 1551261.46 tokens/sec
Step 6168 | loss: 3.339128 | lr:4.9073e-04 | norm 0.3042 | dt 337.93ms | 1551461.74 tokens/sec
Step 6169 | loss: 3.307186 | lr:4.9069e-04 | norm 0.2996 | dt 337.57ms | 1553106.47 tokens/sec
Step 6170 | loss: 3.294210 | lr:4.9065e-04 | norm 0.2720 | dt 338.49ms | 1548891.53 tokens/sec
Step 6171 | loss: 3.275460 | lr:4.9062e-04 | norm 0.2932 | dt 338.23ms | 1550104.54 tokens/sec
Step 6172 | loss: 3.297960 | lr:4.9058e-04 | norm 0.2679 | dt 338.05ms | 1550932.14 tokens/sec
Step 6173 | loss: 3.385872 | lr:4.9054e-04 | norm 0.2693 | dt 338.02ms | 1551055.75 tokens/sec
Step 6174 | loss: 3.267480 | lr:4.9051e-04 | norm 0.2584 | dt 338.89ms | 1547071.75 tokens/sec
Step 6175 | loss: 3.450845 | lr:4.9047e-04 | norm 0.2749 | dt 338.34ms | 1549607.53 tokens/sec
Step 6176 | loss: 3.326082 | lr:4.9043e-04 | norm 0.3156 | dt 337.86ms | 1551801.14 tokens/sec
Step 6177 | loss: 3.254420 | lr:4.9039e-04 | norm 0.3239 | dt 338.06ms | 1550862.14 tokens/sec
Step 6178 | loss: 3.279365 | lr:4.9036e-04 | norm 0.2809 | dt 338.39ms | 1549367.33 tokens/sec
Step 6179 | loss: 3.240674 | lr:4.9032e-04 | norm 0.2796 | dt 337.57ms | 1553139.37 tokens/sec
Step 6180 | loss: 3.281777 | lr:4.9028e-04 | norm 0.2775 | dt 338.85ms | 1547277.49 tokens/sec
Step 6181 | loss: 3.256352 | lr:4.9025e-04 | norm 0.2832 | dt 337.83ms | 1551940.23 tokens/sec
Step 6182 | loss: 3.240771 | lr:4.9021e-04 | norm 0.2668 | dt 338.06ms | 1550886.20 tokens/sec
Step 6183 | loss: 3.274810 | lr:4.9017e-04 | norm 0.3147 | dt 338.62ms | 1548305.90 tokens/sec
Step 6184 | loss: 3.229239 | lr:4.9013e-04 | norm 0.3049 | dt 337.69ms | 1552594.38 tokens/sec
Step 6185 | loss: 3.294662 | lr:4.9010e-04 | norm 0.2680 | dt 338.09ms | 1550736.37 tokens/sec
Step 6186 | loss: 3.244777 | lr:4.9006e-04 | norm 0.2926 | dt 338.67ms | 1548085.72 tokens/sec
Step 6187 | loss: 3.273011 | lr:4.9002e-04 | norm 0.3054 | dt 337.92ms | 1551509.91 tokens/sec
Step 6188 | loss: 3.276904 | lr:4.8999e-04 | norm 0.3015 | dt 338.02ms | 1551074.35 tokens/sec
Step 6189 | loss: 3.253669 | lr:4.8995e-04 | norm 0.2816 | dt 338.53ms | 1548715.90 tokens/sec
Step 6190 | loss: 3.298461 | lr:4.8991e-04 | norm 0.3144 | dt 339.88ms | 1542549.62 tokens/sec
Step 6191 | loss: 3.343665 | lr:4.8987e-04 | norm 0.2969 | dt 338.02ms | 1551052.47 tokens/sec
Step 6192 | loss: 3.327364 | lr:4.8984e-04 | norm 0.2877 | dt 338.57ms | 1548534.86 tokens/sec
Step 6193 | loss: 3.349984 | lr:4.8980e-04 | norm 0.2894 | dt 338.34ms | 1549598.80 tokens/sec
Step 6194 | loss: 3.264571 | lr:4.8976e-04 | norm 0.2831 | dt 338.50ms | 1548871.89 tokens/sec
Step 6195 | loss: 3.320965 | lr:4.8972e-04 | norm 0.2788 | dt 337.88ms | 1551690.55 tokens/sec
Step 6196 | loss: 3.296597 | lr:4.8969e-04 | norm 0.2930 | dt 337.79ms | 1552091.39 tokens/sec
Step 6197 | loss: 3.330118 | lr:4.8965e-04 | norm 0.2958 | dt 338.05ms | 1550916.83 tokens/sec
Step 6198 | loss: 3.345398 | lr:4.8961e-04 | norm 0.3077 | dt 338.22ms | 1550132.95 tokens/sec
Step 6199 | loss: 3.323440 | lr:4.8958e-04 | norm 0.2848 | dt 337.49ms | 1553493.77 tokens/sec
Step 6200 | loss: 3.312892 | lr:4.8954e-04 | norm 0.2691 | dt 338.39ms | 1549376.07 tokens/sec
Step 6201 | loss: 3.267071 | lr:4.8950e-04 | norm 0.2881 | dt 337.79ms | 1552103.44 tokens/sec
Step 6202 | loss: 3.278266 | lr:4.8946e-04 | norm 0.2700 | dt 337.47ms | 1553572.79 tokens/sec
Step 6203 | loss: 3.311275 | lr:4.8943e-04 | norm 0.2758 | dt 337.81ms | 1552023.47 tokens/sec
Step 6204 | loss: 3.313229 | lr:4.8939e-04 | norm 0.2631 | dt 337.98ms | 1551261.46 tokens/sec
Step 6205 | loss: 3.286224 | lr:4.8935e-04 | norm 0.2794 | dt 338.14ms | 1550513.31 tokens/sec
Step 6206 | loss: 3.299550 | lr:4.8931e-04 | norm 0.2607 | dt 338.15ms | 1550440.07 tokens/sec
Step 6207 | loss: 3.254559 | lr:4.8928e-04 | norm 0.2845 | dt 337.28ms | 1554448.05 tokens/sec
Step 6208 | loss: 3.289840 | lr:4.8924e-04 | norm 0.2834 | dt 338.31ms | 1549723.29 tokens/sec
Step 6209 | loss: 3.307534 | lr:4.8920e-04 | norm 0.2581 | dt 338.17ms | 1550349.34 tokens/sec
Step 6210 | loss: 3.295365 | lr:4.8917e-04 | norm 0.2953 | dt 337.08ms | 1555369.40 tokens/sec
Step 6211 | loss: 3.283085 | lr:4.8913e-04 | norm 0.2755 | dt 337.79ms | 1552093.58 tokens/sec
Step 6212 | loss: 3.283370 | lr:4.8909e-04 | norm 0.2604 | dt 338.12ms | 1550583.28 tokens/sec
Step 6213 | loss: 3.216596 | lr:4.8905e-04 | norm 0.2515 | dt 337.90ms | 1551589.82 tokens/sec
Step 6214 | loss: 3.309280 | lr:4.8902e-04 | norm 0.2637 | dt 337.53ms | 1553301.74 tokens/sec
Step 6215 | loss: 3.288827 | lr:4.8898e-04 | norm 0.2770 | dt 337.66ms | 1552719.35 tokens/sec
Step 6216 | loss: 3.265688 | lr:4.8894e-04 | norm 0.2868 | dt 338.08ms | 1550772.46 tokens/sec
Step 6217 | loss: 3.313104 | lr:4.8890e-04 | norm 0.3469 | dt 337.77ms | 1552191.08 tokens/sec
Step 6218 | loss: 3.248951 | lr:4.8887e-04 | norm 0.3712 | dt 338.07ms | 1550816.20 tokens/sec
Step 6219 | loss: 3.259056 | lr:4.8883e-04 | norm 0.2814 | dt 337.64ms | 1552824.61 tokens/sec
Step 6220 | loss: 3.209791 | lr:4.8879e-04 | norm 0.3343 | dt 337.59ms | 1553025.30 tokens/sec
Step 6221 | loss: 3.340701 | lr:4.8875e-04 | norm 0.3240 | dt 337.93ms | 1551488.01 tokens/sec
Step 6222 | loss: 3.222513 | lr:4.8872e-04 | norm 0.3289 | dt 337.96ms | 1551321.65 tokens/sec
Step 6223 | loss: 3.324437 | lr:4.8868e-04 | norm 0.2951 | dt 337.73ms | 1552404.76 tokens/sec
Step 6224 | loss: 3.310407 | lr:4.8864e-04 | norm 0.2760 | dt 337.44ms | 1553725.37 tokens/sec
Step 6225 | loss: 3.305717 | lr:4.8860e-04 | norm 0.3035 | dt 338.03ms | 1550989.02 tokens/sec
Step 6226 | loss: 3.305352 | lr:4.8857e-04 | norm 0.2778 | dt 338.54ms | 1548685.36 tokens/sec
Step 6227 | loss: 3.323646 | lr:4.8853e-04 | norm 0.3079 | dt 338.31ms | 1549709.09 tokens/sec
Step 6228 | loss: 3.415041 | lr:4.8849e-04 | norm 0.2749 | dt 337.76ms | 1552256.82 tokens/sec
Step 6229 | loss: 3.310110 | lr:4.8846e-04 | norm 0.2631 | dt 337.79ms | 1552127.54 tokens/sec
Step 6230 | loss: 3.301755 | lr:4.8842e-04 | norm 0.2942 | dt 338.30ms | 1549771.35 tokens/sec
Step 6231 | loss: 3.349808 | lr:4.8838e-04 | norm 0.2996 | dt 337.96ms | 1551328.21 tokens/sec
Step 6232 | loss: 3.331305 | lr:4.8834e-04 | norm 0.3019 | dt 337.29ms | 1554411.79 tokens/sec
Step 6233 | loss: 3.294320 | lr:4.8831e-04 | norm 0.2989 | dt 339.52ms | 1544184.17 tokens/sec
Step 6234 | loss: 3.350775 | lr:4.8827e-04 | norm 0.2808 | dt 337.52ms | 1553376.35 tokens/sec
Step 6235 | loss: 3.349728 | lr:4.8823e-04 | norm 0.2607 | dt 338.58ms | 1548509.78 tokens/sec
Step 6236 | loss: 3.390904 | lr:4.8819e-04 | norm 0.3221 | dt 895.06ms | 585755.31 tokens/sec
Step 6237 | loss: 3.297146 | lr:4.8816e-04 | norm 0.3289 | dt 334.68ms | 1566548.21 tokens/sec
Step 6238 | loss: 3.307109 | lr:4.8812e-04 | norm 0.3289 | dt 337.43ms | 1553760.50 tokens/sec
Step 6239 | loss: 3.261142 | lr:4.8808e-04 | norm 0.3254 | dt 339.16ms | 1545856.95 tokens/sec
Step 6240 | loss: 3.315051 | lr:4.8804e-04 | norm 0.2937 | dt 337.56ms | 1553177.77 tokens/sec
Step 6241 | loss: 3.326208 | lr:4.8801e-04 | norm 0.2790 | dt 337.35ms | 1554129.46 tokens/sec
Step 6242 | loss: 3.316865 | lr:4.8797e-04 | norm 0.2786 | dt 337.85ms | 1551844.94 tokens/sec
Step 6243 | loss: 3.272061 | lr:4.8793e-04 | norm 0.2695 | dt 337.24ms | 1554638.17 tokens/sec
Step 6244 | loss: 3.272397 | lr:4.8789e-04 | norm 0.2632 | dt 337.43ms | 1553790.14 tokens/sec
Step 6245 | loss: 3.255598 | lr:4.8786e-04 | norm 0.2909 | dt 338.17ms | 1550384.32 tokens/sec
Step 6246 | loss: 3.256978 | lr:4.8782e-04 | norm 0.2763 | dt 337.56ms | 1553154.73 tokens/sec
Step 6247 | loss: 3.198005 | lr:4.8778e-04 | norm 0.2753 | dt 338.53ms | 1548702.81 tokens/sec
Step 6248 | loss: 3.258435 | lr:4.8774e-04 | norm 0.2901 | dt 337.46ms | 1553632.07 tokens/sec
Step 6249 | loss: 3.260946 | lr:4.8771e-04 | norm 0.3125 | dt 338.11ms | 1550629.20 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 6250: 3.3058
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2812/10042=0.2800


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, and if it's like a mouse, the first thing you want to do is type the word 'the' you need
rank 5 sample 1 >Hello, I'm a language model, here we have to work at an open source model. We'll begin by opening in our browser. You want to put
rank 5 sample 2 >Hello, I'm a language model, but it will not necessarily be one at all. I'm not quite sure how many language models I'll be able to
rank 5 sample 3 >Hello, I'm a language model, don't the ones I write about.
[The problem is, I've learned a little way. I don't




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not really talking to language models, so I get a real goliath to have for. I mean.
rank 3 sample 1 >Hello, I'm a language model, so this one is a more abstract type. This is not a good idea.
I'm not a language but you
rank 3 sample 2 >Hello, I'm a language model, so it would be just a one-liners or a one-liners, if it were. Okay. So why I
rank 3 sample 3 >Hello, I'm a language model, so that's how I started working on it. It just keeps popping and popping every couple of days just like a song




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, thanks in part to the IETF's RFCs. It was a shame to think that the original RFC only included RFC
rank 2 sample 1 >Hello, I'm a language model, how do we get people to understand one another? Of course not at all. It's like having the right tools on
rank 2 sample 2 >Hello, I'm a language model, but I can't think of it with anyone.
For those reasons, you may wonder what this was about. If
rank 2 sample 3 >Hello, I'm a language model, and we try to model the language models on the real world but we don't see it well on the real world.




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, my language model is different every day, and I've had some mistakes in learning and I get frustrated.
I need
rank 6 sample 1 >Hello, I'm a language model, which is very similar to a real language where the same language is used with the same grammar and vocabulary, as well as


ddp_rank 4: ####### Printing generated samples ####### 

rank 6 sample 2 >Hello, I'm a language model, and a very hard question for anyone to answer. I'm a great language-modeler, and I don't have
rank 6 sample 3 >Hello, I'm a language model, I want to give you a general overview of what I'm doing here. So I want to explain the syntax of sentences


rank 4 sample 0 >Hello, I'm a language model, but I'd like to see how it feels. Could you describe the situation to me without explaining it? Could you explain
rank 4 sample 1 >Hello, I'm a language model, am it? Have you ever asked you? Can it, in particular, do you believe you can, and perhaps do
rank 4 sample 2 >Hello, I'm a language model, I thought I understand a small bit of it. I'm really proud of this but I'm really wondering if we could
rank 4 sample 3 >Hello, I'm a language model, so you couldn't get into it first: how do you solve a graph problem?
There I am, for instance




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to have to think from the viewpoint of the language structure. Well, how, in my opinion,
rank 7 sample 1 >Hello, I'm a language model,” she said.
I feel like I'm struggling to explain the difference between the two words “and�
rank 7 sample 2 >Hello, I'm a language model, and I want to read this first since the previous program. I've created a program that will be more useful as well
rank 7 sample 3 >Hello, I'm a language model, working in a language class; I see in practice how to translate and edit text and put it in the correct order...




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, because we are all taught in the kindergarten and even elementary school environment. I wanted to do that with my children. I


ddp_rank 0: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, not an object model. I am writing on the blog because I write because I want someone to comment. I'm not
rank 1 sample 2 >Hello, I'm a language model, and your language model is a language model.
And you may not have had a good experience at that, or could
rank 0 sample 0 >Hello, I'm a language model, so I need to understand them better. Now lets do that. Say some languages are used for text and we want text
rank 1 sample 3 >Hello, I'm a language model, so I'm writing this code to write up a little paragraph to demonstrate it at the high level.
I don'trank 0 sample 1 >Hello, I'm a language model, and how to create it. I teach a course so a student could speak the same as me. What is the point



rank 0 sample 2 >Hello, I'm a language model, but I got the basics here. Let's learn how to talk English to a Japanese speaker.
- What's a
rank 0 sample 3 >Hello, I'm a language model, and how to get it right? (Well - I'm saying in the above snippet), or one of the following example


Step 6250 | loss: 3.227396 | lr:4.8767e-04 | norm 0.3016 | dt 12381.02ms | 42346.11 tokens/sec
Step 6251 | loss: 3.182405 | lr:4.8763e-04 | norm 0.2750 | dt 335.76ms | 1561496.87 tokens/sec
Step 6252 | loss: 3.278293 | lr:4.8759e-04 | norm 0.2877 | dt 337.03ms | 1555600.46 tokens/sec
Step 6253 | loss: 3.275601 | lr:4.8756e-04 | norm 0.2997 | dt 337.42ms | 1553794.54 tokens/sec
Step 6254 | loss: 3.280587 | lr:4.8752e-04 | norm 0.2611 | dt 337.39ms | 1553944.96 tokens/sec
Step 6255 | loss: 3.244438 | lr:4.8748e-04 | norm 0.2793 | dt 336.54ms | 1557854.12 tokens/sec
Step 6256 | loss: 3.283009 | lr:4.8744e-04 | norm 0.3204 | dt 335.84ms | 1561136.60 tokens/sec
Step 6257 | loss: 3.336925 | lr:4.8741e-04 | norm 0.2573 | dt 336.99ms | 1555782.05 tokens/sec
Step 6258 | loss: 3.435784 | lr:4.8737e-04 | norm 0.3192 | dt 336.59ms | 1557636.73 tokens/sec
Step 6259 | loss: 3.358318 | lr:4.8733e-04 | norm 0.3301 | dt 336.85ms | 1556464.79 tokens/sec
Step 6260 | loss: 3.316236 | lr:4.8729e-04 | norm 0.3466 | dt 336.96ms | 1555942.77 tokens/sec
Step 6261 | loss: 3.286198 | lr:4.8726e-04 | norm 0.2878 | dt 337.35ms | 1554114.09 tokens/sec
Step 6262 | loss: 3.292206 | lr:4.8722e-04 | norm 0.3384 | dt 337.28ms | 1554437.07 tokens/sec
Step 6263 | loss: 3.299217 | lr:4.8718e-04 | norm 0.3218 | dt 337.94ms | 1551435.47 tokens/sec
Step 6264 | loss: 3.310850 | lr:4.8714e-04 | norm 0.3180 | dt 337.86ms | 1551768.29 tokens/sec
Step 6265 | loss: 3.311638 | lr:4.8711e-04 | norm 0.2684 | dt 336.75ms | 1556897.86 tokens/sec
Step 6266 | loss: 3.312407 | lr:4.8707e-04 | norm 0.2827 | dt 337.47ms | 1553573.89 tokens/sec
Step 6267 | loss: 3.299495 | lr:4.8703e-04 | norm 0.2809 | dt 336.76ms | 1556873.61 tokens/sec
Step 6268 | loss: 3.330687 | lr:4.8699e-04 | norm 0.2618 | dt 336.88ms | 1556317.18 tokens/sec
Step 6269 | loss: 3.255769 | lr:4.8696e-04 | norm 0.2877 | dt 1045.34ms | 501545.73 tokens/sec
Step 6270 | loss: 3.360030 | lr:4.8692e-04 | norm 0.2657 | dt 335.86ms | 1561040.18 tokens/sec
Step 6271 | loss: 3.301691 | lr:4.8688e-04 | norm 0.2593 | dt 337.34ms | 1554184.38 tokens/sec
Step 6272 | loss: 3.352887 | lr:4.8684e-04 | norm 0.2519 | dt 337.45ms | 1553666.09 tokens/sec
Step 6273 | loss: 3.305631 | lr:4.8680e-04 | norm 0.2681 | dt 336.50ms | 1558052.80 tokens/sec
Step 6274 | loss: 3.302528 | lr:4.8677e-04 | norm 0.2890 | dt 338.41ms | 1549286.56 tokens/sec
Step 6275 | loss: 3.342523 | lr:4.8673e-04 | norm 0.2747 | dt 337.60ms | 1552988.01 tokens/sec
Step 6276 | loss: 3.281599 | lr:4.8669e-04 | norm 0.2558 | dt 337.39ms | 1553942.77 tokens/sec
Step 6277 | loss: 3.317475 | lr:4.8665e-04 | norm 0.2822 | dt 338.78ms | 1547582.38 tokens/sec
Step 6278 | loss: 3.255202 | lr:4.8662e-04 | norm 0.2626 | dt 338.67ms | 1548059.56 tokens/sec
Step 6279 | loss: 3.254071 | lr:4.8658e-04 | norm 0.2647 | dt 338.52ms | 1548783.53 tokens/sec
Step 6280 | loss: 3.221272 | lr:4.8654e-04 | norm 0.2631 | dt 339.19ms | 1545705.92 tokens/sec
Step 6281 | loss: 3.237974 | lr:4.8650e-04 | norm 0.2831 | dt 338.14ms | 1550502.38 tokens/sec
Step 6282 | loss: 3.216668 | lr:4.8647e-04 | norm 0.2923 | dt 337.79ms | 1552096.87 tokens/sec
Step 6283 | loss: 3.246838 | lr:4.8643e-04 | norm 0.3290 | dt 339.59ms | 1543887.11 tokens/sec
Step 6284 | loss: 3.217932 | lr:4.8639e-04 | norm 0.2854 | dt 340.27ms | 1540782.49 tokens/sec
Step 6285 | loss: 3.276086 | lr:4.8635e-04 | norm 0.2445 | dt 338.25ms | 1549985.45 tokens/sec
Step 6286 | loss: 3.261903 | lr:4.8632e-04 | norm 0.2829 | dt 338.82ms | 1547402.70 tokens/sec
Step 6287 | loss: 3.247944 | lr:4.8628e-04 | norm 0.2852 | dt 340.10ms | 1541589.35 tokens/sec
Step 6288 | loss: 3.310192 | lr:4.8624e-04 | norm 0.2557 | dt 341.41ms | 1535668.30 tokens/sec
Step 6289 | loss: 3.264853 | lr:4.8620e-04 | norm 0.2645 | dt 338.35ms | 1549535.47 tokens/sec
Step 6290 | loss: 3.235550 | lr:4.8616e-04 | norm 0.2758 | dt 338.95ms | 1546799.70 tokens/sec
Step 6291 | loss: 3.306126 | lr:4.8613e-04 | norm 0.2603 | dt 340.95ms | 1537706.45 tokens/sec
Step 6292 | loss: 3.370511 | lr:4.8609e-04 | norm 0.3014 | dt 340.51ms | 1539709.06 tokens/sec
Step 6293 | loss: 3.200706 | lr:4.8605e-04 | norm 0.2933 | dt 338.70ms | 1547928.80 tokens/sec
Step 6294 | loss: 3.238230 | lr:4.8601e-04 | norm 0.2882 | dt 339.22ms | 1545552.74 tokens/sec
Step 6295 | loss: 3.294123 | lr:4.8598e-04 | norm 0.3038 | dt 338.35ms | 1549558.40 tokens/sec
Step 6296 | loss: 3.308628 | lr:4.8594e-04 | norm 0.2645 | dt 338.96ms | 1546751.82 tokens/sec
Step 6297 | loss: 3.294410 | lr:4.8590e-04 | norm 0.2825 | dt 338.76ms | 1547669.52 tokens/sec
Step 6298 | loss: 3.281492 | lr:4.8586e-04 | norm 0.2673 | dt 338.93ms | 1546906.33 tokens/sec
Step 6299 | loss: 3.329320 | lr:4.8583e-04 | norm 0.2878 | dt 338.38ms | 1549416.46 tokens/sec
Step 6300 | loss: 3.310401 | lr:4.8579e-04 | norm 0.2867 | dt 338.23ms | 1550082.69 tokens/sec
Step 6301 | loss: 3.205183 | lr:4.8575e-04 | norm 0.3156 | dt 337.91ms | 1551574.49 tokens/sec
Step 6302 | loss: 3.319937 | lr:4.8571e-04 | norm 0.3338 | dt 338.78ms | 1547564.95 tokens/sec
Step 6303 | loss: 3.330938 | lr:4.8567e-04 | norm 0.3188 | dt 339.20ms | 1545682.02 tokens/sec
Step 6304 | loss: 3.322547 | lr:4.8564e-04 | norm 0.3172 | dt 338.08ms | 1550776.83 tokens/sec
Step 6305 | loss: 3.346496 | lr:4.8560e-04 | norm 0.3025 | dt 338.83ms | 1547344.99 tokens/sec
Step 6306 | loss: 3.288595 | lr:4.8556e-04 | norm 0.3384 | dt 338.74ms | 1547764.29 tokens/sec
Step 6307 | loss: 3.347118 | lr:4.8552e-04 | norm 0.2865 | dt 341.77ms | 1534048.51 tokens/sec
Step 6308 | loss: 3.296725 | lr:4.8549e-04 | norm 0.3022 | dt 339.06ms | 1546289.58 tokens/sec
Step 6309 | loss: 3.323673 | lr:4.8545e-04 | norm 0.3016 | dt 338.46ms | 1549056.28 tokens/sec
Step 6310 | loss: 3.310607 | lr:4.8541e-04 | norm 0.2778 | dt 339.12ms | 1546017.80 tokens/sec
Step 6311 | loss: 3.330288 | lr:4.8537e-04 | norm 0.3182 | dt 339.00ms | 1546574.51 tokens/sec
Step 6312 | loss: 3.317283 | lr:4.8533e-04 | norm 0.2881 | dt 338.18ms | 1550344.97 tokens/sec
Step 6313 | loss: 3.296604 | lr:4.8530e-04 | norm 0.3111 | dt 338.84ms | 1547325.39 tokens/sec
Step 6314 | loss: 3.256144 | lr:4.8526e-04 | norm 0.2998 | dt 337.88ms | 1551681.79 tokens/sec
Step 6315 | loss: 3.177651 | lr:4.8522e-04 | norm 0.2768 | dt 339.78ms | 1543002.05 tokens/sec
Step 6316 | loss: 3.275233 | lr:4.8518e-04 | norm 0.2981 | dt 338.86ms | 1547233.94 tokens/sec
Step 6317 | loss: 3.270140 | lr:4.8515e-04 | norm 0.2685 | dt 338.33ms | 1549622.82 tokens/sec
Step 6318 | loss: 3.362936 | lr:4.8511e-04 | norm 0.3062 | dt 339.17ms | 1545798.27 tokens/sec
Step 6319 | loss: 3.279051 | lr:4.8507e-04 | norm 0.2516 | dt 338.46ms | 1549023.54 tokens/sec
Step 6320 | loss: 3.266301 | lr:4.8503e-04 | norm 0.2977 | dt 337.53ms | 1553313.81 tokens/sec
Step 6321 | loss: 3.234877 | lr:4.8499e-04 | norm 0.2775 | dt 338.97ms | 1546693.08 tokens/sec
Step 6322 | loss: 3.252121 | lr:4.8496e-04 | norm 0.2589 | dt 338.05ms | 1550910.26 tokens/sec
Step 6323 | loss: 3.261467 | lr:4.8492e-04 | norm 0.2716 | dt 337.39ms | 1553931.78 tokens/sec
Step 6324 | loss: 3.272728 | lr:4.8488e-04 | norm 0.2838 | dt 338.12ms | 1550602.96 tokens/sec
Step 6325 | loss: 3.251817 | lr:4.8484e-04 | norm 0.3076 | dt 337.77ms | 1552183.41 tokens/sec
Step 6326 | loss: 3.242859 | lr:4.8480e-04 | norm 0.2739 | dt 337.97ms | 1551298.66 tokens/sec
Step 6327 | loss: 3.267318 | lr:4.8477e-04 | norm 0.2720 | dt 338.66ms | 1548120.60 tokens/sec
Step 6328 | loss: 3.278934 | lr:4.8473e-04 | norm 0.2778 | dt 338.82ms | 1547380.92 tokens/sec
Step 6329 | loss: 3.238074 | lr:4.8469e-04 | norm 0.2724 | dt 339.48ms | 1544385.88 tokens/sec
Step 6330 | loss: 3.289197 | lr:4.8465e-04 | norm 0.2830 | dt 338.85ms | 1547236.12 tokens/sec
Step 6331 | loss: 3.255479 | lr:4.8462e-04 | norm 0.2652 | dt 339.00ms | 1546590.82 tokens/sec
Step 6332 | loss: 3.291387 | lr:4.8458e-04 | norm 0.2691 | dt 339.11ms | 1546069.98 tokens/sec
Step 6333 | loss: 3.268669 | lr:4.8454e-04 | norm 0.2593 | dt 338.76ms | 1547659.71 tokens/sec
Step 6334 | loss: 3.267506 | lr:4.8450e-04 | norm 0.2628 | dt 339.15ms | 1545899.34 tokens/sec
Step 6335 | loss: 3.298650 | lr:4.8446e-04 | norm 0.2499 | dt 339.58ms | 1543914.21 tokens/sec
Step 6336 | loss: 3.354880 | lr:4.8443e-04 | norm 0.2707 | dt 338.88ms | 1547106.58 tokens/sec
Step 6337 | loss: 3.332139 | lr:4.8439e-04 | norm 0.2781 | dt 338.23ms | 1550114.38 tokens/sec
Step 6338 | loss: 3.292027 | lr:4.8435e-04 | norm 0.2386 | dt 338.39ms | 1549373.88 tokens/sec
Step 6339 | loss: 3.318772 | lr:4.8431e-04 | norm 0.2586 | dt 338.55ms | 1548633.01 tokens/sec
Step 6340 | loss: 3.263911 | lr:4.8427e-04 | norm 0.2567 | dt 338.28ms | 1549863.10 tokens/sec
Step 6341 | loss: 3.341535 | lr:4.8424e-04 | norm 0.2723 | dt 338.36ms | 1549509.26 tokens/sec
Step 6342 | loss: 3.295131 | lr:4.8420e-04 | norm 0.2941 | dt 338.01ms | 1551118.12 tokens/sec
Step 6343 | loss: 3.310381 | lr:4.8416e-04 | norm 0.2774 | dt 339.46ms | 1544493.27 tokens/sec
Step 6344 | loss: 3.326950 | lr:4.8412e-04 | norm 0.3008 | dt 339.71ms | 1543334.50 tokens/sec
Step 6345 | loss: 3.346776 | lr:4.8408e-04 | norm 0.2762 | dt 339.02ms | 1546465.74 tokens/sec
Step 6346 | loss: 3.255561 | lr:4.8405e-04 | norm 0.2902 | dt 339.24ms | 1545495.17 tokens/sec
Step 6347 | loss: 3.280135 | lr:4.8401e-04 | norm 0.2488 | dt 338.85ms | 1547257.89 tokens/sec
Step 6348 | loss: 3.273321 | lr:4.8397e-04 | norm 0.2756 | dt 339.16ms | 1545839.57 tokens/sec
Step 6349 | loss: 3.315645 | lr:4.8393e-04 | norm 0.2759 | dt 339.86ms | 1542665.41 tokens/sec
Step 6350 | loss: 3.193230 | lr:4.8389e-04 | norm 0.2638 | dt 339.65ms | 1543628.10 tokens/sec
Step 6351 | loss: 3.283955 | lr:4.8386e-04 | norm 0.2655 | dt 339.63ms | 1543681.19 tokens/sec
Step 6352 | loss: 3.325081 | lr:4.8382e-04 | norm 0.3004 | dt 339.32ms | 1545094.47 tokens/sec
Step 6353 | loss: 3.238208 | lr:4.8378e-04 | norm 0.2770 | dt 338.84ms | 1547303.61 tokens/sec
Step 6354 | loss: 3.271267 | lr:4.8374e-04 | norm 0.2880 | dt 338.40ms | 1549313.85 tokens/sec
Step 6355 | loss: 3.264643 | lr:4.8370e-04 | norm 0.2508 | dt 339.76ms | 1543105.99 tokens/sec
Step 6356 | loss: 3.247964 | lr:4.8367e-04 | norm 0.2601 | dt 339.02ms | 1546475.53 tokens/sec
Step 6357 | loss: 3.237121 | lr:4.8363e-04 | norm 0.2945 | dt 338.44ms | 1549152.31 tokens/sec
Step 6358 | loss: 3.266051 | lr:4.8359e-04 | norm 0.2822 | dt 338.97ms | 1546698.52 tokens/sec
Step 6359 | loss: 3.214958 | lr:4.8355e-04 | norm 0.2734 | dt 339.00ms | 1546552.75 tokens/sec
Step 6360 | loss: 3.292433 | lr:4.8351e-04 | norm 0.2604 | dt 339.47ms | 1544423.84 tokens/sec
Step 6361 | loss: 3.266857 | lr:4.8348e-04 | norm 0.2535 | dt 341.15ms | 1536846.72 tokens/sec
Step 6362 | loss: 3.281447 | lr:4.8344e-04 | norm 0.2889 | dt 338.93ms | 1546913.95 tokens/sec
Step 6363 | loss: 3.279705 | lr:4.8340e-04 | norm 0.2722 | dt 338.92ms | 1546947.68 tokens/sec
Step 6364 | loss: 3.309966 | lr:4.8336e-04 | norm 0.2667 | dt 338.47ms | 1549007.18 tokens/sec
Step 6365 | loss: 3.252473 | lr:4.8332e-04 | norm 0.2896 | dt 339.13ms | 1545975.41 tokens/sec
Step 6366 | loss: 3.443075 | lr:4.8329e-04 | norm 0.3825 | dt 339.25ms | 1545413.71 tokens/sec
Step 6367 | loss: 3.283681 | lr:4.8325e-04 | norm 0.3883 | dt 338.33ms | 1549651.21 tokens/sec
Step 6368 | loss: 3.287960 | lr:4.8321e-04 | norm 0.3387 | dt 338.75ms | 1547713.09 tokens/sec
Step 6369 | loss: 3.283603 | lr:4.8317e-04 | norm 0.3596 | dt 338.71ms | 1547892.84 tokens/sec
Step 6370 | loss: 3.291430 | lr:4.8313e-04 | norm 0.3678 | dt 338.20ms | 1550230.21 tokens/sec
Step 6371 | loss: 3.330857 | lr:4.8310e-04 | norm 0.3179 | dt 339.35ms | 1544968.55 tokens/sec
Step 6372 | loss: 3.335128 | lr:4.8306e-04 | norm 0.3175 | dt 339.10ms | 1546105.85 tokens/sec
Step 6373 | loss: 3.410485 | lr:4.8302e-04 | norm 0.3585 | dt 338.78ms | 1547582.38 tokens/sec
Step 6374 | loss: 3.304622 | lr:4.8298e-04 | norm 0.3989 | dt 338.94ms | 1546830.16 tokens/sec
Step 6375 | loss: 3.307817 | lr:4.8294e-04 | norm 0.3182 | dt 338.25ms | 1549981.08 tokens/sec
Step 6376 | loss: 3.324649 | lr:4.8291e-04 | norm 0.3317 | dt 338.80ms | 1547481.10 tokens/sec
Step 6377 | loss: 3.307429 | lr:4.8287e-04 | norm 0.3207 | dt 338.69ms | 1547975.65 tokens/sec
Step 6378 | loss: 3.291030 | lr:4.8283e-04 | norm 0.2980 | dt 338.30ms | 1549763.70 tokens/sec
Step 6379 | loss: 3.364185 | lr:4.8279e-04 | norm 0.2913 | dt 339.59ms | 1543880.61 tokens/sec
Step 6380 | loss: 3.344878 | lr:4.8275e-04 | norm 0.2645 | dt 338.93ms | 1546909.59 tokens/sec
Step 6381 | loss: 3.350013 | lr:4.8272e-04 | norm 0.3061 | dt 339.30ms | 1545193.27 tokens/sec
Step 6382 | loss: 3.291558 | lr:4.8268e-04 | norm 0.2685 | dt 339.21ms | 1545611.40 tokens/sec
Step 6383 | loss: 3.242761 | lr:4.8264e-04 | norm 0.2559 | dt 338.68ms | 1548049.76 tokens/sec
Step 6384 | loss: 3.299407 | lr:4.8260e-04 | norm 0.2814 | dt 338.47ms | 1549009.36 tokens/sec
Step 6385 | loss: 3.219295 | lr:4.8256e-04 | norm 0.2570 | dt 338.93ms | 1546869.33 tokens/sec
Step 6386 | loss: 3.310503 | lr:4.8253e-04 | norm 0.2665 | dt 339.14ms | 1545918.90 tokens/sec
Step 6387 | loss: 3.272525 | lr:4.8249e-04 | norm 0.2835 | dt 338.53ms | 1548722.44 tokens/sec
Step 6388 | loss: 3.255003 | lr:4.8245e-04 | norm 0.2692 | dt 338.78ms | 1547574.76 tokens/sec
Step 6389 | loss: 3.240235 | lr:4.8241e-04 | norm 0.2486 | dt 340.02ms | 1541937.42 tokens/sec
Step 6390 | loss: 3.242507 | lr:4.8237e-04 | norm 0.2379 | dt 338.50ms | 1548872.98 tokens/sec
Step 6391 | loss: 3.269883 | lr:4.8233e-04 | norm 0.2487 | dt 338.33ms | 1549622.82 tokens/sec
Step 6392 | loss: 3.245887 | lr:4.8230e-04 | norm 0.2784 | dt 337.87ms | 1551733.25 tokens/sec
Step 6393 | loss: 3.333838 | lr:4.8226e-04 | norm 0.2871 | dt 338.24ms | 1550045.54 tokens/sec
Step 6394 | loss: 3.233242 | lr:4.8222e-04 | norm 0.3184 | dt 337.76ms | 1552244.77 tokens/sec
Step 6395 | loss: 3.218923 | lr:4.8218e-04 | norm 0.2806 | dt 338.27ms | 1549925.36 tokens/sec
Step 6396 | loss: 3.285044 | lr:4.8214e-04 | norm 0.2960 | dt 339.85ms | 1542708.70 tokens/sec
Step 6397 | loss: 3.276345 | lr:4.8211e-04 | norm 0.3289 | dt 337.85ms | 1551851.51 tokens/sec
Step 6398 | loss: 3.355326 | lr:4.8207e-04 | norm 0.3289 | dt 337.35ms | 1554151.43 tokens/sec
Step 6399 | loss: 3.303999 | lr:4.8203e-04 | norm 0.2772 | dt 338.65ms | 1548151.11 tokens/sec
Step 6400 | loss: 3.295856 | lr:4.8199e-04 | norm 0.2874 | dt 338.64ms | 1548213.24 tokens/sec
Step 6401 | loss: 3.319789 | lr:4.8195e-04 | norm 0.3077 | dt 337.42ms | 1553791.24 tokens/sec
Step 6402 | loss: 3.312630 | lr:4.8191e-04 | norm 0.2798 | dt 338.39ms | 1549378.25 tokens/sec
Step 6403 | loss: 3.318062 | lr:4.8188e-04 | norm 0.2536 | dt 338.23ms | 1550091.43 tokens/sec
Step 6404 | loss: 3.278996 | lr:4.8184e-04 | norm 0.2703 | dt 337.67ms | 1552645.90 tokens/sec
Step 6405 | loss: 3.254515 | lr:4.8180e-04 | norm 0.2837 | dt 337.95ms | 1551355.57 tokens/sec
Step 6406 | loss: 3.288516 | lr:4.8176e-04 | norm 0.2582 | dt 337.87ms | 1551752.96 tokens/sec
Step 6407 | loss: 3.290796 | lr:4.8172e-04 | norm 0.2764 | dt 337.49ms | 1553498.16 tokens/sec
Step 6408 | loss: 3.284649 | lr:4.8169e-04 | norm 0.2977 | dt 337.81ms | 1552030.04 tokens/sec
Step 6409 | loss: 3.315922 | lr:4.8165e-04 | norm 0.2972 | dt 338.28ms | 1549869.65 tokens/sec
Step 6410 | loss: 3.381344 | lr:4.8161e-04 | norm 0.2882 | dt 338.37ms | 1549452.49 tokens/sec
Step 6411 | loss: 3.357841 | lr:4.8157e-04 | norm 0.3101 | dt 337.42ms | 1553834.06 tokens/sec
Step 6412 | loss: 3.287878 | lr:4.8153e-04 | norm 0.2718 | dt 338.18ms | 1550318.74 tokens/sec
Step 6413 | loss: 3.310426 | lr:4.8149e-04 | norm 0.3027 | dt 338.40ms | 1549311.66 tokens/sec
Step 6414 | loss: 3.365034 | lr:4.8146e-04 | norm 0.4069 | dt 338.00ms | 1551153.13 tokens/sec
Step 6415 | loss: 3.312207 | lr:4.8142e-04 | norm 0.2895 | dt 337.84ms | 1551875.61 tokens/sec
Step 6416 | loss: 3.365696 | lr:4.8138e-04 | norm 0.3036 | dt 338.31ms | 1549713.46 tokens/sec
Step 6417 | loss: 3.250891 | lr:4.8134e-04 | norm 0.2934 | dt 338.56ms | 1548586.12 tokens/sec
Step 6418 | loss: 3.279213 | lr:4.8130e-04 | norm 0.3142 | dt 338.95ms | 1546799.70 tokens/sec
Step 6419 | loss: 3.254256 | lr:4.8126e-04 | norm 0.2820 | dt 338.08ms | 1550788.86 tokens/sec
Step 6420 | loss: 3.222801 | lr:4.8123e-04 | norm 0.2938 | dt 339.04ms | 1546382.01 tokens/sec
Step 6421 | loss: 3.245111 | lr:4.8119e-04 | norm 0.2820 | dt 338.62ms | 1548305.90 tokens/sec
Step 6422 | loss: 3.260075 | lr:4.8115e-04 | norm 0.2774 | dt 337.89ms | 1551675.22 tokens/sec
Step 6423 | loss: 3.208936 | lr:4.8111e-04 | norm 0.2845 | dt 338.59ms | 1548467.26 tokens/sec
Step 6424 | loss: 3.288746 | lr:4.8107e-04 | norm 0.2808 | dt 338.54ms | 1548665.73 tokens/sec
Step 6425 | loss: 3.291543 | lr:4.8103e-04 | norm 0.2725 | dt 1019.73ms | 514142.45 tokens/sec
Step 6426 | loss: 3.273146 | lr:4.8100e-04 | norm 0.2805 | dt 336.99ms | 1555819.48 tokens/sec
Step 6427 | loss: 3.218191 | lr:4.8096e-04 | norm 0.2980 | dt 338.06ms | 1550890.58 tokens/sec
Step 6428 | loss: 3.227007 | lr:4.8092e-04 | norm 0.2753 | dt 338.17ms | 1550374.48 tokens/sec
Step 6429 | loss: 3.222819 | lr:4.8088e-04 | norm 0.2875 | dt 337.08ms | 1555393.60 tokens/sec
Step 6430 | loss: 3.295042 | lr:4.8084e-04 | norm 0.2727 | dt 337.18ms | 1554937.18 tokens/sec
Step 6431 | loss: 3.279145 | lr:4.8080e-04 | norm 0.2833 | dt 337.55ms | 1553210.68 tokens/sec
Step 6432 | loss: 3.361063 | lr:4.8077e-04 | norm 0.2929 | dt 338.06ms | 1550885.11 tokens/sec
Step 6433 | loss: 3.268955 | lr:4.8073e-04 | norm 0.2918 | dt 337.22ms | 1554751.38 tokens/sec
Step 6434 | loss: 3.306537 | lr:4.8069e-04 | norm 0.2712 | dt 337.17ms | 1554962.47 tokens/sec
Step 6435 | loss: 3.258488 | lr:4.8065e-04 | norm 0.2633 | dt 338.76ms | 1547651.00 tokens/sec
Step 6436 | loss: 3.273841 | lr:4.8061e-04 | norm 0.2636 | dt 337.76ms | 1552250.25 tokens/sec
Step 6437 | loss: 3.282081 | lr:4.8057e-04 | norm 0.2711 | dt 339.52ms | 1544182.00 tokens/sec
Step 6438 | loss: 3.346926 | lr:4.8054e-04 | norm 0.2757 | dt 338.67ms | 1548069.37 tokens/sec
Step 6439 | loss: 3.276059 | lr:4.8050e-04 | norm 0.2555 | dt 338.84ms | 1547319.95 tokens/sec
Step 6440 | loss: 3.304945 | lr:4.8046e-04 | norm 0.2840 | dt 339.74ms | 1543207.79 tokens/sec
Step 6441 | loss: 3.402544 | lr:4.8042e-04 | norm 0.2805 | dt 339.09ms | 1546163.46 tokens/sec
Step 6442 | loss: 3.367569 | lr:4.8038e-04 | norm 0.2806 | dt 338.09ms | 1550722.15 tokens/sec
Step 6443 | loss: 3.267221 | lr:4.8034e-04 | norm 0.2685 | dt 339.12ms | 1546015.63 tokens/sec
Step 6444 | loss: 3.321556 | lr:4.8031e-04 | norm 0.2712 | dt 339.22ms | 1545553.83 tokens/sec
Step 6445 | loss: 3.299057 | lr:4.8027e-04 | norm 0.2721 | dt 338.85ms | 1547278.57 tokens/sec
Step 6446 | loss: 3.332305 | lr:4.8023e-04 | norm 0.2623 | dt 340.21ms | 1541078.35 tokens/sec
Step 6447 | loss: 3.242544 | lr:4.8019e-04 | norm 0.2641 | dt 339.03ms | 1546433.12 tokens/sec
Step 6448 | loss: 3.263323 | lr:4.8015e-04 | norm 0.2871 | dt 339.22ms | 1545584.24 tokens/sec
Step 6449 | loss: 3.393936 | lr:4.8011e-04 | norm 0.3033 | dt 338.70ms | 1547930.98 tokens/sec
Step 6450 | loss: 3.357322 | lr:4.8008e-04 | norm 0.2622 | dt 339.84ms | 1542746.58 tokens/sec
Step 6451 | loss: 3.266705 | lr:4.8004e-04 | norm 0.2701 | dt 339.85ms | 1542695.71 tokens/sec
Step 6452 | loss: 3.275469 | lr:4.8000e-04 | norm 0.2586 | dt 338.42ms | 1549245.08 tokens/sec
Step 6453 | loss: 3.219730 | lr:4.7996e-04 | norm 0.2616 | dt 338.45ms | 1549106.48 tokens/sec
Step 6454 | loss: 3.262848 | lr:4.7992e-04 | norm 0.2947 | dt 338.76ms | 1547660.80 tokens/sec
Step 6455 | loss: 3.248679 | lr:4.7988e-04 | norm 0.2587 | dt 338.18ms | 1550332.94 tokens/sec
Step 6456 | loss: 3.249025 | lr:4.7985e-04 | norm 0.2651 | dt 338.47ms | 1548989.72 tokens/sec
Step 6457 | loss: 3.194163 | lr:4.7981e-04 | norm 0.2430 | dt 337.94ms | 1551432.19 tokens/sec
Step 6458 | loss: 3.286687 | lr:4.7977e-04 | norm 0.2630 | dt 337.29ms | 1554397.51 tokens/sec
Step 6459 | loss: 3.284787 | lr:4.7973e-04 | norm 0.2562 | dt 996.03ms | 526376.03 tokens/sec
Step 6460 | loss: 3.216805 | lr:4.7969e-04 | norm 0.2645 | dt 337.28ms | 1554464.54 tokens/sec
Step 6461 | loss: 3.249982 | lr:4.7965e-04 | norm 0.2547 | dt 338.05ms | 1550916.83 tokens/sec
Step 6462 | loss: 3.244774 | lr:4.7961e-04 | norm 0.2690 | dt 338.32ms | 1549686.16 tokens/sec
Step 6463 | loss: 3.323056 | lr:4.7958e-04 | norm 0.3298 | dt 338.34ms | 1549567.13 tokens/sec
Step 6464 | loss: 3.260361 | lr:4.7954e-04 | norm 0.2703 | dt 337.61ms | 1552949.62 tokens/sec
Step 6465 | loss: 3.381763 | lr:4.7950e-04 | norm 0.2769 | dt 337.60ms | 1552997.88 tokens/sec
Step 6466 | loss: 3.332558 | lr:4.7946e-04 | norm 0.2951 | dt 338.06ms | 1550876.36 tokens/sec
Step 6467 | loss: 3.260716 | lr:4.7942e-04 | norm 0.2765 | dt 337.10ms | 1555300.10 tokens/sec
Step 6468 | loss: 3.291932 | lr:4.7938e-04 | norm 0.3004 | dt 338.00ms | 1551125.77 tokens/sec
Step 6469 | loss: 3.277454 | lr:4.7935e-04 | norm 0.2649 | dt 337.57ms | 1553136.08 tokens/sec
Step 6470 | loss: 3.303163 | lr:4.7931e-04 | norm 0.2953 | dt 337.42ms | 1553791.24 tokens/sec
Step 6471 | loss: 3.287964 | lr:4.7927e-04 | norm 0.2854 | dt 340.86ms | 1538128.07 tokens/sec
Step 6472 | loss: 3.305676 | lr:4.7923e-04 | norm 0.2736 | dt 338.69ms | 1547996.36 tokens/sec
Step 6473 | loss: 3.269591 | lr:4.7919e-04 | norm 0.2939 | dt 338.56ms | 1548600.29 tokens/sec
Step 6474 | loss: 3.295033 | lr:4.7915e-04 | norm 0.2784 | dt 339.64ms | 1543662.77 tokens/sec
Step 6475 | loss: 3.344492 | lr:4.7911e-04 | norm 0.2937 | dt 338.46ms | 1549030.09 tokens/sec
Step 6476 | loss: 3.388750 | lr:4.7908e-04 | norm 0.3625 | dt 337.30ms | 1554374.44 tokens/sec
Step 6477 | loss: 3.284708 | lr:4.7904e-04 | norm 0.3329 | dt 337.84ms | 1551877.80 tokens/sec
Step 6478 | loss: 3.285707 | lr:4.7900e-04 | norm 0.2991 | dt 338.99ms | 1546631.07 tokens/sec
Step 6479 | loss: 3.328577 | lr:4.7896e-04 | norm 0.2898 | dt 337.97ms | 1551274.59 tokens/sec
Step 6480 | loss: 3.311539 | lr:4.7892e-04 | norm 0.3267 | dt 337.66ms | 1552716.06 tokens/sec
Step 6481 | loss: 3.311854 | lr:4.7888e-04 | norm 0.2897 | dt 338.88ms | 1547108.76 tokens/sec
Step 6482 | loss: 3.299377 | lr:4.7884e-04 | norm 0.3142 | dt 337.99ms | 1551177.20 tokens/sec
Step 6483 | loss: 3.292484 | lr:4.7881e-04 | norm 0.3091 | dt 338.09ms | 1550721.06 tokens/sec
Step 6484 | loss: 3.296386 | lr:4.7877e-04 | norm 0.2684 | dt 338.15ms | 1550461.93 tokens/sec
Step 6485 | loss: 3.323399 | lr:4.7873e-04 | norm 0.2854 | dt 338.18ms | 1550306.71 tokens/sec
Step 6486 | loss: 3.272846 | lr:4.7869e-04 | norm 0.2741 | dt 337.80ms | 1552081.53 tokens/sec
Step 6487 | loss: 3.251589 | lr:4.7865e-04 | norm 0.2773 | dt 337.98ms | 1551258.17 tokens/sec
Step 6488 | loss: 3.253829 | lr:4.7861e-04 | norm 0.2767 | dt 338.20ms | 1550250.97 tokens/sec
Step 6489 | loss: 3.267583 | lr:4.7857e-04 | norm 0.2864 | dt 337.98ms | 1551245.04 tokens/sec
Step 6490 | loss: 3.203064 | lr:4.7854e-04 | norm 0.2841 | dt 338.60ms | 1548410.56 tokens/sec
Step 6491 | loss: 3.256985 | lr:4.7850e-04 | norm 0.2643 | dt 337.57ms | 1553145.96 tokens/sec
Step 6492 | loss: 3.215488 | lr:4.7846e-04 | norm 0.2878 | dt 339.34ms | 1545034.76 tokens/sec
Step 6493 | loss: 3.341279 | lr:4.7842e-04 | norm 0.2990 | dt 339.87ms | 1542615.63 tokens/sec
Step 6494 | loss: 3.239801 | lr:4.7838e-04 | norm 0.3239 | dt 338.52ms | 1548772.62 tokens/sec
Step 6495 | loss: 3.285967 | lr:4.7834e-04 | norm 0.2835 | dt 338.61ms | 1548345.14 tokens/sec
Step 6496 | loss: 3.255188 | lr:4.7830e-04 | norm 0.2996 | dt 339.10ms | 1546125.42 tokens/sec
Step 6497 | loss: 3.235265 | lr:4.7827e-04 | norm 0.3062 | dt 338.20ms | 1550219.28 tokens/sec
Step 6498 | loss: 3.310608 | lr:4.7823e-04 | norm 0.2867 | dt 339.69ms | 1543439.58 tokens/sec
Step 6499 | loss: 3.321050 | lr:4.7819e-04 | norm 0.2948 | dt 338.18ms | 1550303.43 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 6500: 3.2958
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2811/10042=0.2799


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but the problem is I'm not a designer, I don't even need to write code. But I don't think
rank 5 sample 1 >Hello, I'm a language model, how should I start when my students are doing learning? How about introducing the more complicated concept in my middle school language classes

rank 5 sample 2 >Hello, I'm a language model, so I had a basic understanding of all the concepts and practices of Spanish, in the US, and Spanish. I don

ddp_rank 4: ####### Printing generated samples ####### 

rank 5 sample 3 >Hello, I'm a language model, learning how to speak, to write, and to understand a language, but how am i learning from the beginning of the


rank 4 sample 0 >Hello, I'm a language model, so I am not sure you can read more about it. So, please see my first step in creating my first blog
rank 4 sample 1 >Hello, I'm a language model, working as a grammar, a grammatical grammar, a structural grammar, with an eye to the grammar in general.

rank 4 sample 2 >Hello, I'm a language model, I try to translate my word into other languages, but I am completely lost. Let's call them L2 because I
rank 4 sample 3 >Hello, I'm a language model, and you mean my brother, and one I am looking at (that's the right way to me) is he is




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, only in my last few posts, because it is an example of a book I want to read. I was thinking,
rank 2 sample 1 >Hello, I'm a language model, but I work at a very basic level without any explicit or implied knowledge. I'm not really sure how I get from
rank 2 sample 2 >Hello, I'm a language model, so I can't get a job or ask for help. My problem is with it.
And my problem is with
rank 2 sample 3 >Hello, I'm a language model, but if I'm going to be doing something very special, why not? Maybe we should let my students speak in my




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, or language model. So with the first language, a person who learns the language in the first language, is not very
rank 6 sample 1 >Hello, I'm a language model, which is what I need. The problem now is not that I'm speaking English. The language is a little bit different
rank 6 sample 2 >Hello, I'm a language model, but I'm a lot older and you'll see it in my second year. I'm a language model student I've
rank 6 sample 3 >Hello, I'm a language model, and my only comment in the section on “Language” describes the function. The difference is based on the problem




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm not even kidding. I feel like you're thinking about trying to think about grammar a bunch. I do
rank 7 sample 1 >Hello, I'm a language model, is better.
So I think so.
Why wouldn't you use it when you have to make a comment that
rank 7 sample 2 >Hello, I'm a language model, and I've taken what I wrote last night. How to use it...
- I'm a language model; you
rank 7 sample 3 >Hello, I'm a language model, even though I'm a teacher. Even though I'm a teacher I get to use some of the languages I already knew




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I think I should write more programs before coding. I don't know where to find data for my first program;
rank 0 sample 1 >Hello, I'm a language model, so i'm not a language model I'm just asking what I should be? I'll answer, no.
There
rank 0 sample 2 >Hello, I'm a language model, so I get it this way. The first is the one with this. The second is the one with this, and
rank 0 sample 3 >Hello, I'm a language model, but when I start learning I don't get one. I'm really tired and bored. If you use it as another




ddp_rank 1: ####### Printing generated samples ####### 



ddp_rank 3: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and as I'm talking, I started thinking it might be difficult to understand anything. But that wasn't the case.
rank 3 sample 0 >Hello, I'm a language model, and I'm a model-moodle.
In my mind, something just like my brother's father's son could
rank 1 sample 1 >Hello, I'm a language model, not an interpreter. I'm not trying to understand all syntax, but I'm making them talk on the same level.
rank 1 sample 2 >Hello, I'm a language model, but because you know how to use it, I'm afraid not all people should learn it. If you want, I
rank 3 sample 1 >Hello, I'm a language model, and a teacher. I will give you some more of your own.
I'm a language model, and if there
rank 3 sample 2 >Hello, I'm a language model, and it works, pretty much anything. A new language is only a new type of language (with no special properties likerank 1 sample 3 >Hello, I'm a language model, and I'm looking at grammar. This thing is different is, we'll also have many subroutines like,



rank 3 sample 3 >Hello, I'm a language model, and do not understand the use of a language, so you should know what you're looking at. English, French,


Step 6500 | loss: 3.259426 | lr:4.7815e-04 | norm 0.3159 | dt 18654.39ms | 28105.35 tokens/sec
Step 6501 | loss: 3.343853 | lr:4.7811e-04 | norm 0.3165 | dt 333.43ms | 1572388.64 tokens/sec
Step 6502 | loss: 3.311743 | lr:4.7807e-04 | norm 0.2657 | dt 335.49ms | 1562734.16 tokens/sec
Step 6503 | loss: 3.359759 | lr:4.7803e-04 | norm 0.3135 | dt 337.74ms | 1552362.02 tokens/sec
Step 6504 | loss: 3.274049 | lr:4.7800e-04 | norm 0.2909 | dt 335.94ms | 1560655.75 tokens/sec
Step 6505 | loss: 3.305227 | lr:4.7796e-04 | norm 0.2893 | dt 335.91ms | 1560821.91 tokens/sec
Step 6506 | loss: 3.305852 | lr:4.7792e-04 | norm 0.2770 | dt 335.78ms | 1561423.70 tokens/sec
Step 6507 | loss: 3.310531 | lr:4.7788e-04 | norm 0.2740 | dt 335.55ms | 1562496.54 tokens/sec
Step 6508 | loss: 3.253443 | lr:4.7784e-04 | norm 0.2810 | dt 336.21ms | 1559416.22 tokens/sec
Step 6509 | loss: 3.326170 | lr:4.7780e-04 | norm 0.2677 | dt 335.93ms | 1560686.76 tokens/sec
Step 6510 | loss: 3.290620 | lr:4.7776e-04 | norm 0.2808 | dt 336.60ms | 1557606.95 tokens/sec
Step 6511 | loss: 3.358508 | lr:4.7772e-04 | norm 0.2798 | dt 335.56ms | 1562423.27 tokens/sec
Step 6512 | loss: 3.229852 | lr:4.7769e-04 | norm 0.2837 | dt 335.50ms | 1562727.50 tokens/sec
Step 6513 | loss: 3.327899 | lr:4.7765e-04 | norm 0.2750 | dt 336.50ms | 1558060.53 tokens/sec
Step 6514 | loss: 3.310889 | lr:4.7761e-04 | norm 0.2763 | dt 336.17ms | 1559589.86 tokens/sec
Step 6515 | loss: 3.294528 | lr:4.7757e-04 | norm 0.2872 | dt 335.80ms | 1561302.86 tokens/sec
Step 6516 | loss: 3.302628 | lr:4.7753e-04 | norm 0.3200 | dt 336.71ms | 1557098.50 tokens/sec
Step 6517 | loss: 3.344646 | lr:4.7749e-04 | norm 0.2731 | dt 336.60ms | 1557589.29 tokens/sec
Step 6518 | loss: 3.316126 | lr:4.7745e-04 | norm 0.2780 | dt 336.12ms | 1559816.64 tokens/sec
Step 6519 | loss: 3.259023 | lr:4.7742e-04 | norm 0.2720 | dt 336.90ms | 1556224.66 tokens/sec
Step 6520 | loss: 3.318570 | lr:4.7738e-04 | norm 0.2809 | dt 336.64ms | 1557400.66 tokens/sec
Step 6521 | loss: 3.258313 | lr:4.7734e-04 | norm 0.2670 | dt 336.78ms | 1556753.47 tokens/sec
Step 6522 | loss: 3.246798 | lr:4.7730e-04 | norm 0.2628 | dt 339.39ms | 1544793.81 tokens/sec
Step 6523 | loss: 3.247503 | lr:4.7726e-04 | norm 0.2531 | dt 337.22ms | 1554729.40 tokens/sec
Step 6524 | loss: 3.283426 | lr:4.7722e-04 | norm 0.2634 | dt 336.43ms | 1558380.73 tokens/sec
Step 6525 | loss: 3.291003 | lr:4.7718e-04 | norm 0.3299 | dt 336.88ms | 1556301.76 tokens/sec
Step 6526 | loss: 3.228705 | lr:4.7714e-04 | norm 0.2869 | dt 337.31ms | 1554307.42 tokens/sec
Step 6527 | loss: 3.257562 | lr:4.7711e-04 | norm 0.2731 | dt 336.59ms | 1557668.73 tokens/sec
Step 6528 | loss: 3.287179 | lr:4.7707e-04 | norm 0.3012 | dt 337.31ms | 1554339.28 tokens/sec
Step 6529 | loss: 3.280734 | lr:4.7703e-04 | norm 0.3346 | dt 337.35ms | 1554122.87 tokens/sec
Step 6530 | loss: 3.231401 | lr:4.7699e-04 | norm 0.2613 | dt 337.30ms | 1554371.14 tokens/sec
Step 6531 | loss: 3.296960 | lr:4.7695e-04 | norm 0.2994 | dt 336.73ms | 1557013.61 tokens/sec
Step 6532 | loss: 3.255304 | lr:4.7691e-04 | norm 0.2771 | dt 338.42ms | 1549205.79 tokens/sec
Step 6533 | loss: 3.279003 | lr:4.7687e-04 | norm 0.2907 | dt 337.41ms | 1553845.04 tokens/sec
Step 6534 | loss: 3.303719 | lr:4.7683e-04 | norm 0.2805 | dt 337.09ms | 1555344.10 tokens/sec
Step 6535 | loss: 3.298997 | lr:4.7680e-04 | norm 0.2948 | dt 338.95ms | 1546807.31 tokens/sec
Step 6536 | loss: 3.288792 | lr:4.7676e-04 | norm 0.2802 | dt 338.22ms | 1550123.12 tokens/sec
Step 6537 | loss: 3.317624 | lr:4.7672e-04 | norm 0.2633 | dt 338.18ms | 1550329.67 tokens/sec
Step 6538 | loss: 3.333950 | lr:4.7668e-04 | norm 0.2721 | dt 338.18ms | 1550343.87 tokens/sec
Step 6539 | loss: 3.268661 | lr:4.7664e-04 | norm 0.2685 | dt 338.82ms | 1547399.43 tokens/sec
Step 6540 | loss: 3.340894 | lr:4.7660e-04 | norm 0.2681 | dt 338.61ms | 1548361.50 tokens/sec
Step 6541 | loss: 3.287865 | lr:4.7656e-04 | norm 0.2850 | dt 338.42ms | 1549234.17 tokens/sec
Step 6542 | loss: 3.348230 | lr:4.7652e-04 | norm 0.2933 | dt 338.96ms | 1546745.30 tokens/sec
Step 6543 | loss: 3.285016 | lr:4.7649e-04 | norm 0.2953 | dt 339.10ms | 1546127.59 tokens/sec
Step 6544 | loss: 3.273952 | lr:4.7645e-04 | norm 0.3152 | dt 339.21ms | 1545596.19 tokens/sec
Step 6545 | loss: 3.342572 | lr:4.7641e-04 | norm 0.2946 | dt 339.01ms | 1546509.25 tokens/sec
Step 6546 | loss: 3.258652 | lr:4.7637e-04 | norm 0.2818 | dt 338.93ms | 1546895.45 tokens/sec
Step 6547 | loss: 3.275415 | lr:4.7633e-04 | norm 0.2824 | dt 339.18ms | 1545740.69 tokens/sec
Step 6548 | loss: 3.371599 | lr:4.7629e-04 | norm 0.2825 | dt 337.52ms | 1553337.95 tokens/sec
Step 6549 | loss: 3.324928 | lr:4.7625e-04 | norm 0.3067 | dt 338.23ms | 1550080.50 tokens/sec
Step 6550 | loss: 3.318287 | lr:4.7621e-04 | norm 0.2888 | dt 338.57ms | 1548544.68 tokens/sec
Step 6551 | loss: 3.295861 | lr:4.7617e-04 | norm 0.2955 | dt 338.79ms | 1547541.00 tokens/sec
Step 6552 | loss: 3.319284 | lr:4.7614e-04 | norm 0.2516 | dt 338.63ms | 1548251.39 tokens/sec
Step 6553 | loss: 3.296152 | lr:4.7610e-04 | norm 0.2729 | dt 338.06ms | 1550873.07 tokens/sec
Step 6554 | loss: 3.293738 | lr:4.7606e-04 | norm 0.2666 | dt 338.60ms | 1548418.19 tokens/sec
Step 6555 | loss: 3.271504 | lr:4.7602e-04 | norm 0.2664 | dt 337.86ms | 1551772.67 tokens/sec
Step 6556 | loss: 3.267536 | lr:4.7598e-04 | norm 0.2657 | dt 337.87ms | 1551762.81 tokens/sec
Step 6557 | loss: 3.280707 | lr:4.7594e-04 | norm 0.2682 | dt 338.60ms | 1548383.30 tokens/sec
Step 6558 | loss: 3.280902 | lr:4.7590e-04 | norm 0.2708 | dt 337.96ms | 1551350.10 tokens/sec
Step 6559 | loss: 3.282711 | lr:4.7586e-04 | norm 0.2664 | dt 337.60ms | 1552996.78 tokens/sec
Step 6560 | loss: 3.226229 | lr:4.7582e-04 | norm 0.2787 | dt 337.94ms | 1551444.23 tokens/sec
Step 6561 | loss: 3.277417 | lr:4.7579e-04 | norm 0.2874 | dt 338.29ms | 1549829.24 tokens/sec
Step 6562 | loss: 3.230931 | lr:4.7575e-04 | norm 0.2831 | dt 337.93ms | 1551486.92 tokens/sec
Step 6563 | loss: 3.266516 | lr:4.7571e-04 | norm 0.3005 | dt 337.25ms | 1554584.32 tokens/sec
Step 6564 | loss: 3.265632 | lr:4.7567e-04 | norm 0.2890 | dt 337.68ms | 1552617.40 tokens/sec
Step 6565 | loss: 3.237773 | lr:4.7563e-04 | norm 0.2665 | dt 338.21ms | 1550183.22 tokens/sec
Step 6566 | loss: 3.253878 | lr:4.7559e-04 | norm 0.2797 | dt 338.67ms | 1548099.89 tokens/sec
Step 6567 | loss: 3.243058 | lr:4.7555e-04 | norm 0.2914 | dt 338.25ms | 1549987.63 tokens/sec
Step 6568 | loss: 3.286258 | lr:4.7551e-04 | norm 0.3387 | dt 339.02ms | 1546459.22 tokens/sec
Step 6569 | loss: 3.250258 | lr:4.7547e-04 | norm 0.2727 | dt 338.22ms | 1550120.93 tokens/sec
Step 6570 | loss: 3.254625 | lr:4.7544e-04 | norm 0.3107 | dt 338.04ms | 1550954.02 tokens/sec
Step 6571 | loss: 3.264353 | lr:4.7540e-04 | norm 0.3381 | dt 338.63ms | 1548243.76 tokens/sec
Step 6572 | loss: 3.296643 | lr:4.7536e-04 | norm 0.2724 | dt 338.23ms | 1550082.69 tokens/sec
Step 6573 | loss: 3.273321 | lr:4.7532e-04 | norm 0.3053 | dt 338.02ms | 1551071.07 tokens/sec
Step 6574 | loss: 3.316187 | lr:4.7528e-04 | norm 0.2953 | dt 337.53ms | 1553320.39 tokens/sec
Step 6575 | loss: 3.278111 | lr:4.7524e-04 | norm 0.2877 | dt 338.67ms | 1548077.00 tokens/sec
Step 6576 | loss: 3.269236 | lr:4.7520e-04 | norm 0.3096 | dt 338.87ms | 1547155.56 tokens/sec
Step 6577 | loss: 3.305305 | lr:4.7516e-04 | norm 0.2969 | dt 338.45ms | 1549106.48 tokens/sec
Step 6578 | loss: 3.290987 | lr:4.7512e-04 | norm 0.2907 | dt 338.88ms | 1547109.84 tokens/sec
Step 6579 | loss: 3.313977 | lr:4.7508e-04 | norm 0.2941 | dt 339.14ms | 1545952.59 tokens/sec
Step 6580 | loss: 3.300842 | lr:4.7505e-04 | norm 0.2691 | dt 339.02ms | 1546474.45 tokens/sec
Step 6581 | loss: 3.277128 | lr:4.7501e-04 | norm 0.2923 | dt 338.88ms | 1547100.05 tokens/sec
Step 6582 | loss: 3.313143 | lr:4.7497e-04 | norm 0.2686 | dt 338.56ms | 1548581.75 tokens/sec
Step 6583 | loss: 3.305744 | lr:4.7493e-04 | norm 0.2690 | dt 338.59ms | 1548459.62 tokens/sec
Step 6584 | loss: 3.333819 | lr:4.7489e-04 | norm 0.2754 | dt 338.73ms | 1547822.03 tokens/sec
Step 6585 | loss: 3.293423 | lr:4.7485e-04 | norm 0.2976 | dt 339.36ms | 1544927.30 tokens/sec
Step 6586 | loss: 3.272083 | lr:4.7481e-04 | norm 0.2990 | dt 338.45ms | 1549070.47 tokens/sec
Step 6587 | loss: 3.267769 | lr:4.7477e-04 | norm 0.2641 | dt 338.59ms | 1548423.64 tokens/sec
Step 6588 | loss: 3.317193 | lr:4.7473e-04 | norm 0.2421 | dt 339.93ms | 1542327.83 tokens/sec
Step 6589 | loss: 3.253174 | lr:4.7470e-04 | norm 0.2660 | dt 338.43ms | 1549176.32 tokens/sec
Step 6590 | loss: 3.304935 | lr:4.7466e-04 | norm 0.2578 | dt 338.55ms | 1548615.56 tokens/sec
Step 6591 | loss: 3.262482 | lr:4.7462e-04 | norm 0.2751 | dt 338.82ms | 1547404.87 tokens/sec
Step 6592 | loss: 3.217974 | lr:4.7458e-04 | norm 0.2402 | dt 343.60ms | 1525865.02 tokens/sec
Step 6593 | loss: 3.233054 | lr:4.7454e-04 | norm 0.2479 | dt 338.80ms | 1547477.83 tokens/sec
Step 6594 | loss: 3.323132 | lr:4.7450e-04 | norm 0.2704 | dt 339.20ms | 1545678.76 tokens/sec
Step 6595 | loss: 3.224487 | lr:4.7446e-04 | norm 0.2757 | dt 339.28ms | 1545274.70 tokens/sec
Step 6596 | loss: 3.266376 | lr:4.7442e-04 | norm 0.3070 | dt 338.73ms | 1547817.67 tokens/sec
Step 6597 | loss: 3.248535 | lr:4.7438e-04 | norm 0.2981 | dt 338.77ms | 1547606.34 tokens/sec
Step 6598 | loss: 3.228325 | lr:4.7434e-04 | norm 0.2635 | dt 339.01ms | 1546536.44 tokens/sec
Step 6599 | loss: 3.225772 | lr:4.7430e-04 | norm 0.2687 | dt 338.69ms | 1548008.35 tokens/sec
Step 6600 | loss: 3.236676 | lr:4.7427e-04 | norm 0.3106 | dt 339.17ms | 1545805.88 tokens/sec
Step 6601 | loss: 3.215980 | lr:4.7423e-04 | norm 0.2860 | dt 338.61ms | 1548335.33 tokens/sec
Step 6602 | loss: 3.310593 | lr:4.7419e-04 | norm 0.3042 | dt 339.31ms | 1545144.41 tokens/sec
Step 6603 | loss: 3.288821 | lr:4.7415e-04 | norm 0.3134 | dt 339.12ms | 1546009.11 tokens/sec
Step 6604 | loss: 3.295902 | lr:4.7411e-04 | norm 0.2790 | dt 339.06ms | 1546317.85 tokens/sec
Step 6605 | loss: 3.331848 | lr:4.7407e-04 | norm 0.3032 | dt 337.78ms | 1552170.27 tokens/sec
Step 6606 | loss: 3.344454 | lr:4.7403e-04 | norm 0.2918 | dt 338.95ms | 1546806.22 tokens/sec
Step 6607 | loss: 3.299727 | lr:4.7399e-04 | norm 0.2610 | dt 338.20ms | 1550218.19 tokens/sec
Step 6608 | loss: 3.325037 | lr:4.7395e-04 | norm 0.3098 | dt 337.96ms | 1551310.70 tokens/sec
Step 6609 | loss: 3.314391 | lr:4.7391e-04 | norm 0.3029 | dt 337.63ms | 1552847.64 tokens/sec
Step 6610 | loss: 3.293258 | lr:4.7387e-04 | norm 0.2788 | dt 337.71ms | 1552477.09 tokens/sec
Step 6611 | loss: 3.328185 | lr:4.7384e-04 | norm 0.2940 | dt 338.05ms | 1550908.08 tokens/sec
Step 6612 | loss: 3.317457 | lr:4.7380e-04 | norm 0.2920 | dt 338.37ms | 1549452.49 tokens/sec
Step 6613 | loss: 3.320828 | lr:4.7376e-04 | norm 0.2467 | dt 338.35ms | 1549535.47 tokens/sec
Step 6614 | loss: 3.313310 | lr:4.7372e-04 | norm 0.2815 | dt 1029.49ms | 509267.64 tokens/sec
Step 6615 | loss: 3.327460 | lr:4.7368e-04 | norm 0.3191 | dt 335.28ms | 1563720.96 tokens/sec
Step 6616 | loss: 3.316081 | lr:4.7364e-04 | norm 0.3227 | dt 336.25ms | 1559230.46 tokens/sec
Step 6617 | loss: 3.296764 | lr:4.7360e-04 | norm 0.2803 | dt 338.53ms | 1548725.72 tokens/sec
Step 6618 | loss: 3.358364 | lr:4.7356e-04 | norm 0.2971 | dt 337.84ms | 1551874.51 tokens/sec
Step 6619 | loss: 3.280099 | lr:4.7352e-04 | norm 0.3201 | dt 337.17ms | 1554980.06 tokens/sec
Step 6620 | loss: 3.306480 | lr:4.7348e-04 | norm 0.3255 | dt 337.98ms | 1551236.29 tokens/sec
Step 6621 | loss: 3.283795 | lr:4.7344e-04 | norm 0.3016 | dt 339.10ms | 1546116.72 tokens/sec
Step 6622 | loss: 3.279011 | lr:4.7341e-04 | norm 0.3221 | dt 337.91ms | 1551551.50 tokens/sec
Step 6623 | loss: 3.254688 | lr:4.7337e-04 | norm 0.3011 | dt 337.93ms | 1551477.07 tokens/sec
Step 6624 | loss: 3.279361 | lr:4.7333e-04 | norm 0.2865 | dt 340.22ms | 1541039.47 tokens/sec
Step 6625 | loss: 3.269629 | lr:4.7329e-04 | norm 0.2916 | dt 339.09ms | 1546140.63 tokens/sec
Step 6626 | loss: 3.260296 | lr:4.7325e-04 | norm 0.2785 | dt 339.34ms | 1545013.05 tokens/sec
Step 6627 | loss: 3.254787 | lr:4.7321e-04 | norm 0.2583 | dt 339.00ms | 1546559.28 tokens/sec
Step 6628 | loss: 3.290781 | lr:4.7317e-04 | norm 0.2843 | dt 338.52ms | 1548783.53 tokens/sec
Step 6629 | loss: 3.227463 | lr:4.7313e-04 | norm 0.2858 | dt 337.14ms | 1555094.42 tokens/sec
Step 6630 | loss: 3.249190 | lr:4.7309e-04 | norm 0.2767 | dt 339.04ms | 1546405.93 tokens/sec
Step 6631 | loss: 3.267028 | lr:4.7305e-04 | norm 0.2636 | dt 338.63ms | 1548249.21 tokens/sec
Step 6632 | loss: 3.289083 | lr:4.7301e-04 | norm 0.2472 | dt 337.64ms | 1552811.45 tokens/sec
Step 6633 | loss: 3.315486 | lr:4.7297e-04 | norm 0.2702 | dt 338.42ms | 1549219.98 tokens/sec
Step 6634 | loss: 3.351018 | lr:4.7294e-04 | norm 0.2620 | dt 339.19ms | 1545702.66 tokens/sec
Step 6635 | loss: 3.291343 | lr:4.7290e-04 | norm 0.2972 | dt 338.18ms | 1550342.78 tokens/sec
Step 6636 | loss: 3.302331 | lr:4.7286e-04 | norm 0.2767 | dt 338.83ms | 1547368.94 tokens/sec
Step 6637 | loss: 3.279600 | lr:4.7282e-04 | norm 0.2793 | dt 339.01ms | 1546529.91 tokens/sec
Step 6638 | loss: 3.326656 | lr:4.7278e-04 | norm 0.3166 | dt 337.99ms | 1551192.52 tokens/sec
Step 6639 | loss: 3.293858 | lr:4.7274e-04 | norm 0.3005 | dt 337.73ms | 1552403.66 tokens/sec
Step 6640 | loss: 3.353389 | lr:4.7270e-04 | norm 0.3661 | dt 339.96ms | 1542204.52 tokens/sec
Step 6641 | loss: 3.256782 | lr:4.7266e-04 | norm 0.2983 | dt 338.09ms | 1550754.96 tokens/sec
Step 6642 | loss: 3.334723 | lr:4.7262e-04 | norm 0.3394 | dt 338.23ms | 1550104.54 tokens/sec
Step 6643 | loss: 3.299071 | lr:4.7258e-04 | norm 0.4222 | dt 340.15ms | 1541325.71 tokens/sec
Step 6644 | loss: 3.400209 | lr:4.7254e-04 | norm 0.3164 | dt 338.84ms | 1547289.46 tokens/sec
Step 6645 | loss: 3.292925 | lr:4.7250e-04 | norm 0.3181 | dt 338.22ms | 1550152.62 tokens/sec
Step 6646 | loss: 3.302825 | lr:4.7246e-04 | norm 0.2956 | dt 338.26ms | 1549937.38 tokens/sec
Step 6647 | loss: 3.325580 | lr:4.7243e-04 | norm 0.3071 | dt 338.37ms | 1549453.58 tokens/sec
Step 6648 | loss: 3.376174 | lr:4.7239e-04 | norm 0.2859 | dt 338.16ms | 1550398.53 tokens/sec
Step 6649 | loss: 3.290827 | lr:4.7235e-04 | norm 0.3143 | dt 1004.72ms | 521827.11 tokens/sec
Step 6650 | loss: 3.256132 | lr:4.7231e-04 | norm 0.2802 | dt 338.19ms | 1550296.88 tokens/sec
Step 6651 | loss: 3.283659 | lr:4.7227e-04 | norm 0.2913 | dt 338.22ms | 1550128.58 tokens/sec
Step 6652 | loss: 3.245527 | lr:4.7223e-04 | norm 0.2757 | dt 338.11ms | 1550658.73 tokens/sec
Step 6653 | loss: 3.313026 | lr:4.7219e-04 | norm 0.2658 | dt 337.86ms | 1551808.81 tokens/sec
Step 6654 | loss: 3.317275 | lr:4.7215e-04 | norm 0.2828 | dt 338.17ms | 1550346.06 tokens/sec
Step 6655 | loss: 3.332869 | lr:4.7211e-04 | norm 0.2925 | dt 337.57ms | 1553113.05 tokens/sec
Step 6656 | loss: 3.233027 | lr:4.7207e-04 | norm 0.2865 | dt 337.96ms | 1551317.27 tokens/sec
Step 6657 | loss: 3.281316 | lr:4.7203e-04 | norm 0.3037 | dt 338.69ms | 1547996.36 tokens/sec
Step 6658 | loss: 3.294996 | lr:4.7199e-04 | norm 0.3012 | dt 337.85ms | 1551824.14 tokens/sec
Step 6659 | loss: 3.233171 | lr:4.7195e-04 | norm 0.3187 | dt 337.97ms | 1551277.87 tokens/sec
Step 6660 | loss: 3.229496 | lr:4.7191e-04 | norm 0.3027 | dt 338.48ms | 1548960.26 tokens/sec
Step 6661 | loss: 3.253705 | lr:4.7188e-04 | norm 0.2685 | dt 338.22ms | 1550119.84 tokens/sec
Step 6662 | loss: 3.306400 | lr:4.7184e-04 | norm 0.2693 | dt 337.23ms | 1554705.22 tokens/sec
Step 6663 | loss: 3.206487 | lr:4.7180e-04 | norm 0.2584 | dt 338.20ms | 1550228.02 tokens/sec
Step 6664 | loss: 3.235001 | lr:4.7176e-04 | norm 0.2730 | dt 337.80ms | 1552078.24 tokens/sec
Step 6665 | loss: 3.257711 | lr:4.7172e-04 | norm 0.2574 | dt 338.08ms | 1550765.89 tokens/sec
Step 6666 | loss: 3.252383 | lr:4.7168e-04 | norm 0.2718 | dt 338.16ms | 1550391.97 tokens/sec
Step 6667 | loss: 3.288426 | lr:4.7164e-04 | norm 0.2659 | dt 337.89ms | 1551665.36 tokens/sec
Step 6668 | loss: 3.280228 | lr:4.7160e-04 | norm 0.2599 | dt 337.59ms | 1553022.01 tokens/sec
Step 6669 | loss: 3.250934 | lr:4.7156e-04 | norm 0.2734 | dt 337.33ms | 1554232.72 tokens/sec
Step 6670 | loss: 3.290080 | lr:4.7152e-04 | norm 0.2949 | dt 338.43ms | 1549182.87 tokens/sec
Step 6671 | loss: 3.321005 | lr:4.7148e-04 | norm 0.2555 | dt 340.10ms | 1541580.71 tokens/sec
Step 6672 | loss: 3.256000 | lr:4.7144e-04 | norm 0.3088 | dt 338.71ms | 1547878.68 tokens/sec
Step 6673 | loss: 3.393641 | lr:4.7140e-04 | norm 0.2859 | dt 338.21ms | 1550164.64 tokens/sec
Step 6674 | loss: 3.333792 | lr:4.7136e-04 | norm 0.2979 | dt 337.92ms | 1551494.58 tokens/sec
Step 6675 | loss: 3.329888 | lr:4.7132e-04 | norm 0.2907 | dt 337.69ms | 1552572.45 tokens/sec
Step 6676 | loss: 3.257109 | lr:4.7129e-04 | norm 0.3084 | dt 338.05ms | 1550906.98 tokens/sec
Step 6677 | loss: 3.345226 | lr:4.7125e-04 | norm 0.3508 | dt 337.45ms | 1553658.41 tokens/sec
Step 6678 | loss: 3.289375 | lr:4.7121e-04 | norm 0.3059 | dt 338.47ms | 1549005.00 tokens/sec
Step 6679 | loss: 3.337846 | lr:4.7117e-04 | norm 0.2946 | dt 339.24ms | 1545473.45 tokens/sec
Step 6680 | loss: 3.315442 | lr:4.7113e-04 | norm 0.2985 | dt 338.69ms | 1547988.73 tokens/sec
Step 6681 | loss: 3.315771 | lr:4.7109e-04 | norm 0.2690 | dt 339.53ms | 1544154.89 tokens/sec
Step 6682 | loss: 3.291362 | lr:4.7105e-04 | norm 0.3024 | dt 338.77ms | 1547630.30 tokens/sec
Step 6683 | loss: 3.308639 | lr:4.7101e-04 | norm 0.2806 | dt 338.50ms | 1548874.07 tokens/sec
Step 6684 | loss: 3.296082 | lr:4.7097e-04 | norm 0.3089 | dt 339.49ms | 1544352.26 tokens/sec
Step 6685 | loss: 3.282983 | lr:4.7093e-04 | norm 0.2724 | dt 338.72ms | 1547850.35 tokens/sec
Step 6686 | loss: 3.315385 | lr:4.7089e-04 | norm 0.2733 | dt 339.16ms | 1545861.30 tokens/sec
Step 6687 | loss: 3.231483 | lr:4.7085e-04 | norm 0.2838 | dt 339.12ms | 1546002.59 tokens/sec
Step 6688 | loss: 3.288312 | lr:4.7081e-04 | norm 0.2629 | dt 338.49ms | 1548888.25 tokens/sec
Step 6689 | loss: 3.326455 | lr:4.7077e-04 | norm 0.2933 | dt 338.71ms | 1547910.28 tokens/sec
Step 6690 | loss: 3.303613 | lr:4.7073e-04 | norm 0.2931 | dt 339.00ms | 1546556.02 tokens/sec
Step 6691 | loss: 3.258636 | lr:4.7069e-04 | norm 0.2645 | dt 339.26ms | 1545405.02 tokens/sec
Step 6692 | loss: 3.203688 | lr:4.7065e-04 | norm 0.3036 | dt 338.72ms | 1547850.35 tokens/sec
Step 6693 | loss: 3.236719 | lr:4.7062e-04 | norm 0.2858 | dt 338.07ms | 1550807.45 tokens/sec
Step 6694 | loss: 3.248869 | lr:4.7058e-04 | norm 0.2936 | dt 337.54ms | 1553284.19 tokens/sec
Step 6695 | loss: 3.258122 | lr:4.7054e-04 | norm 0.3061 | dt 337.11ms | 1555244.00 tokens/sec
Step 6696 | loss: 3.207675 | lr:4.7050e-04 | norm 0.2855 | dt 337.71ms | 1552460.65 tokens/sec
Step 6697 | loss: 3.255608 | lr:4.7046e-04 | norm 0.2898 | dt 337.53ms | 1553325.88 tokens/sec
Step 6698 | loss: 3.244372 | lr:4.7042e-04 | norm 0.3071 | dt 337.58ms | 1553073.56 tokens/sec
Step 6699 | loss: 3.276298 | lr:4.7038e-04 | norm 0.2916 | dt 338.45ms | 1549081.38 tokens/sec
Step 6700 | loss: 3.288251 | lr:4.7034e-04 | norm 0.2732 | dt 337.26ms | 1554543.66 tokens/sec
Step 6701 | loss: 3.280797 | lr:4.7030e-04 | norm 0.2979 | dt 337.34ms | 1554193.17 tokens/sec
Step 6702 | loss: 3.297704 | lr:4.7026e-04 | norm 0.2655 | dt 338.01ms | 1551111.55 tokens/sec
Step 6703 | loss: 3.293188 | lr:4.7022e-04 | norm 0.2672 | dt 337.82ms | 1551961.04 tokens/sec
Step 6704 | loss: 3.273480 | lr:4.7018e-04 | norm 0.2662 | dt 337.48ms | 1553528.89 tokens/sec
Step 6705 | loss: 3.250876 | lr:4.7014e-04 | norm 0.3086 | dt 338.10ms | 1550675.13 tokens/sec
Step 6706 | loss: 3.260874 | lr:4.7010e-04 | norm 0.3147 | dt 339.36ms | 1544941.41 tokens/sec
Step 6707 | loss: 3.271430 | lr:4.7006e-04 | norm 0.2651 | dt 339.13ms | 1545976.50 tokens/sec
Step 6708 | loss: 3.261775 | lr:4.7002e-04 | norm 0.2936 | dt 338.73ms | 1547827.47 tokens/sec
Step 6709 | loss: 3.406696 | lr:4.6998e-04 | norm 0.3076 | dt 339.39ms | 1544780.79 tokens/sec
Step 6710 | loss: 3.326321 | lr:4.6994e-04 | norm 0.3465 | dt 338.06ms | 1550871.98 tokens/sec
Step 6711 | loss: 3.309659 | lr:4.6990e-04 | norm 0.3298 | dt 338.78ms | 1547584.56 tokens/sec
Step 6712 | loss: 3.326665 | lr:4.6987e-04 | norm 0.3064 | dt 340.15ms | 1541321.38 tokens/sec
Step 6713 | loss: 3.251871 | lr:4.6983e-04 | norm 0.2951 | dt 338.86ms | 1547232.85 tokens/sec
Step 6714 | loss: 3.337578 | lr:4.6979e-04 | norm 0.3098 | dt 339.19ms | 1545726.56 tokens/sec
Step 6715 | loss: 3.363202 | lr:4.6975e-04 | norm 0.3466 | dt 338.89ms | 1547079.37 tokens/sec
Step 6716 | loss: 3.299313 | lr:4.6971e-04 | norm 0.2832 | dt 338.71ms | 1547908.10 tokens/sec
Step 6717 | loss: 3.305178 | lr:4.6967e-04 | norm 0.2937 | dt 338.73ms | 1547810.04 tokens/sec
Step 6718 | loss: 3.315132 | lr:4.6963e-04 | norm 0.2875 | dt 339.00ms | 1546594.09 tokens/sec
Step 6719 | loss: 3.304855 | lr:4.6959e-04 | norm 0.2946 | dt 338.38ms | 1549407.73 tokens/sec
Step 6720 | loss: 3.276343 | lr:4.6955e-04 | norm 0.2940 | dt 338.63ms | 1548259.02 tokens/sec
Step 6721 | loss: 3.301407 | lr:4.6951e-04 | norm 0.2851 | dt 338.30ms | 1549773.53 tokens/sec
Step 6722 | loss: 3.357429 | lr:4.6947e-04 | norm 0.3193 | dt 339.34ms | 1545041.27 tokens/sec
Step 6723 | loss: 3.344473 | lr:4.6943e-04 | norm 0.2692 | dt 338.52ms | 1548774.80 tokens/sec
Step 6724 | loss: 3.282579 | lr:4.6939e-04 | norm 0.2834 | dt 339.34ms | 1545010.88 tokens/sec
Step 6725 | loss: 3.247256 | lr:4.6935e-04 | norm 0.2721 | dt 338.22ms | 1550118.75 tokens/sec
Step 6726 | loss: 3.242640 | lr:4.6931e-04 | norm 0.2719 | dt 338.47ms | 1549002.81 tokens/sec
Step 6727 | loss: 3.265587 | lr:4.6927e-04 | norm 0.2798 | dt 339.13ms | 1545967.81 tokens/sec
Step 6728 | loss: 3.258747 | lr:4.6923e-04 | norm 0.2414 | dt 338.25ms | 1550010.58 tokens/sec
Step 6729 | loss: 3.290435 | lr:4.6919e-04 | norm 0.3105 | dt 338.63ms | 1548254.66 tokens/sec
Step 6730 | loss: 3.192941 | lr:4.6915e-04 | norm 0.3104 | dt 339.10ms | 1546099.33 tokens/sec
Step 6731 | loss: 3.233918 | lr:4.6911e-04 | norm 0.2939 | dt 338.04ms | 1550978.08 tokens/sec
Step 6732 | loss: 3.307182 | lr:4.6907e-04 | norm 0.2651 | dt 339.29ms | 1545233.44 tokens/sec
Step 6733 | loss: 3.209016 | lr:4.6903e-04 | norm 0.2805 | dt 338.86ms | 1547216.52 tokens/sec
Step 6734 | loss: 3.284029 | lr:4.6899e-04 | norm 0.2639 | dt 338.15ms | 1550479.42 tokens/sec
Step 6735 | loss: 3.254458 | lr:4.6896e-04 | norm 0.2700 | dt 338.08ms | 1550768.08 tokens/sec
Step 6736 | loss: 3.290148 | lr:4.6892e-04 | norm 0.2861 | dt 338.36ms | 1549497.25 tokens/sec
Step 6737 | loss: 3.319175 | lr:4.6888e-04 | norm 0.2989 | dt 338.32ms | 1549697.08 tokens/sec
Step 6738 | loss: 3.354227 | lr:4.6884e-04 | norm 0.2774 | dt 338.06ms | 1550888.39 tokens/sec
Step 6739 | loss: 3.295251 | lr:4.6880e-04 | norm 0.3127 | dt 337.94ms | 1551403.73 tokens/sec
Step 6740 | loss: 3.238122 | lr:4.6876e-04 | norm 0.2850 | dt 337.47ms | 1553576.09 tokens/sec
Step 6741 | loss: 3.272531 | lr:4.6872e-04 | norm 0.2894 | dt 338.13ms | 1550563.60 tokens/sec
Step 6742 | loss: 3.271885 | lr:4.6868e-04 | norm 0.2679 | dt 338.33ms | 1549649.03 tokens/sec
Step 6743 | loss: 3.299489 | lr:4.6864e-04 | norm 0.2768 | dt 338.41ms | 1549266.91 tokens/sec
Step 6744 | loss: 3.310739 | lr:4.6860e-04 | norm 0.2725 | dt 338.39ms | 1549360.78 tokens/sec
Step 6745 | loss: 3.302149 | lr:4.6856e-04 | norm 0.2725 | dt 339.03ms | 1546447.26 tokens/sec
Step 6746 | loss: 3.332206 | lr:4.6852e-04 | norm 0.2878 | dt 338.48ms | 1548963.53 tokens/sec
Step 6747 | loss: 3.258229 | lr:4.6848e-04 | norm 0.2554 | dt 337.01ms | 1555725.92 tokens/sec
Step 6748 | loss: 3.272380 | lr:4.6844e-04 | norm 0.2596 | dt 338.16ms | 1550407.27 tokens/sec
Step 6749 | loss: 3.324674 | lr:4.6840e-04 | norm 0.2912 | dt 338.35ms | 1549547.48 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 6750: 3.2844
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2844/10042=0.2832


ddp_rank 3: ####### Printing generated samples ####### 



ddp_rank 5: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not talking too much about things. But that's the conclusion ...
You say, to be able to
rank 3 sample 1 >Hello, I'm a language model, so this isn't a task I'd like to get across. I'm going to teach you something, and to teach
rank 5 sample 0 >Hello, I'm a language model, and my first language is English. I didn't like the idea of writing a book on the English language because I wanted
rank 3 sample 2 >Hello, I'm a language model, so it can be simplified. A language model would be a kind of a dictionary. A dictionary (a dictionary containing English
rank 5 sample 1 >Hello, I'm a language model, writing something about it but doing nothing for you
I'd love to see some ways that you can teach children how to
rank 3 sample 3 >Hello, I'm a language model, so in my last post I discussed how I could make sure that it works for something and write a message.
This


rank 5 sample 2 >Hello, I'm a language model, I've only got seven languages! You can just type in words you think are related to each other and you can have
rank 5 sample 3 >Hello, I'm a language model, here are a few things to know about the program
This is the first page you can read any text. I used




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so there are a lot of things one might encounter while living out in the United States: language (e.g.,
rank 1 sample 1 >Hello, I'm a language model, you're not as far off as I used to be but I just made an excuse now, right. I'm not
rank 1 sample 2 >Hello, I'm a language model, and once upon a time I was able to write a paper which would make a lot of sense in terms of writing a
rank 1 sample 3 >Hello, I'm a language model, so I'm using the HTML template I invented. I write HTML - XML that is quite difficult to read, but I




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, no doubt, for the past two years, and I'd been thinking of it all the time, but when I saw
rank 2 sample 1 >Hello, I'm a language model, and I’m not sure to feel that I’m wrong. But I don't mean that I think
rank 2 sample 2 >Hello, I'm a language model, I'm not sure why. You probably had a textbook where I'd recommend that to someone. What was my advice and
rank 2 sample 3 >Hello, I'm a language model, and that says,
I'm not exactly sure. But my model is wrong. The real math problem is, is




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to make a class my own, but I'm not going to have the feeling that you're really
rank 7 sample 1 >Hello, I'm a language model, all of us are talking to someone today. I live inside a lot of different homes. We live in a very urban
rank 7 sample 2 >Hello, I'm a language model, so I'll keep posting this information tomorrow and I'll send you this for the rest of the week.<|endoftext|>It�


ddp_rank 6: ####### Printing generated samples ####### 

rank 7 sample 3 >Hello, I'm a language model, language modeling program. In it, I'm building a model that I understand. It's a kind of language model where


rank 6 sample 0 >Hello, I'm a language model, there's a lot of differences in how I'm used to using each language. It's mostly the same way I see
rank 6 sample 1 >Hello, I'm a language model, so I want to create an animation for someone to make a movie. In my example, the animation was the actual movie
rank 6 sample 2 >Hello, I'm a language model, and this is the part that makes your life better.
The part of me that makes your life better is because you
rank 6 sample 3 >Hello, I'm a language model, so if you want to use Python in your programming language, you're going to want the languages to express each other visually




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, I'm learning how to speak English, my native English is English, there's a great app called Language Learning that helps
rank 4 sample 1 >Hello, I'm a language model, using Python. Since you don't know about it there are a lot of ways to describe the language as well. For
rank 4 sample 2 >Hello, I'm a language model, but then I really think, I know I'm a language model. I'm always on the left!
This is
rank 4 sample 3 >Hello, I'm a language model, so you type that and press the Insert and Save... which tells me I want to use this interface to create a file




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I think I'll start this series soon.
[Image above]: You can also say this in Spanish, as
rank 0 sample 1 >Hello, I'm a language model, and i'm trying to design my very own way towards getting my own data, so we are trying to create a simple
rank 0 sample 2 >Hello, I'm a language model, I'm curious. Any ideas?<|endoftext|>The "W" shape has three distinct points, which are shown in Figure 2
rank 0 sample 3 >Hello, I'm a language model, so to speak. I'm a designer and model programmer. I'm a model model for Java, so I want everyone


Step 6750 | loss: 3.342729 | lr:4.6836e-04 | norm 0.2655 | dt 14580.13ms | 35959.08 tokens/sec
Step 6751 | loss: 3.325776 | lr:4.6832e-04 | norm 0.2700 | dt 333.97ms | 1569847.30 tokens/sec
Step 6752 | loss: 3.317076 | lr:4.6828e-04 | norm 0.2780 | dt 337.15ms | 1555063.63 tokens/sec
Step 6753 | loss: 3.342595 | lr:4.6824e-04 | norm 0.2722 | dt 335.94ms | 1560649.10 tokens/sec
Step 6754 | loss: 3.309807 | lr:4.6820e-04 | norm 0.3066 | dt 335.52ms | 1562610.90 tokens/sec
Step 6755 | loss: 3.325197 | lr:4.6816e-04 | norm 0.2726 | dt 335.87ms | 1560990.32 tokens/sec
Step 6756 | loss: 3.285482 | lr:4.6812e-04 | norm 0.2977 | dt 336.55ms | 1557808.87 tokens/sec
Step 6757 | loss: 3.300824 | lr:4.6808e-04 | norm 0.2989 | dt 336.37ms | 1558652.45 tokens/sec
Step 6758 | loss: 3.257753 | lr:4.6804e-04 | norm 0.2794 | dt 336.52ms | 1557967.80 tokens/sec
Step 6759 | loss: 3.224377 | lr:4.6800e-04 | norm 0.2641 | dt 336.69ms | 1557166.86 tokens/sec
Step 6760 | loss: 3.250590 | lr:4.6796e-04 | norm 0.3035 | dt 336.25ms | 1559234.88 tokens/sec
Step 6761 | loss: 3.238363 | lr:4.6792e-04 | norm 0.3094 | dt 337.01ms | 1555692.90 tokens/sec
Step 6762 | loss: 3.260822 | lr:4.6788e-04 | norm 0.2844 | dt 336.53ms | 1557917.03 tokens/sec
Step 6763 | loss: 3.270201 | lr:4.6784e-04 | norm 0.2730 | dt 336.75ms | 1556890.14 tokens/sec
Step 6764 | loss: 3.235329 | lr:4.6780e-04 | norm 0.2792 | dt 336.82ms | 1556593.69 tokens/sec
Step 6765 | loss: 3.199565 | lr:4.6776e-04 | norm 0.3760 | dt 336.85ms | 1556443.85 tokens/sec
Step 6766 | loss: 3.214221 | lr:4.6773e-04 | norm 0.3118 | dt 336.60ms | 1557589.29 tokens/sec
Step 6767 | loss: 3.231129 | lr:4.6769e-04 | norm 0.2757 | dt 337.32ms | 1554282.15 tokens/sec
Step 6768 | loss: 3.222324 | lr:4.6765e-04 | norm 0.2556 | dt 337.48ms | 1553553.04 tokens/sec
Step 6769 | loss: 3.278601 | lr:4.6761e-04 | norm 0.2720 | dt 337.45ms | 1553675.97 tokens/sec
Step 6770 | loss: 3.216412 | lr:4.6757e-04 | norm 0.3252 | dt 338.03ms | 1551010.90 tokens/sec
Step 6771 | loss: 3.277049 | lr:4.6753e-04 | norm 0.3020 | dt 337.25ms | 1554618.39 tokens/sec
Step 6772 | loss: 3.248954 | lr:4.6749e-04 | norm 0.2777 | dt 337.32ms | 1554282.15 tokens/sec
Step 6773 | loss: 3.231884 | lr:4.6745e-04 | norm 0.2968 | dt 337.54ms | 1553283.09 tokens/sec
Step 6774 | loss: 3.294698 | lr:4.6741e-04 | norm 0.2986 | dt 337.49ms | 1553514.63 tokens/sec
Step 6775 | loss: 3.295078 | lr:4.6737e-04 | norm 0.3205 | dt 337.24ms | 1554660.15 tokens/sec
Step 6776 | loss: 3.260302 | lr:4.6733e-04 | norm 0.2989 | dt 336.92ms | 1556100.22 tokens/sec
Step 6777 | loss: 3.256064 | lr:4.6729e-04 | norm 0.2759 | dt 337.83ms | 1551950.08 tokens/sec
Step 6778 | loss: 3.333150 | lr:4.6725e-04 | norm 0.3222 | dt 338.42ms | 1549234.17 tokens/sec
Step 6779 | loss: 3.238480 | lr:4.6721e-04 | norm 0.2884 | dt 337.90ms | 1551596.39 tokens/sec
Step 6780 | loss: 3.301512 | lr:4.6717e-04 | norm 0.3082 | dt 337.67ms | 1552663.44 tokens/sec
Step 6781 | loss: 3.242130 | lr:4.6713e-04 | norm 0.3027 | dt 337.06ms | 1555475.02 tokens/sec
Step 6782 | loss: 3.365703 | lr:4.6709e-04 | norm 0.3149 | dt 338.19ms | 1550258.62 tokens/sec
Step 6783 | loss: 3.343967 | lr:4.6705e-04 | norm 0.3144 | dt 338.37ms | 1549462.31 tokens/sec
Step 6784 | loss: 3.318655 | lr:4.6701e-04 | norm 0.2729 | dt 338.09ms | 1550723.24 tokens/sec
Step 6785 | loss: 3.343209 | lr:4.6697e-04 | norm 0.2924 | dt 337.28ms | 1554456.84 tokens/sec
Step 6786 | loss: 3.249483 | lr:4.6693e-04 | norm 0.2954 | dt 337.98ms | 1551217.69 tokens/sec
Step 6787 | loss: 3.274662 | lr:4.6689e-04 | norm 0.2880 | dt 338.03ms | 1551015.28 tokens/sec
Step 6788 | loss: 3.286079 | lr:4.6685e-04 | norm 0.2610 | dt 336.88ms | 1556288.54 tokens/sec
Step 6789 | loss: 3.329917 | lr:4.6681e-04 | norm 0.2941 | dt 338.61ms | 1548341.87 tokens/sec
Step 6790 | loss: 3.275698 | lr:4.6677e-04 | norm 0.3019 | dt 337.65ms | 1552742.38 tokens/sec
Step 6791 | loss: 3.345624 | lr:4.6673e-04 | norm 0.3011 | dt 338.17ms | 1550379.95 tokens/sec
Step 6792 | loss: 3.306656 | lr:4.6669e-04 | norm 0.3585 | dt 337.83ms | 1551919.42 tokens/sec
Step 6793 | loss: 3.354213 | lr:4.6665e-04 | norm 0.3337 | dt 337.45ms | 1553679.27 tokens/sec
Step 6794 | loss: 3.270151 | lr:4.6661e-04 | norm 0.2740 | dt 338.00ms | 1551143.28 tokens/sec
Step 6795 | loss: 3.239653 | lr:4.6657e-04 | norm 0.3020 | dt 338.50ms | 1548847.89 tokens/sec
Step 6796 | loss: 3.275849 | lr:4.6653e-04 | norm 0.2829 | dt 337.83ms | 1551951.18 tokens/sec
Step 6797 | loss: 3.281070 | lr:4.6649e-04 | norm 0.2638 | dt 338.17ms | 1550358.08 tokens/sec
Step 6798 | loss: 3.262890 | lr:4.6645e-04 | norm 0.2930 | dt 337.98ms | 1551225.35 tokens/sec
Step 6799 | loss: 3.246834 | lr:4.6641e-04 | norm 0.2747 | dt 337.82ms | 1551988.42 tokens/sec
Step 6800 | loss: 3.263281 | lr:4.6637e-04 | norm 0.2745 | dt 339.24ms | 1545477.79 tokens/sec
Step 6801 | loss: 3.182685 | lr:4.6633e-04 | norm 0.2858 | dt 338.00ms | 1551147.66 tokens/sec
Step 6802 | loss: 3.233412 | lr:4.6629e-04 | norm 0.2594 | dt 339.60ms | 1543825.33 tokens/sec
Step 6803 | loss: 3.229921 | lr:4.6625e-04 | norm 0.2728 | dt 896.13ms | 585058.54 tokens/sec
Step 6804 | loss: 3.245396 | lr:4.6621e-04 | norm 0.2466 | dt 336.55ms | 1557823.22 tokens/sec
Step 6805 | loss: 3.277786 | lr:4.6617e-04 | norm 0.2936 | dt 338.49ms | 1548888.25 tokens/sec
Step 6806 | loss: 3.265687 | lr:4.6613e-04 | norm 0.2840 | dt 339.07ms | 1546230.87 tokens/sec
Step 6807 | loss: 3.307245 | lr:4.6609e-04 | norm 0.2840 | dt 338.00ms | 1551159.69 tokens/sec
Step 6808 | loss: 3.237067 | lr:4.6605e-04 | norm 0.2857 | dt 337.82ms | 1551971.99 tokens/sec
Step 6809 | loss: 3.281878 | lr:4.6601e-04 | norm 0.2792 | dt 338.89ms | 1547072.84 tokens/sec
Step 6810 | loss: 3.276670 | lr:4.6597e-04 | norm 0.2684 | dt 338.15ms | 1550470.68 tokens/sec
Step 6811 | loss: 3.250132 | lr:4.6593e-04 | norm 0.2907 | dt 339.23ms | 1545510.38 tokens/sec
Step 6812 | loss: 3.261635 | lr:4.6589e-04 | norm 0.2548 | dt 337.27ms | 1554497.50 tokens/sec
Step 6813 | loss: 3.317552 | lr:4.6585e-04 | norm 0.2772 | dt 338.04ms | 1550952.92 tokens/sec
Step 6814 | loss: 3.287899 | lr:4.6581e-04 | norm 0.2772 | dt 338.64ms | 1548206.70 tokens/sec
Step 6815 | loss: 3.269656 | lr:4.6577e-04 | norm 0.2578 | dt 338.25ms | 1550023.69 tokens/sec
Step 6816 | loss: 3.286627 | lr:4.6573e-04 | norm 0.2644 | dt 338.43ms | 1549178.50 tokens/sec
Step 6817 | loss: 3.289518 | lr:4.6569e-04 | norm 0.2582 | dt 339.16ms | 1545834.13 tokens/sec
Step 6818 | loss: 3.319829 | lr:4.6565e-04 | norm 0.2868 | dt 338.63ms | 1548265.56 tokens/sec
Step 6819 | loss: 3.287527 | lr:4.6561e-04 | norm 0.2742 | dt 337.81ms | 1552019.09 tokens/sec
Step 6820 | loss: 3.288729 | lr:4.6557e-04 | norm 0.2723 | dt 338.05ms | 1550936.52 tokens/sec
Step 6821 | loss: 3.275156 | lr:4.6553e-04 | norm 0.2879 | dt 338.47ms | 1548985.36 tokens/sec
Step 6822 | loss: 3.292072 | lr:4.6549e-04 | norm 0.3160 | dt 338.08ms | 1550758.24 tokens/sec
Step 6823 | loss: 3.277377 | lr:4.6545e-04 | norm 0.2682 | dt 338.33ms | 1549641.38 tokens/sec
Step 6824 | loss: 3.333792 | lr:4.6541e-04 | norm 0.3023 | dt 338.36ms | 1549478.69 tokens/sec
Step 6825 | loss: 3.324363 | lr:4.6537e-04 | norm 0.2805 | dt 338.61ms | 1548375.67 tokens/sec
Step 6826 | loss: 3.279521 | lr:4.6533e-04 | norm 0.3006 | dt 338.58ms | 1548503.24 tokens/sec
Step 6827 | loss: 3.266563 | lr:4.6529e-04 | norm 0.2922 | dt 337.98ms | 1551222.06 tokens/sec
Step 6828 | loss: 3.267067 | lr:4.6525e-04 | norm 0.2903 | dt 338.24ms | 1550045.54 tokens/sec
Step 6829 | loss: 3.195052 | lr:4.6521e-04 | norm 0.2521 | dt 338.20ms | 1550225.84 tokens/sec
Step 6830 | loss: 3.272947 | lr:4.6517e-04 | norm 0.2683 | dt 337.85ms | 1551823.04 tokens/sec
Step 6831 | loss: 3.230061 | lr:4.6513e-04 | norm 0.2641 | dt 338.10ms | 1550674.04 tokens/sec
Step 6832 | loss: 3.250991 | lr:4.6509e-04 | norm 0.2681 | dt 337.92ms | 1551537.27 tokens/sec
Step 6833 | loss: 3.293942 | lr:4.6505e-04 | norm 0.2804 | dt 338.43ms | 1549180.69 tokens/sec
Step 6834 | loss: 3.249457 | lr:4.6501e-04 | norm 0.2496 | dt 337.68ms | 1552598.76 tokens/sec
Step 6835 | loss: 3.372051 | lr:4.6497e-04 | norm 0.2746 | dt 338.66ms | 1548133.67 tokens/sec
Step 6836 | loss: 3.203818 | lr:4.6493e-04 | norm 0.3182 | dt 338.02ms | 1551037.16 tokens/sec
Step 6837 | loss: 3.255510 | lr:4.6489e-04 | norm 0.3028 | dt 337.65ms | 1552736.89 tokens/sec
Step 6838 | loss: 3.240422 | lr:4.6485e-04 | norm 0.2815 | dt 337.16ms | 1555015.25 tokens/sec
Step 6839 | loss: 3.263941 | lr:4.6481e-04 | norm 0.3134 | dt 1007.05ms | 520617.26 tokens/sec
Step 6840 | loss: 3.213940 | lr:4.6477e-04 | norm 0.2805 | dt 338.18ms | 1550344.97 tokens/sec
Step 6841 | loss: 3.292371 | lr:4.6473e-04 | norm 0.3134 | dt 338.20ms | 1550232.40 tokens/sec
Step 6842 | loss: 3.315023 | lr:4.6469e-04 | norm 0.3396 | dt 337.84ms | 1551869.04 tokens/sec
Step 6843 | loss: 3.273599 | lr:4.6465e-04 | norm 0.3876 | dt 337.39ms | 1553968.02 tokens/sec
Step 6844 | loss: 3.274174 | lr:4.6461e-04 | norm 0.3192 | dt 338.17ms | 1550371.20 tokens/sec
Step 6845 | loss: 3.250306 | lr:4.6457e-04 | norm 0.3402 | dt 338.01ms | 1551097.33 tokens/sec
Step 6846 | loss: 3.288290 | lr:4.6453e-04 | norm 0.2864 | dt 337.53ms | 1553318.20 tokens/sec
Step 6847 | loss: 3.238683 | lr:4.6449e-04 | norm 0.3230 | dt 338.01ms | 1551102.80 tokens/sec
Step 6848 | loss: 3.255927 | lr:4.6445e-04 | norm 0.2789 | dt 337.74ms | 1552362.02 tokens/sec
Step 6849 | loss: 3.262527 | lr:4.6441e-04 | norm 0.2907 | dt 337.29ms | 1554430.47 tokens/sec
Step 6850 | loss: 3.243116 | lr:4.6437e-04 | norm 0.3171 | dt 337.21ms | 1554778.87 tokens/sec
Step 6851 | loss: 3.317039 | lr:4.6433e-04 | norm 0.2896 | dt 337.20ms | 1554839.33 tokens/sec
Step 6852 | loss: 3.284635 | lr:4.6429e-04 | norm 0.2969 | dt 338.05ms | 1550938.70 tokens/sec
Step 6853 | loss: 3.261698 | lr:4.6425e-04 | norm 0.2952 | dt 337.70ms | 1552530.80 tokens/sec
Step 6854 | loss: 3.248748 | lr:4.6421e-04 | norm 0.2610 | dt 337.37ms | 1554061.37 tokens/sec
Step 6855 | loss: 3.222891 | lr:4.6417e-04 | norm 0.2781 | dt 336.94ms | 1556048.47 tokens/sec
Step 6856 | loss: 3.285608 | lr:4.6413e-04 | norm 0.2622 | dt 338.21ms | 1550163.55 tokens/sec
Step 6857 | loss: 3.329843 | lr:4.6409e-04 | norm 0.2709 | dt 337.63ms | 1552832.29 tokens/sec
Step 6858 | loss: 3.326884 | lr:4.6405e-04 | norm 0.2959 | dt 337.43ms | 1553761.60 tokens/sec
Step 6859 | loss: 3.232488 | lr:4.6401e-04 | norm 0.3414 | dt 338.96ms | 1546749.65 tokens/sec
Step 6860 | loss: 3.232974 | lr:4.6397e-04 | norm 0.3082 | dt 338.19ms | 1550281.58 tokens/sec
Step 6861 | loss: 3.289577 | lr:4.6393e-04 | norm 0.3324 | dt 337.45ms | 1553685.85 tokens/sec
Step 6862 | loss: 3.211507 | lr:4.6389e-04 | norm 0.3057 | dt 337.75ms | 1552274.35 tokens/sec
Step 6863 | loss: 3.223979 | lr:4.6385e-04 | norm 0.2858 | dt 339.26ms | 1545399.59 tokens/sec
Step 6864 | loss: 3.238119 | lr:4.6381e-04 | norm 0.2965 | dt 337.41ms | 1553880.18 tokens/sec
Step 6865 | loss: 3.269938 | lr:4.6377e-04 | norm 0.2667 | dt 336.94ms | 1556026.45 tokens/sec
Step 6866 | loss: 3.240132 | lr:4.6373e-04 | norm 0.2663 | dt 339.03ms | 1546442.91 tokens/sec
Step 6867 | loss: 3.217486 | lr:4.6369e-04 | norm 0.2687 | dt 337.58ms | 1553093.30 tokens/sec
Step 6868 | loss: 3.234196 | lr:4.6365e-04 | norm 0.2706 | dt 338.18ms | 1550337.32 tokens/sec
Step 6869 | loss: 3.203093 | lr:4.6361e-04 | norm 0.2860 | dt 338.38ms | 1549419.74 tokens/sec
Step 6870 | loss: 3.232676 | lr:4.6357e-04 | norm 0.2788 | dt 338.30ms | 1549794.28 tokens/sec
Step 6871 | loss: 3.271173 | lr:4.6353e-04 | norm 0.2880 | dt 337.56ms | 1553168.99 tokens/sec
Step 6872 | loss: 3.226164 | lr:4.6349e-04 | norm 0.2801 | dt 337.87ms | 1551734.34 tokens/sec
Step 6873 | loss: 3.246704 | lr:4.6345e-04 | norm 0.2820 | dt 338.28ms | 1549864.19 tokens/sec
Step 6874 | loss: 3.285087 | lr:4.6341e-04 | norm 0.3050 | dt 337.05ms | 1555522.33 tokens/sec
Step 6875 | loss: 3.281949 | lr:4.6337e-04 | norm 0.2499 | dt 338.62ms | 1548299.36 tokens/sec
Step 6876 | loss: 3.250618 | lr:4.6333e-04 | norm 0.2608 | dt 338.22ms | 1550137.32 tokens/sec
Step 6877 | loss: 3.246583 | lr:4.6329e-04 | norm 0.2801 | dt 339.26ms | 1545382.21 tokens/sec
Step 6878 | loss: 3.265849 | lr:4.6325e-04 | norm 0.3387 | dt 338.76ms | 1547674.96 tokens/sec
Step 6879 | loss: 3.253185 | lr:4.6321e-04 | norm 0.2898 | dt 338.86ms | 1547230.67 tokens/sec
Step 6880 | loss: 3.249339 | lr:4.6317e-04 | norm 0.2681 | dt 338.52ms | 1548757.35 tokens/sec
Step 6881 | loss: 3.286697 | lr:4.6313e-04 | norm 0.3024 | dt 337.91ms | 1551578.87 tokens/sec
Step 6882 | loss: 3.258646 | lr:4.6309e-04 | norm 0.3097 | dt 338.07ms | 1550826.04 tokens/sec
Step 6883 | loss: 3.280069 | lr:4.6305e-04 | norm 0.3577 | dt 337.91ms | 1551576.68 tokens/sec
Step 6884 | loss: 3.359273 | lr:4.6301e-04 | norm 0.3090 | dt 338.51ms | 1548809.71 tokens/sec
Step 6885 | loss: 3.267197 | lr:4.6297e-04 | norm 0.3095 | dt 338.16ms | 1550424.76 tokens/sec
Step 6886 | loss: 3.275961 | lr:4.6293e-04 | norm 0.3069 | dt 338.10ms | 1550674.04 tokens/sec
Step 6887 | loss: 3.280334 | lr:4.6289e-04 | norm 0.3024 | dt 338.04ms | 1550974.80 tokens/sec
Step 6888 | loss: 3.280447 | lr:4.6285e-04 | norm 0.2823 | dt 337.79ms | 1552093.58 tokens/sec
Step 6889 | loss: 3.264971 | lr:4.6281e-04 | norm 0.2660 | dt 337.14ms | 1555107.62 tokens/sec
Step 6890 | loss: 3.267038 | lr:4.6277e-04 | norm 0.2838 | dt 338.05ms | 1550928.86 tokens/sec
Step 6891 | loss: 3.339155 | lr:4.6273e-04 | norm 0.2636 | dt 337.67ms | 1552652.48 tokens/sec
Step 6892 | loss: 3.323507 | lr:4.6269e-04 | norm 0.2756 | dt 338.15ms | 1550468.49 tokens/sec
Step 6893 | loss: 3.280410 | lr:4.6265e-04 | norm 0.2556 | dt 336.97ms | 1555895.43 tokens/sec
Step 6894 | loss: 3.245486 | lr:4.6261e-04 | norm 0.2572 | dt 337.23ms | 1554694.23 tokens/sec
Step 6895 | loss: 3.358901 | lr:4.6257e-04 | norm 0.2580 | dt 337.17ms | 1554943.77 tokens/sec
Step 6896 | loss: 3.286325 | lr:4.6252e-04 | norm 0.2825 | dt 337.82ms | 1551969.80 tokens/sec
Step 6897 | loss: 3.325040 | lr:4.6248e-04 | norm 0.2737 | dt 337.03ms | 1555612.56 tokens/sec
Step 6898 | loss: 3.225065 | lr:4.6244e-04 | norm 0.2640 | dt 338.41ms | 1549278.92 tokens/sec
Step 6899 | loss: 3.327177 | lr:4.6240e-04 | norm 0.2708 | dt 337.51ms | 1553398.30 tokens/sec
Step 6900 | loss: 3.197246 | lr:4.6236e-04 | norm 0.2874 | dt 337.05ms | 1555539.94 tokens/sec
Step 6901 | loss: 3.231640 | lr:4.6232e-04 | norm 0.2555 | dt 337.42ms | 1553797.83 tokens/sec
Step 6902 | loss: 3.232688 | lr:4.6228e-04 | norm 0.2689 | dt 337.70ms | 1552530.80 tokens/sec
Step 6903 | loss: 3.278154 | lr:4.6224e-04 | norm 0.2562 | dt 338.42ms | 1549209.06 tokens/sec
Step 6904 | loss: 3.251106 | lr:4.6220e-04 | norm 0.2755 | dt 337.44ms | 1553713.30 tokens/sec
Step 6905 | loss: 3.272111 | lr:4.6216e-04 | norm 0.2647 | dt 337.21ms | 1554772.27 tokens/sec
Step 6906 | loss: 3.266520 | lr:4.6212e-04 | norm 0.2481 | dt 337.90ms | 1551609.53 tokens/sec
Step 6907 | loss: 3.239584 | lr:4.6208e-04 | norm 0.2707 | dt 337.41ms | 1553872.49 tokens/sec
Step 6908 | loss: 3.257697 | lr:4.6204e-04 | norm 0.2873 | dt 337.66ms | 1552698.52 tokens/sec
Step 6909 | loss: 3.297091 | lr:4.6200e-04 | norm 0.2700 | dt 338.01ms | 1551086.39 tokens/sec
Step 6910 | loss: 3.240287 | lr:4.6196e-04 | norm 0.2740 | dt 337.92ms | 1551518.66 tokens/sec
Step 6911 | loss: 3.294234 | lr:4.6192e-04 | norm 0.2701 | dt 337.33ms | 1554242.60 tokens/sec
Step 6912 | loss: 3.279329 | lr:4.6188e-04 | norm 0.2675 | dt 337.60ms | 1552996.78 tokens/sec
Step 6913 | loss: 3.315177 | lr:4.6184e-04 | norm 0.2709 | dt 338.62ms | 1548322.25 tokens/sec
Step 6914 | loss: 3.242052 | lr:4.6180e-04 | norm 0.2767 | dt 338.46ms | 1549033.37 tokens/sec
Step 6915 | loss: 3.250022 | lr:4.6176e-04 | norm 0.2693 | dt 338.04ms | 1550974.80 tokens/sec
Step 6916 | loss: 3.270480 | lr:4.6172e-04 | norm 0.2695 | dt 338.07ms | 1550818.39 tokens/sec
Step 6917 | loss: 3.278687 | lr:4.6168e-04 | norm 0.2683 | dt 337.64ms | 1552808.16 tokens/sec
Step 6918 | loss: 3.223511 | lr:4.6164e-04 | norm 0.2567 | dt 338.74ms | 1547778.45 tokens/sec
Step 6919 | loss: 3.248085 | lr:4.6160e-04 | norm 0.2796 | dt 337.86ms | 1551775.95 tokens/sec
Step 6920 | loss: 3.308950 | lr:4.6156e-04 | norm 0.2666 | dt 336.96ms | 1555950.48 tokens/sec
Step 6921 | loss: 3.319413 | lr:4.6152e-04 | norm 0.3012 | dt 339.57ms | 1543990.09 tokens/sec
Step 6922 | loss: 3.245744 | lr:4.6148e-04 | norm 0.2933 | dt 339.67ms | 1543519.75 tokens/sec
Step 6923 | loss: 3.221703 | lr:4.6144e-04 | norm 0.2577 | dt 338.51ms | 1548810.80 tokens/sec
Step 6924 | loss: 3.232929 | lr:4.6140e-04 | norm 0.2686 | dt 338.42ms | 1549215.61 tokens/sec
Step 6925 | loss: 3.260685 | lr:4.6136e-04 | norm 0.2507 | dt 339.05ms | 1546346.12 tokens/sec
Step 6926 | loss: 3.268759 | lr:4.6132e-04 | norm 0.2612 | dt 338.52ms | 1548745.35 tokens/sec
Step 6927 | loss: 3.225205 | lr:4.6128e-04 | norm 0.2742 | dt 338.41ms | 1549262.54 tokens/sec
Step 6928 | loss: 3.267525 | lr:4.6123e-04 | norm 0.2737 | dt 338.38ms | 1549405.54 tokens/sec
Step 6929 | loss: 3.223202 | lr:4.6119e-04 | norm 0.2802 | dt 338.60ms | 1548378.94 tokens/sec
Step 6930 | loss: 3.273305 | lr:4.6115e-04 | norm 0.2671 | dt 338.73ms | 1547790.43 tokens/sec
Step 6931 | loss: 3.226081 | lr:4.6111e-04 | norm 0.2875 | dt 339.17ms | 1545780.89 tokens/sec
Step 6932 | loss: 3.234632 | lr:4.6107e-04 | norm 0.2748 | dt 337.94ms | 1551411.39 tokens/sec
Step 6933 | loss: 3.162683 | lr:4.6103e-04 | norm 0.2815 | dt 337.90ms | 1551607.34 tokens/sec
Step 6934 | loss: 3.221092 | lr:4.6099e-04 | norm 0.2903 | dt 338.66ms | 1548134.76 tokens/sec
Step 6935 | loss: 3.254355 | lr:4.6095e-04 | norm 0.2790 | dt 338.59ms | 1548428.00 tokens/sec
Step 6936 | loss: 3.241343 | lr:4.6091e-04 | norm 0.2792 | dt 337.43ms | 1553785.75 tokens/sec
Step 6937 | loss: 3.239474 | lr:4.6087e-04 | norm 0.2786 | dt 338.06ms | 1550853.39 tokens/sec
Step 6938 | loss: 3.211507 | lr:4.6083e-04 | norm 0.2801 | dt 338.05ms | 1550931.05 tokens/sec
Step 6939 | loss: 3.252346 | lr:4.6079e-04 | norm 0.2870 | dt 338.66ms | 1548146.75 tokens/sec
Step 6940 | loss: 3.236229 | lr:4.6075e-04 | norm 0.2787 | dt 337.74ms | 1552362.02 tokens/sec
Step 6941 | loss: 3.205120 | lr:4.6071e-04 | norm 0.3059 | dt 338.14ms | 1550490.35 tokens/sec
Step 6942 | loss: 3.240980 | lr:4.6067e-04 | norm 0.2816 | dt 338.16ms | 1550401.81 tokens/sec
Step 6943 | loss: 3.302783 | lr:4.6063e-04 | norm 0.2796 | dt 338.12ms | 1550617.18 tokens/sec
Step 6944 | loss: 3.297767 | lr:4.6059e-04 | norm 0.2873 | dt 338.58ms | 1548497.79 tokens/sec
Step 6945 | loss: 3.315554 | lr:4.6055e-04 | norm 0.2963 | dt 338.11ms | 1550630.30 tokens/sec
Step 6946 | loss: 3.355794 | lr:4.6051e-04 | norm 0.3267 | dt 337.87ms | 1551748.58 tokens/sec
Step 6947 | loss: 3.357753 | lr:4.6047e-04 | norm 0.3112 | dt 338.10ms | 1550697.00 tokens/sec
Step 6948 | loss: 3.229619 | lr:4.6043e-04 | norm 0.3022 | dt 338.38ms | 1549421.92 tokens/sec
Step 6949 | loss: 3.310535 | lr:4.6039e-04 | norm 0.3153 | dt 338.35ms | 1549560.58 tokens/sec
Step 6950 | loss: 3.295119 | lr:4.6035e-04 | norm 0.2708 | dt 337.83ms | 1551951.18 tokens/sec
Step 6951 | loss: 3.259936 | lr:4.6031e-04 | norm 0.3334 | dt 338.27ms | 1549903.51 tokens/sec
Step 6952 | loss: 3.246868 | lr:4.6026e-04 | norm 0.3238 | dt 337.63ms | 1552839.96 tokens/sec
Step 6953 | loss: 3.216871 | lr:4.6022e-04 | norm 0.2836 | dt 338.24ms | 1550035.71 tokens/sec
Step 6954 | loss: 3.333018 | lr:4.6018e-04 | norm 0.2873 | dt 337.70ms | 1552503.40 tokens/sec
Step 6955 | loss: 3.345580 | lr:4.6014e-04 | norm 0.2964 | dt 338.63ms | 1548265.56 tokens/sec
Step 6956 | loss: 3.271440 | lr:4.6010e-04 | norm 0.2856 | dt 339.02ms | 1546476.62 tokens/sec
Step 6957 | loss: 3.298860 | lr:4.6006e-04 | norm 0.2879 | dt 338.34ms | 1549581.33 tokens/sec
Step 6958 | loss: 3.266878 | lr:4.6002e-04 | norm 0.2900 | dt 338.32ms | 1549666.50 tokens/sec
Step 6959 | loss: 3.286031 | lr:4.5998e-04 | norm 0.2776 | dt 338.14ms | 1550490.35 tokens/sec
Step 6960 | loss: 3.243814 | lr:4.5994e-04 | norm 0.2853 | dt 338.35ms | 1549548.57 tokens/sec
Step 6961 | loss: 3.239030 | lr:4.5990e-04 | norm 0.2949 | dt 338.33ms | 1549612.99 tokens/sec
Step 6962 | loss: 3.246874 | lr:4.5986e-04 | norm 0.2898 | dt 338.13ms | 1550529.71 tokens/sec
Step 6963 | loss: 3.325137 | lr:4.5982e-04 | norm 0.2985 | dt 339.06ms | 1546318.94 tokens/sec
Step 6964 | loss: 3.288477 | lr:4.5978e-04 | norm 0.2751 | dt 338.33ms | 1549657.77 tokens/sec
Step 6965 | loss: 3.258091 | lr:4.5974e-04 | norm 0.2562 | dt 337.39ms | 1553936.18 tokens/sec
Step 6966 | loss: 3.247348 | lr:4.5970e-04 | norm 0.2781 | dt 337.79ms | 1552125.35 tokens/sec
Step 6967 | loss: 3.281427 | lr:4.5966e-04 | norm 0.2677 | dt 337.91ms | 1551547.12 tokens/sec
Step 6968 | loss: 3.270873 | lr:4.5962e-04 | norm 0.2959 | dt 337.43ms | 1553780.26 tokens/sec
Step 6969 | loss: 3.195951 | lr:4.5958e-04 | norm 0.3086 | dt 337.60ms | 1552968.27 tokens/sec
Step 6970 | loss: 3.266055 | lr:4.5954e-04 | norm 0.2586 | dt 336.92ms | 1556127.75 tokens/sec
Step 6971 | loss: 3.245744 | lr:4.5949e-04 | norm 0.2952 | dt 338.70ms | 1547928.80 tokens/sec
Step 6972 | loss: 3.303660 | lr:4.5945e-04 | norm 0.3028 | dt 337.13ms | 1555160.41 tokens/sec
Step 6973 | loss: 3.263802 | lr:4.5941e-04 | norm 0.2702 | dt 337.18ms | 1554933.88 tokens/sec
Step 6974 | loss: 3.199214 | lr:4.5937e-04 | norm 0.3041 | dt 338.71ms | 1547876.50 tokens/sec
Step 6975 | loss: 3.287045 | lr:4.5933e-04 | norm 0.2845 | dt 337.91ms | 1551575.59 tokens/sec
Step 6976 | loss: 3.224043 | lr:4.5929e-04 | norm 0.2792 | dt 337.86ms | 1551794.57 tokens/sec
Step 6977 | loss: 3.260310 | lr:4.5925e-04 | norm 0.2847 | dt 338.17ms | 1550347.15 tokens/sec
Step 6978 | loss: 3.337048 | lr:4.5921e-04 | norm 0.2930 | dt 337.80ms | 1552051.95 tokens/sec
Step 6979 | loss: 3.275617 | lr:4.5917e-04 | norm 0.2852 | dt 337.75ms | 1552284.22 tokens/sec
Step 6980 | loss: 3.316101 | lr:4.5913e-04 | norm 0.2857 | dt 337.17ms | 1554984.46 tokens/sec
Step 6981 | loss: 3.259141 | lr:4.5909e-04 | norm 0.2929 | dt 338.25ms | 1550013.85 tokens/sec
Step 6982 | loss: 3.267925 | lr:4.5905e-04 | norm 0.3086 | dt 338.23ms | 1550075.04 tokens/sec
Step 6983 | loss: 3.226818 | lr:4.5901e-04 | norm 0.3304 | dt 338.15ms | 1550458.65 tokens/sec
Step 6984 | loss: 3.255528 | lr:4.5897e-04 | norm 0.3107 | dt 337.73ms | 1552385.03 tokens/sec
Step 6985 | loss: 3.285508 | lr:4.5893e-04 | norm 0.3312 | dt 338.45ms | 1549105.39 tokens/sec
Step 6986 | loss: 3.265020 | lr:4.5889e-04 | norm 0.3014 | dt 337.94ms | 1551434.38 tokens/sec
Step 6987 | loss: 3.340687 | lr:4.5885e-04 | norm 0.3227 | dt 337.53ms | 1553298.45 tokens/sec
Step 6988 | loss: 3.258342 | lr:4.5881e-04 | norm 0.2888 | dt 338.85ms | 1547277.49 tokens/sec
Step 6989 | loss: 3.282956 | lr:4.5876e-04 | norm 0.3256 | dt 337.38ms | 1553985.59 tokens/sec
Step 6990 | loss: 3.311212 | lr:4.5872e-04 | norm 0.3321 | dt 337.47ms | 1553606.82 tokens/sec
Step 6991 | loss: 3.254774 | lr:4.5868e-04 | norm 0.3001 | dt 338.98ms | 1546645.21 tokens/sec
Step 6992 | loss: 3.275600 | lr:4.5864e-04 | norm 0.2971 | dt 1026.57ms | 510716.75 tokens/sec
Step 6993 | loss: 3.296371 | lr:4.5860e-04 | norm 0.2869 | dt 337.29ms | 1554432.67 tokens/sec
Step 6994 | loss: 3.265756 | lr:4.5856e-04 | norm 0.2635 | dt 337.80ms | 1552078.24 tokens/sec
Step 6995 | loss: 3.301386 | lr:4.5852e-04 | norm 0.3104 | dt 338.81ms | 1547423.38 tokens/sec
Step 6996 | loss: 3.258533 | lr:4.5848e-04 | norm 0.2773 | dt 337.77ms | 1552203.13 tokens/sec
Step 6997 | loss: 3.291042 | lr:4.5844e-04 | norm 0.2622 | dt 337.92ms | 1551496.77 tokens/sec
Step 6998 | loss: 3.235049 | lr:4.5840e-04 | norm 0.2950 | dt 338.26ms | 1549947.21 tokens/sec
Step 6999 | loss: 3.262818 | lr:4.5836e-04 | norm 0.2734 | dt 337.66ms | 1552688.65 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 7000: 3.2762
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2868/10042=0.2856


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I am not a model. I can show my computer to me to understand it and I can't show me. It
rank 5 sample 1 >Hello, I'm a language model, meaning in which we want all children to understand grammar. So if you are in any way confused or embarrassed.
I
rank 5 sample 2 >Hello, I'm a language model, and you've probably also heard the expression "The question is, are you doing the writing?"
I'm not the
rank 5 sample 3 >Hello, I'm a language model, that is part of my writing assignment. So, here I am, and the purpose was that everyone would be writing that




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I like to help other people. What are you doing when I need to start thinking again?
In this article

ddp_rank 1: ####### Printing generated samples ####### 


rank 4 sample 1 >Hello, I'm a language model, don't you seem to think twice about writing "phonetic" with your computer? I think so. Thank yourank 1 sample 0 >Hello, I'm a language model, I use it in my classroom. By doing so, I show students how to use language, in a very intuitive way

rank 1 sample 1 >Hello, I'm a language model, a person with dyslexia. And they're just people who come from countries that like math or English. They're
rank 4 sample 2 >Hello, I'm a language model, but sometimes I come into an argument in which I'm asking the same question: “What does what I said in
rank 1 sample 2 >Hello, I'm a language model, I did some grammar and vocabulary work in my class. It helped me learn what I was taught and was very effective in
rank 4 sample 3 >Hello, I'm a language model, so you definitely think it's important anyway if there are 2 teachers out there, but I think a teacher can always have


rank 1 sample 3 >Hello, I'm a language model, so I'm sure I mean, let's talk about getting your computer a place to hang out, and I can't




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I will be posting more on how does it work.
I just said just about do our job, and they
rank 7 sample 1 >Hello, I'm a language model, though, since I'm not writing at all. Most people have not heard of some languages like English, German, etc
rank 7 sample 2 >Hello, I'm a language model, I'm not talking to you today about our children's first language. That's what we're going to talk about today
rank 7 sample 3 >Hello, I'm a language model, is it good to use or not or will take a lot of practice!
So, I went for a whole-




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, right? I was trying to learn French. So I was thinking about my first sentence. I'm trying you to say


ddp_rank 3: ####### Printing generated samples ####### 

rank 2 sample 1 >Hello, I'm a language model, I'm constantly looking for ways to build these. Not only is learning this difficult, but the learning style of the person
rank 2 sample 2 >Hello, I'm a language model, and I want to learn more! Thanks!
And please, let's go.
Now in my second and fifth
rank 3 sample 0 >Hello, I'm a language model, so I'm not the type. There are a couple of variables in code/code where these are expressed as numbers to
rank 2 sample 3 >Hello, I'm a language model, I like and feel good about it. (I hate that term, but in this blog i dont know the meaning of


rank 3 sample 1 >Hello, I'm a language model, so if we're going to go to a computer, you'll find that it's a little bit more complicated if the
rank 3 sample 2 >Hello, I'm a language model, so let's use it. This is going to be the simplest way of using this. You’re done!
rank 3 sample 3 >Hello, I'm a language model, so when I first started learning how to use language models (and, sometimes, all my languages), I had a very




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, but that's not really something you do in your daily life."
The problem with this advice is that you can also
rank 6 sample 1 >Hello, I'm a language model, my book, I love to learn English through this method. I also get to know my students well. My friend,
rank 6 sample 2 >Hello, I'm a language model, I want to have a good life. I love learning English and French and Spanish. I want to stay my Spanish,
rank 6 sample 3 >Hello, I'm a language model, but I think it's important if we use a lot of the same language as the one discussed in class in class.




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I think I'll show a demo there ...
|A1D-1: Using Google's Search Engine for
rank 0 sample 1 >Hello, I'm a language model, I mean, I mean, I used to think twice what I wanted to talk to my clients, you can't tell
rank 0 sample 2 >Hello, I'm a language model, and I thought, it wouldn't have been practical. I can see it in the future.
I'm not suggesting
rank 0 sample 3 >Hello, I'm a language model, I hope you can help me out. I wanted to make sure I understood my code to avoid any technical problems that arose


Step 7000 | loss: 3.292643 | lr:4.5832e-04 | norm 0.2746 | dt 13829.24ms | 37911.57 tokens/sec
Step 7001 | loss: 3.211913 | lr:4.5828e-04 | norm 0.2573 | dt 334.53ms | 1567220.32 tokens/sec
Step 7002 | loss: 3.239541 | lr:4.5824e-04 | norm 0.2609 | dt 336.97ms | 1555902.04 tokens/sec
Step 7003 | loss: 3.163626 | lr:4.5820e-04 | norm 0.3190 | dt 336.36ms | 1558706.59 tokens/sec
Step 7004 | loss: 3.245663 | lr:4.5815e-04 | norm 0.2748 | dt 336.14ms | 1559721.49 tokens/sec
Step 7005 | loss: 3.254249 | lr:4.5811e-04 | norm 0.2638 | dt 336.21ms | 1559429.49 tokens/sec
Step 7006 | loss: 3.244481 | lr:4.5807e-04 | norm 0.2727 | dt 336.59ms | 1557651.08 tokens/sec
Step 7007 | loss: 3.262096 | lr:4.5803e-04 | norm 0.2659 | dt 336.41ms | 1558460.25 tokens/sec
Step 7008 | loss: 3.257599 | lr:4.5799e-04 | norm 0.2798 | dt 336.34ms | 1558797.19 tokens/sec
Step 7009 | loss: 3.277570 | lr:4.5795e-04 | norm 0.2574 | dt 336.11ms | 1559865.32 tokens/sec
Step 7010 | loss: 3.267131 | lr:4.5791e-04 | norm 0.2775 | dt 336.90ms | 1556191.62 tokens/sec
Step 7011 | loss: 3.265757 | lr:4.5787e-04 | norm 0.2797 | dt 336.17ms | 1559584.32 tokens/sec
Step 7012 | loss: 3.237064 | lr:4.5783e-04 | norm 0.2715 | dt 337.03ms | 1555623.57 tokens/sec
Step 7013 | loss: 3.293466 | lr:4.5779e-04 | norm 0.2840 | dt 338.11ms | 1550641.23 tokens/sec
Step 7014 | loss: 3.266240 | lr:4.5775e-04 | norm 0.2923 | dt 337.74ms | 1552331.34 tokens/sec
Step 7015 | loss: 3.247954 | lr:4.5771e-04 | norm 0.2700 | dt 337.21ms | 1554782.16 tokens/sec
Step 7016 | loss: 3.299546 | lr:4.5767e-04 | norm 0.2824 | dt 337.23ms | 1554689.83 tokens/sec
Step 7017 | loss: 3.334406 | lr:4.5763e-04 | norm 0.2785 | dt 337.26ms | 1554560.14 tokens/sec
Step 7018 | loss: 3.284878 | lr:4.5759e-04 | norm 0.2986 | dt 337.22ms | 1554733.80 tokens/sec
Step 7019 | loss: 3.258583 | lr:4.5754e-04 | norm 0.3154 | dt 337.70ms | 1552511.07 tokens/sec
Step 7020 | loss: 3.274252 | lr:4.5750e-04 | norm 0.2659 | dt 336.68ms | 1557244.05 tokens/sec
Step 7021 | loss: 3.256426 | lr:4.5746e-04 | norm 0.2835 | dt 337.68ms | 1552618.49 tokens/sec
Step 7022 | loss: 3.264915 | lr:4.5742e-04 | norm 0.3363 | dt 337.43ms | 1553764.89 tokens/sec
Step 7023 | loss: 3.245833 | lr:4.5738e-04 | norm 0.3054 | dt 337.20ms | 1554817.34 tokens/sec
Step 7024 | loss: 3.230730 | lr:4.5734e-04 | norm 0.2735 | dt 337.91ms | 1551553.69 tokens/sec
Step 7025 | loss: 3.276932 | lr:4.5730e-04 | norm 0.2901 | dt 338.53ms | 1548734.44 tokens/sec
Step 7026 | loss: 3.259894 | lr:4.5726e-04 | norm 0.2984 | dt 338.60ms | 1548416.01 tokens/sec
Step 7027 | loss: 3.277773 | lr:4.5722e-04 | norm 0.2753 | dt 338.20ms | 1550248.79 tokens/sec
Step 7028 | loss: 3.361779 | lr:4.5718e-04 | norm 0.3001 | dt 338.45ms | 1549093.38 tokens/sec
Step 7029 | loss: 3.251589 | lr:4.5714e-04 | norm 0.2593 | dt 1045.36ms | 501540.59 tokens/sec
Step 7030 | loss: 3.344074 | lr:4.5710e-04 | norm 0.3185 | dt 338.76ms | 1547667.34 tokens/sec
Step 7031 | loss: 3.341864 | lr:4.5706e-04 | norm 0.3428 | dt 337.67ms | 1552686.46 tokens/sec
Step 7032 | loss: 3.281158 | lr:4.5701e-04 | norm 0.3141 | dt 338.81ms | 1547417.94 tokens/sec
Step 7033 | loss: 3.314907 | lr:4.5697e-04 | norm 0.2751 | dt 338.41ms | 1549277.83 tokens/sec
Step 7034 | loss: 3.278628 | lr:4.5693e-04 | norm 0.3313 | dt 337.76ms | 1552253.54 tokens/sec
Step 7035 | loss: 3.265550 | lr:4.5689e-04 | norm 0.3129 | dt 338.65ms | 1548169.64 tokens/sec
Step 7036 | loss: 3.258119 | lr:4.5685e-04 | norm 0.2931 | dt 338.63ms | 1548278.64 tokens/sec
Step 7037 | loss: 3.252196 | lr:4.5681e-04 | norm 0.2949 | dt 338.30ms | 1549785.55 tokens/sec
Step 7038 | loss: 3.237642 | lr:4.5677e-04 | norm 0.2830 | dt 337.97ms | 1551295.38 tokens/sec
Step 7039 | loss: 3.205049 | lr:4.5673e-04 | norm 0.3112 | dt 339.44ms | 1544584.39 tokens/sec
Step 7040 | loss: 3.271846 | lr:4.5669e-04 | norm 0.3182 | dt 341.38ms | 1535800.22 tokens/sec
Step 7041 | loss: 3.231957 | lr:4.5665e-04 | norm 0.3454 | dt 338.58ms | 1548497.79 tokens/sec
Step 7042 | loss: 3.229290 | lr:4.5661e-04 | norm 0.2862 | dt 339.64ms | 1543655.18 tokens/sec
Step 7043 | loss: 3.241594 | lr:4.5657e-04 | norm 0.2880 | dt 338.36ms | 1549478.69 tokens/sec
Step 7044 | loss: 3.229859 | lr:4.5653e-04 | norm 0.2828 | dt 338.69ms | 1547988.73 tokens/sec
Step 7045 | loss: 3.316035 | lr:4.5648e-04 | norm 0.3131 | dt 339.76ms | 1543113.57 tokens/sec
Step 7046 | loss: 3.244147 | lr:4.5644e-04 | norm 0.2888 | dt 338.28ms | 1549858.73 tokens/sec
Step 7047 | loss: 3.277034 | lr:4.5640e-04 | norm 0.2948 | dt 339.73ms | 1543252.19 tokens/sec
Step 7048 | loss: 3.294338 | lr:4.5636e-04 | norm 0.3159 | dt 338.19ms | 1550278.30 tokens/sec
Step 7049 | loss: 3.279101 | lr:4.5632e-04 | norm 0.3012 | dt 338.57ms | 1548539.22 tokens/sec
Step 7050 | loss: 3.250619 | lr:4.5628e-04 | norm 0.3079 | dt 339.95ms | 1542262.93 tokens/sec
Step 7051 | loss: 3.310680 | lr:4.5624e-04 | norm 0.2781 | dt 338.61ms | 1548360.41 tokens/sec
Step 7052 | loss: 3.311542 | lr:4.5620e-04 | norm 0.2627 | dt 338.67ms | 1548057.38 tokens/sec
Step 7053 | loss: 3.285620 | lr:4.5616e-04 | norm 0.2658 | dt 338.90ms | 1547018.42 tokens/sec
Step 7054 | loss: 3.301094 | lr:4.5612e-04 | norm 0.2652 | dt 339.95ms | 1542236.97 tokens/sec
Step 7055 | loss: 3.304394 | lr:4.5608e-04 | norm 0.2778 | dt 338.54ms | 1548667.91 tokens/sec
Step 7056 | loss: 3.281354 | lr:4.5604e-04 | norm 0.2645 | dt 338.88ms | 1547121.82 tokens/sec
Step 7057 | loss: 3.217866 | lr:4.5599e-04 | norm 0.2436 | dt 339.31ms | 1545137.89 tokens/sec
Step 7058 | loss: 3.260117 | lr:4.5595e-04 | norm 0.2534 | dt 338.94ms | 1546824.72 tokens/sec
Step 7059 | loss: 3.229785 | lr:4.5591e-04 | norm 0.2740 | dt 338.76ms | 1547685.85 tokens/sec
Step 7060 | loss: 3.258066 | lr:4.5587e-04 | norm 0.2802 | dt 338.81ms | 1547459.32 tokens/sec
Step 7061 | loss: 3.251364 | lr:4.5583e-04 | norm 0.2456 | dt 338.10ms | 1550709.03 tokens/sec
Step 7062 | loss: 3.277489 | lr:4.5579e-04 | norm 0.2807 | dt 338.29ms | 1549836.88 tokens/sec
Step 7063 | loss: 3.220423 | lr:4.5575e-04 | norm 0.2668 | dt 337.43ms | 1553749.52 tokens/sec
Step 7064 | loss: 3.344879 | lr:4.5571e-04 | norm 0.2868 | dt 337.49ms | 1553489.38 tokens/sec
Step 7065 | loss: 3.365199 | lr:4.5567e-04 | norm 0.3485 | dt 338.05ms | 1550908.08 tokens/sec
Step 7066 | loss: 3.287801 | lr:4.5563e-04 | norm 0.3347 | dt 337.51ms | 1553410.37 tokens/sec
Step 7067 | loss: 3.278391 | lr:4.5559e-04 | norm 0.2795 | dt 338.21ms | 1550195.24 tokens/sec
Step 7068 | loss: 3.241955 | lr:4.5554e-04 | norm 0.3174 | dt 337.43ms | 1553787.95 tokens/sec
Step 7069 | loss: 3.302594 | lr:4.5550e-04 | norm 0.2751 | dt 337.20ms | 1554830.53 tokens/sec
Step 7070 | loss: 3.244055 | lr:4.5546e-04 | norm 0.2928 | dt 338.63ms | 1548284.10 tokens/sec
Step 7071 | loss: 3.202569 | lr:4.5542e-04 | norm 0.3252 | dt 338.15ms | 1550458.65 tokens/sec
Step 7072 | loss: 3.246657 | lr:4.5538e-04 | norm 0.3244 | dt 337.20ms | 1554818.44 tokens/sec
Step 7073 | loss: 3.267479 | lr:4.5534e-04 | norm 0.3267 | dt 338.35ms | 1549545.29 tokens/sec
Step 7074 | loss: 3.298819 | lr:4.5530e-04 | norm 0.3156 | dt 337.32ms | 1554281.05 tokens/sec
Step 7075 | loss: 3.218938 | lr:4.5526e-04 | norm 0.3400 | dt 337.40ms | 1553892.25 tokens/sec
Step 7076 | loss: 3.193563 | lr:4.5522e-04 | norm 0.3475 | dt 338.34ms | 1549593.34 tokens/sec
Step 7077 | loss: 3.215968 | lr:4.5518e-04 | norm 0.3069 | dt 337.70ms | 1552516.55 tokens/sec
Step 7078 | loss: 3.262291 | lr:4.5514e-04 | norm 0.2933 | dt 338.53ms | 1548712.63 tokens/sec
Step 7079 | loss: 3.287082 | lr:4.5509e-04 | norm 0.2971 | dt 336.83ms | 1556514.36 tokens/sec
Step 7080 | loss: 3.281330 | lr:4.5505e-04 | norm 0.2952 | dt 337.82ms | 1551964.32 tokens/sec
Step 7081 | loss: 3.279460 | lr:4.5501e-04 | norm 0.3017 | dt 338.31ms | 1549717.83 tokens/sec
Step 7082 | loss: 3.257352 | lr:4.5497e-04 | norm 0.3056 | dt 337.38ms | 1553984.49 tokens/sec
Step 7083 | loss: 3.261173 | lr:4.5493e-04 | norm 0.3151 | dt 337.64ms | 1552800.49 tokens/sec
Step 7084 | loss: 3.306125 | lr:4.5489e-04 | norm 0.2822 | dt 337.78ms | 1552176.84 tokens/sec
Step 7085 | loss: 3.328475 | lr:4.5485e-04 | norm 0.3020 | dt 337.65ms | 1552739.09 tokens/sec
Step 7086 | loss: 3.281442 | lr:4.5481e-04 | norm 0.2679 | dt 337.73ms | 1552404.76 tokens/sec
Step 7087 | loss: 3.289632 | lr:4.5477e-04 | norm 0.3124 | dt 338.22ms | 1550132.95 tokens/sec
Step 7088 | loss: 3.396797 | lr:4.5473e-04 | norm 0.3991 | dt 337.36ms | 1554097.61 tokens/sec
Step 7089 | loss: 3.259362 | lr:4.5468e-04 | norm 0.3486 | dt 338.13ms | 1550552.67 tokens/sec
Step 7090 | loss: 3.263350 | lr:4.5464e-04 | norm 0.2937 | dt 337.54ms | 1553261.15 tokens/sec
Step 7091 | loss: 3.350160 | lr:4.5460e-04 | norm 0.3035 | dt 337.48ms | 1553551.94 tokens/sec
Step 7092 | loss: 3.244866 | lr:4.5456e-04 | norm 0.3127 | dt 337.48ms | 1553549.75 tokens/sec
Step 7093 | loss: 3.316031 | lr:4.5452e-04 | norm 0.3284 | dt 338.50ms | 1548860.98 tokens/sec
Step 7094 | loss: 3.256985 | lr:4.5448e-04 | norm 0.2934 | dt 337.59ms | 1553027.49 tokens/sec
Step 7095 | loss: 3.357934 | lr:4.5444e-04 | norm 0.2915 | dt 337.72ms | 1552420.10 tokens/sec
Step 7096 | loss: 3.281329 | lr:4.5440e-04 | norm 0.2996 | dt 338.13ms | 1550570.16 tokens/sec
Step 7097 | loss: 3.254711 | lr:4.5436e-04 | norm 0.2698 | dt 337.53ms | 1553326.98 tokens/sec
Step 7098 | loss: 3.272761 | lr:4.5432e-04 | norm 0.3056 | dt 337.59ms | 1553049.43 tokens/sec
Step 7099 | loss: 3.317967 | lr:4.5427e-04 | norm 0.2671 | dt 337.91ms | 1551549.31 tokens/sec
Step 7100 | loss: 3.291400 | lr:4.5423e-04 | norm 0.2693 | dt 337.49ms | 1553505.85 tokens/sec
Step 7101 | loss: 3.220861 | lr:4.5419e-04 | norm 0.2656 | dt 337.85ms | 1551826.33 tokens/sec
Step 7102 | loss: 3.238763 | lr:4.5415e-04 | norm 0.2642 | dt 337.37ms | 1554066.86 tokens/sec
Step 7103 | loss: 3.296568 | lr:4.5411e-04 | norm 0.2625 | dt 337.54ms | 1553271.02 tokens/sec
Step 7104 | loss: 3.235010 | lr:4.5407e-04 | norm 0.2721 | dt 337.99ms | 1551216.59 tokens/sec
Step 7105 | loss: 3.241540 | lr:4.5403e-04 | norm 0.2638 | dt 337.33ms | 1554251.39 tokens/sec
Step 7106 | loss: 3.274076 | lr:4.5399e-04 | norm 0.2441 | dt 337.94ms | 1551417.96 tokens/sec
Step 7107 | loss: 3.237240 | lr:4.5395e-04 | norm 0.2611 | dt 338.34ms | 1549588.97 tokens/sec
Step 7108 | loss: 3.273892 | lr:4.5391e-04 | norm 0.2466 | dt 339.84ms | 1542754.15 tokens/sec
Step 7109 | loss: 3.252516 | lr:4.5386e-04 | norm 0.2783 | dt 338.20ms | 1550242.23 tokens/sec
Step 7110 | loss: 3.230505 | lr:4.5382e-04 | norm 0.2622 | dt 339.00ms | 1546582.12 tokens/sec
Step 7111 | loss: 3.227118 | lr:4.5378e-04 | norm 0.2746 | dt 338.87ms | 1547156.65 tokens/sec
Step 7112 | loss: 3.282811 | lr:4.5374e-04 | norm 0.2860 | dt 339.24ms | 1545460.41 tokens/sec
Step 7113 | loss: 3.253458 | lr:4.5370e-04 | norm 0.2733 | dt 339.36ms | 1544941.41 tokens/sec
Step 7114 | loss: 3.260576 | lr:4.5366e-04 | norm 0.2804 | dt 339.18ms | 1545773.28 tokens/sec
Step 7115 | loss: 3.217648 | lr:4.5362e-04 | norm 0.2464 | dt 338.17ms | 1550347.15 tokens/sec
Step 7116 | loss: 3.286731 | lr:4.5358e-04 | norm 0.3081 | dt 339.44ms | 1544588.73 tokens/sec
Step 7117 | loss: 3.235227 | lr:4.5354e-04 | norm 0.3014 | dt 338.81ms | 1547424.47 tokens/sec
Step 7118 | loss: 3.284081 | lr:4.5349e-04 | norm 0.2757 | dt 338.95ms | 1546816.02 tokens/sec
Step 7119 | loss: 3.309520 | lr:4.5345e-04 | norm 0.2988 | dt 338.59ms | 1548444.36 tokens/sec
Step 7120 | loss: 3.292212 | lr:4.5341e-04 | norm 0.3222 | dt 339.55ms | 1544055.14 tokens/sec
Step 7121 | loss: 3.252289 | lr:4.5337e-04 | norm 0.2943 | dt 338.43ms | 1549155.59 tokens/sec
Step 7122 | loss: 3.292714 | lr:4.5333e-04 | norm 0.2598 | dt 339.06ms | 1546284.14 tokens/sec
Step 7123 | loss: 3.290440 | lr:4.5329e-04 | norm 0.3071 | dt 339.12ms | 1546029.76 tokens/sec
Step 7124 | loss: 3.251144 | lr:4.5325e-04 | norm 0.2905 | dt 338.12ms | 1550584.38 tokens/sec
Step 7125 | loss: 3.290491 | lr:4.5321e-04 | norm 0.3236 | dt 337.88ms | 1551703.68 tokens/sec
Step 7126 | loss: 3.298672 | lr:4.5317e-04 | norm 0.2885 | dt 340.78ms | 1538496.11 tokens/sec
Step 7127 | loss: 3.246950 | lr:4.5312e-04 | norm 0.2929 | dt 337.79ms | 1552102.34 tokens/sec
Step 7128 | loss: 3.289148 | lr:4.5308e-04 | norm 0.2991 | dt 338.05ms | 1550904.79 tokens/sec
Step 7129 | loss: 3.273321 | lr:4.5304e-04 | norm 0.2800 | dt 337.81ms | 1552012.52 tokens/sec
Step 7130 | loss: 3.281909 | lr:4.5300e-04 | norm 0.2817 | dt 337.83ms | 1551907.37 tokens/sec
Step 7131 | loss: 3.284307 | lr:4.5296e-04 | norm 0.3122 | dt 338.20ms | 1550228.02 tokens/sec
Step 7132 | loss: 3.288766 | lr:4.5292e-04 | norm 0.2663 | dt 338.78ms | 1547581.29 tokens/sec
Step 7133 | loss: 3.325966 | lr:4.5288e-04 | norm 0.2866 | dt 337.71ms | 1552478.19 tokens/sec
Step 7134 | loss: 3.241362 | lr:4.5284e-04 | norm 0.2733 | dt 339.49ms | 1544355.51 tokens/sec
Step 7135 | loss: 3.287541 | lr:4.5280e-04 | norm 0.2616 | dt 338.94ms | 1546846.48 tokens/sec
Step 7136 | loss: 3.245206 | lr:4.5275e-04 | norm 0.2580 | dt 337.71ms | 1552478.19 tokens/sec
Step 7137 | loss: 3.269356 | lr:4.5271e-04 | norm 0.2615 | dt 338.75ms | 1547730.52 tokens/sec
Step 7138 | loss: 3.238781 | lr:4.5267e-04 | norm 0.2592 | dt 338.70ms | 1547937.52 tokens/sec
Step 7139 | loss: 3.227324 | lr:4.5263e-04 | norm 0.2588 | dt 339.23ms | 1545545.13 tokens/sec
Step 7140 | loss: 3.236216 | lr:4.5259e-04 | norm 0.2736 | dt 337.87ms | 1551738.72 tokens/sec
Step 7141 | loss: 3.222178 | lr:4.5255e-04 | norm 0.2698 | dt 338.01ms | 1551091.86 tokens/sec
Step 7142 | loss: 3.199944 | lr:4.5251e-04 | norm 0.2546 | dt 339.08ms | 1546189.56 tokens/sec
Step 7143 | loss: 3.210261 | lr:4.5247e-04 | norm 0.2528 | dt 338.70ms | 1547956.04 tokens/sec
Step 7144 | loss: 3.237935 | lr:4.5243e-04 | norm 0.2745 | dt 338.34ms | 1549592.24 tokens/sec
Step 7145 | loss: 3.186843 | lr:4.5238e-04 | norm 0.2754 | dt 338.02ms | 1551045.91 tokens/sec
Step 7146 | loss: 3.176485 | lr:4.5234e-04 | norm 0.2636 | dt 338.73ms | 1547792.61 tokens/sec
Step 7147 | loss: 3.181041 | lr:4.5230e-04 | norm 0.2605 | dt 338.49ms | 1548925.35 tokens/sec
Step 7148 | loss: 3.233233 | lr:4.5226e-04 | norm 0.2970 | dt 338.92ms | 1546937.89 tokens/sec
Step 7149 | loss: 3.256330 | lr:4.5222e-04 | norm 0.2826 | dt 339.12ms | 1546012.37 tokens/sec
Step 7150 | loss: 3.224525 | lr:4.5218e-04 | norm 0.2692 | dt 339.71ms | 1543361.58 tokens/sec
Step 7151 | loss: 3.325664 | lr:4.5214e-04 | norm 0.3359 | dt 338.62ms | 1548291.73 tokens/sec
Step 7152 | loss: 3.316207 | lr:4.5210e-04 | norm 0.3552 | dt 338.30ms | 1549761.52 tokens/sec
Step 7153 | loss: 3.269966 | lr:4.5205e-04 | norm 0.3008 | dt 339.18ms | 1545747.21 tokens/sec
Step 7154 | loss: 3.270507 | lr:4.5201e-04 | norm 0.3232 | dt 339.16ms | 1545826.53 tokens/sec
Step 7155 | loss: 3.330169 | lr:4.5197e-04 | norm 0.3020 | dt 339.45ms | 1544499.77 tokens/sec
Step 7156 | loss: 3.295691 | lr:4.5193e-04 | norm 0.2817 | dt 339.38ms | 1544818.77 tokens/sec
Step 7157 | loss: 3.264449 | lr:4.5189e-04 | norm 0.3003 | dt 338.80ms | 1547495.26 tokens/sec
Step 7158 | loss: 3.256461 | lr:4.5185e-04 | norm 0.3045 | dt 338.75ms | 1547734.87 tokens/sec
Step 7159 | loss: 3.307045 | lr:4.5181e-04 | norm 0.3116 | dt 338.11ms | 1550640.14 tokens/sec
Step 7160 | loss: 3.299597 | lr:4.5177e-04 | norm 0.3093 | dt 338.09ms | 1550748.40 tokens/sec
Step 7161 | loss: 3.285563 | lr:4.5172e-04 | norm 0.2763 | dt 337.58ms | 1553084.53 tokens/sec
Step 7162 | loss: 3.292204 | lr:4.5168e-04 | norm 0.3065 | dt 337.29ms | 1554397.51 tokens/sec
Step 7163 | loss: 3.301204 | lr:4.5164e-04 | norm 0.2621 | dt 337.53ms | 1553328.07 tokens/sec
Step 7164 | loss: 3.244313 | lr:4.5160e-04 | norm 0.2698 | dt 338.20ms | 1550250.97 tokens/sec
Step 7165 | loss: 3.287422 | lr:4.5156e-04 | norm 0.2725 | dt 337.67ms | 1552660.15 tokens/sec
Step 7166 | loss: 3.260201 | lr:4.5152e-04 | norm 0.2832 | dt 338.21ms | 1550205.07 tokens/sec
Step 7167 | loss: 3.284272 | lr:4.5148e-04 | norm 0.3015 | dt 337.88ms | 1551678.50 tokens/sec
Step 7168 | loss: 3.275412 | lr:4.5144e-04 | norm 0.2822 | dt 338.09ms | 1550715.59 tokens/sec
Step 7169 | loss: 3.373074 | lr:4.5139e-04 | norm 0.3058 | dt 338.93ms | 1546876.95 tokens/sec
Step 7170 | loss: 3.219635 | lr:4.5135e-04 | norm 0.3044 | dt 338.62ms | 1548309.17 tokens/sec
Step 7171 | loss: 3.292753 | lr:4.5131e-04 | norm 0.2848 | dt 338.92ms | 1546916.12 tokens/sec
Step 7172 | loss: 3.276847 | lr:4.5127e-04 | norm 0.2922 | dt 339.24ms | 1545476.70 tokens/sec
Step 7173 | loss: 3.247901 | lr:4.5123e-04 | norm 0.2886 | dt 338.85ms | 1547279.66 tokens/sec
Step 7174 | loss: 3.202909 | lr:4.5119e-04 | norm 0.2914 | dt 337.73ms | 1552375.17 tokens/sec
Step 7175 | loss: 3.238276 | lr:4.5115e-04 | norm 0.2935 | dt 338.80ms | 1547491.99 tokens/sec
Step 7176 | loss: 3.199645 | lr:4.5111e-04 | norm 0.2806 | dt 339.61ms | 1543812.32 tokens/sec
Step 7177 | loss: 3.226135 | lr:4.5106e-04 | norm 0.2624 | dt 338.50ms | 1548859.89 tokens/sec
Step 7178 | loss: 3.248468 | lr:4.5102e-04 | norm 0.2754 | dt 338.62ms | 1548287.37 tokens/sec
Step 7179 | loss: 3.247591 | lr:4.5098e-04 | norm 0.2887 | dt 338.51ms | 1548824.98 tokens/sec
Step 7180 | loss: 3.289119 | lr:4.5094e-04 | norm 0.2847 | dt 339.49ms | 1544319.72 tokens/sec
Step 7181 | loss: 3.210944 | lr:4.5090e-04 | norm 0.2859 | dt 901.06ms | 581856.70 tokens/sec
Step 7182 | loss: 3.254701 | lr:4.5086e-04 | norm 0.2582 | dt 335.53ms | 1562585.36 tokens/sec
Step 7183 | loss: 3.230651 | lr:4.5082e-04 | norm 0.2715 | dt 339.10ms | 1546122.15 tokens/sec
Step 7184 | loss: 3.239699 | lr:4.5077e-04 | norm 0.2590 | dt 341.03ms | 1537352.77 tokens/sec
Step 7185 | loss: 3.290921 | lr:4.5073e-04 | norm 0.2952 | dt 337.45ms | 1553679.27 tokens/sec
Step 7186 | loss: 3.392634 | lr:4.5069e-04 | norm 0.3131 | dt 338.71ms | 1547907.01 tokens/sec
Step 7187 | loss: 3.339296 | lr:4.5065e-04 | norm 0.2659 | dt 340.07ms | 1541720.13 tokens/sec
Step 7188 | loss: 3.393781 | lr:4.5061e-04 | norm 0.3507 | dt 339.18ms | 1545767.85 tokens/sec
Step 7189 | loss: 3.276796 | lr:4.5057e-04 | norm 0.3344 | dt 338.64ms | 1548205.61 tokens/sec
Step 7190 | loss: 3.268991 | lr:4.5053e-04 | norm 0.2705 | dt 340.14ms | 1541381.88 tokens/sec
Step 7191 | loss: 3.251163 | lr:4.5049e-04 | norm 0.2905 | dt 337.76ms | 1552253.54 tokens/sec
Step 7192 | loss: 3.289981 | lr:4.5044e-04 | norm 0.2625 | dt 338.08ms | 1550781.20 tokens/sec
Step 7193 | loss: 3.331946 | lr:4.5040e-04 | norm 0.2780 | dt 337.69ms | 1552594.38 tokens/sec
Step 7194 | loss: 3.267449 | lr:4.5036e-04 | norm 0.2626 | dt 338.93ms | 1546879.13 tokens/sec
Step 7195 | loss: 3.293384 | lr:4.5032e-04 | norm 0.2818 | dt 340.82ms | 1538333.59 tokens/sec
Step 7196 | loss: 3.288963 | lr:4.5028e-04 | norm 0.2885 | dt 338.83ms | 1547333.01 tokens/sec
Step 7197 | loss: 3.284911 | lr:4.5024e-04 | norm 0.2708 | dt 339.20ms | 1545671.15 tokens/sec
Step 7198 | loss: 3.298057 | lr:4.5020e-04 | norm 0.2545 | dt 339.35ms | 1544964.20 tokens/sec
Step 7199 | loss: 3.239028 | lr:4.5015e-04 | norm 0.2802 | dt 339.14ms | 1545953.68 tokens/sec
Step 7200 | loss: 3.262986 | lr:4.5011e-04 | norm 0.2598 | dt 338.45ms | 1549067.19 tokens/sec
Step 7201 | loss: 3.254068 | lr:4.5007e-04 | norm 0.2803 | dt 339.23ms | 1545511.46 tokens/sec
Step 7202 | loss: 3.278497 | lr:4.5003e-04 | norm 0.2720 | dt 337.57ms | 1553125.11 tokens/sec
Step 7203 | loss: 3.256887 | lr:4.4999e-04 | norm 0.2756 | dt 338.13ms | 1550536.27 tokens/sec
Step 7204 | loss: 3.346911 | lr:4.4995e-04 | norm 0.2784 | dt 338.84ms | 1547294.91 tokens/sec
Step 7205 | loss: 3.282444 | lr:4.4991e-04 | norm 0.2895 | dt 338.35ms | 1549552.94 tokens/sec
Step 7206 | loss: 3.275918 | lr:4.4986e-04 | norm 0.3280 | dt 337.64ms | 1552792.81 tokens/sec
Step 7207 | loss: 3.279605 | lr:4.4982e-04 | norm 0.2882 | dt 339.34ms | 1545004.37 tokens/sec
Step 7208 | loss: 3.243382 | lr:4.4978e-04 | norm 0.2899 | dt 337.89ms | 1551636.90 tokens/sec
Step 7209 | loss: 3.247084 | lr:4.4974e-04 | norm 0.3000 | dt 337.80ms | 1552071.67 tokens/sec
Step 7210 | loss: 3.244706 | lr:4.4970e-04 | norm 0.2825 | dt 338.72ms | 1547851.44 tokens/sec
Step 7211 | loss: 3.270332 | lr:4.4966e-04 | norm 0.2876 | dt 338.32ms | 1549689.44 tokens/sec
Step 7212 | loss: 3.274198 | lr:4.4962e-04 | norm 0.2728 | dt 337.83ms | 1551942.42 tokens/sec
Step 7213 | loss: 3.245167 | lr:4.4958e-04 | norm 0.2755 | dt 338.42ms | 1549202.51 tokens/sec
Step 7214 | loss: 3.197553 | lr:4.4953e-04 | norm 0.2973 | dt 338.85ms | 1547257.89 tokens/sec
Step 7215 | loss: 3.221441 | lr:4.4949e-04 | norm 0.3120 | dt 338.12ms | 1550575.63 tokens/sec
Step 7216 | loss: 3.203495 | lr:4.4945e-04 | norm 0.3490 | dt 338.66ms | 1548121.69 tokens/sec
Step 7217 | loss: 3.196475 | lr:4.4941e-04 | norm 0.3187 | dt 337.73ms | 1552393.80 tokens/sec
Step 7218 | loss: 3.234063 | lr:4.4937e-04 | norm 0.3005 | dt 338.47ms | 1549010.45 tokens/sec
Step 7219 | loss: 3.316248 | lr:4.4933e-04 | norm 0.3416 | dt 1051.36ms | 498674.47 tokens/sec
Step 7220 | loss: 3.275775 | lr:4.4928e-04 | norm 0.2993 | dt 336.04ms | 1560177.41 tokens/sec
Step 7221 | loss: 3.271518 | lr:4.4924e-04 | norm 0.2924 | dt 337.49ms | 1553493.77 tokens/sec
Step 7222 | loss: 3.275286 | lr:4.4920e-04 | norm 0.3112 | dt 338.78ms | 1547598.72 tokens/sec
Step 7223 | loss: 3.287554 | lr:4.4916e-04 | norm 0.2891 | dt 337.27ms | 1554490.91 tokens/sec
Step 7224 | loss: 3.269124 | lr:4.4912e-04 | norm 0.3000 | dt 337.44ms | 1553700.12 tokens/sec
Step 7225 | loss: 3.255725 | lr:4.4908e-04 | norm 0.2697 | dt 337.53ms | 1553316.01 tokens/sec
Step 7226 | loss: 3.272354 | lr:4.4904e-04 | norm 0.2767 | dt 338.36ms | 1549476.51 tokens/sec
Step 7227 | loss: 3.310087 | lr:4.4899e-04 | norm 0.3893 | dt 339.75ms | 1543139.56 tokens/sec
Step 7228 | loss: 3.285530 | lr:4.4895e-04 | norm 0.3096 | dt 338.49ms | 1548919.89 tokens/sec
Step 7229 | loss: 3.255089 | lr:4.4891e-04 | norm 0.2889 | dt 338.91ms | 1546998.83 tokens/sec
Step 7230 | loss: 3.290020 | lr:4.4887e-04 | norm 0.2715 | dt 338.56ms | 1548564.30 tokens/sec
Step 7231 | loss: 3.284081 | lr:4.4883e-04 | norm 0.2828 | dt 338.93ms | 1546910.68 tokens/sec
Step 7232 | loss: 3.246489 | lr:4.4879e-04 | norm 0.2780 | dt 343.65ms | 1525652.23 tokens/sec
Step 7233 | loss: 3.332183 | lr:4.4875e-04 | norm 0.2649 | dt 338.68ms | 1548037.77 tokens/sec
Step 7234 | loss: 3.288148 | lr:4.4870e-04 | norm 0.2740 | dt 341.30ms | 1536155.33 tokens/sec
Step 7235 | loss: 3.232656 | lr:4.4866e-04 | norm 0.2780 | dt 339.28ms | 1545296.42 tokens/sec
Step 7236 | loss: 3.247914 | lr:4.4862e-04 | norm 0.2846 | dt 342.57ms | 1530445.21 tokens/sec
Step 7237 | loss: 3.310140 | lr:4.4858e-04 | norm 0.3118 | dt 339.58ms | 1543908.79 tokens/sec
Step 7238 | loss: 3.279688 | lr:4.4854e-04 | norm 0.2678 | dt 338.67ms | 1548081.36 tokens/sec
Step 7239 | loss: 3.327061 | lr:4.4850e-04 | norm 0.2871 | dt 339.90ms | 1542486.86 tokens/sec
Step 7240 | loss: 3.264462 | lr:4.4846e-04 | norm 0.2927 | dt 338.94ms | 1546855.19 tokens/sec
Step 7241 | loss: 3.230944 | lr:4.4841e-04 | norm 0.2831 | dt 338.93ms | 1546906.33 tokens/sec
Step 7242 | loss: 3.213228 | lr:4.4837e-04 | norm 0.2795 | dt 338.31ms | 1549721.11 tokens/sec
Step 7243 | loss: 3.249581 | lr:4.4833e-04 | norm 0.2747 | dt 338.83ms | 1547347.17 tokens/sec
Step 7244 | loss: 3.208841 | lr:4.4829e-04 | norm 0.3054 | dt 340.71ms | 1538798.63 tokens/sec
Step 7245 | loss: 3.192255 | lr:4.4825e-04 | norm 0.2832 | dt 338.21ms | 1550165.73 tokens/sec
Step 7246 | loss: 3.191595 | lr:4.4821e-04 | norm 0.2823 | dt 338.98ms | 1546666.97 tokens/sec
Step 7247 | loss: 3.233596 | lr:4.4816e-04 | norm 0.2617 | dt 338.11ms | 1550653.26 tokens/sec
Step 7248 | loss: 3.248436 | lr:4.4812e-04 | norm 0.2593 | dt 338.05ms | 1550922.30 tokens/sec
Step 7249 | loss: 3.240345 | lr:4.4808e-04 | norm 0.2575 | dt 338.31ms | 1549704.72 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 7250: 3.2713
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2846/10042=0.2834


ddp_rank 5: ####### Printing generated samples ####### 


rank 5 sample 0 >Hello, I'm a language model, but a little bit like the one that uses a native English. Here I'm speaking English, and I'm learning it
rank 5 sample 1 >Hello, I'm a language model, like, in, how-to, what’s in some cases actually an expression. (So) to take

ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 2 >Hello, I'm a language model, and you have a number of good ways to communicate with your students. I've had teachers who have used the language and
rank 5 sample 3 >Hello, I'm a language model, as you read.
This is a good way to understand the world of your work in the area you're studying.


rank 3 sample 0 >Hello, I'm a language model, so I'm not saying the language system of my model is backwards, especially about the future!
Well, you get
rank 3 sample 1 >Hello, I'm a language model, so it was a little post that I had to leave that I was never really finished.
But, I can think
rank 3 sample 2 >Hello, I'm a language model, so let's write I'm a language model. And, what's that?" If you have another word that uses that
rank 3 sample 3 >Hello, I'm a language model, so today I'll look at my favorite. Here I've looked at some of the ways I can't translate into English




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, language learner for the beginner, as a teacher, teacher or a writer, in the classroom, in front of students
rank 2 sample 1 >Hello, I'm a language model, meaning I hate that! I'm thinking:
Okay, let's write a sentence like this, with a word for
rank 2 sample 2 >Hello, I'm a language model, and I'll show you how I have designed my own language for your learning. Here's how...
In the second
rank 2 sample 3 >Hello, I'm a language model, but this problem is what's the problem to me of the book.
That's it I could write an answer as




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I think it's really important to explain the terms to you. As I teach the grammar basics and grammar, it
rank 7 sample 1 >Hello, I'm a language model, learning to say "hello, thank you" is called greeting. Here's a cute example from the dictionary, you don
rank 7 sample 2 >Hello, I'm a language model, so I'll keep it. However, this is all about the use of the language, so I'll leave all this
rank 7 sample 3 >Hello, I'm a language model, is not a language, the answer is rather in the opposite. I understand that language is a linguistic structure, but for




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, here in the U.S., the language model in this post [source: BBC].<|endoftext|>The International Space Station (
rank 6 sample 1 >Hello, I'm a language model, so I don't know if anyone is working on my own.
And I've just been learning to code.

rank 6 sample 2 >Hello, I'm a language model, but what is it really about the first letter of the alphabet, when I speak English, that's another part! I
rank 6 sample 3 >Hello, I'm a language model, so if you want to learn some of the most common words in your language (like Spanish English, Turkish French, Serbian




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I will be writing and typing for free by the end of Year 3 in the year 2039!<|endoftext|>The history
rank 0 sample 1 >Hello, I'm a language model, and can't do that. So today I want to know how to add some text into a text so that it is
rank 0 sample 2 >Hello, I'm a language model, and I really love coding.
As a high school student it seems like I'm not as good as I think I
rank 0 sample 3 >Hello, I'm a language model, so for the first time I've learnt how to write a program written in a 2D printer, without having to have





ddp_rank 4: ####### Printing generated samples ####### 


ddp_rank 1: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I am a teacher. I am doing my best to be a writer and I don't forget to get a job
rank 4 sample 1 >Hello, I'm a language model, thanks for the post I wrote at the end. And just like for those people at the beginning, this post gets allrank 1 sample 0 >Hello, I'm a language model, so please help me out by leaving, so far? The one I'm always looking for is English.<|endoftext|>The first

rank 1 sample 1 >Hello, I'm a language model, a teacher and a teacher. I'll work with English speakers in New Zealand in preparation and take online courses. I'm
rank 4 sample 2 >Hello, I'm a language model, I teach for years in London. But my only job is for high school students. My dream job is to work in
rank 4 sample 3 >Hello, I'm a language model, so here have an article that I am trying to describe in class:
Now, I am in a class which has


rank 1 sample 2 >Hello, I'm a language model, but just wanted to be a teacher. I'm a big student. And I'm a good language model, right?
rank 1 sample 3 >Hello, I'm a language model, so I'm sure I did some programming right now. Since I used an OpenOffice template instead of the Google docs,


Step 7250 | loss: 3.246058 | lr:4.4804e-04 | norm 0.2650 | dt 18742.45ms | 27973.29 tokens/sec
Step 7251 | loss: 3.263061 | lr:4.4800e-04 | norm 0.2559 | dt 335.67ms | 1561934.97 tokens/sec
Step 7252 | loss: 3.235516 | lr:4.4796e-04 | norm 0.2907 | dt 335.68ms | 1561846.22 tokens/sec
Step 7253 | loss: 3.223302 | lr:4.4792e-04 | norm 0.2811 | dt 336.92ms | 1556127.75 tokens/sec
Step 7254 | loss: 3.273653 | lr:4.4787e-04 | norm 0.2765 | dt 339.05ms | 1546348.30 tokens/sec
Step 7255 | loss: 3.170912 | lr:4.4783e-04 | norm 0.2795 | dt 336.04ms | 1560185.16 tokens/sec
Step 7256 | loss: 3.230715 | lr:4.4779e-04 | norm 0.2933 | dt 335.65ms | 1562022.62 tokens/sec
Step 7257 | loss: 3.294911 | lr:4.4775e-04 | norm 0.3445 | dt 336.23ms | 1559303.43 tokens/sec
Step 7258 | loss: 3.322587 | lr:4.4771e-04 | norm 0.3747 | dt 338.16ms | 1550429.13 tokens/sec
Step 7259 | loss: 3.277277 | lr:4.4767e-04 | norm 0.3240 | dt 337.03ms | 1555603.76 tokens/sec
Step 7260 | loss: 3.248525 | lr:4.4762e-04 | norm 0.3216 | dt 337.24ms | 1554665.65 tokens/sec
Step 7261 | loss: 3.286666 | lr:4.4758e-04 | norm 0.2922 | dt 338.34ms | 1549611.90 tokens/sec
Step 7262 | loss: 3.246984 | lr:4.4754e-04 | norm 0.3007 | dt 337.33ms | 1554216.24 tokens/sec
Step 7263 | loss: 3.222646 | lr:4.4750e-04 | norm 0.2840 | dt 337.92ms | 1551537.27 tokens/sec
Step 7264 | loss: 3.320838 | lr:4.4746e-04 | norm 0.2841 | dt 337.07ms | 1555413.41 tokens/sec
Step 7265 | loss: 3.287498 | lr:4.4742e-04 | norm 0.2938 | dt 337.76ms | 1552250.25 tokens/sec
Step 7266 | loss: 3.288206 | lr:4.4737e-04 | norm 0.2775 | dt 337.33ms | 1554206.35 tokens/sec
Step 7267 | loss: 3.302576 | lr:4.4733e-04 | norm 0.3241 | dt 339.23ms | 1545520.15 tokens/sec
Step 7268 | loss: 3.303190 | lr:4.4729e-04 | norm 0.2891 | dt 338.62ms | 1548313.53 tokens/sec
Step 7269 | loss: 3.239810 | lr:4.4725e-04 | norm 0.2867 | dt 338.49ms | 1548882.80 tokens/sec
Step 7270 | loss: 3.276022 | lr:4.4721e-04 | norm 0.2961 | dt 337.51ms | 1553404.88 tokens/sec
Step 7271 | loss: 3.287774 | lr:4.4717e-04 | norm 0.3200 | dt 336.97ms | 1555907.54 tokens/sec
Step 7272 | loss: 3.250265 | lr:4.4712e-04 | norm 0.2759 | dt 337.84ms | 1551883.27 tokens/sec
Step 7273 | loss: 3.216718 | lr:4.4708e-04 | norm 0.2537 | dt 338.86ms | 1547192.57 tokens/sec
Step 7274 | loss: 3.305046 | lr:4.4704e-04 | norm 0.2943 | dt 337.69ms | 1552571.36 tokens/sec
Step 7275 | loss: 3.244475 | lr:4.4700e-04 | norm 0.2633 | dt 337.92ms | 1551515.38 tokens/sec
Step 7276 | loss: 3.220303 | lr:4.4696e-04 | norm 0.2561 | dt 338.07ms | 1550823.86 tokens/sec
Step 7277 | loss: 3.256172 | lr:4.4692e-04 | norm 0.2567 | dt 339.11ms | 1546060.19 tokens/sec
Step 7278 | loss: 3.259386 | lr:4.4687e-04 | norm 0.2654 | dt 337.69ms | 1552571.36 tokens/sec
Step 7279 | loss: 3.228033 | lr:4.4683e-04 | norm 0.2453 | dt 339.02ms | 1546490.76 tokens/sec
Step 7280 | loss: 3.279883 | lr:4.4679e-04 | norm 0.2849 | dt 338.15ms | 1550445.53 tokens/sec
Step 7281 | loss: 3.270153 | lr:4.4675e-04 | norm 0.2783 | dt 338.26ms | 1549975.62 tokens/sec
Step 7282 | loss: 3.195629 | lr:4.4671e-04 | norm 0.2768 | dt 338.48ms | 1548938.44 tokens/sec
Step 7283 | loss: 3.229552 | lr:4.4667e-04 | norm 0.2836 | dt 339.76ms | 1543108.16 tokens/sec
Step 7284 | loss: 3.232309 | lr:4.4663e-04 | norm 0.2867 | dt 337.56ms | 1553148.15 tokens/sec
Step 7285 | loss: 3.247134 | lr:4.4658e-04 | norm 0.2703 | dt 337.51ms | 1553377.45 tokens/sec
Step 7286 | loss: 3.227444 | lr:4.4654e-04 | norm 0.2644 | dt 338.09ms | 1550741.83 tokens/sec
Step 7287 | loss: 3.228900 | lr:4.4650e-04 | norm 0.2451 | dt 339.09ms | 1546148.24 tokens/sec
Step 7288 | loss: 3.335898 | lr:4.4646e-04 | norm 0.2747 | dt 338.75ms | 1547694.57 tokens/sec
Step 7289 | loss: 3.241775 | lr:4.4642e-04 | norm 0.2773 | dt 339.47ms | 1544445.54 tokens/sec
Step 7290 | loss: 3.232226 | lr:4.4637e-04 | norm 0.2483 | dt 340.27ms | 1540783.57 tokens/sec
Step 7291 | loss: 3.315028 | lr:4.4633e-04 | norm 0.3394 | dt 338.91ms | 1546983.59 tokens/sec
Step 7292 | loss: 3.236428 | lr:4.4629e-04 | norm 0.3318 | dt 339.70ms | 1543397.33 tokens/sec
Step 7293 | loss: 3.222494 | lr:4.4625e-04 | norm 0.3179 | dt 339.00ms | 1546591.91 tokens/sec
Step 7294 | loss: 3.243966 | lr:4.4621e-04 | norm 0.2938 | dt 339.60ms | 1543831.83 tokens/sec
Step 7295 | loss: 3.238698 | lr:4.4617e-04 | norm 0.2927 | dt 338.56ms | 1548575.21 tokens/sec
Step 7296 | loss: 3.231688 | lr:4.4612e-04 | norm 0.3261 | dt 339.63ms | 1543719.12 tokens/sec
Step 7297 | loss: 3.271774 | lr:4.4608e-04 | norm 0.2840 | dt 338.66ms | 1548124.96 tokens/sec
Step 7298 | loss: 3.300238 | lr:4.4604e-04 | norm 0.3086 | dt 339.16ms | 1545862.39 tokens/sec
Step 7299 | loss: 3.302662 | lr:4.4600e-04 | norm 0.2796 | dt 337.58ms | 1553096.59 tokens/sec
Step 7300 | loss: 3.283784 | lr:4.4596e-04 | norm 0.2975 | dt 338.03ms | 1550995.59 tokens/sec
Step 7301 | loss: 3.336547 | lr:4.4592e-04 | norm 0.3027 | dt 337.85ms | 1551826.33 tokens/sec
Step 7302 | loss: 3.281337 | lr:4.4587e-04 | norm 0.2949 | dt 338.09ms | 1550741.83 tokens/sec
Step 7303 | loss: 3.276886 | lr:4.4583e-04 | norm 0.2933 | dt 337.95ms | 1551374.18 tokens/sec
Step 7304 | loss: 3.262552 | lr:4.4579e-04 | norm 0.2657 | dt 337.99ms | 1551173.92 tokens/sec
Step 7305 | loss: 3.297591 | lr:4.4575e-04 | norm 0.2779 | dt 338.24ms | 1550063.02 tokens/sec
Step 7306 | loss: 3.234283 | lr:4.4571e-04 | norm 0.2687 | dt 337.96ms | 1551313.99 tokens/sec
Step 7307 | loss: 3.259011 | lr:4.4567e-04 | norm 0.2897 | dt 338.63ms | 1548274.28 tokens/sec
Step 7308 | loss: 3.223989 | lr:4.4562e-04 | norm 0.2718 | dt 338.05ms | 1550933.23 tokens/sec
Step 7309 | loss: 3.286965 | lr:4.4558e-04 | norm 0.2764 | dt 338.32ms | 1549703.63 tokens/sec
Step 7310 | loss: 3.293525 | lr:4.4554e-04 | norm 0.2934 | dt 338.35ms | 1549558.40 tokens/sec
Step 7311 | loss: 3.294383 | lr:4.4550e-04 | norm 0.3023 | dt 338.16ms | 1550429.13 tokens/sec
Step 7312 | loss: 3.214992 | lr:4.4546e-04 | norm 0.2685 | dt 337.90ms | 1551605.15 tokens/sec
Step 7313 | loss: 3.233455 | lr:4.4542e-04 | norm 0.2592 | dt 337.67ms | 1552677.69 tokens/sec
Step 7314 | loss: 3.194577 | lr:4.4537e-04 | norm 0.2795 | dt 339.13ms | 1545986.28 tokens/sec
Step 7315 | loss: 3.320208 | lr:4.4533e-04 | norm 0.2912 | dt 337.85ms | 1551830.71 tokens/sec
Step 7316 | loss: 3.248524 | lr:4.4529e-04 | norm 0.2852 | dt 338.72ms | 1547859.07 tokens/sec
Step 7317 | loss: 3.253206 | lr:4.4525e-04 | norm 0.2998 | dt 337.87ms | 1551746.39 tokens/sec
Step 7318 | loss: 3.246563 | lr:4.4521e-04 | norm 0.3145 | dt 338.50ms | 1548878.43 tokens/sec
Step 7319 | loss: 3.285721 | lr:4.4516e-04 | norm 0.2805 | dt 338.00ms | 1551159.69 tokens/sec
Step 7320 | loss: 3.218478 | lr:4.4512e-04 | norm 0.2713 | dt 338.66ms | 1548134.76 tokens/sec
Step 7321 | loss: 3.243152 | lr:4.4508e-04 | norm 0.2695 | dt 339.07ms | 1546252.61 tokens/sec
Step 7322 | loss: 3.243273 | lr:4.4504e-04 | norm 0.2764 | dt 338.20ms | 1550231.30 tokens/sec
Step 7323 | loss: 3.266290 | lr:4.4500e-04 | norm 0.2632 | dt 337.97ms | 1551296.48 tokens/sec
Step 7324 | loss: 3.270717 | lr:4.4496e-04 | norm 0.2726 | dt 338.08ms | 1550796.52 tokens/sec
Step 7325 | loss: 3.224717 | lr:4.4491e-04 | norm 0.3009 | dt 338.38ms | 1549426.29 tokens/sec
Step 7326 | loss: 3.192466 | lr:4.4487e-04 | norm 0.2727 | dt 338.29ms | 1549808.48 tokens/sec
Step 7327 | loss: 3.253567 | lr:4.4483e-04 | norm 0.2816 | dt 338.85ms | 1547251.36 tokens/sec
Step 7328 | loss: 3.275970 | lr:4.4479e-04 | norm 0.2890 | dt 337.70ms | 1552511.07 tokens/sec
Step 7329 | loss: 3.277056 | lr:4.4475e-04 | norm 0.2697 | dt 338.24ms | 1550069.58 tokens/sec
Step 7330 | loss: 3.239480 | lr:4.4470e-04 | norm 0.2894 | dt 337.90ms | 1551586.54 tokens/sec
Step 7331 | loss: 3.246639 | lr:4.4466e-04 | norm 0.2596 | dt 337.95ms | 1551362.14 tokens/sec
Step 7332 | loss: 3.229517 | lr:4.4462e-04 | norm 0.2657 | dt 338.09ms | 1550756.05 tokens/sec
Step 7333 | loss: 3.304854 | lr:4.4458e-04 | norm 0.2799 | dt 337.30ms | 1554359.05 tokens/sec
Step 7334 | loss: 3.321107 | lr:4.4454e-04 | norm 0.3019 | dt 338.05ms | 1550908.08 tokens/sec
Step 7335 | loss: 3.221935 | lr:4.4450e-04 | norm 0.2895 | dt 338.04ms | 1550971.52 tokens/sec
Step 7336 | loss: 3.293380 | lr:4.4445e-04 | norm 0.2679 | dt 337.79ms | 1552124.25 tokens/sec
Step 7337 | loss: 3.276104 | lr:4.4441e-04 | norm 0.2813 | dt 338.04ms | 1550950.74 tokens/sec
Step 7338 | loss: 3.263289 | lr:4.4437e-04 | norm 0.2940 | dt 338.48ms | 1548947.17 tokens/sec
Step 7339 | loss: 3.262417 | lr:4.4433e-04 | norm 0.2596 | dt 337.38ms | 1554006.46 tokens/sec
Step 7340 | loss: 3.277074 | lr:4.4429e-04 | norm 0.2905 | dt 338.06ms | 1550871.98 tokens/sec
Step 7341 | loss: 3.226630 | lr:4.4424e-04 | norm 0.2917 | dt 337.64ms | 1552792.81 tokens/sec
Step 7342 | loss: 3.263701 | lr:4.4420e-04 | norm 0.2862 | dt 338.66ms | 1548122.78 tokens/sec
Step 7343 | loss: 3.250162 | lr:4.4416e-04 | norm 0.2830 | dt 338.01ms | 1551097.33 tokens/sec
Step 7344 | loss: 3.289915 | lr:4.4412e-04 | norm 0.2709 | dt 337.48ms | 1553529.99 tokens/sec
Step 7345 | loss: 3.205733 | lr:4.4408e-04 | norm 0.2759 | dt 338.65ms | 1548184.90 tokens/sec
Step 7346 | loss: 3.267585 | lr:4.4403e-04 | norm 0.2932 | dt 337.61ms | 1552940.85 tokens/sec
Step 7347 | loss: 3.246319 | lr:4.4399e-04 | norm 0.3261 | dt 337.94ms | 1551430.00 tokens/sec
Step 7348 | loss: 3.272654 | lr:4.4395e-04 | norm 0.3693 | dt 337.98ms | 1551245.04 tokens/sec
Step 7349 | loss: 3.271021 | lr:4.4391e-04 | norm 0.3505 | dt 338.25ms | 1550014.95 tokens/sec
Step 7350 | loss: 3.250122 | lr:4.4387e-04 | norm 0.2836 | dt 339.06ms | 1546305.89 tokens/sec
Step 7351 | loss: 3.224493 | lr:4.4383e-04 | norm 0.3004 | dt 337.88ms | 1551687.26 tokens/sec
Step 7352 | loss: 3.235017 | lr:4.4378e-04 | norm 0.2909 | dt 338.11ms | 1550633.58 tokens/sec
Step 7353 | loss: 3.208341 | lr:4.4374e-04 | norm 0.2890 | dt 337.92ms | 1551523.04 tokens/sec
Step 7354 | loss: 3.263013 | lr:4.4370e-04 | norm 0.2670 | dt 337.98ms | 1551252.70 tokens/sec
Step 7355 | loss: 3.210927 | lr:4.4366e-04 | norm 0.2781 | dt 338.44ms | 1549145.76 tokens/sec
Step 7356 | loss: 3.235945 | lr:4.4362e-04 | norm 0.2671 | dt 337.81ms | 1552023.47 tokens/sec
Step 7357 | loss: 3.258973 | lr:4.4357e-04 | norm 0.2716 | dt 337.80ms | 1552061.81 tokens/sec
Step 7358 | loss: 3.159088 | lr:4.4353e-04 | norm 0.3363 | dt 337.85ms | 1551852.61 tokens/sec
Step 7359 | loss: 3.257099 | lr:4.4349e-04 | norm 0.2826 | dt 338.39ms | 1549376.07 tokens/sec
Step 7360 | loss: 3.331558 | lr:4.4345e-04 | norm 0.3344 | dt 337.43ms | 1553759.40 tokens/sec
Step 7361 | loss: 3.174114 | lr:4.4341e-04 | norm 0.3350 | dt 337.69ms | 1552556.01 tokens/sec
Step 7362 | loss: 3.219943 | lr:4.4336e-04 | norm 0.2772 | dt 338.11ms | 1550620.46 tokens/sec
Step 7363 | loss: 3.232634 | lr:4.4332e-04 | norm 0.2848 | dt 338.31ms | 1549732.03 tokens/sec
Step 7364 | loss: 3.203565 | lr:4.4328e-04 | norm 0.2962 | dt 337.84ms | 1551892.04 tokens/sec
Step 7365 | loss: 3.248276 | lr:4.4324e-04 | norm 0.2894 | dt 337.99ms | 1551178.29 tokens/sec
Step 7366 | loss: 3.265701 | lr:4.4320e-04 | norm 0.2819 | dt 337.93ms | 1551461.74 tokens/sec
Step 7367 | loss: 3.295937 | lr:4.4315e-04 | norm 0.2628 | dt 338.36ms | 1549483.06 tokens/sec
Step 7368 | loss: 3.289109 | lr:4.4311e-04 | norm 0.2640 | dt 339.12ms | 1546037.37 tokens/sec
Step 7369 | loss: 3.241754 | lr:4.4307e-04 | norm 0.2504 | dt 338.52ms | 1548754.08 tokens/sec
Step 7370 | loss: 3.314662 | lr:4.4303e-04 | norm 0.2883 | dt 1018.22ms | 514908.00 tokens/sec
Step 7371 | loss: 3.245688 | lr:4.4299e-04 | norm 0.2996 | dt 336.38ms | 1558617.10 tokens/sec
Step 7372 | loss: 3.238526 | lr:4.4294e-04 | norm 0.2788 | dt 337.65ms | 1552778.56 tokens/sec
Step 7373 | loss: 3.330892 | lr:4.4290e-04 | norm 0.3366 | dt 338.33ms | 1549617.36 tokens/sec
Step 7374 | loss: 3.288041 | lr:4.4286e-04 | norm 0.2870 | dt 338.28ms | 1549875.11 tokens/sec
Step 7375 | loss: 3.238807 | lr:4.4282e-04 | norm 0.2709 | dt 337.50ms | 1553434.51 tokens/sec
Step 7376 | loss: 3.271744 | lr:4.4278e-04 | norm 0.3087 | dt 338.62ms | 1548306.99 tokens/sec
Step 7377 | loss: 3.239247 | lr:4.4273e-04 | norm 0.3333 | dt 337.27ms | 1554494.20 tokens/sec
Step 7378 | loss: 3.252779 | lr:4.4269e-04 | norm 0.2810 | dt 337.76ms | 1552254.63 tokens/sec
Step 7379 | loss: 3.277964 | lr:4.4265e-04 | norm 0.2717 | dt 338.68ms | 1548053.03 tokens/sec
Step 7380 | loss: 3.270458 | lr:4.4261e-04 | norm 0.2814 | dt 338.30ms | 1549753.87 tokens/sec
Step 7381 | loss: 3.236813 | lr:4.4257e-04 | norm 0.3025 | dt 338.34ms | 1549588.97 tokens/sec
Step 7382 | loss: 3.219837 | lr:4.4252e-04 | norm 0.2955 | dt 338.70ms | 1547929.89 tokens/sec
Step 7383 | loss: 3.233094 | lr:4.4248e-04 | norm 0.2625 | dt 339.08ms | 1546188.47 tokens/sec
Step 7384 | loss: 3.236706 | lr:4.4244e-04 | norm 0.2736 | dt 338.16ms | 1550402.90 tokens/sec
Step 7385 | loss: 3.263596 | lr:4.4240e-04 | norm 0.2735 | dt 337.98ms | 1551249.42 tokens/sec
Step 7386 | loss: 3.188495 | lr:4.4236e-04 | norm 0.2773 | dt 338.03ms | 1551008.71 tokens/sec
Step 7387 | loss: 3.304224 | lr:4.4231e-04 | norm 0.3092 | dt 338.63ms | 1548245.94 tokens/sec
Step 7388 | loss: 3.224206 | lr:4.4227e-04 | norm 0.2744 | dt 337.73ms | 1552382.84 tokens/sec
Step 7389 | loss: 3.243798 | lr:4.4223e-04 | norm 0.2715 | dt 338.16ms | 1550406.18 tokens/sec
Step 7390 | loss: 3.274659 | lr:4.4219e-04 | norm 0.2746 | dt 337.86ms | 1551805.52 tokens/sec
Step 7391 | loss: 3.183837 | lr:4.4215e-04 | norm 0.2734 | dt 338.18ms | 1550311.09 tokens/sec
Step 7392 | loss: 3.214224 | lr:4.4210e-04 | norm 0.3076 | dt 338.06ms | 1550870.89 tokens/sec
Step 7393 | loss: 3.263935 | lr:4.4206e-04 | norm 0.2925 | dt 337.87ms | 1551759.53 tokens/sec
Step 7394 | loss: 3.245339 | lr:4.4202e-04 | norm 0.2984 | dt 339.18ms | 1545767.85 tokens/sec
Step 7395 | loss: 3.271998 | lr:4.4198e-04 | norm 0.3077 | dt 339.56ms | 1544036.71 tokens/sec
Step 7396 | loss: 3.247458 | lr:4.4194e-04 | norm 0.3160 | dt 338.90ms | 1547010.80 tokens/sec
Step 7397 | loss: 3.208346 | lr:4.4189e-04 | norm 0.2751 | dt 338.87ms | 1547178.42 tokens/sec
Step 7398 | loss: 3.244343 | lr:4.4185e-04 | norm 0.3157 | dt 338.01ms | 1551120.30 tokens/sec
Step 7399 | loss: 3.266413 | lr:4.4181e-04 | norm 0.2806 | dt 338.30ms | 1549760.42 tokens/sec
Step 7400 | loss: 3.282984 | lr:4.4177e-04 | norm 0.3495 | dt 338.80ms | 1547507.23 tokens/sec
Step 7401 | loss: 3.268389 | lr:4.4173e-04 | norm 0.3345 | dt 338.56ms | 1548586.12 tokens/sec
Step 7402 | loss: 3.266096 | lr:4.4168e-04 | norm 0.3429 | dt 338.41ms | 1549264.73 tokens/sec
Step 7403 | loss: 3.244668 | lr:4.4164e-04 | norm 0.2929 | dt 339.03ms | 1546433.12 tokens/sec
Step 7404 | loss: 3.251615 | lr:4.4160e-04 | norm 0.2950 | dt 338.95ms | 1546809.49 tokens/sec
Step 7405 | loss: 3.233681 | lr:4.4156e-04 | norm 0.3066 | dt 338.08ms | 1550799.80 tokens/sec
Step 7406 | loss: 3.276509 | lr:4.4152e-04 | norm 0.3229 | dt 338.73ms | 1547826.38 tokens/sec
Step 7407 | loss: 3.264344 | lr:4.4147e-04 | norm 0.3049 | dt 337.79ms | 1552119.87 tokens/sec
Step 7408 | loss: 3.313004 | lr:4.4143e-04 | norm 0.3177 | dt 338.59ms | 1548436.73 tokens/sec
Step 7409 | loss: 3.308822 | lr:4.4139e-04 | norm 0.2729 | dt 1044.74ms | 501835.88 tokens/sec
Step 7410 | loss: 3.280643 | lr:4.4135e-04 | norm 0.2864 | dt 334.09ms | 1569283.79 tokens/sec
Step 7411 | loss: 3.231576 | lr:4.4131e-04 | norm 0.2896 | dt 338.53ms | 1548710.45 tokens/sec
Step 7412 | loss: 3.262929 | lr:4.4126e-04 | norm 0.2578 | dt 340.55ms | 1539515.03 tokens/sec
Step 7413 | loss: 3.332026 | lr:4.4122e-04 | norm 0.2942 | dt 336.89ms | 1556266.51 tokens/sec
Step 7414 | loss: 3.230618 | lr:4.4118e-04 | norm 0.2722 | dt 337.44ms | 1553700.12 tokens/sec
Step 7415 | loss: 3.233080 | lr:4.4114e-04 | norm 0.2957 | dt 337.48ms | 1553556.33 tokens/sec
Step 7416 | loss: 3.267034 | lr:4.4109e-04 | norm 0.2850 | dt 336.77ms | 1556803.07 tokens/sec
Step 7417 | loss: 3.296940 | lr:4.4105e-04 | norm 0.2690 | dt 339.12ms | 1546004.76 tokens/sec
Step 7418 | loss: 3.285541 | lr:4.4101e-04 | norm 0.2839 | dt 339.06ms | 1546309.15 tokens/sec
Step 7419 | loss: 3.286887 | lr:4.4097e-04 | norm 0.2598 | dt 337.31ms | 1554308.52 tokens/sec
Step 7420 | loss: 3.192658 | lr:4.4093e-04 | norm 0.2944 | dt 337.78ms | 1552163.69 tokens/sec
Step 7421 | loss: 3.211884 | lr:4.4088e-04 | norm 0.3099 | dt 338.00ms | 1551167.35 tokens/sec
Step 7422 | loss: 3.209873 | lr:4.4084e-04 | norm 0.3162 | dt 337.11ms | 1555260.50 tokens/sec
Step 7423 | loss: 3.277970 | lr:4.4080e-04 | norm 0.2845 | dt 337.67ms | 1552644.80 tokens/sec
Step 7424 | loss: 3.281981 | lr:4.4076e-04 | norm 0.2992 | dt 337.69ms | 1552584.51 tokens/sec
Step 7425 | loss: 3.217189 | lr:4.4072e-04 | norm 0.3139 | dt 337.60ms | 1552991.30 tokens/sec
Step 7426 | loss: 3.272195 | lr:4.4067e-04 | norm 0.2781 | dt 338.18ms | 1550309.99 tokens/sec
Step 7427 | loss: 3.287192 | lr:4.4063e-04 | norm 0.3055 | dt 338.29ms | 1549833.60 tokens/sec
Step 7428 | loss: 3.263128 | lr:4.4059e-04 | norm 0.2733 | dt 337.68ms | 1552618.49 tokens/sec
Step 7429 | loss: 3.280500 | lr:4.4055e-04 | norm 0.2807 | dt 338.40ms | 1549298.56 tokens/sec
Step 7430 | loss: 3.325376 | lr:4.4051e-04 | norm 0.2794 | dt 338.05ms | 1550901.51 tokens/sec
Step 7431 | loss: 3.193977 | lr:4.4046e-04 | norm 0.2986 | dt 338.33ms | 1549629.37 tokens/sec
Step 7432 | loss: 3.282573 | lr:4.4042e-04 | norm 0.3106 | dt 337.61ms | 1552917.82 tokens/sec
Step 7433 | loss: 3.220285 | lr:4.4038e-04 | norm 0.2828 | dt 338.88ms | 1547127.26 tokens/sec
Step 7434 | loss: 3.297114 | lr:4.4034e-04 | norm 0.2989 | dt 339.36ms | 1544953.35 tokens/sec
Step 7435 | loss: 3.272628 | lr:4.4029e-04 | norm 0.2975 | dt 337.90ms | 1551596.39 tokens/sec
Step 7436 | loss: 3.224164 | lr:4.4025e-04 | norm 0.2846 | dt 337.91ms | 1551563.55 tokens/sec
Step 7437 | loss: 3.190804 | lr:4.4021e-04 | norm 0.2956 | dt 338.30ms | 1549772.44 tokens/sec
Step 7438 | loss: 3.284877 | lr:4.4017e-04 | norm 0.3199 | dt 337.68ms | 1552640.42 tokens/sec
Step 7439 | loss: 3.289031 | lr:4.4013e-04 | norm 0.3163 | dt 337.88ms | 1551688.36 tokens/sec
Step 7440 | loss: 3.278397 | lr:4.4008e-04 | norm 0.2982 | dt 338.13ms | 1550548.30 tokens/sec
Step 7441 | loss: 3.265244 | lr:4.4004e-04 | norm 0.3045 | dt 338.06ms | 1550852.29 tokens/sec
Step 7442 | loss: 3.238129 | lr:4.4000e-04 | norm 0.2862 | dt 338.70ms | 1547926.62 tokens/sec
Step 7443 | loss: 3.257358 | lr:4.3996e-04 | norm 0.2927 | dt 337.77ms | 1552184.51 tokens/sec
Step 7444 | loss: 3.235168 | lr:4.3991e-04 | norm 0.2670 | dt 337.62ms | 1552882.73 tokens/sec
Step 7445 | loss: 3.241198 | lr:4.3987e-04 | norm 0.2579 | dt 337.90ms | 1551586.54 tokens/sec
Step 7446 | loss: 3.232889 | lr:4.3983e-04 | norm 0.2967 | dt 338.58ms | 1548487.97 tokens/sec
Step 7447 | loss: 3.267175 | lr:4.3979e-04 | norm 0.2789 | dt 338.80ms | 1547477.83 tokens/sec
Step 7448 | loss: 3.278969 | lr:4.3975e-04 | norm 0.2953 | dt 339.29ms | 1545250.81 tokens/sec
Step 7449 | loss: 3.245444 | lr:4.3970e-04 | norm 0.2778 | dt 338.46ms | 1549051.92 tokens/sec
Step 7450 | loss: 3.194890 | lr:4.3966e-04 | norm 0.2741 | dt 338.37ms | 1549448.12 tokens/sec
Step 7451 | loss: 3.287592 | lr:4.3962e-04 | norm 0.2753 | dt 338.70ms | 1547962.58 tokens/sec
Step 7452 | loss: 3.226501 | lr:4.3958e-04 | norm 0.3058 | dt 339.48ms | 1544399.98 tokens/sec
Step 7453 | loss: 3.214120 | lr:4.3953e-04 | norm 0.2825 | dt 338.48ms | 1548939.53 tokens/sec
Step 7454 | loss: 3.216366 | lr:4.3949e-04 | norm 0.2675 | dt 338.65ms | 1548182.72 tokens/sec
Step 7455 | loss: 3.247965 | lr:4.3945e-04 | norm 0.3033 | dt 338.87ms | 1547176.24 tokens/sec
Step 7456 | loss: 3.227915 | lr:4.3941e-04 | norm 0.2722 | dt 338.09ms | 1550721.06 tokens/sec
Step 7457 | loss: 3.205738 | lr:4.3937e-04 | norm 0.2699 | dt 338.01ms | 1551114.83 tokens/sec
Step 7458 | loss: 3.253540 | lr:4.3932e-04 | norm 0.2856 | dt 337.39ms | 1553944.96 tokens/sec
Step 7459 | loss: 3.259005 | lr:4.3928e-04 | norm 0.2845 | dt 337.71ms | 1552461.75 tokens/sec
Step 7460 | loss: 3.239133 | lr:4.3924e-04 | norm 0.2733 | dt 337.69ms | 1552576.84 tokens/sec
Step 7461 | loss: 3.268185 | lr:4.3920e-04 | norm 0.2886 | dt 338.28ms | 1549841.25 tokens/sec
Step 7462 | loss: 3.276028 | lr:4.3915e-04 | norm 0.3132 | dt 338.07ms | 1550846.83 tokens/sec
Step 7463 | loss: 3.259511 | lr:4.3911e-04 | norm 0.3123 | dt 337.65ms | 1552759.92 tokens/sec
Step 7464 | loss: 3.259202 | lr:4.3907e-04 | norm 0.2690 | dt 338.77ms | 1547623.77 tokens/sec
Step 7465 | loss: 3.278857 | lr:4.3903e-04 | norm 0.3096 | dt 338.32ms | 1549667.59 tokens/sec
Step 7466 | loss: 3.275841 | lr:4.3899e-04 | norm 0.3047 | dt 338.72ms | 1547863.43 tokens/sec
Step 7467 | loss: 3.281868 | lr:4.3894e-04 | norm 0.3162 | dt 338.37ms | 1549466.68 tokens/sec
Step 7468 | loss: 3.309248 | lr:4.3890e-04 | norm 0.3025 | dt 338.91ms | 1547005.36 tokens/sec
Step 7469 | loss: 3.207757 | lr:4.3886e-04 | norm 0.2742 | dt 338.99ms | 1546597.35 tokens/sec
Step 7470 | loss: 3.280155 | lr:4.3882e-04 | norm 0.3091 | dt 338.28ms | 1549866.37 tokens/sec
Step 7471 | loss: 3.266134 | lr:4.3877e-04 | norm 0.3021 | dt 338.00ms | 1551143.28 tokens/sec
Step 7472 | loss: 3.272996 | lr:4.3873e-04 | norm 0.3216 | dt 338.58ms | 1548489.06 tokens/sec
Step 7473 | loss: 3.348129 | lr:4.3869e-04 | norm 0.3685 | dt 338.66ms | 1548115.15 tokens/sec
Step 7474 | loss: 3.266865 | lr:4.3865e-04 | norm 0.3349 | dt 338.42ms | 1549221.07 tokens/sec
Step 7475 | loss: 3.223104 | lr:4.3860e-04 | norm 0.3089 | dt 338.09ms | 1550745.12 tokens/sec
Step 7476 | loss: 3.225482 | lr:4.3856e-04 | norm 0.2903 | dt 339.33ms | 1545069.50 tokens/sec
Step 7477 | loss: 3.295343 | lr:4.3852e-04 | norm 0.2959 | dt 338.89ms | 1547056.51 tokens/sec
Step 7478 | loss: 3.308461 | lr:4.3848e-04 | norm 0.2922 | dt 337.80ms | 1552085.91 tokens/sec
Step 7479 | loss: 3.255075 | lr:4.3844e-04 | norm 0.2914 | dt 339.44ms | 1544570.29 tokens/sec
Step 7480 | loss: 3.214649 | lr:4.3839e-04 | norm 0.2895 | dt 337.65ms | 1552755.53 tokens/sec
Step 7481 | loss: 3.271932 | lr:4.3835e-04 | norm 0.3275 | dt 337.61ms | 1552924.40 tokens/sec
Step 7482 | loss: 3.258867 | lr:4.3831e-04 | norm 0.2917 | dt 338.31ms | 1549746.23 tokens/sec
Step 7483 | loss: 3.215126 | lr:4.3827e-04 | norm 0.3190 | dt 338.46ms | 1549035.55 tokens/sec
Step 7484 | loss: 3.217447 | lr:4.3822e-04 | norm 0.3305 | dt 337.34ms | 1554172.30 tokens/sec
Step 7485 | loss: 3.196054 | lr:4.3818e-04 | norm 0.2792 | dt 337.62ms | 1552891.50 tokens/sec
Step 7486 | loss: 3.223958 | lr:4.3814e-04 | norm 0.3643 | dt 338.15ms | 1550453.18 tokens/sec
Step 7487 | loss: 3.246893 | lr:4.3810e-04 | norm 0.3681 | dt 337.25ms | 1554577.72 tokens/sec
Step 7488 | loss: 3.328019 | lr:4.3805e-04 | norm 0.3102 | dt 337.68ms | 1552618.49 tokens/sec
Step 7489 | loss: 3.284824 | lr:4.3801e-04 | norm 0.3102 | dt 337.95ms | 1551380.75 tokens/sec
Step 7490 | loss: 3.257629 | lr:4.3797e-04 | norm 0.2722 | dt 337.78ms | 1552139.59 tokens/sec
Step 7491 | loss: 3.221745 | lr:4.3793e-04 | norm 0.2954 | dt 337.77ms | 1552189.99 tokens/sec
Step 7492 | loss: 3.225137 | lr:4.3789e-04 | norm 0.2594 | dt 338.27ms | 1549906.79 tokens/sec
Step 7493 | loss: 3.270839 | lr:4.3784e-04 | norm 0.2765 | dt 338.30ms | 1549764.79 tokens/sec
Step 7494 | loss: 3.287340 | lr:4.3780e-04 | norm 0.2566 | dt 338.16ms | 1550414.92 tokens/sec
Step 7495 | loss: 3.329992 | lr:4.3776e-04 | norm 0.2849 | dt 337.24ms | 1554657.95 tokens/sec
Step 7496 | loss: 3.349201 | lr:4.3772e-04 | norm 0.2873 | dt 338.22ms | 1550126.40 tokens/sec
Step 7497 | loss: 3.269551 | lr:4.3767e-04 | norm 0.2855 | dt 339.01ms | 1546548.40 tokens/sec
Step 7498 | loss: 3.298963 | lr:4.3763e-04 | norm 0.3056 | dt 338.00ms | 1551136.72 tokens/sec
Step 7499 | loss: 3.282120 | lr:4.3759e-04 | norm 0.3064 | dt 338.26ms | 1549952.67 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 7500: 3.2631
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2899/10042=0.2887


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, and my model is like this:
There's a way to go, but you're not.
The word for
rank 5 sample 1 >Hello, I'm a language model, am in it? So this is what I know. But if you want to go into a language of another language,
rank 5 sample 2 >Hello, I'm a language model, but it does a variety of calculations I can't do. I like the layout of objects, and it's a neat
rank 5 sample 3 >Hello, I'm a language model, no, it's not very different from a Lisp machine (because I'm working right on it because it's not Lisp





ddp_rank 2: ####### Printing generated samples ####### 


ddp_rank 3: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, anyway - it would be much more appropriate for me to say it's true, or would be better to talk about how
rank 3 sample 0 >Hello, I'm a language model, so I'm not so familiar with Haskell.
But there are some language languages I think should be widely used, though
rank 2 sample 1 >Hello, I'm a language model, and I started my class by taking off reading a question and then going backwards. I'm a language-model, and
rank 3 sample 1 >Hello, I'm a language model, so it has a lot of differences from other languages. Here's an example of a language model, the Language Prof.
rank 2 sample 2 >Hello, I'm a language model, but I don't like to make new and complicated things. The way I write a paragraph, paragraph format, quotes.rank 3 sample 2 >Hello, I'm a language model, so let's assume you're studying a grammar structure and you're studying the meaning of the language like a grammar table for

rank 2 sample 3 >Hello, I'm a language model, and if I'm a teacher, I get a point for doing this.<|endoftext|>I am curious that there are two classesrank 3 sample 3 >Hello, I'm a language model, so when I get to know this, it is really good to understand the language learning process, and just how much it







ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I think it might give you a slightly different perspective on what it's all about.
Yeah, that's good
rank 7 sample 1 >Hello, I'm a language model, learning to understand the concept behind how learning languages works. Most of the time, my work will be working on learning and
rank 7 sample 2 >Hello, I'm a language model, and I've started off with your little ideas. As I've explained below, you can use the 'Hello'. If
rank 7 sample 3 >Hello, I'm a language model, but it's not like an editor/librarian. I've got something like the example below!
I'm so




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and it's a bit of a project for language teachers. How do I encourage my learners to become fluent speakers?

rank 1 sample 1 >Hello, I'm a language model, not an expert, but I'll get one of his other books a day. How?
C.S.E


ddp_rank 6: ####### Printing generated samples ####### 



ddp_rank 4: ####### Printing generated samples ####### 

rank 1 sample 2 >Hello, I'm a language model, and don't need to know anything about it.
So in my opinion, I'm going to go with "L
rank 1 sample 3 >Hello, I'm a language model, so I'm doing this work. And today I'm adding an entire series on the work of language models. In this


rank 6 sample 0 >Hello, I'm a language model, that you can use anywhere within your application, and even on a system. That's what it's all about. Let
rank 4 sample 0 >Hello, I'm a language model, but I want to take some time to put this to the test. Let's say your job is to build a simple
rank 6 sample 1 >Hello, I'm a language model, you're right, that is the way she's supposed to be. We're talking about a bunch of language models,
rank 4 sample 1 >Hello, I'm a language model, don't know (but I will, for another reason, be so busy, please!)
My students make the
rank 6 sample 2 >Hello, I'm a language model, and so am a student in this group. It's a very basic language because it's so easy to understand: it
rank 4 sample 2 >Hello, I'm a language model, I always teach students that you can do that with a simple example of a language but not with complex sentence structures; that
rank 6 sample 3 >Hello, I'm a language model, so that's the problem. That's pretty easy.
I think that it is the worst thing we know about any


rank 4 sample 3 >Hello, I'm a language model, so you find myself as a language or you get so impressed that you say that you're not very familiar with me and




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't think he was so interesting by definition. But for good reason, I never actually had this kind of
rank 0 sample 1 >Hello, I'm a language model, and one of the first things to go is that learning and understanding that language makes you more knowledgeable. A great example of
rank 0 sample 2 >Hello, I'm a language model, but I use that language very often as a method of language instruction (for example, I'm using the word 'my
rank 0 sample 3 >Hello, I'm a language model, and in that way I'm a fluent person today. I'm the author and an author of a dozen books and an


Step 7500 | loss: 3.239388 | lr:4.3755e-04 | norm 0.2779 | dt 12272.40ms | 42720.91 tokens/sec
Step 7501 | loss: 3.286965 | lr:4.3750e-04 | norm 0.2700 | dt 335.82ms | 1561213.07 tokens/sec
Step 7502 | loss: 3.252385 | lr:4.3746e-04 | norm 0.2816 | dt 336.16ms | 1559660.65 tokens/sec
Step 7503 | loss: 3.244144 | lr:4.3742e-04 | norm 0.2490 | dt 336.24ms | 1559279.11 tokens/sec
Step 7504 | loss: 3.260642 | lr:4.3738e-04 | norm 0.3145 | dt 336.87ms | 1556353.53 tokens/sec
Step 7505 | loss: 3.283403 | lr:4.3733e-04 | norm 0.3272 | dt 337.44ms | 1553741.84 tokens/sec
Step 7506 | loss: 3.325425 | lr:4.3729e-04 | norm 0.3318 | dt 336.18ms | 1559555.57 tokens/sec
Step 7507 | loss: 3.265190 | lr:4.3725e-04 | norm 0.2868 | dt 336.54ms | 1557897.16 tokens/sec
Step 7508 | loss: 3.217094 | lr:4.3721e-04 | norm 0.2970 | dt 336.88ms | 1556301.76 tokens/sec
Step 7509 | loss: 3.265329 | lr:4.3716e-04 | norm 0.3073 | dt 336.54ms | 1557870.67 tokens/sec
Step 7510 | loss: 3.320127 | lr:4.3712e-04 | norm 0.2742 | dt 336.47ms | 1558198.53 tokens/sec
Step 7511 | loss: 3.274648 | lr:4.3708e-04 | norm 0.2833 | dt 337.02ms | 1555662.09 tokens/sec
Step 7512 | loss: 3.224795 | lr:4.3704e-04 | norm 0.2689 | dt 337.22ms | 1554749.19 tokens/sec
Step 7513 | loss: 3.252018 | lr:4.3699e-04 | norm 0.2953 | dt 336.71ms | 1557100.70 tokens/sec
Step 7514 | loss: 3.223347 | lr:4.3695e-04 | norm 0.3185 | dt 336.07ms | 1560054.55 tokens/sec
Step 7515 | loss: 3.305527 | lr:4.3691e-04 | norm 0.2756 | dt 337.87ms | 1551729.96 tokens/sec
Step 7516 | loss: 3.317050 | lr:4.3687e-04 | norm 0.2665 | dt 336.68ms | 1557235.23 tokens/sec
Step 7517 | loss: 3.227478 | lr:4.3683e-04 | norm 0.2627 | dt 336.69ms | 1557160.25 tokens/sec
Step 7518 | loss: 3.293852 | lr:4.3678e-04 | norm 0.2760 | dt 338.08ms | 1550763.71 tokens/sec
Step 7519 | loss: 3.238158 | lr:4.3674e-04 | norm 0.2745 | dt 337.25ms | 1554600.80 tokens/sec
Step 7520 | loss: 3.232091 | lr:4.3670e-04 | norm 0.2602 | dt 336.33ms | 1558830.34 tokens/sec
Step 7521 | loss: 3.252827 | lr:4.3666e-04 | norm 0.2702 | dt 338.13ms | 1550543.92 tokens/sec
Step 7522 | loss: 3.216134 | lr:4.3661e-04 | norm 0.2542 | dt 337.24ms | 1554622.78 tokens/sec
Step 7523 | loss: 3.266152 | lr:4.3657e-04 | norm 0.2636 | dt 338.17ms | 1550389.78 tokens/sec
Step 7524 | loss: 3.269382 | lr:4.3653e-04 | norm 0.2673 | dt 337.30ms | 1554386.52 tokens/sec
Step 7525 | loss: 3.199327 | lr:4.3649e-04 | norm 0.2661 | dt 337.63ms | 1552857.51 tokens/sec
Step 7526 | loss: 3.269289 | lr:4.3644e-04 | norm 0.2887 | dt 338.26ms | 1549966.88 tokens/sec
Step 7527 | loss: 3.241472 | lr:4.3640e-04 | norm 0.3117 | dt 337.95ms | 1551361.05 tokens/sec
Step 7528 | loss: 3.224374 | lr:4.3636e-04 | norm 0.2873 | dt 337.59ms | 1553024.20 tokens/sec
Step 7529 | loss: 3.215659 | lr:4.3632e-04 | norm 0.3914 | dt 337.43ms | 1553773.68 tokens/sec
Step 7530 | loss: 3.257721 | lr:4.3627e-04 | norm 0.3462 | dt 338.71ms | 1547887.40 tokens/sec
Step 7531 | loss: 3.275325 | lr:4.3623e-04 | norm 0.3112 | dt 338.59ms | 1548443.27 tokens/sec
Step 7532 | loss: 3.264004 | lr:4.3619e-04 | norm 0.2923 | dt 337.47ms | 1553584.87 tokens/sec
Step 7533 | loss: 3.266232 | lr:4.3615e-04 | norm 0.3042 | dt 338.45ms | 1549067.19 tokens/sec
Step 7534 | loss: 3.247139 | lr:4.3610e-04 | norm 0.2809 | dt 338.54ms | 1548683.18 tokens/sec
Step 7535 | loss: 3.219558 | lr:4.3606e-04 | norm 0.2965 | dt 338.46ms | 1549037.73 tokens/sec
Step 7536 | loss: 3.287623 | lr:4.3602e-04 | norm 0.2869 | dt 337.63ms | 1552853.12 tokens/sec
Step 7537 | loss: 3.285582 | lr:4.3598e-04 | norm 0.2994 | dt 337.96ms | 1551323.84 tokens/sec
Step 7538 | loss: 3.247367 | lr:4.3593e-04 | norm 0.2870 | dt 337.80ms | 1552070.57 tokens/sec
Step 7539 | loss: 3.319994 | lr:4.3589e-04 | norm 0.3005 | dt 338.86ms | 1547204.55 tokens/sec
Step 7540 | loss: 3.213881 | lr:4.3585e-04 | norm 0.3044 | dt 338.48ms | 1548949.35 tokens/sec
Step 7541 | loss: 3.308770 | lr:4.3581e-04 | norm 0.3045 | dt 337.18ms | 1554939.38 tokens/sec
Step 7542 | loss: 3.266804 | lr:4.3576e-04 | norm 0.3246 | dt 338.28ms | 1549869.65 tokens/sec
Step 7543 | loss: 3.210181 | lr:4.3572e-04 | norm 0.3060 | dt 338.55ms | 1548626.47 tokens/sec
Step 7544 | loss: 3.333130 | lr:4.3568e-04 | norm 0.3202 | dt 337.42ms | 1553801.12 tokens/sec
Step 7545 | loss: 3.311170 | lr:4.3564e-04 | norm 0.3130 | dt 338.04ms | 1550980.27 tokens/sec
Step 7546 | loss: 3.271819 | lr:4.3559e-04 | norm 0.3189 | dt 338.02ms | 1551049.19 tokens/sec
Step 7547 | loss: 3.288249 | lr:4.3555e-04 | norm 0.2799 | dt 337.83ms | 1551940.23 tokens/sec
Step 7548 | loss: 3.264514 | lr:4.3551e-04 | norm 0.2959 | dt 338.06ms | 1550874.17 tokens/sec
Step 7549 | loss: 3.328116 | lr:4.3547e-04 | norm 0.3141 | dt 337.59ms | 1553011.04 tokens/sec
Step 7550 | loss: 3.308480 | lr:4.3542e-04 | norm 0.3313 | dt 337.75ms | 1552315.99 tokens/sec
Step 7551 | loss: 3.275167 | lr:4.3538e-04 | norm 0.3665 | dt 338.20ms | 1550243.32 tokens/sec
Step 7552 | loss: 3.246220 | lr:4.3534e-04 | norm 0.2836 | dt 337.74ms | 1552356.54 tokens/sec
Step 7553 | loss: 3.243582 | lr:4.3530e-04 | norm 0.3062 | dt 336.97ms | 1555886.63 tokens/sec
Step 7554 | loss: 3.240863 | lr:4.3525e-04 | norm 0.2945 | dt 338.09ms | 1550730.90 tokens/sec
Step 7555 | loss: 3.244809 | lr:4.3521e-04 | norm 0.2806 | dt 338.01ms | 1551089.67 tokens/sec
Step 7556 | loss: 3.240321 | lr:4.3517e-04 | norm 0.2807 | dt 337.25ms | 1554587.61 tokens/sec
Step 7557 | loss: 3.257800 | lr:4.3513e-04 | norm 0.2780 | dt 337.57ms | 1553125.11 tokens/sec
Step 7558 | loss: 3.210077 | lr:4.3508e-04 | norm 0.2871 | dt 338.20ms | 1550220.37 tokens/sec
Step 7559 | loss: 3.224100 | lr:4.3504e-04 | norm 0.2593 | dt 897.65ms | 584066.97 tokens/sec
Step 7560 | loss: 3.249797 | lr:4.3500e-04 | norm 0.2728 | dt 334.90ms | 1565505.46 tokens/sec
Step 7561 | loss: 3.253006 | lr:4.3496e-04 | norm 0.2607 | dt 337.44ms | 1553703.42 tokens/sec
Step 7562 | loss: 3.285614 | lr:4.3491e-04 | norm 0.2507 | dt 340.89ms | 1538019.42 tokens/sec
Step 7563 | loss: 3.275494 | lr:4.3487e-04 | norm 0.2612 | dt 338.75ms | 1547698.93 tokens/sec
Step 7564 | loss: 3.250150 | lr:4.3483e-04 | norm 0.2625 | dt 338.61ms | 1548354.96 tokens/sec
Step 7565 | loss: 3.283390 | lr:4.3478e-04 | norm 0.2624 | dt 337.89ms | 1551650.03 tokens/sec
Step 7566 | loss: 3.254610 | lr:4.3474e-04 | norm 0.2716 | dt 339.74ms | 1543226.20 tokens/sec
Step 7567 | loss: 3.234797 | lr:4.3470e-04 | norm 0.2630 | dt 338.25ms | 1550012.76 tokens/sec
Step 7568 | loss: 3.224397 | lr:4.3466e-04 | norm 0.2559 | dt 338.23ms | 1550104.54 tokens/sec
Step 7569 | loss: 3.282830 | lr:4.3461e-04 | norm 0.2848 | dt 339.02ms | 1546494.02 tokens/sec
Step 7570 | loss: 3.255769 | lr:4.3457e-04 | norm 0.3122 | dt 337.24ms | 1554635.97 tokens/sec
Step 7571 | loss: 3.248959 | lr:4.3453e-04 | norm 0.2825 | dt 337.56ms | 1553163.51 tokens/sec
Step 7572 | loss: 3.279460 | lr:4.3449e-04 | norm 0.3473 | dt 338.73ms | 1547816.58 tokens/sec
Step 7573 | loss: 3.257780 | lr:4.3444e-04 | norm 0.3197 | dt 338.26ms | 1549933.01 tokens/sec
Step 7574 | loss: 3.285587 | lr:4.3440e-04 | norm 0.2890 | dt 337.09ms | 1555324.30 tokens/sec
Step 7575 | loss: 3.251935 | lr:4.3436e-04 | norm 0.3002 | dt 338.09ms | 1550741.83 tokens/sec
Step 7576 | loss: 3.280197 | lr:4.3432e-04 | norm 0.2851 | dt 337.96ms | 1551312.89 tokens/sec
Step 7577 | loss: 3.269701 | lr:4.3427e-04 | norm 0.2676 | dt 337.66ms | 1552724.83 tokens/sec
Step 7578 | loss: 3.359616 | lr:4.3423e-04 | norm 0.3114 | dt 339.04ms | 1546382.01 tokens/sec
Step 7579 | loss: 3.329956 | lr:4.3419e-04 | norm 0.2885 | dt 338.88ms | 1547098.96 tokens/sec
Step 7580 | loss: 3.251113 | lr:4.3415e-04 | norm 0.2755 | dt 337.69ms | 1552570.26 tokens/sec
Step 7581 | loss: 3.243274 | lr:4.3410e-04 | norm 0.3185 | dt 338.11ms | 1550622.64 tokens/sec
Step 7582 | loss: 3.274344 | lr:4.3406e-04 | norm 0.2556 | dt 338.35ms | 1549537.65 tokens/sec
Step 7583 | loss: 3.282753 | lr:4.3402e-04 | norm 0.2794 | dt 337.41ms | 1553871.39 tokens/sec
Step 7584 | loss: 3.233937 | lr:4.3398e-04 | norm 0.2870 | dt 338.39ms | 1549353.14 tokens/sec
Step 7585 | loss: 3.268054 | lr:4.3393e-04 | norm 0.2644 | dt 337.42ms | 1553794.54 tokens/sec
Step 7586 | loss: 3.301801 | lr:4.3389e-04 | norm 0.2606 | dt 338.13ms | 1550562.51 tokens/sec
Step 7587 | loss: 3.214487 | lr:4.3385e-04 | norm 0.2677 | dt 338.43ms | 1549186.14 tokens/sec
Step 7588 | loss: 3.219833 | lr:4.3380e-04 | norm 0.2981 | dt 337.37ms | 1554044.89 tokens/sec
Step 7589 | loss: 3.277423 | lr:4.3376e-04 | norm 0.2807 | dt 338.07ms | 1550836.98 tokens/sec
Step 7590 | loss: 3.182899 | lr:4.3372e-04 | norm 0.2793 | dt 338.08ms | 1550765.89 tokens/sec
Step 7591 | loss: 3.272292 | lr:4.3368e-04 | norm 0.3042 | dt 337.33ms | 1554226.13 tokens/sec
Step 7592 | loss: 3.223991 | lr:4.3363e-04 | norm 0.2972 | dt 339.09ms | 1546162.38 tokens/sec
Step 7593 | loss: 3.298308 | lr:4.3359e-04 | norm 0.2778 | dt 337.72ms | 1552428.87 tokens/sec
Step 7594 | loss: 3.187552 | lr:4.3355e-04 | norm 0.2770 | dt 337.36ms | 1554077.84 tokens/sec
Step 7595 | loss: 3.268094 | lr:4.3351e-04 | norm 0.2635 | dt 338.33ms | 1549652.30 tokens/sec
Step 7596 | loss: 3.280468 | lr:4.3346e-04 | norm 0.2669 | dt 338.76ms | 1547651.00 tokens/sec
Step 7597 | loss: 3.174947 | lr:4.3342e-04 | norm 0.2811 | dt 337.46ms | 1553634.26 tokens/sec
Step 7598 | loss: 3.281325 | lr:4.3338e-04 | norm 0.2620 | dt 337.80ms | 1552054.14 tokens/sec
Step 7599 | loss: 3.255605 | lr:4.3334e-04 | norm 0.2833 | dt 949.54ms | 552149.94 tokens/sec
Step 7600 | loss: 3.257695 | lr:4.3329e-04 | norm 0.2906 | dt 336.81ms | 1556623.44 tokens/sec
Step 7601 | loss: 3.236184 | lr:4.3325e-04 | norm 0.2892 | dt 337.99ms | 1551177.20 tokens/sec
Step 7602 | loss: 3.269376 | lr:4.3321e-04 | norm 0.2642 | dt 337.92ms | 1551520.85 tokens/sec
Step 7603 | loss: 3.354219 | lr:4.3316e-04 | norm 0.3251 | dt 337.24ms | 1554663.45 tokens/sec
Step 7604 | loss: 3.306247 | lr:4.3312e-04 | norm 0.3390 | dt 338.06ms | 1550864.32 tokens/sec
Step 7605 | loss: 3.274944 | lr:4.3308e-04 | norm 0.2981 | dt 337.81ms | 1552003.75 tokens/sec
Step 7606 | loss: 3.188299 | lr:4.3304e-04 | norm 0.2845 | dt 338.12ms | 1550611.71 tokens/sec
Step 7607 | loss: 3.292009 | lr:4.3299e-04 | norm 0.2972 | dt 337.03ms | 1555614.77 tokens/sec
Step 7608 | loss: 3.263276 | lr:4.3295e-04 | norm 0.2934 | dt 338.79ms | 1547543.17 tokens/sec
Step 7609 | loss: 3.266732 | lr:4.3291e-04 | norm 0.2905 | dt 337.96ms | 1551338.06 tokens/sec
Step 7610 | loss: 3.230408 | lr:4.3287e-04 | norm 0.3132 | dt 336.71ms | 1557070.94 tokens/sec
Step 7611 | loss: 3.259454 | lr:4.3282e-04 | norm 0.2923 | dt 337.15ms | 1555061.43 tokens/sec
Step 7612 | loss: 3.289348 | lr:4.3278e-04 | norm 0.3155 | dt 338.75ms | 1547694.57 tokens/sec
Step 7613 | loss: 3.230359 | lr:4.3274e-04 | norm 0.2896 | dt 337.76ms | 1552231.62 tokens/sec
Step 7614 | loss: 3.333637 | lr:4.3269e-04 | norm 0.2909 | dt 337.61ms | 1552954.01 tokens/sec
Step 7615 | loss: 3.278122 | lr:4.3265e-04 | norm 0.3080 | dt 337.83ms | 1551923.80 tokens/sec
Step 7616 | loss: 3.228380 | lr:4.3261e-04 | norm 0.2667 | dt 337.98ms | 1551248.33 tokens/sec
Step 7617 | loss: 3.307550 | lr:4.3257e-04 | norm 0.2752 | dt 338.15ms | 1550455.37 tokens/sec
Step 7618 | loss: 3.234357 | lr:4.3252e-04 | norm 0.2885 | dt 337.66ms | 1552727.03 tokens/sec
Step 7619 | loss: 3.232254 | lr:4.3248e-04 | norm 0.2909 | dt 338.02ms | 1551061.22 tokens/sec
Step 7620 | loss: 3.280981 | lr:4.3244e-04 | norm 0.2798 | dt 337.63ms | 1552839.96 tokens/sec
Step 7621 | loss: 3.267829 | lr:4.3240e-04 | norm 0.2830 | dt 337.91ms | 1551577.78 tokens/sec
Step 7622 | loss: 3.173588 | lr:4.3235e-04 | norm 0.2698 | dt 337.69ms | 1552552.72 tokens/sec
Step 7623 | loss: 3.222268 | lr:4.3231e-04 | norm 0.2604 | dt 337.27ms | 1554505.19 tokens/sec
Step 7624 | loss: 3.232101 | lr:4.3227e-04 | norm 0.2688 | dt 338.06ms | 1550851.20 tokens/sec
Step 7625 | loss: 3.233020 | lr:4.3222e-04 | norm 0.3014 | dt 338.69ms | 1548006.17 tokens/sec
Step 7626 | loss: 3.272206 | lr:4.3218e-04 | norm 0.2779 | dt 337.42ms | 1553796.73 tokens/sec
Step 7627 | loss: 3.250913 | lr:4.3214e-04 | norm 0.3052 | dt 338.16ms | 1550412.74 tokens/sec
Step 7628 | loss: 3.262381 | lr:4.3210e-04 | norm 0.3150 | dt 339.13ms | 1545976.50 tokens/sec
Step 7629 | loss: 3.286191 | lr:4.3205e-04 | norm 0.2690 | dt 337.46ms | 1553630.97 tokens/sec
Step 7630 | loss: 3.248007 | lr:4.3201e-04 | norm 0.2957 | dt 338.08ms | 1550792.14 tokens/sec
Step 7631 | loss: 3.267377 | lr:4.3197e-04 | norm 0.2831 | dt 338.56ms | 1548595.93 tokens/sec
Step 7632 | loss: 3.290563 | lr:4.3192e-04 | norm 0.3013 | dt 338.13ms | 1550551.58 tokens/sec
Step 7633 | loss: 3.296072 | lr:4.3188e-04 | norm 0.2840 | dt 337.46ms | 1553636.46 tokens/sec
Step 7634 | loss: 3.296677 | lr:4.3184e-04 | norm 0.2966 | dt 338.85ms | 1547273.13 tokens/sec
Step 7635 | loss: 3.301502 | lr:4.3180e-04 | norm 0.2761 | dt 338.16ms | 1550413.83 tokens/sec
Step 7636 | loss: 3.282114 | lr:4.3175e-04 | norm 0.2998 | dt 337.52ms | 1553358.80 tokens/sec
Step 7637 | loss: 3.284161 | lr:4.3171e-04 | norm 0.2750 | dt 338.14ms | 1550499.10 tokens/sec
Step 7638 | loss: 3.308547 | lr:4.3167e-04 | norm 0.2939 | dt 338.39ms | 1549350.96 tokens/sec
Step 7639 | loss: 3.300740 | lr:4.3163e-04 | norm 0.2946 | dt 337.58ms | 1553085.63 tokens/sec
Step 7640 | loss: 3.208848 | lr:4.3158e-04 | norm 0.2557 | dt 338.60ms | 1548419.28 tokens/sec
Step 7641 | loss: 3.284645 | lr:4.3154e-04 | norm 0.2867 | dt 338.02ms | 1551034.97 tokens/sec
Step 7642 | loss: 3.227408 | lr:4.3150e-04 | norm 0.2553 | dt 338.58ms | 1548486.88 tokens/sec
Step 7643 | loss: 3.197570 | lr:4.3145e-04 | norm 0.2678 | dt 339.40ms | 1544752.57 tokens/sec
Step 7644 | loss: 3.256018 | lr:4.3141e-04 | norm 0.2721 | dt 338.15ms | 1550451.00 tokens/sec
Step 7645 | loss: 3.310190 | lr:4.3137e-04 | norm 0.2932 | dt 338.23ms | 1550071.76 tokens/sec
Step 7646 | loss: 3.302562 | lr:4.3133e-04 | norm 0.2853 | dt 338.38ms | 1549427.38 tokens/sec
Step 7647 | loss: 3.288492 | lr:4.3128e-04 | norm 0.2619 | dt 337.52ms | 1553359.90 tokens/sec
Step 7648 | loss: 3.288546 | lr:4.3124e-04 | norm 0.2609 | dt 338.06ms | 1550852.29 tokens/sec
Step 7649 | loss: 3.315778 | lr:4.3120e-04 | norm 0.2718 | dt 337.59ms | 1553011.04 tokens/sec
Step 7650 | loss: 3.229534 | lr:4.3115e-04 | norm 0.2832 | dt 337.12ms | 1555219.80 tokens/sec
Step 7651 | loss: 3.275453 | lr:4.3111e-04 | norm 0.3326 | dt 338.29ms | 1549808.48 tokens/sec
Step 7652 | loss: 3.263592 | lr:4.3107e-04 | norm 0.2844 | dt 337.78ms | 1552148.35 tokens/sec
Step 7653 | loss: 3.340671 | lr:4.3103e-04 | norm 0.2953 | dt 337.24ms | 1554640.37 tokens/sec
Step 7654 | loss: 3.269239 | lr:4.3098e-04 | norm 0.3203 | dt 337.74ms | 1552321.47 tokens/sec
Step 7655 | loss: 3.215966 | lr:4.3094e-04 | norm 0.2661 | dt 338.52ms | 1548747.53 tokens/sec
Step 7656 | loss: 3.270369 | lr:4.3090e-04 | norm 0.3126 | dt 337.46ms | 1553624.38 tokens/sec
Step 7657 | loss: 3.213385 | lr:4.3085e-04 | norm 0.2881 | dt 337.54ms | 1553284.19 tokens/sec
Step 7658 | loss: 3.252712 | lr:4.3081e-04 | norm 0.2659 | dt 338.29ms | 1549818.31 tokens/sec
Step 7659 | loss: 3.263370 | lr:4.3077e-04 | norm 0.2571 | dt 338.51ms | 1548809.71 tokens/sec
Step 7660 | loss: 3.265131 | lr:4.3073e-04 | norm 0.3060 | dt 338.66ms | 1548134.76 tokens/sec
Step 7661 | loss: 3.248463 | lr:4.3068e-04 | norm 0.2822 | dt 338.30ms | 1549751.69 tokens/sec
Step 7662 | loss: 3.223754 | lr:4.3064e-04 | norm 0.2541 | dt 337.87ms | 1551727.77 tokens/sec
Step 7663 | loss: 3.217036 | lr:4.3060e-04 | norm 0.3209 | dt 337.54ms | 1553246.88 tokens/sec
Step 7664 | loss: 3.231811 | lr:4.3055e-04 | norm 0.2972 | dt 338.40ms | 1549301.84 tokens/sec
Step 7665 | loss: 3.257304 | lr:4.3051e-04 | norm 0.3187 | dt 339.04ms | 1546405.93 tokens/sec
Step 7666 | loss: 3.245398 | lr:4.3047e-04 | norm 0.3071 | dt 338.09ms | 1550725.43 tokens/sec
Step 7667 | loss: 3.187200 | lr:4.3043e-04 | norm 0.3103 | dt 337.87ms | 1551766.10 tokens/sec
Step 7668 | loss: 3.259696 | lr:4.3038e-04 | norm 0.3008 | dt 337.82ms | 1551954.46 tokens/sec
Step 7669 | loss: 3.241679 | lr:4.3034e-04 | norm 0.2867 | dt 338.15ms | 1550475.05 tokens/sec
Step 7670 | loss: 3.209833 | lr:4.3030e-04 | norm 0.3211 | dt 337.39ms | 1553965.83 tokens/sec
Step 7671 | loss: 3.280246 | lr:4.3025e-04 | norm 0.3292 | dt 337.80ms | 1552065.10 tokens/sec
Step 7672 | loss: 3.244389 | lr:4.3021e-04 | norm 0.2892 | dt 337.87ms | 1551763.91 tokens/sec
Step 7673 | loss: 3.187607 | lr:4.3017e-04 | norm 0.3385 | dt 337.98ms | 1551240.67 tokens/sec
Step 7674 | loss: 3.291968 | lr:4.3013e-04 | norm 0.2727 | dt 337.88ms | 1551678.50 tokens/sec
Step 7675 | loss: 3.355389 | lr:4.3008e-04 | norm 0.3035 | dt 337.93ms | 1551481.44 tokens/sec
Step 7676 | loss: 3.336176 | lr:4.3004e-04 | norm 0.3208 | dt 337.78ms | 1552170.27 tokens/sec
Step 7677 | loss: 3.313426 | lr:4.3000e-04 | norm 0.3184 | dt 337.29ms | 1554406.30 tokens/sec
Step 7678 | loss: 3.300912 | lr:4.2995e-04 | norm 0.2958 | dt 338.53ms | 1548732.26 tokens/sec
Step 7679 | loss: 3.281013 | lr:4.2991e-04 | norm 0.3217 | dt 337.47ms | 1553591.45 tokens/sec
Step 7680 | loss: 3.292028 | lr:4.2987e-04 | norm 0.2850 | dt 337.11ms | 1555226.40 tokens/sec
Step 7681 | loss: 3.256361 | lr:4.2982e-04 | norm 0.2997 | dt 338.16ms | 1550414.92 tokens/sec
Step 7682 | loss: 3.296503 | lr:4.2978e-04 | norm 0.3001 | dt 338.45ms | 1549099.93 tokens/sec
Step 7683 | loss: 3.253911 | lr:4.2974e-04 | norm 0.3123 | dt 339.21ms | 1545624.44 tokens/sec
Step 7684 | loss: 3.176674 | lr:4.2970e-04 | norm 0.2770 | dt 337.84ms | 1551896.42 tokens/sec
Step 7685 | loss: 3.348800 | lr:4.2965e-04 | norm 0.6089 | dt 338.31ms | 1549721.11 tokens/sec
Step 7686 | loss: 3.282218 | lr:4.2961e-04 | norm 0.3152 | dt 337.18ms | 1554915.19 tokens/sec
Step 7687 | loss: 3.265277 | lr:4.2957e-04 | norm 0.2976 | dt 338.02ms | 1551040.44 tokens/sec
Step 7688 | loss: 3.205780 | lr:4.2952e-04 | norm 0.2660 | dt 338.13ms | 1550554.86 tokens/sec
Step 7689 | loss: 3.300198 | lr:4.2948e-04 | norm 0.3052 | dt 338.16ms | 1550432.41 tokens/sec
Step 7690 | loss: 3.236070 | lr:4.2944e-04 | norm 0.2755 | dt 337.57ms | 1553129.50 tokens/sec
Step 7691 | loss: 3.194762 | lr:4.2940e-04 | norm 0.2974 | dt 337.95ms | 1551386.22 tokens/sec
Step 7692 | loss: 3.278419 | lr:4.2935e-04 | norm 0.3296 | dt 338.04ms | 1550984.65 tokens/sec
Step 7693 | loss: 3.263350 | lr:4.2931e-04 | norm 0.2844 | dt 337.37ms | 1554039.40 tokens/sec
Step 7694 | loss: 3.234743 | lr:4.2927e-04 | norm 0.3078 | dt 338.12ms | 1550580.00 tokens/sec
Step 7695 | loss: 3.345128 | lr:4.2922e-04 | norm 0.2979 | dt 338.33ms | 1549621.73 tokens/sec
Step 7696 | loss: 3.251585 | lr:4.2918e-04 | norm 0.2811 | dt 338.27ms | 1549913.35 tokens/sec
Step 7697 | loss: 3.228652 | lr:4.2914e-04 | norm 0.3115 | dt 338.20ms | 1550252.07 tokens/sec
Step 7698 | loss: 3.265814 | lr:4.2909e-04 | norm 0.2981 | dt 338.18ms | 1550300.16 tokens/sec
Step 7699 | loss: 3.234184 | lr:4.2905e-04 | norm 0.2874 | dt 338.75ms | 1547722.89 tokens/sec
Step 7700 | loss: 3.246088 | lr:4.2901e-04 | norm 0.2886 | dt 338.86ms | 1547233.94 tokens/sec
Step 7701 | loss: 3.335695 | lr:4.2897e-04 | norm 0.2936 | dt 337.63ms | 1552868.47 tokens/sec
Step 7702 | loss: 3.225632 | lr:4.2892e-04 | norm 0.2926 | dt 337.30ms | 1554370.04 tokens/sec
Step 7703 | loss: 3.229714 | lr:4.2888e-04 | norm 0.2696 | dt 339.49ms | 1544343.58 tokens/sec
Step 7704 | loss: 3.213424 | lr:4.2884e-04 | norm 0.2827 | dt 337.88ms | 1551719.01 tokens/sec
Step 7705 | loss: 3.202109 | lr:4.2879e-04 | norm 0.2779 | dt 337.61ms | 1552946.33 tokens/sec
Step 7706 | loss: 3.243848 | lr:4.2875e-04 | norm 0.2699 | dt 338.42ms | 1549241.81 tokens/sec
Step 7707 | loss: 3.258337 | lr:4.2871e-04 | norm 0.2518 | dt 338.11ms | 1550637.95 tokens/sec
Step 7708 | loss: 3.175539 | lr:4.2866e-04 | norm 0.2581 | dt 337.22ms | 1554730.50 tokens/sec
Step 7709 | loss: 3.200808 | lr:4.2862e-04 | norm 0.2651 | dt 338.45ms | 1549084.65 tokens/sec
Step 7710 | loss: 3.252427 | lr:4.2858e-04 | norm 0.2483 | dt 338.12ms | 1550588.75 tokens/sec
Step 7711 | loss: 3.294367 | lr:4.2854e-04 | norm 0.2845 | dt 337.98ms | 1551235.19 tokens/sec
Step 7712 | loss: 3.258112 | lr:4.2849e-04 | norm 0.2811 | dt 338.32ms | 1549688.34 tokens/sec
Step 7713 | loss: 3.242933 | lr:4.2845e-04 | norm 0.2623 | dt 338.05ms | 1550928.86 tokens/sec
Step 7714 | loss: 3.314913 | lr:4.2841e-04 | norm 0.2559 | dt 337.66ms | 1552700.71 tokens/sec
Step 7715 | loss: 3.298377 | lr:4.2836e-04 | norm 0.2817 | dt 337.83ms | 1551913.94 tokens/sec
Step 7716 | loss: 3.274681 | lr:4.2832e-04 | norm 0.2586 | dt 338.43ms | 1549161.04 tokens/sec
Step 7717 | loss: 3.283784 | lr:4.2828e-04 | norm 0.2759 | dt 338.50ms | 1548864.25 tokens/sec
Step 7718 | loss: 3.276190 | lr:4.2823e-04 | norm 0.2757 | dt 338.35ms | 1549561.67 tokens/sec
Step 7719 | loss: 3.331834 | lr:4.2819e-04 | norm 0.2499 | dt 338.57ms | 1548539.22 tokens/sec
Step 7720 | loss: 3.248486 | lr:4.2815e-04 | norm 0.2816 | dt 338.30ms | 1549780.08 tokens/sec
Step 7721 | loss: 3.343825 | lr:4.2811e-04 | norm 0.2803 | dt 338.59ms | 1548445.45 tokens/sec
Step 7722 | loss: 3.232684 | lr:4.2806e-04 | norm 0.2654 | dt 338.43ms | 1549188.33 tokens/sec
Step 7723 | loss: 3.187133 | lr:4.2802e-04 | norm 0.2531 | dt 337.94ms | 1551435.47 tokens/sec
Step 7724 | loss: 3.224491 | lr:4.2798e-04 | norm 0.2627 | dt 338.19ms | 1550292.50 tokens/sec
Step 7725 | loss: 3.263193 | lr:4.2793e-04 | norm 0.3963 | dt 338.62ms | 1548325.52 tokens/sec
Step 7726 | loss: 3.238826 | lr:4.2789e-04 | norm 0.3183 | dt 338.02ms | 1551044.81 tokens/sec
Step 7727 | loss: 3.353117 | lr:4.2785e-04 | norm 0.3049 | dt 338.74ms | 1547750.12 tokens/sec
Step 7728 | loss: 3.208176 | lr:4.2780e-04 | norm 0.3078 | dt 339.29ms | 1545243.21 tokens/sec
Step 7729 | loss: 3.270372 | lr:4.2776e-04 | norm 0.2961 | dt 337.93ms | 1551446.42 tokens/sec
Step 7730 | loss: 3.291776 | lr:4.2772e-04 | norm 0.3043 | dt 338.06ms | 1550876.36 tokens/sec
Step 7731 | loss: 3.210618 | lr:4.2767e-04 | norm 0.3075 | dt 339.55ms | 1544061.64 tokens/sec
Step 7732 | loss: 3.292128 | lr:4.2763e-04 | norm 0.3268 | dt 338.48ms | 1548951.53 tokens/sec
Step 7733 | loss: 3.261277 | lr:4.2759e-04 | norm 0.2703 | dt 339.22ms | 1545578.81 tokens/sec
Step 7734 | loss: 3.242316 | lr:4.2755e-04 | norm 0.3191 | dt 338.42ms | 1549217.79 tokens/sec
Step 7735 | loss: 3.195329 | lr:4.2750e-04 | norm 0.2827 | dt 338.70ms | 1547959.31 tokens/sec
Step 7736 | loss: 3.270826 | lr:4.2746e-04 | norm 0.2845 | dt 338.36ms | 1549516.90 tokens/sec
Step 7737 | loss: 3.240341 | lr:4.2742e-04 | norm 0.2725 | dt 338.50ms | 1548860.98 tokens/sec
Step 7738 | loss: 3.336315 | lr:4.2737e-04 | norm 0.2950 | dt 338.66ms | 1548122.78 tokens/sec
Step 7739 | loss: 3.258415 | lr:4.2733e-04 | norm 0.3178 | dt 338.97ms | 1546707.22 tokens/sec
Step 7740 | loss: 3.279337 | lr:4.2729e-04 | norm 0.2786 | dt 337.90ms | 1551586.54 tokens/sec
Step 7741 | loss: 3.240454 | lr:4.2724e-04 | norm 0.2740 | dt 338.92ms | 1546928.09 tokens/sec
Step 7742 | loss: 3.285645 | lr:4.2720e-04 | norm 0.2700 | dt 338.98ms | 1546652.83 tokens/sec
Step 7743 | loss: 3.232529 | lr:4.2716e-04 | norm 0.2864 | dt 338.30ms | 1549763.70 tokens/sec
Step 7744 | loss: 3.233938 | lr:4.2711e-04 | norm 0.2583 | dt 338.44ms | 1549138.12 tokens/sec
Step 7745 | loss: 3.258729 | lr:4.2707e-04 | norm 0.2818 | dt 337.88ms | 1551702.59 tokens/sec
Step 7746 | loss: 3.265542 | lr:4.2703e-04 | norm 0.2719 | dt 338.17ms | 1550347.15 tokens/sec
Step 7747 | loss: 3.269875 | lr:4.2699e-04 | norm 0.2705 | dt 338.47ms | 1549009.36 tokens/sec
Step 7748 | loss: 3.402626 | lr:4.2694e-04 | norm 0.3071 | dt 902.33ms | 581037.88 tokens/sec
Step 7749 | loss: 3.315470 | lr:4.2690e-04 | norm 0.3287 | dt 337.22ms | 1554748.09 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 7750: 3.2565
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2877/10042=0.2865


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not so confident in any languages, and I find that no person in my team makes you a problem...
rank 3 sample 1 >Hello, I'm a language model, and the reason I'm coming into my work is that the language model is the language model of the world. You've
rank 3 sample 2 >Hello, I'm a language model, and you know me all about learning a new language. If I'm a little bit confused, which language do most people
rank 3 sample 3 >Hello, I'm a language model, and have a good relationship with me.
So if there is one of the few problems that I try to solve,




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, of my mind but I'm a big one, but that's not how it gets published. I'm very happy about

rank 2 sample 1 >Hello, I'm a language model, so I feel a little bit a little of a buffer-type that looks like the one above, so it's going

ddp_rank 1: ####### Printing generated samples ####### 

rank 2 sample 2 >Hello, I'm a language model, but I've seen the same language to ask my kids to create something to write and publish, because if they could get
rank 2 sample 3 >Hello, I'm a language model, so the function is called "class", if you mean "an object", type, you won't know how it would


rank 1 sample 0 >Hello, I'm a language model, so we can get all of the meanings we see, right or left, through the end of our sentence.
I
rank 1 sample 1 >Hello, I'm a language model, a native language language (L2) creator. I had a copy of "Let them help us with our projects"
rank 1 sample 2 >Hello, I'm a language model, so many different languages are used.
I'm a model editor and the aim is to write in C. In my
rank 1 sample 3 >Hello, I'm a language model, and I'm doing my little math problem after a quick Internet search. This seems to run when I'm done working on




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, so what's the use of it? But what if you can learn how to do it? How can we just add


ddp_rank 7: ####### Printing generated samples ####### 

rank 5 sample 1 >Hello, I'm a language model, because a lot of these sentences will end in words. But you can also have many sentences with some other, more than
rank 5 sample 2 >Hello, I'm a language model, but it works better not with those
- In a single word order, all of an expression is an expression. That
rank 7 sample 0 >Hello, I'm a language model, and I'm a computer-based software to program software that gives you instant access to a vast reservoir of data.

rank 5 sample 3 >Hello, I'm a language model, let's use this to say, "We are trying to learn to use Python using this method again. We have tried


rank 7 sample 1 >Hello, I'm a language model, since it looks like the computer has changed my language every 30 years. I'm able to write a good article, for
rank 7 sample 2 >Hello, I'm a language model, so I'll share my thoughts. And what I wanted to say with you is, "I'm a native learner
rank 7 sample 3 >Hello, I'm a language model, but it's not quite easy to grasp. By the time you get to the end of this exercise, you will know




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, I like to learn from myself, and it's the most interesting of the topics. I use it to help me find
rank 6 sample 1 >Hello, I'm a language model, which is what I just learned to do using a native speaker. I think that it would make a great difference in the
rank 6 sample 2 >Hello, I'm a language model, so if you want to write in Java, but you don't want to make it difficult to write in Perl I'll
rank 6 sample 3 >Hello, I'm a language model, I just got my word out here and posted the word out here. I get the picture posted here using an internet server




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I'd like to give you an answer to how to do it?
As for reading, if the word is
rank 4 sample 1 >Hello, I'm a language model, meaning the language could be the basis of language. So using the language we use and understanding the language are important: the
rank 4 sample 2 >Hello, I'm a language model, I see when kids say a word like this. I'm trying to make it explicit in my vocabulary; I am not
rank 4 sample 3 >Hello, I'm a language model, and it might get complicated for you on my website's A language guide at www.language.ucs.edu,




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I like to think in words I try to do what I understand a bit. It's hard, and I can
rank 0 sample 1 >Hello, I'm a language model, so there's a good chance you can use a number of resources for these activities. If you have one or more of
rank 0 sample 2 >Hello, I'm a language model, but I thought you saw why you were thinking I'm a great writer. I'm not sure why you're thinking,
rank 0 sample 3 >Hello, I'm a language model, so now I'm going to do some exercises specifically for you. Let's start your class, by telling you about where


Step 7750 | loss: 3.228095 | lr:4.2686e-04 | norm 0.2977 | dt 14003.83ms | 37438.91 tokens/sec
Step 7751 | loss: 3.225580 | lr:4.2681e-04 | norm 0.2722 | dt 335.76ms | 1561499.09 tokens/sec
Step 7752 | loss: 3.250431 | lr:4.2677e-04 | norm 0.3209 | dt 336.69ms | 1557165.76 tokens/sec
Step 7753 | loss: 3.206109 | lr:4.2673e-04 | norm 0.2935 | dt 336.84ms | 1556468.09 tokens/sec
Step 7754 | loss: 3.300329 | lr:4.2668e-04 | norm 0.3633 | dt 337.35ms | 1554115.19 tokens/sec
Step 7755 | loss: 3.224332 | lr:4.2664e-04 | norm 0.2870 | dt 337.12ms | 1555213.20 tokens/sec
Step 7756 | loss: 3.239202 | lr:4.2660e-04 | norm 0.3006 | dt 336.82ms | 1556559.53 tokens/sec
Step 7757 | loss: 3.265487 | lr:4.2655e-04 | norm 0.2805 | dt 336.59ms | 1557664.32 tokens/sec
Step 7758 | loss: 3.201903 | lr:4.2651e-04 | norm 0.2619 | dt 336.67ms | 1557272.72 tokens/sec
Step 7759 | loss: 3.219638 | lr:4.2647e-04 | norm 0.3012 | dt 335.86ms | 1561032.43 tokens/sec
Step 7760 | loss: 3.291131 | lr:4.2642e-04 | norm 0.2843 | dt 336.79ms | 1556700.58 tokens/sec
Step 7761 | loss: 3.228456 | lr:4.2638e-04 | norm 0.2741 | dt 337.19ms | 1554894.30 tokens/sec
Step 7762 | loss: 3.235788 | lr:4.2634e-04 | norm 0.2726 | dt 337.24ms | 1554660.15 tokens/sec
Step 7763 | loss: 3.240806 | lr:4.2629e-04 | norm 0.2658 | dt 336.95ms | 1555981.31 tokens/sec
Step 7764 | loss: 3.246118 | lr:4.2625e-04 | norm 0.2594 | dt 338.21ms | 1550202.89 tokens/sec
Step 7765 | loss: 3.273604 | lr:4.2621e-04 | norm 0.2610 | dt 337.04ms | 1555572.95 tokens/sec
Step 7766 | loss: 3.240844 | lr:4.2617e-04 | norm 0.2613 | dt 338.44ms | 1549149.04 tokens/sec
Step 7767 | loss: 3.281158 | lr:4.2612e-04 | norm 0.2610 | dt 337.80ms | 1552087.01 tokens/sec
Step 7768 | loss: 3.212174 | lr:4.2608e-04 | norm 0.2479 | dt 338.09ms | 1550731.99 tokens/sec
Step 7769 | loss: 3.283680 | lr:4.2604e-04 | norm 0.2880 | dt 337.29ms | 1554427.18 tokens/sec
Step 7770 | loss: 3.224637 | lr:4.2599e-04 | norm 0.2707 | dt 344.52ms | 1521774.33 tokens/sec
Step 7771 | loss: 3.245893 | lr:4.2595e-04 | norm 0.2845 | dt 339.13ms | 1545969.98 tokens/sec
Step 7772 | loss: 3.265919 | lr:4.2591e-04 | norm 0.2762 | dt 337.85ms | 1551853.70 tokens/sec
Step 7773 | loss: 3.218848 | lr:4.2586e-04 | norm 0.3070 | dt 338.34ms | 1549597.70 tokens/sec
Step 7774 | loss: 3.243930 | lr:4.2582e-04 | norm 0.2760 | dt 338.41ms | 1549286.56 tokens/sec
Step 7775 | loss: 3.253486 | lr:4.2578e-04 | norm 0.2897 | dt 338.52ms | 1548763.89 tokens/sec
Step 7776 | loss: 3.256509 | lr:4.2573e-04 | norm 0.3278 | dt 337.60ms | 1553003.36 tokens/sec
Step 7777 | loss: 3.223011 | lr:4.2569e-04 | norm 0.2872 | dt 338.58ms | 1548503.24 tokens/sec
Step 7778 | loss: 3.264030 | lr:4.2565e-04 | norm 0.2981 | dt 338.74ms | 1547761.02 tokens/sec
Step 7779 | loss: 3.232755 | lr:4.2560e-04 | norm 0.2926 | dt 337.64ms | 1552793.91 tokens/sec
Step 7780 | loss: 3.240762 | lr:4.2556e-04 | norm 0.2766 | dt 338.42ms | 1549201.42 tokens/sec
Step 7781 | loss: 3.287558 | lr:4.2552e-04 | norm 0.3229 | dt 338.93ms | 1546895.45 tokens/sec
Step 7782 | loss: 3.234552 | lr:4.2547e-04 | norm 0.3065 | dt 338.73ms | 1547819.85 tokens/sec
Step 7783 | loss: 3.297503 | lr:4.2543e-04 | norm 0.2825 | dt 338.20ms | 1550213.82 tokens/sec
Step 7784 | loss: 3.256687 | lr:4.2539e-04 | norm 0.2859 | dt 339.20ms | 1545646.16 tokens/sec
Step 7785 | loss: 3.250381 | lr:4.2534e-04 | norm 0.2931 | dt 339.39ms | 1544804.66 tokens/sec
Step 7786 | loss: 3.214139 | lr:4.2530e-04 | norm 0.2575 | dt 339.46ms | 1544490.01 tokens/sec
Step 7787 | loss: 3.293727 | lr:4.2526e-04 | norm 0.2856 | dt 338.10ms | 1550704.65 tokens/sec
Step 7788 | loss: 3.230283 | lr:4.2521e-04 | norm 0.2947 | dt 338.31ms | 1549716.74 tokens/sec
Step 7789 | loss: 3.229316 | lr:4.2517e-04 | norm 0.2896 | dt 1032.36ms | 507852.87 tokens/sec
Step 7790 | loss: 3.134251 | lr:4.2513e-04 | norm 0.3000 | dt 337.42ms | 1553830.77 tokens/sec
Step 7791 | loss: 3.232371 | lr:4.2508e-04 | norm 0.2584 | dt 337.48ms | 1553560.72 tokens/sec
Step 7792 | loss: 3.237222 | lr:4.2504e-04 | norm 0.2755 | dt 338.57ms | 1548516.32 tokens/sec
Step 7793 | loss: 3.265292 | lr:4.2500e-04 | norm 0.2729 | dt 338.24ms | 1550030.24 tokens/sec
Step 7794 | loss: 3.208545 | lr:4.2496e-04 | norm 0.2896 | dt 337.52ms | 1553343.44 tokens/sec
Step 7795 | loss: 3.219638 | lr:4.2491e-04 | norm 0.2901 | dt 337.99ms | 1551200.18 tokens/sec
Step 7796 | loss: 3.354434 | lr:4.2487e-04 | norm 0.2998 | dt 338.48ms | 1548953.71 tokens/sec
Step 7797 | loss: 3.182110 | lr:4.2483e-04 | norm 0.3631 | dt 338.17ms | 1550367.92 tokens/sec
Step 7798 | loss: 3.236409 | lr:4.2478e-04 | norm 0.3105 | dt 337.99ms | 1551205.65 tokens/sec
Step 7799 | loss: 3.209934 | lr:4.2474e-04 | norm 0.2896 | dt 340.24ms | 1540950.92 tokens/sec
Step 7800 | loss: 3.268750 | lr:4.2470e-04 | norm 0.3015 | dt 338.18ms | 1550319.83 tokens/sec
Step 7801 | loss: 3.222368 | lr:4.2465e-04 | norm 0.2777 | dt 337.50ms | 1553450.97 tokens/sec
Step 7802 | loss: 3.192430 | lr:4.2461e-04 | norm 0.2733 | dt 338.05ms | 1550938.70 tokens/sec
Step 7803 | loss: 3.305595 | lr:4.2457e-04 | norm 0.2828 | dt 337.42ms | 1553828.57 tokens/sec
Step 7804 | loss: 3.226783 | lr:4.2452e-04 | norm 0.3265 | dt 339.88ms | 1542561.52 tokens/sec
Step 7805 | loss: 3.178946 | lr:4.2448e-04 | norm 0.2918 | dt 337.83ms | 1551927.08 tokens/sec
Step 7806 | loss: 3.271329 | lr:4.2444e-04 | norm 0.2976 | dt 338.71ms | 1547910.28 tokens/sec
Step 7807 | loss: 3.245868 | lr:4.2439e-04 | norm 0.3003 | dt 338.92ms | 1546948.77 tokens/sec
Step 7808 | loss: 3.260719 | lr:4.2435e-04 | norm 0.2809 | dt 338.23ms | 1550085.97 tokens/sec
Step 7809 | loss: 3.233364 | lr:4.2431e-04 | norm 0.2952 | dt 338.76ms | 1547674.96 tokens/sec
Step 7810 | loss: 3.276318 | lr:4.2426e-04 | norm 0.2713 | dt 338.44ms | 1549108.66 tokens/sec
Step 7811 | loss: 3.264625 | lr:4.2422e-04 | norm 0.3101 | dt 338.71ms | 1547912.46 tokens/sec
Step 7812 | loss: 3.250273 | lr:4.2418e-04 | norm 0.2696 | dt 338.60ms | 1548381.12 tokens/sec
Step 7813 | loss: 3.290704 | lr:4.2413e-04 | norm 0.2756 | dt 338.41ms | 1549259.27 tokens/sec
Step 7814 | loss: 3.239064 | lr:4.2409e-04 | norm 0.2825 | dt 339.02ms | 1546502.72 tokens/sec
Step 7815 | loss: 3.263697 | lr:4.2405e-04 | norm 0.2555 | dt 339.05ms | 1546331.99 tokens/sec
Step 7816 | loss: 3.354695 | lr:4.2400e-04 | norm 0.2935 | dt 338.10ms | 1550668.57 tokens/sec
Step 7817 | loss: 3.249174 | lr:4.2396e-04 | norm 0.2732 | dt 338.37ms | 1549448.12 tokens/sec
Step 7818 | loss: 3.294090 | lr:4.2392e-04 | norm 0.2820 | dt 339.87ms | 1542630.78 tokens/sec
Step 7819 | loss: 3.263021 | lr:4.2387e-04 | norm 0.2669 | dt 338.46ms | 1549032.27 tokens/sec
Step 7820 | loss: 3.291991 | lr:4.2383e-04 | norm 0.3126 | dt 338.98ms | 1546678.93 tokens/sec
Step 7821 | loss: 3.267025 | lr:4.2379e-04 | norm 0.3068 | dt 338.78ms | 1547585.65 tokens/sec
Step 7822 | loss: 3.273875 | lr:4.2374e-04 | norm 0.2666 | dt 338.30ms | 1549778.99 tokens/sec
Step 7823 | loss: 3.307051 | lr:4.2370e-04 | norm 0.2759 | dt 338.25ms | 1549978.89 tokens/sec
Step 7824 | loss: 3.300637 | lr:4.2366e-04 | norm 0.3161 | dt 337.80ms | 1552089.20 tokens/sec
Step 7825 | loss: 3.250864 | lr:4.2361e-04 | norm 0.3080 | dt 338.39ms | 1549346.59 tokens/sec
Step 7826 | loss: 3.242556 | lr:4.2357e-04 | norm 0.2944 | dt 338.85ms | 1547275.31 tokens/sec
Step 7827 | loss: 3.180641 | lr:4.2353e-04 | norm 0.3099 | dt 338.45ms | 1549072.65 tokens/sec
Step 7828 | loss: 3.215309 | lr:4.2348e-04 | norm 0.2948 | dt 338.57ms | 1548550.13 tokens/sec
Step 7829 | loss: 3.204165 | lr:4.2344e-04 | norm 0.2841 | dt 338.76ms | 1547647.73 tokens/sec
Step 7830 | loss: 3.278815 | lr:4.2340e-04 | norm 0.3066 | dt 339.07ms | 1546275.45 tokens/sec
Step 7831 | loss: 3.249428 | lr:4.2335e-04 | norm 0.2976 | dt 338.10ms | 1550669.66 tokens/sec
Step 7832 | loss: 3.241201 | lr:4.2331e-04 | norm 0.2958 | dt 338.98ms | 1546646.30 tokens/sec
Step 7833 | loss: 3.241779 | lr:4.2327e-04 | norm 0.2741 | dt 338.41ms | 1549281.10 tokens/sec
Step 7834 | loss: 3.284118 | lr:4.2322e-04 | norm 0.3177 | dt 338.68ms | 1548035.59 tokens/sec
Step 7835 | loss: 3.223618 | lr:4.2318e-04 | norm 0.2678 | dt 339.43ms | 1544599.58 tokens/sec
Step 7836 | loss: 3.250189 | lr:4.2314e-04 | norm 0.2910 | dt 338.12ms | 1550576.72 tokens/sec
Step 7837 | loss: 3.321671 | lr:4.2309e-04 | norm 0.2822 | dt 338.64ms | 1548237.22 tokens/sec
Step 7838 | loss: 3.308927 | lr:4.2305e-04 | norm 0.2818 | dt 339.74ms | 1543201.29 tokens/sec
Step 7839 | loss: 3.266573 | lr:4.2301e-04 | norm 0.3005 | dt 338.70ms | 1547923.35 tokens/sec
Step 7840 | loss: 3.228059 | lr:4.2296e-04 | norm 0.2658 | dt 338.20ms | 1550231.30 tokens/sec
Step 7841 | loss: 3.284738 | lr:4.2292e-04 | norm 0.3187 | dt 338.51ms | 1548796.62 tokens/sec
Step 7842 | loss: 3.302024 | lr:4.2288e-04 | norm 0.2928 | dt 338.26ms | 1549942.84 tokens/sec
Step 7843 | loss: 3.201190 | lr:4.2283e-04 | norm 0.2753 | dt 337.72ms | 1552439.83 tokens/sec
Step 7844 | loss: 3.214561 | lr:4.2279e-04 | norm 0.2755 | dt 338.07ms | 1550828.23 tokens/sec
Step 7845 | loss: 3.266278 | lr:4.2275e-04 | norm 0.2622 | dt 338.43ms | 1549198.15 tokens/sec
Step 7846 | loss: 3.269972 | lr:4.2270e-04 | norm 0.2676 | dt 338.69ms | 1547994.18 tokens/sec
Step 7847 | loss: 3.248827 | lr:4.2266e-04 | norm 0.2579 | dt 338.88ms | 1547108.76 tokens/sec
Step 7848 | loss: 3.274304 | lr:4.2262e-04 | norm 0.2760 | dt 339.00ms | 1546563.63 tokens/sec
Step 7849 | loss: 3.239952 | lr:4.2257e-04 | norm 0.2551 | dt 339.69ms | 1543408.16 tokens/sec
Step 7850 | loss: 3.396420 | lr:4.2253e-04 | norm 0.2883 | dt 339.14ms | 1545917.81 tokens/sec
Step 7851 | loss: 3.228255 | lr:4.2249e-04 | norm 0.2873 | dt 338.82ms | 1547397.25 tokens/sec
Step 7852 | loss: 3.319751 | lr:4.2244e-04 | norm 0.2714 | dt 338.70ms | 1547935.34 tokens/sec
Step 7853 | loss: 3.305534 | lr:4.2240e-04 | norm 0.2849 | dt 338.61ms | 1548366.95 tokens/sec
Step 7854 | loss: 3.330474 | lr:4.2236e-04 | norm 0.2706 | dt 339.22ms | 1545562.52 tokens/sec
Step 7855 | loss: 3.235666 | lr:4.2231e-04 | norm 0.2821 | dt 339.08ms | 1546212.39 tokens/sec
Step 7856 | loss: 3.285604 | lr:4.2227e-04 | norm 0.2766 | dt 338.44ms | 1549152.31 tokens/sec
Step 7857 | loss: 3.245353 | lr:4.2222e-04 | norm 0.2530 | dt 339.29ms | 1545234.53 tokens/sec
Step 7858 | loss: 3.325822 | lr:4.2218e-04 | norm 0.3009 | dt 338.57ms | 1548554.49 tokens/sec
Step 7859 | loss: 3.242263 | lr:4.2214e-04 | norm 0.2835 | dt 338.16ms | 1550401.81 tokens/sec
Step 7860 | loss: 3.250387 | lr:4.2209e-04 | norm 0.2985 | dt 338.81ms | 1547446.25 tokens/sec
Step 7861 | loss: 3.220033 | lr:4.2205e-04 | norm 0.2691 | dt 339.70ms | 1543370.25 tokens/sec
Step 7862 | loss: 3.229460 | lr:4.2201e-04 | norm 0.2772 | dt 338.33ms | 1549645.75 tokens/sec
Step 7863 | loss: 3.233997 | lr:4.2196e-04 | norm 0.2662 | dt 338.33ms | 1549620.64 tokens/sec
Step 7864 | loss: 3.266757 | lr:4.2192e-04 | norm 0.2799 | dt 339.42ms | 1544648.41 tokens/sec
Step 7865 | loss: 3.261594 | lr:4.2188e-04 | norm 0.2645 | dt 339.38ms | 1544839.39 tokens/sec
Step 7866 | loss: 3.276934 | lr:4.2183e-04 | norm 0.3414 | dt 339.29ms | 1545235.61 tokens/sec
Step 7867 | loss: 3.242314 | lr:4.2179e-04 | norm 0.2891 | dt 338.75ms | 1547700.02 tokens/sec
Step 7868 | loss: 3.285898 | lr:4.2175e-04 | norm 0.2947 | dt 339.05ms | 1546341.77 tokens/sec
Step 7869 | loss: 3.186182 | lr:4.2170e-04 | norm 0.2848 | dt 338.83ms | 1547344.99 tokens/sec
Step 7870 | loss: 3.271762 | lr:4.2166e-04 | norm 0.2891 | dt 339.08ms | 1546225.43 tokens/sec
Step 7871 | loss: 3.245933 | lr:4.2162e-04 | norm 0.3599 | dt 338.14ms | 1550491.45 tokens/sec
Step 7872 | loss: 3.216472 | lr:4.2157e-04 | norm 0.2913 | dt 339.36ms | 1544951.18 tokens/sec
Step 7873 | loss: 3.249484 | lr:4.2153e-04 | norm 0.3037 | dt 339.59ms | 1543897.95 tokens/sec
Step 7874 | loss: 3.258374 | lr:4.2149e-04 | norm 0.2890 | dt 338.41ms | 1549289.83 tokens/sec
Step 7875 | loss: 3.280142 | lr:4.2144e-04 | norm 0.2784 | dt 338.71ms | 1547915.72 tokens/sec
Step 7876 | loss: 3.284990 | lr:4.2140e-04 | norm 0.2933 | dt 339.04ms | 1546373.31 tokens/sec
Step 7877 | loss: 3.238883 | lr:4.2136e-04 | norm 0.2779 | dt 338.93ms | 1546880.21 tokens/sec
Step 7878 | loss: 3.218148 | lr:4.2131e-04 | norm 0.2588 | dt 338.53ms | 1548725.72 tokens/sec
Step 7879 | loss: 3.296651 | lr:4.2127e-04 | norm 0.3006 | dt 338.14ms | 1550526.43 tokens/sec
Step 7880 | loss: 3.250521 | lr:4.2123e-04 | norm 0.2901 | dt 338.54ms | 1548675.54 tokens/sec
Step 7881 | loss: 3.263009 | lr:4.2118e-04 | norm 0.2798 | dt 337.77ms | 1552226.14 tokens/sec
Step 7882 | loss: 3.240589 | lr:4.2114e-04 | norm 0.3181 | dt 337.50ms | 1553444.39 tokens/sec
Step 7883 | loss: 3.253510 | lr:4.2109e-04 | norm 0.3318 | dt 337.98ms | 1551260.36 tokens/sec
Step 7884 | loss: 3.205424 | lr:4.2105e-04 | norm 0.3457 | dt 337.87ms | 1551731.06 tokens/sec
Step 7885 | loss: 3.297050 | lr:4.2101e-04 | norm 0.3064 | dt 337.39ms | 1553932.88 tokens/sec
Step 7886 | loss: 3.273474 | lr:4.2096e-04 | norm 0.3121 | dt 337.66ms | 1552708.39 tokens/sec
Step 7887 | loss: 3.265282 | lr:4.2092e-04 | norm 0.3059 | dt 337.45ms | 1553681.46 tokens/sec
Step 7888 | loss: 3.298087 | lr:4.2088e-04 | norm 0.3229 | dt 337.77ms | 1552206.42 tokens/sec
Step 7889 | loss: 3.240591 | lr:4.2083e-04 | norm 0.2968 | dt 338.09ms | 1550738.55 tokens/sec
Step 7890 | loss: 3.272867 | lr:4.2079e-04 | norm 0.2960 | dt 337.33ms | 1554223.93 tokens/sec
Step 7891 | loss: 3.275440 | lr:4.2075e-04 | norm 0.2910 | dt 338.40ms | 1549318.21 tokens/sec
Step 7892 | loss: 3.435995 | lr:4.2070e-04 | norm 0.2967 | dt 338.16ms | 1550401.81 tokens/sec
Step 7893 | loss: 3.225677 | lr:4.2066e-04 | norm 0.2969 | dt 337.77ms | 1552207.52 tokens/sec
Step 7894 | loss: 3.300494 | lr:4.2062e-04 | norm 0.3130 | dt 337.17ms | 1554988.86 tokens/sec
Step 7895 | loss: 3.227898 | lr:4.2057e-04 | norm 0.2873 | dt 338.08ms | 1550765.89 tokens/sec
Step 7896 | loss: 3.201079 | lr:4.2053e-04 | norm 0.2906 | dt 337.75ms | 1552297.37 tokens/sec
Step 7897 | loss: 3.220845 | lr:4.2049e-04 | norm 0.2955 | dt 337.35ms | 1554127.27 tokens/sec
Step 7898 | loss: 3.201485 | lr:4.2044e-04 | norm 0.2972 | dt 337.29ms | 1554395.31 tokens/sec
Step 7899 | loss: 3.271995 | lr:4.2040e-04 | norm 0.2769 | dt 338.00ms | 1551148.75 tokens/sec
Step 7900 | loss: 3.209599 | lr:4.2036e-04 | norm 0.2814 | dt 338.12ms | 1550575.63 tokens/sec
Step 7901 | loss: 3.235807 | lr:4.2031e-04 | norm 0.2548 | dt 338.13ms | 1550535.18 tokens/sec
Step 7902 | loss: 3.207171 | lr:4.2027e-04 | norm 0.2661 | dt 337.60ms | 1552992.40 tokens/sec
Step 7903 | loss: 3.229516 | lr:4.2022e-04 | norm 0.2794 | dt 337.69ms | 1552550.53 tokens/sec
Step 7904 | loss: 3.251379 | lr:4.2018e-04 | norm 0.2809 | dt 338.02ms | 1551078.73 tokens/sec
Step 7905 | loss: 3.273535 | lr:4.2014e-04 | norm 0.2802 | dt 337.32ms | 1554277.76 tokens/sec
Step 7906 | loss: 3.232522 | lr:4.2009e-04 | norm 0.2894 | dt 337.99ms | 1551175.01 tokens/sec
Step 7907 | loss: 3.295368 | lr:4.2005e-04 | norm 0.3016 | dt 338.58ms | 1548505.42 tokens/sec
Step 7908 | loss: 3.275239 | lr:4.2001e-04 | norm 0.2873 | dt 337.45ms | 1553679.27 tokens/sec
Step 7909 | loss: 3.233377 | lr:4.1996e-04 | norm 0.2870 | dt 337.56ms | 1553149.25 tokens/sec
Step 7910 | loss: 3.253662 | lr:4.1992e-04 | norm 0.2846 | dt 339.95ms | 1542261.84 tokens/sec
Step 7911 | loss: 3.333851 | lr:4.1988e-04 | norm 0.3076 | dt 338.48ms | 1548942.80 tokens/sec
Step 7912 | loss: 3.309602 | lr:4.1983e-04 | norm 0.2950 | dt 337.67ms | 1552684.27 tokens/sec
Step 7913 | loss: 3.239545 | lr:4.1979e-04 | norm 0.2826 | dt 337.49ms | 1553495.97 tokens/sec
Step 7914 | loss: 3.279283 | lr:4.1975e-04 | norm 0.2759 | dt 337.62ms | 1552877.25 tokens/sec
Step 7915 | loss: 3.213003 | lr:4.1970e-04 | norm 0.2961 | dt 338.37ms | 1549445.94 tokens/sec
Step 7916 | loss: 3.255351 | lr:4.1966e-04 | norm 0.2802 | dt 337.67ms | 1552665.63 tokens/sec
Step 7917 | loss: 3.222261 | lr:4.1961e-04 | norm 0.2632 | dt 337.96ms | 1551313.99 tokens/sec
Step 7918 | loss: 3.315269 | lr:4.1957e-04 | norm 0.2876 | dt 338.52ms | 1548770.44 tokens/sec
Step 7919 | loss: 3.249672 | lr:4.1953e-04 | norm 0.2782 | dt 338.15ms | 1550472.86 tokens/sec
Step 7920 | loss: 3.290832 | lr:4.1948e-04 | norm 0.5202 | dt 337.29ms | 1554413.99 tokens/sec
Step 7921 | loss: 3.275320 | lr:4.1944e-04 | norm 0.3018 | dt 337.42ms | 1553819.79 tokens/sec
Step 7922 | loss: 3.229939 | lr:4.1940e-04 | norm 0.2614 | dt 337.90ms | 1551612.81 tokens/sec
Step 7923 | loss: 3.304111 | lr:4.1935e-04 | norm 0.2846 | dt 337.30ms | 1554385.42 tokens/sec
Step 7924 | loss: 3.302202 | lr:4.1931e-04 | norm 0.2633 | dt 338.11ms | 1550642.33 tokens/sec
Step 7925 | loss: 3.263352 | lr:4.1927e-04 | norm 0.2830 | dt 337.50ms | 1553433.42 tokens/sec
Step 7926 | loss: 3.263533 | lr:4.1922e-04 | norm 0.2688 | dt 338.33ms | 1549635.92 tokens/sec
Step 7927 | loss: 3.382429 | lr:4.1918e-04 | norm 0.3101 | dt 337.19ms | 1554863.51 tokens/sec
Step 7928 | loss: 3.249144 | lr:4.1913e-04 | norm 0.3313 | dt 337.98ms | 1551223.16 tokens/sec
Step 7929 | loss: 3.237064 | lr:4.1909e-04 | norm 0.3185 | dt 338.70ms | 1547928.80 tokens/sec
Step 7930 | loss: 3.218829 | lr:4.1905e-04 | norm 0.2701 | dt 338.58ms | 1548509.78 tokens/sec
Step 7931 | loss: 3.222269 | lr:4.1900e-04 | norm 0.2902 | dt 338.77ms | 1547636.84 tokens/sec
Step 7932 | loss: 3.233094 | lr:4.1896e-04 | norm 0.2899 | dt 337.46ms | 1553633.16 tokens/sec
Step 7933 | loss: 3.187131 | lr:4.1892e-04 | norm 0.3746 | dt 339.62ms | 1543739.71 tokens/sec
Step 7934 | loss: 3.290610 | lr:4.1887e-04 | norm 0.3715 | dt 337.87ms | 1551738.72 tokens/sec
Step 7935 | loss: 3.224138 | lr:4.1883e-04 | norm 0.3451 | dt 337.82ms | 1551992.80 tokens/sec
Step 7936 | loss: 3.263170 | lr:4.1879e-04 | norm 0.2979 | dt 338.27ms | 1549906.79 tokens/sec
Step 7937 | loss: 3.246911 | lr:4.1874e-04 | norm 0.3186 | dt 902.25ms | 581088.70 tokens/sec
Step 7938 | loss: 3.290437 | lr:4.1870e-04 | norm 0.2932 | dt 334.71ms | 1566415.42 tokens/sec
Step 7939 | loss: 3.252412 | lr:4.1865e-04 | norm 0.3353 | dt 339.86ms | 1542658.91 tokens/sec
Step 7940 | loss: 3.315032 | lr:4.1861e-04 | norm 0.3103 | dt 337.95ms | 1551387.31 tokens/sec
Step 7941 | loss: 3.247677 | lr:4.1857e-04 | norm 0.2956 | dt 337.53ms | 1553316.01 tokens/sec
Step 7942 | loss: 3.245490 | lr:4.1852e-04 | norm 0.3170 | dt 337.61ms | 1552949.62 tokens/sec
Step 7943 | loss: 3.242822 | lr:4.1848e-04 | norm 0.3005 | dt 337.13ms | 1555140.61 tokens/sec
Step 7944 | loss: 3.241725 | lr:4.1844e-04 | norm 0.2891 | dt 337.78ms | 1552154.93 tokens/sec
Step 7945 | loss: 3.304326 | lr:4.1839e-04 | norm 0.2647 | dt 338.13ms | 1550534.08 tokens/sec
Step 7946 | loss: 3.227666 | lr:4.1835e-04 | norm 0.2901 | dt 337.22ms | 1554753.58 tokens/sec
Step 7947 | loss: 3.247021 | lr:4.1831e-04 | norm 0.2783 | dt 336.94ms | 1556044.06 tokens/sec
Step 7948 | loss: 3.246529 | lr:4.1826e-04 | norm 0.2902 | dt 338.36ms | 1549479.78 tokens/sec
Step 7949 | loss: 3.261003 | lr:4.1822e-04 | norm 0.2641 | dt 337.32ms | 1554273.36 tokens/sec
Step 7950 | loss: 3.280758 | lr:4.1817e-04 | norm 0.3020 | dt 337.67ms | 1552683.17 tokens/sec
Step 7951 | loss: 3.273574 | lr:4.1813e-04 | norm 0.2673 | dt 338.31ms | 1549718.92 tokens/sec
Step 7952 | loss: 3.280952 | lr:4.1809e-04 | norm 0.2725 | dt 337.79ms | 1552106.72 tokens/sec
Step 7953 | loss: 3.337155 | lr:4.1804e-04 | norm 0.2467 | dt 338.78ms | 1547575.85 tokens/sec
Step 7954 | loss: 3.244565 | lr:4.1800e-04 | norm 0.2774 | dt 337.81ms | 1552037.71 tokens/sec
Step 7955 | loss: 3.268634 | lr:4.1796e-04 | norm 0.2679 | dt 337.55ms | 1553216.17 tokens/sec
Step 7956 | loss: 3.297198 | lr:4.1791e-04 | norm 0.2764 | dt 338.92ms | 1546923.74 tokens/sec
Step 7957 | loss: 3.264825 | lr:4.1787e-04 | norm 0.2741 | dt 337.73ms | 1552379.55 tokens/sec
Step 7958 | loss: 3.345513 | lr:4.1783e-04 | norm 0.2691 | dt 338.32ms | 1549670.87 tokens/sec
Step 7959 | loss: 3.278829 | lr:4.1778e-04 | norm 0.2658 | dt 338.35ms | 1549525.64 tokens/sec
Step 7960 | loss: 3.212461 | lr:4.1774e-04 | norm 0.2624 | dt 337.37ms | 1554038.31 tokens/sec
Step 7961 | loss: 3.276079 | lr:4.1769e-04 | norm 0.2594 | dt 337.95ms | 1551359.95 tokens/sec
Step 7962 | loss: 3.251634 | lr:4.1765e-04 | norm 0.3132 | dt 337.92ms | 1551500.05 tokens/sec
Step 7963 | loss: 3.220594 | lr:4.1761e-04 | norm 0.3062 | dt 338.21ms | 1550205.07 tokens/sec
Step 7964 | loss: 3.199688 | lr:4.1756e-04 | norm 0.2820 | dt 337.88ms | 1551706.97 tokens/sec
Step 7965 | loss: 3.235259 | lr:4.1752e-04 | norm 0.3212 | dt 338.25ms | 1550005.11 tokens/sec
Step 7966 | loss: 3.258988 | lr:4.1748e-04 | norm 0.2887 | dt 341.35ms | 1535916.07 tokens/sec
Step 7967 | loss: 3.243479 | lr:4.1743e-04 | norm 0.2955 | dt 337.35ms | 1554153.63 tokens/sec
Step 7968 | loss: 3.278074 | lr:4.1739e-04 | norm 0.2862 | dt 338.25ms | 1550022.59 tokens/sec
Step 7969 | loss: 3.255514 | lr:4.1734e-04 | norm 0.2678 | dt 339.11ms | 1546063.45 tokens/sec
Step 7970 | loss: 3.263387 | lr:4.1730e-04 | norm 0.3047 | dt 337.66ms | 1552721.55 tokens/sec
Step 7971 | loss: 3.247773 | lr:4.1726e-04 | norm 0.2840 | dt 338.04ms | 1550983.55 tokens/sec
Step 7972 | loss: 3.200379 | lr:4.1721e-04 | norm 0.3329 | dt 338.42ms | 1549210.15 tokens/sec
Step 7973 | loss: 3.166069 | lr:4.1717e-04 | norm 0.2738 | dt 337.95ms | 1551369.80 tokens/sec
Step 7974 | loss: 3.224039 | lr:4.1713e-04 | norm 0.2701 | dt 337.99ms | 1551173.92 tokens/sec
Step 7975 | loss: 3.234709 | lr:4.1708e-04 | norm 0.2716 | dt 337.48ms | 1553534.38 tokens/sec
Step 7976 | loss: 3.256290 | lr:4.1704e-04 | norm 0.2853 | dt 338.80ms | 1547480.01 tokens/sec
Step 7977 | loss: 3.271233 | lr:4.1699e-04 | norm 0.2745 | dt 337.68ms | 1552595.47 tokens/sec
Step 7978 | loss: 3.290327 | lr:4.1695e-04 | norm 0.2921 | dt 338.09ms | 1550754.96 tokens/sec
Step 7979 | loss: 3.230464 | lr:4.1691e-04 | norm 0.2734 | dt 933.16ms | 561841.59 tokens/sec
Step 7980 | loss: 3.257282 | lr:4.1686e-04 | norm 0.3102 | dt 337.19ms | 1554864.61 tokens/sec
Step 7981 | loss: 3.230131 | lr:4.1682e-04 | norm 0.3086 | dt 338.15ms | 1550464.12 tokens/sec
Step 7982 | loss: 3.284735 | lr:4.1678e-04 | norm 0.2751 | dt 337.70ms | 1552534.09 tokens/sec
Step 7983 | loss: 3.317740 | lr:4.1673e-04 | norm 0.3016 | dt 337.35ms | 1554147.04 tokens/sec
Step 7984 | loss: 3.311072 | lr:4.1669e-04 | norm 0.2536 | dt 337.70ms | 1552542.86 tokens/sec
Step 7985 | loss: 3.255853 | lr:4.1664e-04 | norm 0.2898 | dt 337.71ms | 1552463.94 tokens/sec
Step 7986 | loss: 3.228814 | lr:4.1660e-04 | norm 0.2652 | dt 336.86ms | 1556378.86 tokens/sec
Step 7987 | loss: 3.246141 | lr:4.1656e-04 | norm 0.2896 | dt 337.93ms | 1551460.65 tokens/sec
Step 7988 | loss: 3.281828 | lr:4.1651e-04 | norm 0.2747 | dt 337.51ms | 1553386.23 tokens/sec
Step 7989 | loss: 3.268537 | lr:4.1647e-04 | norm 0.3098 | dt 337.34ms | 1554164.61 tokens/sec
Step 7990 | loss: 3.252757 | lr:4.1643e-04 | norm 0.2790 | dt 337.84ms | 1551874.51 tokens/sec
Step 7991 | loss: 3.269140 | lr:4.1638e-04 | norm 0.3191 | dt 338.00ms | 1551169.54 tokens/sec
Step 7992 | loss: 3.252602 | lr:4.1634e-04 | norm 0.2816 | dt 338.18ms | 1550341.69 tokens/sec
Step 7993 | loss: 3.257164 | lr:4.1629e-04 | norm 0.2820 | dt 337.67ms | 1552663.44 tokens/sec
Step 7994 | loss: 3.294713 | lr:4.1625e-04 | norm 0.2614 | dt 338.34ms | 1549594.43 tokens/sec
Step 7995 | loss: 3.316772 | lr:4.1621e-04 | norm 0.2947 | dt 338.75ms | 1547693.48 tokens/sec
Step 7996 | loss: 3.237530 | lr:4.1616e-04 | norm 0.2792 | dt 338.90ms | 1547016.24 tokens/sec
Step 7997 | loss: 3.243828 | lr:4.1612e-04 | norm 0.2840 | dt 338.97ms | 1546711.57 tokens/sec
Step 7998 | loss: 3.234667 | lr:4.1608e-04 | norm 0.2822 | dt 338.57ms | 1548533.77 tokens/sec
Step 7999 | loss: 3.199439 | lr:4.1603e-04 | norm 0.2786 | dt 338.16ms | 1550408.37 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 8000: 3.2502
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2890/10042=0.2878


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but if it's the first time I've seen such an episode before and it doesn't sound good, it's because


ddp_rank 2: ####### Printing generated samples ####### 

rank 5 sample 1 >Hello, I'm a language model, don't you? In English, it's probably not that simple. It still needs some practice. For many, it


ddp_rank 1: ####### Printing generated samples ####### 

rank 5 sample 2 >Hello, I'm a language model, and you have a chance to start building your own language.
First, you have to understand the difference between a real
rank 2 sample 0 >Hello, I'm a language model, trying to make me feel like I'm speaking to the kids, and in a couple of weeks, I think I've
rank 5 sample 3 >Hello, I'm a language model, why do we usually do math in the classroom? What about a book? And it is not often how you do math


rank 2 sample 1 >Hello, I'm a language model, but I live a normal life. There are so many languages that use languages. I'm sure I am right. My
rank 1 sample 0 >Hello, I'm a language model, so we'll be building our own virtual worlds (I want to make sure my kids have an authentic experience!)
-
rank 2 sample 2 >Hello, I'm a language model, and I love to learn how it actually works.
I use this lesson every single day. Every new word gets written
rank 1 sample 1 >Hello, I'm a language model, a teacher in the classroom. I was given a course with a great topic to include all of my classes. I'm
rank 2 sample 3 >Hello, I'm a language model, but when you're talking to a group, you just talk the same thing. And that seems totally like that, doesn


rank 1 sample 2 >Hello, I'm a language model, but also a social media model. I'm a social media writer. My parents are from my home. They don�
rank 1 sample 3 >Hello, I'm a language model, so I'm doing my thinking. Now some of you started to appreciate some different ways of structuring your code.





ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to try to answer and answer every question I've posted. Do you have a clue about the content
rank 7 sample 1 >Hello, I'm a language model, since I think I'm making this an everyday language approach to language learning. I do want to talk about this language because
rank 7 sample 2 >Hello, I'm a language model, so I'll do my best to include all the types of words we hear, but I'm not sure how this sounds
rank 7 sample 3 >Hello, I'm a language model, and you should be aware that you haven't found a proper explanation for grammar. But, when my brother and I both




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I can't say the same thing myself, because I'm a human being. If anybody lets me talk, it
rank 4 sample 1 >Hello, I'm a language model,” she replies. “In English, I hear the word first thing because my parents were not able to pronounce
rank 4 sample 2 >Hello, I'm a language model, I could read myself a hundred times! But, I'm sorry for the confusion
in my own training. What is
rank 4 sample 3 >Hello, I'm a language model, so this class does my best to let my child read using IELTS and I'm in Year 5. That said




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, I know I'm an Australian language teacher. I can speak and write English or English, I'm a native Australian lingu
rank 6 sample 1 >Hello, I'm a language model, so I have to think of some ways why I should use it.<|endoftext|>The story of a group of students sharing their
rank 6 sample 2 >Hello, I'm a language model, but what if you were asked about that? You've got the best answer yet.
I'm guessing. We have
rank 6 sample 3 >Hello, I'm a language model, so here's the problem: you have to use a lot of
- no matter where – you shouldn't speak about




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm gonna teach this one so I can talk about it in two hour. But in the language model there might
rank 3 sample 1 >Hello, I'm a language model, so my students are learning our own language and grammar. Students are also learning the basics of English, so I will have


ddp_rank 0: ####### Printing generated samples ####### 

rank 3 sample 2 >Hello, I'm a language model, so it doesn't speak French much. Can you explain? That's right because they're so proud of their ability to
rank 0 sample 0 >Hello, I'm a language model, so I need to understand some terminology when explaining words. I'm always getting a lot more involved because it helps me think
rank 3 sample 3 >Hello, I'm a language model, so what I have to do is go to http://mikec.com/drambookwork/pam


rank 0 sample 1 >Hello, I'm a language model, and one of the first people I can speak in elementary school. The rest of the course is a long-term memory
rank 0 sample 2 >Hello, I'm a language model, and I really love hearing this person sing. He's a real good friend.
It's a kid's favorite new
rank 0 sample 3 >Hello, I'm a language model, so today I'm going to show you the next step.
1. First let's learn the word "goods


Step 8000 | loss: 3.234645 | lr:4.1599e-04 | norm 0.3161 | dt 18520.73ms | 28308.18 tokens/sec
Step 8001 | loss: 3.216176 | lr:4.1594e-04 | norm 0.2780 | dt 334.28ms | 1568405.18 tokens/sec
Step 8002 | loss: 3.246196 | lr:4.1590e-04 | norm 0.3227 | dt 336.03ms | 1560242.72 tokens/sec
Step 8003 | loss: 3.301770 | lr:4.1586e-04 | norm 0.2894 | dt 336.58ms | 1557672.04 tokens/sec
Step 8004 | loss: 3.255542 | lr:4.1581e-04 | norm 0.2945 | dt 336.37ms | 1558683.39 tokens/sec
Step 8005 | loss: 3.220574 | lr:4.1577e-04 | norm 0.2842 | dt 336.54ms | 1557855.22 tokens/sec
Step 8006 | loss: 3.229101 | lr:4.1573e-04 | norm 0.2772 | dt 336.74ms | 1556939.75 tokens/sec
Step 8007 | loss: 3.237405 | lr:4.1568e-04 | norm 0.2706 | dt 335.94ms | 1560669.04 tokens/sec
Step 8008 | loss: 3.188512 | lr:4.1564e-04 | norm 0.3051 | dt 336.17ms | 1559602.02 tokens/sec
Step 8009 | loss: 3.176816 | lr:4.1559e-04 | norm 0.2927 | dt 336.58ms | 1557669.84 tokens/sec
Step 8010 | loss: 3.241975 | lr:4.1555e-04 | norm 0.2832 | dt 336.59ms | 1557623.50 tokens/sec
Step 8011 | loss: 3.257530 | lr:4.1551e-04 | norm 0.2940 | dt 336.53ms | 1557931.38 tokens/sec
Step 8012 | loss: 3.258110 | lr:4.1546e-04 | norm 0.2939 | dt 336.99ms | 1555810.67 tokens/sec
Step 8013 | loss: 3.192227 | lr:4.1542e-04 | norm 0.3376 | dt 337.56ms | 1553155.83 tokens/sec
Step 8014 | loss: 3.246454 | lr:4.1537e-04 | norm 0.3169 | dt 336.84ms | 1556511.06 tokens/sec
Step 8015 | loss: 3.236671 | lr:4.1533e-04 | norm 0.3256 | dt 337.26ms | 1554549.15 tokens/sec
Step 8016 | loss: 3.265229 | lr:4.1529e-04 | norm 0.3243 | dt 337.11ms | 1555242.90 tokens/sec
Step 8017 | loss: 3.278058 | lr:4.1524e-04 | norm 0.3368 | dt 336.33ms | 1558845.81 tokens/sec
Step 8018 | loss: 3.302829 | lr:4.1520e-04 | norm 0.3238 | dt 336.92ms | 1556129.95 tokens/sec
Step 8019 | loss: 3.278656 | lr:4.1516e-04 | norm 0.3136 | dt 337.87ms | 1551729.96 tokens/sec
Step 8020 | loss: 3.288649 | lr:4.1511e-04 | norm 0.3303 | dt 336.61ms | 1557536.34 tokens/sec
Step 8021 | loss: 3.258540 | lr:4.1507e-04 | norm 0.2946 | dt 337.85ms | 1551851.51 tokens/sec
Step 8022 | loss: 3.231233 | lr:4.1502e-04 | norm 0.3810 | dt 338.21ms | 1550182.13 tokens/sec
Step 8023 | loss: 3.253541 | lr:4.1498e-04 | norm 0.2903 | dt 336.92ms | 1556123.34 tokens/sec
Step 8024 | loss: 3.275526 | lr:4.1494e-04 | norm 0.3097 | dt 337.20ms | 1554817.34 tokens/sec
Step 8025 | loss: 3.357976 | lr:4.1489e-04 | norm 0.3084 | dt 337.31ms | 1554306.32 tokens/sec
Step 8026 | loss: 3.255126 | lr:4.1485e-04 | norm 0.2892 | dt 336.62ms | 1557510.97 tokens/sec
Step 8027 | loss: 3.259651 | lr:4.1480e-04 | norm 0.2717 | dt 337.34ms | 1554198.66 tokens/sec
Step 8028 | loss: 3.301333 | lr:4.1476e-04 | norm 0.2955 | dt 338.18ms | 1550301.25 tokens/sec
Step 8029 | loss: 3.264617 | lr:4.1472e-04 | norm 0.2793 | dt 337.70ms | 1552523.13 tokens/sec
Step 8030 | loss: 3.222404 | lr:4.1467e-04 | norm 0.2649 | dt 336.87ms | 1556354.63 tokens/sec
Step 8031 | loss: 3.238688 | lr:4.1463e-04 | norm 0.2946 | dt 337.11ms | 1555248.40 tokens/sec
Step 8032 | loss: 3.271555 | lr:4.1459e-04 | norm 0.2942 | dt 337.30ms | 1554386.52 tokens/sec
Step 8033 | loss: 3.223115 | lr:4.1454e-04 | norm 0.2983 | dt 336.99ms | 1555809.57 tokens/sec
Step 8034 | loss: 3.219109 | lr:4.1450e-04 | norm 0.2789 | dt 336.68ms | 1557218.69 tokens/sec
Step 8035 | loss: 3.233902 | lr:4.1445e-04 | norm 0.2790 | dt 337.18ms | 1554899.80 tokens/sec
Step 8036 | loss: 3.273868 | lr:4.1441e-04 | norm 0.2678 | dt 337.28ms | 1554449.15 tokens/sec
Step 8037 | loss: 3.252158 | lr:4.1437e-04 | norm 0.2672 | dt 337.92ms | 1551502.24 tokens/sec
Step 8038 | loss: 3.219290 | lr:4.1432e-04 | norm 0.2538 | dt 337.40ms | 1553918.61 tokens/sec
Step 8039 | loss: 3.243097 | lr:4.1428e-04 | norm 0.2749 | dt 337.10ms | 1555310.00 tokens/sec
Step 8040 | loss: 3.211430 | lr:4.1423e-04 | norm 0.2712 | dt 336.93ms | 1556051.77 tokens/sec
Step 8041 | loss: 3.210909 | lr:4.1419e-04 | norm 0.2680 | dt 337.50ms | 1553453.17 tokens/sec
Step 8042 | loss: 3.203901 | lr:4.1415e-04 | norm 0.2949 | dt 337.12ms | 1555178.01 tokens/sec
Step 8043 | loss: 3.294142 | lr:4.1410e-04 | norm 0.2658 | dt 337.30ms | 1554356.86 tokens/sec
Step 8044 | loss: 3.226460 | lr:4.1406e-04 | norm 0.2820 | dt 337.43ms | 1553778.07 tokens/sec
Step 8045 | loss: 3.272795 | lr:4.1401e-04 | norm 0.2680 | dt 338.39ms | 1549346.59 tokens/sec
Step 8046 | loss: 3.227837 | lr:4.1397e-04 | norm 0.2886 | dt 338.46ms | 1549043.19 tokens/sec
Step 8047 | loss: 3.275730 | lr:4.1393e-04 | norm 0.3086 | dt 337.35ms | 1554152.53 tokens/sec
Step 8048 | loss: 3.227598 | lr:4.1388e-04 | norm 0.2841 | dt 337.48ms | 1553560.72 tokens/sec
Step 8049 | loss: 3.247368 | lr:4.1384e-04 | norm 0.2764 | dt 337.82ms | 1551970.89 tokens/sec
Step 8050 | loss: 3.222601 | lr:4.1379e-04 | norm 0.2944 | dt 337.56ms | 1553160.22 tokens/sec
Step 8051 | loss: 3.225572 | lr:4.1375e-04 | norm 0.2617 | dt 337.67ms | 1552653.57 tokens/sec
Step 8052 | loss: 3.235403 | lr:4.1371e-04 | norm 0.3216 | dt 337.44ms | 1553702.32 tokens/sec
Step 8053 | loss: 3.236621 | lr:4.1366e-04 | norm 0.2616 | dt 339.20ms | 1545673.32 tokens/sec
Step 8054 | loss: 3.295001 | lr:4.1362e-04 | norm 0.2873 | dt 338.16ms | 1550393.06 tokens/sec
Step 8055 | loss: 3.256672 | lr:4.1358e-04 | norm 0.3011 | dt 337.07ms | 1555425.51 tokens/sec
Step 8056 | loss: 3.252903 | lr:4.1353e-04 | norm 0.2603 | dt 338.27ms | 1549889.31 tokens/sec
Step 8057 | loss: 3.255711 | lr:4.1349e-04 | norm 0.2805 | dt 338.63ms | 1548245.94 tokens/sec
Step 8058 | loss: 3.327500 | lr:4.1344e-04 | norm 0.3117 | dt 338.34ms | 1549575.87 tokens/sec
Step 8059 | loss: 3.234456 | lr:4.1340e-04 | norm 0.3141 | dt 338.32ms | 1549697.08 tokens/sec
Step 8060 | loss: 3.272234 | lr:4.1336e-04 | norm 0.2901 | dt 338.46ms | 1549030.09 tokens/sec
Step 8061 | loss: 3.300276 | lr:4.1331e-04 | norm 0.2861 | dt 337.83ms | 1551910.65 tokens/sec
Step 8062 | loss: 3.246366 | lr:4.1327e-04 | norm 0.2887 | dt 337.90ms | 1551605.15 tokens/sec
Step 8063 | loss: 3.310598 | lr:4.1322e-04 | norm 0.2990 | dt 338.30ms | 1549787.73 tokens/sec
Step 8064 | loss: 3.215065 | lr:4.1318e-04 | norm 0.2974 | dt 338.87ms | 1547175.16 tokens/sec
Step 8065 | loss: 3.238667 | lr:4.1314e-04 | norm 0.2898 | dt 338.45ms | 1549071.56 tokens/sec
Step 8066 | loss: 3.228785 | lr:4.1309e-04 | norm 0.2855 | dt 337.99ms | 1551185.95 tokens/sec
Step 8067 | loss: 3.232730 | lr:4.1305e-04 | norm 0.2829 | dt 338.53ms | 1548732.26 tokens/sec
Step 8068 | loss: 3.203096 | lr:4.1300e-04 | norm 0.2722 | dt 338.30ms | 1549759.33 tokens/sec
Step 8069 | loss: 3.161069 | lr:4.1296e-04 | norm 0.2736 | dt 337.70ms | 1552526.42 tokens/sec
Step 8070 | loss: 3.205932 | lr:4.1292e-04 | norm 0.2725 | dt 338.35ms | 1549556.21 tokens/sec
Step 8071 | loss: 3.202229 | lr:4.1287e-04 | norm 0.2922 | dt 338.26ms | 1549957.04 tokens/sec
Step 8072 | loss: 3.215438 | lr:4.1283e-04 | norm 0.2533 | dt 338.69ms | 1547992.00 tokens/sec
Step 8073 | loss: 3.239458 | lr:4.1278e-04 | norm 0.2688 | dt 338.40ms | 1549299.66 tokens/sec
Step 8074 | loss: 3.258681 | lr:4.1274e-04 | norm 0.2782 | dt 338.16ms | 1550399.62 tokens/sec
Step 8075 | loss: 3.264728 | lr:4.1270e-04 | norm 0.2714 | dt 338.30ms | 1549776.81 tokens/sec
Step 8076 | loss: 3.223848 | lr:4.1265e-04 | norm 0.2596 | dt 338.60ms | 1548411.65 tokens/sec
Step 8077 | loss: 3.235652 | lr:4.1261e-04 | norm 0.2745 | dt 338.03ms | 1551017.46 tokens/sec
Step 8078 | loss: 3.221044 | lr:4.1256e-04 | norm 0.2973 | dt 338.54ms | 1548660.28 tokens/sec
Step 8079 | loss: 3.211202 | lr:4.1252e-04 | norm 0.2681 | dt 338.15ms | 1550459.74 tokens/sec
Step 8080 | loss: 3.230208 | lr:4.1248e-04 | norm 0.2890 | dt 338.18ms | 1550317.64 tokens/sec
Step 8081 | loss: 3.280508 | lr:4.1243e-04 | norm 0.2675 | dt 337.68ms | 1552595.47 tokens/sec
Step 8082 | loss: 3.291504 | lr:4.1239e-04 | norm 0.2684 | dt 337.43ms | 1553758.31 tokens/sec
Step 8083 | loss: 3.282686 | lr:4.1234e-04 | norm 0.2753 | dt 338.83ms | 1547353.70 tokens/sec
Step 8084 | loss: 3.252614 | lr:4.1230e-04 | norm 0.2773 | dt 337.43ms | 1553755.01 tokens/sec
Step 8085 | loss: 3.314844 | lr:4.1226e-04 | norm 0.3428 | dt 338.06ms | 1550869.79 tokens/sec
Step 8086 | loss: 3.243789 | lr:4.1221e-04 | norm 0.3527 | dt 338.74ms | 1547776.27 tokens/sec
Step 8087 | loss: 3.259810 | lr:4.1217e-04 | norm 0.2988 | dt 337.84ms | 1551881.08 tokens/sec
Step 8088 | loss: 3.376143 | lr:4.1212e-04 | norm 0.3304 | dt 338.56ms | 1548563.21 tokens/sec
Step 8089 | loss: 3.311864 | lr:4.1208e-04 | norm 0.3436 | dt 338.95ms | 1546779.02 tokens/sec
Step 8090 | loss: 3.269432 | lr:4.1204e-04 | norm 0.3220 | dt 338.87ms | 1547177.33 tokens/sec
Step 8091 | loss: 3.196118 | lr:4.1199e-04 | norm 0.2986 | dt 338.40ms | 1549308.39 tokens/sec
Step 8092 | loss: 3.323757 | lr:4.1195e-04 | norm 0.2999 | dt 338.74ms | 1547776.27 tokens/sec
Step 8093 | loss: 3.247215 | lr:4.1190e-04 | norm 0.3188 | dt 338.69ms | 1547995.27 tokens/sec
Step 8094 | loss: 3.295085 | lr:4.1186e-04 | norm 0.2992 | dt 339.59ms | 1543876.27 tokens/sec
Step 8095 | loss: 3.255455 | lr:4.1182e-04 | norm 0.3026 | dt 338.28ms | 1549880.57 tokens/sec
Step 8096 | loss: 3.209128 | lr:4.1177e-04 | norm 0.2796 | dt 338.04ms | 1550972.61 tokens/sec
Step 8097 | loss: 3.270820 | lr:4.1173e-04 | norm 0.2951 | dt 338.59ms | 1548431.27 tokens/sec
Step 8098 | loss: 3.226088 | lr:4.1168e-04 | norm 0.2720 | dt 339.54ms | 1544107.18 tokens/sec
Step 8099 | loss: 3.265110 | lr:4.1164e-04 | norm 0.2925 | dt 338.32ms | 1549690.53 tokens/sec
Step 8100 | loss: 3.261271 | lr:4.1160e-04 | norm 0.2692 | dt 338.03ms | 1551008.71 tokens/sec
Step 8101 | loss: 3.217356 | lr:4.1155e-04 | norm 0.2750 | dt 338.12ms | 1550582.19 tokens/sec
Step 8102 | loss: 3.210531 | lr:4.1151e-04 | norm 0.2582 | dt 338.21ms | 1550162.46 tokens/sec
Step 8103 | loss: 3.227592 | lr:4.1146e-04 | norm 0.2587 | dt 339.67ms | 1543505.66 tokens/sec
Step 8104 | loss: 3.269597 | lr:4.1142e-04 | norm 0.2839 | dt 337.30ms | 1554371.14 tokens/sec
Step 8105 | loss: 3.220111 | lr:4.1138e-04 | norm 0.2860 | dt 338.21ms | 1550207.26 tokens/sec
Step 8106 | loss: 3.241836 | lr:4.1133e-04 | norm 0.2887 | dt 337.43ms | 1553746.23 tokens/sec
Step 8107 | loss: 3.229162 | lr:4.1129e-04 | norm 0.3049 | dt 338.16ms | 1550435.69 tokens/sec
Step 8108 | loss: 3.265256 | lr:4.1124e-04 | norm 0.2753 | dt 338.88ms | 1547116.38 tokens/sec
Step 8109 | loss: 3.260456 | lr:4.1120e-04 | norm 0.2822 | dt 337.69ms | 1552554.91 tokens/sec
Step 8110 | loss: 3.213652 | lr:4.1116e-04 | norm 0.2762 | dt 338.56ms | 1548563.21 tokens/sec
Step 8111 | loss: 3.276616 | lr:4.1111e-04 | norm 0.2868 | dt 338.84ms | 1547315.59 tokens/sec
Step 8112 | loss: 3.199212 | lr:4.1107e-04 | norm 0.2757 | dt 337.94ms | 1551439.85 tokens/sec
Step 8113 | loss: 3.206261 | lr:4.1102e-04 | norm 0.2617 | dt 339.13ms | 1545978.67 tokens/sec
Step 8114 | loss: 3.253786 | lr:4.1098e-04 | norm 0.3532 | dt 337.90ms | 1551597.48 tokens/sec
Step 8115 | loss: 3.261634 | lr:4.1093e-04 | norm 0.3006 | dt 337.73ms | 1552370.79 tokens/sec
Step 8116 | loss: 3.284738 | lr:4.1089e-04 | norm 0.2837 | dt 338.14ms | 1550492.54 tokens/sec
Step 8117 | loss: 3.245115 | lr:4.1085e-04 | norm 0.3098 | dt 337.77ms | 1552198.75 tokens/sec
Step 8118 | loss: 3.301726 | lr:4.1080e-04 | norm 0.2897 | dt 337.52ms | 1553366.48 tokens/sec
Step 8119 | loss: 3.284400 | lr:4.1076e-04 | norm 0.3101 | dt 337.46ms | 1553649.63 tokens/sec
Step 8120 | loss: 3.201373 | lr:4.1071e-04 | norm 0.3021 | dt 338.56ms | 1548589.39 tokens/sec
Step 8121 | loss: 3.255826 | lr:4.1067e-04 | norm 0.3590 | dt 337.17ms | 1554958.07 tokens/sec
Step 8122 | loss: 3.264095 | lr:4.1063e-04 | norm 0.2947 | dt 337.04ms | 1555563.05 tokens/sec
Step 8123 | loss: 3.277185 | lr:4.1058e-04 | norm 0.2996 | dt 338.47ms | 1548992.99 tokens/sec
Step 8124 | loss: 3.225475 | lr:4.1054e-04 | norm 0.2802 | dt 337.51ms | 1553420.25 tokens/sec
Step 8125 | loss: 3.270783 | lr:4.1049e-04 | norm 0.4048 | dt 337.22ms | 1554741.49 tokens/sec
Step 8126 | loss: 3.257597 | lr:4.1045e-04 | norm 0.2758 | dt 900.57ms | 582174.95 tokens/sec
Step 8127 | loss: 3.269835 | lr:4.1041e-04 | norm 0.2905 | dt 335.10ms | 1564565.39 tokens/sec
Step 8128 | loss: 3.264604 | lr:4.1036e-04 | norm 0.2504 | dt 337.21ms | 1554801.95 tokens/sec
Step 8129 | loss: 3.334869 | lr:4.1032e-04 | norm 0.2832 | dt 337.62ms | 1552894.79 tokens/sec
Step 8130 | loss: 3.264106 | lr:4.1027e-04 | norm 0.2928 | dt 337.96ms | 1551344.63 tokens/sec
Step 8131 | loss: 3.253109 | lr:4.1023e-04 | norm 0.2676 | dt 337.17ms | 1554980.06 tokens/sec
Step 8132 | loss: 3.251608 | lr:4.1018e-04 | norm 0.2722 | dt 340.03ms | 1541864.98 tokens/sec
Step 8133 | loss: 3.262100 | lr:4.1014e-04 | norm 0.2694 | dt 338.12ms | 1550594.22 tokens/sec
Step 8134 | loss: 3.277807 | lr:4.1010e-04 | norm 0.2853 | dt 337.60ms | 1553002.27 tokens/sec
Step 8135 | loss: 3.270673 | lr:4.1005e-04 | norm 0.2648 | dt 337.82ms | 1551980.75 tokens/sec
Step 8136 | loss: 3.228927 | lr:4.1001e-04 | norm 0.2985 | dt 337.72ms | 1552412.43 tokens/sec
Step 8137 | loss: 3.189961 | lr:4.0996e-04 | norm 0.2735 | dt 337.68ms | 1552604.24 tokens/sec
Step 8138 | loss: 3.223339 | lr:4.0992e-04 | norm 0.2428 | dt 337.77ms | 1552214.09 tokens/sec
Step 8139 | loss: 3.301940 | lr:4.0988e-04 | norm 0.2752 | dt 337.78ms | 1552168.08 tokens/sec
Step 8140 | loss: 3.212442 | lr:4.0983e-04 | norm 0.2963 | dt 337.97ms | 1551298.66 tokens/sec
Step 8141 | loss: 3.174214 | lr:4.0979e-04 | norm 0.2468 | dt 338.34ms | 1549571.50 tokens/sec
Step 8142 | loss: 3.208925 | lr:4.0974e-04 | norm 0.2777 | dt 338.45ms | 1549101.02 tokens/sec
Step 8143 | loss: 3.225474 | lr:4.0970e-04 | norm 0.2612 | dt 337.78ms | 1552166.98 tokens/sec
Step 8144 | loss: 3.263363 | lr:4.0966e-04 | norm 0.2584 | dt 337.83ms | 1551930.37 tokens/sec
Step 8145 | loss: 3.243186 | lr:4.0961e-04 | norm 0.2773 | dt 338.26ms | 1549970.15 tokens/sec
Step 8146 | loss: 3.261007 | lr:4.0957e-04 | norm 0.2886 | dt 338.80ms | 1547486.54 tokens/sec
Step 8147 | loss: 3.222164 | lr:4.0952e-04 | norm 0.2868 | dt 337.75ms | 1552290.79 tokens/sec
Step 8148 | loss: 3.251890 | lr:4.0948e-04 | norm 0.2648 | dt 338.03ms | 1551024.03 tokens/sec
Step 8149 | loss: 3.255762 | lr:4.0943e-04 | norm 0.2880 | dt 339.62ms | 1543761.39 tokens/sec
Step 8150 | loss: 3.357314 | lr:4.0939e-04 | norm 0.2748 | dt 338.53ms | 1548706.08 tokens/sec
Step 8151 | loss: 3.199281 | lr:4.0935e-04 | norm 0.2883 | dt 338.13ms | 1550536.27 tokens/sec
Step 8152 | loss: 3.335026 | lr:4.0930e-04 | norm 0.2908 | dt 338.24ms | 1550029.15 tokens/sec
Step 8153 | loss: 3.267707 | lr:4.0926e-04 | norm 0.3073 | dt 338.88ms | 1547110.93 tokens/sec
Step 8154 | loss: 3.241431 | lr:4.0921e-04 | norm 0.2724 | dt 338.42ms | 1549219.98 tokens/sec
Step 8155 | loss: 3.246546 | lr:4.0917e-04 | norm 0.2908 | dt 339.33ms | 1545082.53 tokens/sec
Step 8156 | loss: 3.185663 | lr:4.0913e-04 | norm 0.2934 | dt 338.74ms | 1547756.66 tokens/sec
Step 8157 | loss: 3.196261 | lr:4.0908e-04 | norm 0.6063 | dt 338.43ms | 1549165.41 tokens/sec
Step 8158 | loss: 3.243211 | lr:4.0904e-04 | norm 0.3870 | dt 339.13ms | 1545992.80 tokens/sec
Step 8159 | loss: 3.295562 | lr:4.0899e-04 | norm 0.3318 | dt 338.55ms | 1548633.01 tokens/sec
Step 8160 | loss: 3.300566 | lr:4.0895e-04 | norm 0.3288 | dt 339.36ms | 1544941.41 tokens/sec
Step 8161 | loss: 3.229218 | lr:4.0890e-04 | norm 0.3159 | dt 338.59ms | 1548441.09 tokens/sec
Step 8162 | loss: 3.296865 | lr:4.0886e-04 | norm 0.3014 | dt 338.51ms | 1548820.62 tokens/sec
Step 8163 | loss: 3.322412 | lr:4.0882e-04 | norm 0.3039 | dt 338.57ms | 1548553.40 tokens/sec
Step 8164 | loss: 3.355034 | lr:4.0877e-04 | norm 0.2950 | dt 337.83ms | 1551939.13 tokens/sec
Step 8165 | loss: 3.260425 | lr:4.0873e-04 | norm 0.2781 | dt 337.72ms | 1552440.92 tokens/sec
Step 8166 | loss: 3.229074 | lr:4.0868e-04 | norm 0.2769 | dt 337.27ms | 1554495.30 tokens/sec
Step 8167 | loss: 3.297243 | lr:4.0864e-04 | norm 0.2814 | dt 337.68ms | 1552629.46 tokens/sec
Step 8168 | loss: 3.239946 | lr:4.0860e-04 | norm 0.2748 | dt 339.18ms | 1545751.55 tokens/sec
Step 8169 | loss: 3.356493 | lr:4.0855e-04 | norm 0.2980 | dt 1003.97ms | 522216.59 tokens/sec
Step 8170 | loss: 3.259583 | lr:4.0851e-04 | norm 0.2966 | dt 338.47ms | 1549006.09 tokens/sec
Step 8171 | loss: 3.187451 | lr:4.0846e-04 | norm 0.2831 | dt 338.38ms | 1549386.98 tokens/sec
Step 8172 | loss: 3.198837 | lr:4.0842e-04 | norm 0.2764 | dt 337.91ms | 1551567.92 tokens/sec
Step 8173 | loss: 3.238620 | lr:4.0837e-04 | norm 0.2920 | dt 338.21ms | 1550177.75 tokens/sec
Step 8174 | loss: 3.186377 | lr:4.0833e-04 | norm 0.2854 | dt 338.03ms | 1551012.00 tokens/sec
Step 8175 | loss: 3.175390 | lr:4.0829e-04 | norm 0.2748 | dt 338.14ms | 1550490.35 tokens/sec
Step 8176 | loss: 3.202075 | lr:4.0824e-04 | norm 0.2732 | dt 338.82ms | 1547412.50 tokens/sec
Step 8177 | loss: 3.232442 | lr:4.0820e-04 | norm 0.2690 | dt 338.83ms | 1547334.10 tokens/sec
Step 8178 | loss: 3.191294 | lr:4.0815e-04 | norm 0.2474 | dt 338.19ms | 1550299.06 tokens/sec
Step 8179 | loss: 3.215057 | lr:4.0811e-04 | norm 0.2734 | dt 337.82ms | 1551992.80 tokens/sec
Step 8180 | loss: 3.222071 | lr:4.0806e-04 | norm 0.2704 | dt 338.81ms | 1547426.65 tokens/sec
Step 8181 | loss: 3.229340 | lr:4.0802e-04 | norm 0.2735 | dt 338.64ms | 1548227.41 tokens/sec
Step 8182 | loss: 3.218436 | lr:4.0798e-04 | norm 0.3072 | dt 338.15ms | 1550445.53 tokens/sec
Step 8183 | loss: 3.240691 | lr:4.0793e-04 | norm 0.3145 | dt 338.48ms | 1548929.71 tokens/sec
Step 8184 | loss: 3.192504 | lr:4.0789e-04 | norm 0.2581 | dt 338.48ms | 1548955.90 tokens/sec
Step 8185 | loss: 3.276652 | lr:4.0784e-04 | norm 0.3422 | dt 338.43ms | 1549193.78 tokens/sec
Step 8186 | loss: 3.253357 | lr:4.0780e-04 | norm 0.2907 | dt 338.93ms | 1546900.89 tokens/sec
Step 8187 | loss: 3.318826 | lr:4.0775e-04 | norm 0.2903 | dt 338.91ms | 1546970.53 tokens/sec
Step 8188 | loss: 3.277452 | lr:4.0771e-04 | norm 0.2885 | dt 337.27ms | 1554517.28 tokens/sec
Step 8189 | loss: 3.268550 | lr:4.0767e-04 | norm 0.3052 | dt 337.92ms | 1551536.18 tokens/sec
Step 8190 | loss: 3.290674 | lr:4.0762e-04 | norm 0.2924 | dt 338.60ms | 1548381.12 tokens/sec
Step 8191 | loss: 3.318974 | lr:4.0758e-04 | norm 0.3032 | dt 337.22ms | 1554752.48 tokens/sec
Step 8192 | loss: 3.239528 | lr:4.0753e-04 | norm 0.3087 | dt 338.32ms | 1549701.45 tokens/sec
Step 8193 | loss: 3.293743 | lr:4.0749e-04 | norm 0.2789 | dt 338.09ms | 1550719.96 tokens/sec
Step 8194 | loss: 3.236170 | lr:4.0745e-04 | norm 0.3124 | dt 338.12ms | 1550582.19 tokens/sec
Step 8195 | loss: 3.333051 | lr:4.0740e-04 | norm 0.3104 | dt 337.64ms | 1552809.26 tokens/sec
Step 8196 | loss: 3.268675 | lr:4.0736e-04 | norm 0.2608 | dt 338.78ms | 1547591.09 tokens/sec
Step 8197 | loss: 3.286025 | lr:4.0731e-04 | norm 0.2959 | dt 338.47ms | 1549005.00 tokens/sec
Step 8198 | loss: 3.247318 | lr:4.0727e-04 | norm 0.2809 | dt 338.20ms | 1550224.75 tokens/sec
Step 8199 | loss: 3.269570 | lr:4.0722e-04 | norm 0.2685 | dt 339.13ms | 1545977.59 tokens/sec
Step 8200 | loss: 3.238856 | lr:4.0718e-04 | norm 0.3006 | dt 338.60ms | 1548386.57 tokens/sec
Step 8201 | loss: 3.382454 | lr:4.0714e-04 | norm 0.3069 | dt 338.96ms | 1546776.85 tokens/sec
Step 8202 | loss: 3.273933 | lr:4.0709e-04 | norm 0.3151 | dt 337.83ms | 1551921.61 tokens/sec
Step 8203 | loss: 3.245227 | lr:4.0705e-04 | norm 0.2887 | dt 339.13ms | 1545976.50 tokens/sec
Step 8204 | loss: 3.219453 | lr:4.0700e-04 | norm 0.2536 | dt 338.32ms | 1549680.70 tokens/sec
Step 8205 | loss: 3.241534 | lr:4.0696e-04 | norm 0.3018 | dt 338.44ms | 1549111.93 tokens/sec
Step 8206 | loss: 3.245841 | lr:4.0691e-04 | norm 0.2890 | dt 337.79ms | 1552119.87 tokens/sec
Step 8207 | loss: 3.226181 | lr:4.0687e-04 | norm 0.2769 | dt 337.40ms | 1553893.35 tokens/sec
Step 8208 | loss: 3.243958 | lr:4.0683e-04 | norm 0.2595 | dt 338.52ms | 1548774.80 tokens/sec
Step 8209 | loss: 3.217665 | lr:4.0678e-04 | norm 0.2622 | dt 338.45ms | 1549103.20 tokens/sec
Step 8210 | loss: 3.272252 | lr:4.0674e-04 | norm 0.2737 | dt 338.60ms | 1548380.03 tokens/sec
Step 8211 | loss: 3.255227 | lr:4.0669e-04 | norm 0.2619 | dt 337.95ms | 1551368.71 tokens/sec
Step 8212 | loss: 3.238413 | lr:4.0665e-04 | norm 0.3028 | dt 338.32ms | 1549675.24 tokens/sec
Step 8213 | loss: 3.189549 | lr:4.0660e-04 | norm 0.2622 | dt 338.40ms | 1549298.56 tokens/sec
Step 8214 | loss: 3.209934 | lr:4.0656e-04 | norm 0.2800 | dt 338.16ms | 1550395.25 tokens/sec
Step 8215 | loss: 3.256291 | lr:4.0652e-04 | norm 0.2667 | dt 337.98ms | 1551258.17 tokens/sec
Step 8216 | loss: 3.194663 | lr:4.0647e-04 | norm 0.3127 | dt 338.34ms | 1549581.33 tokens/sec
Step 8217 | loss: 3.200536 | lr:4.0643e-04 | norm 0.2947 | dt 338.62ms | 1548322.25 tokens/sec
Step 8218 | loss: 3.231737 | lr:4.0638e-04 | norm 0.3270 | dt 337.57ms | 1553143.76 tokens/sec
Step 8219 | loss: 3.212879 | lr:4.0634e-04 | norm 0.3036 | dt 338.08ms | 1550770.27 tokens/sec
Step 8220 | loss: 3.237152 | lr:4.0629e-04 | norm 0.2804 | dt 338.01ms | 1551091.86 tokens/sec
Step 8221 | loss: 3.260211 | lr:4.0625e-04 | norm 0.2957 | dt 337.94ms | 1551401.54 tokens/sec
Step 8222 | loss: 3.250071 | lr:4.0620e-04 | norm 0.2910 | dt 338.05ms | 1550928.86 tokens/sec
Step 8223 | loss: 3.251033 | lr:4.0616e-04 | norm 0.2740 | dt 337.94ms | 1551445.32 tokens/sec
Step 8224 | loss: 3.271669 | lr:4.0612e-04 | norm 0.3193 | dt 337.21ms | 1554765.67 tokens/sec
Step 8225 | loss: 3.221554 | lr:4.0607e-04 | norm 0.3457 | dt 338.30ms | 1549757.15 tokens/sec
Step 8226 | loss: 3.182932 | lr:4.0603e-04 | norm 0.3286 | dt 338.26ms | 1549952.67 tokens/sec
Step 8227 | loss: 3.231480 | lr:4.0598e-04 | norm 0.3261 | dt 338.29ms | 1549808.48 tokens/sec
Step 8228 | loss: 3.232180 | lr:4.0594e-04 | norm 0.3143 | dt 338.32ms | 1549666.50 tokens/sec
Step 8229 | loss: 3.266240 | lr:4.0589e-04 | norm 0.3034 | dt 338.23ms | 1550092.52 tokens/sec
Step 8230 | loss: 3.308939 | lr:4.0585e-04 | norm 0.3001 | dt 338.51ms | 1548814.07 tokens/sec
Step 8231 | loss: 3.230188 | lr:4.0581e-04 | norm 0.3080 | dt 339.04ms | 1546385.27 tokens/sec
Step 8232 | loss: 3.289639 | lr:4.0576e-04 | norm 0.2995 | dt 338.45ms | 1549097.75 tokens/sec
Step 8233 | loss: 3.238082 | lr:4.0572e-04 | norm 0.3336 | dt 338.41ms | 1549275.64 tokens/sec
Step 8234 | loss: 3.312052 | lr:4.0567e-04 | norm 0.2920 | dt 338.78ms | 1547596.54 tokens/sec
Step 8235 | loss: 3.196232 | lr:4.0563e-04 | norm 0.2883 | dt 338.56ms | 1548576.30 tokens/sec
Step 8236 | loss: 3.244019 | lr:4.0558e-04 | norm 0.2678 | dt 337.62ms | 1552913.43 tokens/sec
Step 8237 | loss: 3.212271 | lr:4.0554e-04 | norm 0.2916 | dt 337.78ms | 1552151.64 tokens/sec
Step 8238 | loss: 3.252090 | lr:4.0550e-04 | norm 0.2749 | dt 338.16ms | 1550405.09 tokens/sec
Step 8239 | loss: 3.231754 | lr:4.0545e-04 | norm 0.2878 | dt 337.60ms | 1552998.98 tokens/sec
Step 8240 | loss: 3.151586 | lr:4.0541e-04 | norm 0.2848 | dt 338.75ms | 1547714.18 tokens/sec
Step 8241 | loss: 3.231790 | lr:4.0536e-04 | norm 0.3103 | dt 338.58ms | 1548499.97 tokens/sec
Step 8242 | loss: 3.226942 | lr:4.0532e-04 | norm 0.2996 | dt 337.54ms | 1553276.51 tokens/sec
Step 8243 | loss: 3.188162 | lr:4.0527e-04 | norm 0.3294 | dt 337.17ms | 1554987.76 tokens/sec
Step 8244 | loss: 3.213265 | lr:4.0523e-04 | norm 0.2846 | dt 337.35ms | 1554117.38 tokens/sec
Step 8245 | loss: 3.182152 | lr:4.0518e-04 | norm 0.3044 | dt 338.19ms | 1550266.27 tokens/sec
Step 8246 | loss: 3.186090 | lr:4.0514e-04 | norm 0.2873 | dt 338.25ms | 1550013.85 tokens/sec
Step 8247 | loss: 3.218218 | lr:4.0510e-04 | norm 0.2908 | dt 338.66ms | 1548104.25 tokens/sec
Step 8248 | loss: 3.192023 | lr:4.0505e-04 | norm 0.2773 | dt 338.63ms | 1548239.40 tokens/sec
Step 8249 | loss: 3.254664 | lr:4.0501e-04 | norm 0.2862 | dt 339.21ms | 1545609.23 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 8250: 3.2454
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2900/10042=0.2888


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, and a lot of people are confused. One thing I've noticed a lot is that the language that I'm teaching has
rank 5 sample 1 >Hello, I'm a language model, why doesn't this explain this problem?
Hi, I'm a linguist! This problem is similar to what I
rank 5 sample 2 >Hello, I'm a language model, but it can't think of enough things to understand the structure of our language, let alone the number of words. This
rank 5 sample 3 >Hello, I'm a language model, thanks.<|endoftext|>To celebrate a new year in May this year, I'm running around on the project "The Year of




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to give you a bunch of instructions on how to speak for various languages. Once I get to that
rank 7 sample 1 >Hello, I'm a language model, do I?
The simplest one way to tell your baby is that he's still speaking his mother tongue and not a
rank 7 sample 2 >Hello, I'm a language model, and I've noticed that the numbers from our other modules are the same except that they have the same letter names - we
rank 7 sample 3 >Hello, I'm a language model, though I'm not going to put out your notes.
On a given day, a group of students will learn the




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, please explain to everyone that I am learning it, and then we'll talk to my 3rd grade teacher in the coming
rank 2 sample 1 >Hello, I'm a language model, and I must do so in a reasonable-to-fail situation. When I'm talking about a program that can take
rank 2 sample 2 >Hello, I'm a language model, but I've also found it quite annoying to say "how is the name of this? So much has happened with that
rank 2 sample 3 >Hello, I'm a language model, and we got to the end of this last section from the very beginning. Anyway, it had many wonderful discussions, and




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, that doesn't understand that "normal" language is like the normal language, except that it doesn't understand the normal language
rank 6 sample 1 >Hello, I'm a language model, which is based on the concept of "talk" that I'm talking about. I'm talking about a word that someone
rank 6 sample 2 >Hello, I'm a language model, and that's what I do in my next project.
So for now, I'm going to put in something that
rank 6 sample 3 >Hello, I'm a language model, I'm a new friend, this time as an English language model.
One thing to not forget: we now have




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not trying to prove something by a formula, but by any type of logic we can show it, for
rank 3 sample 1 >Hello, I'm a language model, so this one is a way.
- In this situation, the input is the same as the output. We'll
rank 3 sample 2 >Hello, I'm a language model, so here we're...
Here's a very simple code with a little twist, but we didn't really notice a
rank 3 sample 3 >Hello, I'm a language model, so what do you think about language? I don't mean that the most important thing we will be defining in our next




ddp_rank 0: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I am going for one! Anyway, with the aid of Python, this is what I wanted for the first timerank 1 sample 0 >Hello, I'm a language model, an editor. I'm not a scientist but you have to create a program for me and my program. You have to

rank 1 sample 1 >Hello, I'm a language model, a computer program and a computer science textbook. I'm learning how to program.
First of all I want to makerank 0 sample 1 >Hello, I'm a language model, and i'm a teacher. I're a native Korean who enjoys the subject, but it's a real learning experience.

rank 0 sample 2 >Hello, I'm a language model, but I never think I am saying I'm stupid.
"My teacher is stupid."
"I'm a poor
rank 1 sample 2 >Hello, I'm a language model, and while we're learning, I'm a language model and as such we're learning. So it's not as hard
rank 0 sample 3 >Hello, I'm a language model, and how do I get there?" This was indeed a problem. This is the issue we faced. Our task, so


rank 1 sample 3 >Hello, I'm a language model, so I'm just gonna see it happen everywhere.
All you would ever need to hear were that the brain isn't




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I know that it's not a very easy task. It's hard to talk or explain one another because you have
rank 4 sample 1 >Hello, I'm a language model, or language model just to show us the basic syntax to this language. For a simple program to work on this sentence structure
rank 4 sample 2 >Hello, I'm a language model, I'm a parent to some of your favorite TV shows. My youngest son is 6-8 months old. You know
rank 4 sample 3 >Hello, I'm a language model, so let me write this one for you here in more detail. I came across this tutorial in other languages, but this


Step 8250 | loss: 3.228972 | lr:4.0496e-04 | norm 0.3258 | dt 18986.12ms | 27614.28 tokens/sec
Step 8251 | loss: 3.252768 | lr:4.0492e-04 | norm 0.2798 | dt 335.93ms | 1560723.32 tokens/sec
Step 8252 | loss: 3.267390 | lr:4.0487e-04 | norm 0.2703 | dt 336.57ms | 1557737.14 tokens/sec
Step 8253 | loss: 3.152946 | lr:4.0483e-04 | norm 0.3065 | dt 336.01ms | 1560315.79 tokens/sec
Step 8254 | loss: 3.195959 | lr:4.0479e-04 | norm 0.2932 | dt 335.76ms | 1561488.00 tokens/sec
Step 8255 | loss: 3.221380 | lr:4.0474e-04 | norm 0.3087 | dt 337.03ms | 1555597.16 tokens/sec
Step 8256 | loss: 3.293509 | lr:4.0470e-04 | norm 0.2731 | dt 336.25ms | 1559199.50 tokens/sec
Step 8257 | loss: 3.269159 | lr:4.0465e-04 | norm 0.2922 | dt 336.91ms | 1556176.20 tokens/sec
Step 8258 | loss: 3.218079 | lr:4.0461e-04 | norm 0.2526 | dt 335.74ms | 1561568.95 tokens/sec
Step 8259 | loss: 3.227492 | lr:4.0456e-04 | norm 0.3017 | dt 335.16ms | 1564298.28 tokens/sec
Step 8260 | loss: 3.217151 | lr:4.0452e-04 | norm 0.2978 | dt 335.04ms | 1564862.66 tokens/sec
Step 8261 | loss: 3.252598 | lr:4.0447e-04 | norm 0.2696 | dt 336.22ms | 1559369.77 tokens/sec
Step 8262 | loss: 3.231320 | lr:4.0443e-04 | norm 0.2749 | dt 336.24ms | 1559261.42 tokens/sec
Step 8263 | loss: 3.296817 | lr:4.0439e-04 | norm 0.2792 | dt 336.21ms | 1559419.54 tokens/sec
Step 8264 | loss: 3.233821 | lr:4.0434e-04 | norm 0.2666 | dt 336.82ms | 1556561.74 tokens/sec
Step 8265 | loss: 3.241403 | lr:4.0430e-04 | norm 0.2926 | dt 335.67ms | 1561912.78 tokens/sec
Step 8266 | loss: 3.231320 | lr:4.0425e-04 | norm 0.2713 | dt 336.77ms | 1556794.25 tokens/sec
Step 8267 | loss: 3.213071 | lr:4.0421e-04 | norm 0.2686 | dt 336.11ms | 1559853.15 tokens/sec
Step 8268 | loss: 3.281300 | lr:4.0416e-04 | norm 0.2713 | dt 335.85ms | 1561084.51 tokens/sec
Step 8269 | loss: 3.231546 | lr:4.0412e-04 | norm 0.2764 | dt 336.66ms | 1557306.91 tokens/sec
Step 8270 | loss: 3.256133 | lr:4.0407e-04 | norm 0.2582 | dt 337.13ms | 1555128.51 tokens/sec
Step 8271 | loss: 3.249586 | lr:4.0403e-04 | norm 0.2551 | dt 336.47ms | 1558197.42 tokens/sec
Step 8272 | loss: 3.237966 | lr:4.0399e-04 | norm 0.2535 | dt 337.19ms | 1554888.80 tokens/sec
Step 8273 | loss: 3.269444 | lr:4.0394e-04 | norm 0.2565 | dt 337.27ms | 1554516.18 tokens/sec
Step 8274 | loss: 3.319173 | lr:4.0390e-04 | norm 0.2743 | dt 337.32ms | 1554261.28 tokens/sec
Step 8275 | loss: 3.215935 | lr:4.0385e-04 | norm 0.2888 | dt 337.73ms | 1552380.65 tokens/sec
Step 8276 | loss: 3.310592 | lr:4.0381e-04 | norm 0.2915 | dt 336.98ms | 1555821.68 tokens/sec
Step 8277 | loss: 3.261705 | lr:4.0376e-04 | norm 0.2948 | dt 337.18ms | 1554914.09 tokens/sec
Step 8278 | loss: 3.277822 | lr:4.0372e-04 | norm 0.2961 | dt 338.23ms | 1550071.76 tokens/sec
Step 8279 | loss: 3.290655 | lr:4.0367e-04 | norm 0.3066 | dt 337.57ms | 1553125.11 tokens/sec
Step 8280 | loss: 3.215769 | lr:4.0363e-04 | norm 0.2631 | dt 337.82ms | 1551989.51 tokens/sec
Step 8281 | loss: 3.240989 | lr:4.0359e-04 | norm 0.3049 | dt 337.00ms | 1555745.73 tokens/sec
Step 8282 | loss: 3.191602 | lr:4.0354e-04 | norm 0.2607 | dt 337.49ms | 1553491.58 tokens/sec
Step 8283 | loss: 3.197880 | lr:4.0350e-04 | norm 0.2812 | dt 337.75ms | 1552314.90 tokens/sec
Step 8284 | loss: 3.185180 | lr:4.0345e-04 | norm 0.2390 | dt 336.57ms | 1557750.39 tokens/sec
Step 8285 | loss: 3.262568 | lr:4.0341e-04 | norm 0.2697 | dt 338.56ms | 1548586.12 tokens/sec
Step 8286 | loss: 3.230059 | lr:4.0336e-04 | norm 0.2844 | dt 337.41ms | 1553856.02 tokens/sec
Step 8287 | loss: 3.235481 | lr:4.0332e-04 | norm 0.2942 | dt 336.87ms | 1556359.03 tokens/sec
Step 8288 | loss: 3.208289 | lr:4.0327e-04 | norm 0.2824 | dt 338.65ms | 1548171.82 tokens/sec
Step 8289 | loss: 3.240057 | lr:4.0323e-04 | norm 0.2741 | dt 337.87ms | 1551736.53 tokens/sec
Step 8290 | loss: 3.231109 | lr:4.0319e-04 | norm 0.2827 | dt 337.53ms | 1553320.39 tokens/sec
Step 8291 | loss: 3.283366 | lr:4.0314e-04 | norm 0.2919 | dt 336.73ms | 1556997.07 tokens/sec
Step 8292 | loss: 3.267281 | lr:4.0310e-04 | norm 0.2989 | dt 337.78ms | 1552156.02 tokens/sec
Step 8293 | loss: 3.254223 | lr:4.0305e-04 | norm 0.2610 | dt 338.46ms | 1549050.82 tokens/sec
Step 8294 | loss: 3.262594 | lr:4.0301e-04 | norm 0.2977 | dt 338.18ms | 1550304.53 tokens/sec
Step 8295 | loss: 3.225197 | lr:4.0296e-04 | norm 0.2783 | dt 338.56ms | 1548586.12 tokens/sec
Step 8296 | loss: 3.267019 | lr:4.0292e-04 | norm 0.2695 | dt 338.45ms | 1549104.29 tokens/sec
Step 8297 | loss: 3.248069 | lr:4.0287e-04 | norm 0.2969 | dt 337.75ms | 1552305.04 tokens/sec
Step 8298 | loss: 3.246582 | lr:4.0283e-04 | norm 0.2575 | dt 337.81ms | 1552033.33 tokens/sec
Step 8299 | loss: 3.280675 | lr:4.0279e-04 | norm 0.2824 | dt 337.36ms | 1554106.40 tokens/sec
Step 8300 | loss: 3.258471 | lr:4.0274e-04 | norm 0.2699 | dt 337.72ms | 1552422.29 tokens/sec
Step 8301 | loss: 3.278606 | lr:4.0270e-04 | norm 0.2716 | dt 337.89ms | 1551634.71 tokens/sec
Step 8302 | loss: 3.241121 | lr:4.0265e-04 | norm 0.2738 | dt 338.68ms | 1548020.33 tokens/sec
Step 8303 | loss: 3.306103 | lr:4.0261e-04 | norm 0.2534 | dt 337.66ms | 1552720.45 tokens/sec
Step 8304 | loss: 3.237798 | lr:4.0256e-04 | norm 0.2551 | dt 337.38ms | 1554008.65 tokens/sec
Step 8305 | loss: 3.226460 | lr:4.0252e-04 | norm 0.2715 | dt 338.11ms | 1550660.91 tokens/sec
Step 8306 | loss: 3.235925 | lr:4.0247e-04 | norm 0.2786 | dt 338.74ms | 1547771.91 tokens/sec
Step 8307 | loss: 3.307559 | lr:4.0243e-04 | norm 0.2913 | dt 338.15ms | 1550477.23 tokens/sec
Step 8308 | loss: 3.275067 | lr:4.0238e-04 | norm 0.2763 | dt 338.26ms | 1549976.71 tokens/sec
Step 8309 | loss: 3.322522 | lr:4.0234e-04 | norm 0.3402 | dt 338.02ms | 1551043.72 tokens/sec
Step 8310 | loss: 3.236032 | lr:4.0230e-04 | norm 0.2818 | dt 338.79ms | 1547520.30 tokens/sec
Step 8311 | loss: 3.175534 | lr:4.0225e-04 | norm 0.2612 | dt 338.26ms | 1549942.84 tokens/sec
Step 8312 | loss: 3.235037 | lr:4.0221e-04 | norm 0.3004 | dt 337.81ms | 1552030.04 tokens/sec
Step 8313 | loss: 3.189987 | lr:4.0216e-04 | norm 0.2785 | dt 337.79ms | 1552111.11 tokens/sec
Step 8314 | loss: 3.204357 | lr:4.0212e-04 | norm 0.3175 | dt 339.40ms | 1544758.00 tokens/sec
Step 8315 | loss: 3.195571 | lr:4.0207e-04 | norm 0.3347 | dt 1029.15ms | 509436.82 tokens/sec
Step 8316 | loss: 3.218703 | lr:4.0203e-04 | norm 0.2685 | dt 337.51ms | 1553397.20 tokens/sec
Step 8317 | loss: 3.188141 | lr:4.0198e-04 | norm 0.3200 | dt 337.92ms | 1551511.00 tokens/sec
Step 8318 | loss: 3.246180 | lr:4.0194e-04 | norm 0.2816 | dt 339.49ms | 1544339.24 tokens/sec
Step 8319 | loss: 3.310283 | lr:4.0189e-04 | norm 0.3057 | dt 338.47ms | 1548975.54 tokens/sec
Step 8320 | loss: 3.196108 | lr:4.0185e-04 | norm 0.2978 | dt 338.34ms | 1549576.96 tokens/sec
Step 8321 | loss: 3.222576 | lr:4.0181e-04 | norm 0.3002 | dt 342.07ms | 1532673.52 tokens/sec
Step 8322 | loss: 3.256859 | lr:4.0176e-04 | norm 0.3035 | dt 339.40ms | 1544748.23 tokens/sec
Step 8323 | loss: 3.245378 | lr:4.0172e-04 | norm 0.2829 | dt 338.72ms | 1547871.05 tokens/sec
Step 8324 | loss: 3.349632 | lr:4.0167e-04 | norm 0.3175 | dt 338.68ms | 1548033.41 tokens/sec
Step 8325 | loss: 3.229936 | lr:4.0163e-04 | norm 0.3924 | dt 338.72ms | 1547839.46 tokens/sec
Step 8326 | loss: 3.321456 | lr:4.0158e-04 | norm 0.3682 | dt 338.92ms | 1546931.36 tokens/sec
Step 8327 | loss: 3.249945 | lr:4.0154e-04 | norm 0.3094 | dt 338.98ms | 1546675.67 tokens/sec
Step 8328 | loss: 3.288447 | lr:4.0149e-04 | norm 0.3406 | dt 337.73ms | 1552378.46 tokens/sec
Step 8329 | loss: 3.267454 | lr:4.0145e-04 | norm 0.3005 | dt 338.41ms | 1549251.63 tokens/sec
Step 8330 | loss: 3.270800 | lr:4.0140e-04 | norm 0.3183 | dt 338.30ms | 1549768.07 tokens/sec
Step 8331 | loss: 3.268271 | lr:4.0136e-04 | norm 0.3097 | dt 338.58ms | 1548509.78 tokens/sec
Step 8332 | loss: 3.280887 | lr:4.0132e-04 | norm 0.3079 | dt 338.19ms | 1550294.69 tokens/sec
Step 8333 | loss: 3.217221 | lr:4.0127e-04 | norm 0.2906 | dt 338.96ms | 1546744.21 tokens/sec
Step 8334 | loss: 3.273390 | lr:4.0123e-04 | norm 0.3168 | dt 338.57ms | 1548550.13 tokens/sec
Step 8335 | loss: 3.297705 | lr:4.0118e-04 | norm 0.3061 | dt 338.37ms | 1549461.22 tokens/sec
Step 8336 | loss: 3.268277 | lr:4.0114e-04 | norm 0.3099 | dt 339.64ms | 1543666.02 tokens/sec
Step 8337 | loss: 3.259862 | lr:4.0109e-04 | norm 0.2972 | dt 338.42ms | 1549234.17 tokens/sec
Step 8338 | loss: 3.233541 | lr:4.0105e-04 | norm 0.3002 | dt 338.76ms | 1547671.69 tokens/sec
Step 8339 | loss: 3.239931 | lr:4.0100e-04 | norm 0.2927 | dt 337.72ms | 1552438.73 tokens/sec
Step 8340 | loss: 3.256307 | lr:4.0096e-04 | norm 0.2794 | dt 338.91ms | 1546982.51 tokens/sec
Step 8341 | loss: 3.260378 | lr:4.0091e-04 | norm 0.2729 | dt 338.71ms | 1547918.99 tokens/sec
Step 8342 | loss: 3.297996 | lr:4.0087e-04 | norm 0.2780 | dt 338.25ms | 1550011.67 tokens/sec
Step 8343 | loss: 3.273688 | lr:4.0083e-04 | norm 0.2926 | dt 337.90ms | 1551608.43 tokens/sec
Step 8344 | loss: 3.225854 | lr:4.0078e-04 | norm 0.2573 | dt 338.88ms | 1547135.97 tokens/sec
Step 8345 | loss: 3.367079 | lr:4.0074e-04 | norm 0.3051 | dt 338.90ms | 1547022.77 tokens/sec
Step 8346 | loss: 3.285992 | lr:4.0069e-04 | norm 0.3369 | dt 338.50ms | 1548866.43 tokens/sec
Step 8347 | loss: 3.238357 | lr:4.0065e-04 | norm 0.2598 | dt 340.00ms | 1542030.41 tokens/sec
Step 8348 | loss: 3.263781 | lr:4.0060e-04 | norm 0.2608 | dt 339.80ms | 1542927.35 tokens/sec
Step 8349 | loss: 3.211326 | lr:4.0056e-04 | norm 0.2927 | dt 340.20ms | 1541118.31 tokens/sec
Step 8350 | loss: 3.218793 | lr:4.0051e-04 | norm 0.2585 | dt 342.18ms | 1532214.31 tokens/sec
Step 8351 | loss: 3.217682 | lr:4.0047e-04 | norm 0.2797 | dt 340.20ms | 1541126.95 tokens/sec
Step 8352 | loss: 3.261585 | lr:4.0042e-04 | norm 0.2570 | dt 340.05ms | 1541818.50 tokens/sec
Step 8353 | loss: 3.231100 | lr:4.0038e-04 | norm 0.2885 | dt 340.16ms | 1541279.25 tokens/sec
Step 8354 | loss: 3.171246 | lr:4.0033e-04 | norm 0.2475 | dt 339.08ms | 1546215.65 tokens/sec
Step 8355 | loss: 3.223976 | lr:4.0029e-04 | norm 0.2802 | dt 337.49ms | 1553487.19 tokens/sec
Step 8356 | loss: 3.178910 | lr:4.0025e-04 | norm 0.2586 | dt 338.97ms | 1546721.36 tokens/sec
Step 8357 | loss: 3.169756 | lr:4.0020e-04 | norm 0.2722 | dt 339.85ms | 1542714.11 tokens/sec
Step 8358 | loss: 3.199781 | lr:4.0016e-04 | norm 0.2837 | dt 340.09ms | 1541617.45 tokens/sec
Step 8359 | loss: 3.205581 | lr:4.0011e-04 | norm 0.2991 | dt 931.35ms | 562931.07 tokens/sec
Step 8360 | loss: 3.226981 | lr:4.0007e-04 | norm 0.2741 | dt 337.55ms | 1553199.71 tokens/sec
Step 8361 | loss: 3.214090 | lr:4.0002e-04 | norm 0.3475 | dt 338.97ms | 1546699.60 tokens/sec
Step 8362 | loss: 3.269319 | lr:3.9998e-04 | norm 0.2752 | dt 337.35ms | 1554148.14 tokens/sec
Step 8363 | loss: 3.260352 | lr:3.9993e-04 | norm 0.2617 | dt 337.42ms | 1553795.63 tokens/sec
Step 8364 | loss: 3.296672 | lr:3.9989e-04 | norm 0.2806 | dt 337.09ms | 1555347.40 tokens/sec
Step 8365 | loss: 3.223419 | lr:3.9984e-04 | norm 0.2806 | dt 338.61ms | 1548346.23 tokens/sec
Step 8366 | loss: 3.235260 | lr:3.9980e-04 | norm 0.2785 | dt 338.59ms | 1548450.90 tokens/sec
Step 8367 | loss: 3.269886 | lr:3.9975e-04 | norm 0.2515 | dt 337.76ms | 1552242.58 tokens/sec
Step 8368 | loss: 3.239802 | lr:3.9971e-04 | norm 0.2954 | dt 337.70ms | 1552516.55 tokens/sec
Step 8369 | loss: 3.260515 | lr:3.9967e-04 | norm 0.3473 | dt 337.57ms | 1553143.76 tokens/sec
Step 8370 | loss: 3.302350 | lr:3.9962e-04 | norm 0.3212 | dt 338.94ms | 1546866.07 tokens/sec
Step 8371 | loss: 3.252178 | lr:3.9958e-04 | norm 0.3278 | dt 338.59ms | 1548435.64 tokens/sec
Step 8372 | loss: 3.294087 | lr:3.9953e-04 | norm 0.3031 | dt 338.93ms | 1546894.36 tokens/sec
Step 8373 | loss: 3.267827 | lr:3.9949e-04 | norm 0.2837 | dt 338.21ms | 1550186.50 tokens/sec
Step 8374 | loss: 3.270714 | lr:3.9944e-04 | norm 0.2777 | dt 341.35ms | 1535927.87 tokens/sec
Step 8375 | loss: 3.289278 | lr:3.9940e-04 | norm 0.2817 | dt 338.15ms | 1550441.16 tokens/sec
Step 8376 | loss: 3.233057 | lr:3.9935e-04 | norm 0.2688 | dt 339.06ms | 1546283.06 tokens/sec
Step 8377 | loss: 3.310224 | lr:3.9931e-04 | norm 0.3015 | dt 339.19ms | 1545702.66 tokens/sec
Step 8378 | loss: 3.230286 | lr:3.9926e-04 | norm 0.2703 | dt 339.13ms | 1545998.24 tokens/sec
Step 8379 | loss: 3.265215 | lr:3.9922e-04 | norm 0.2668 | dt 338.95ms | 1546783.38 tokens/sec
Step 8380 | loss: 3.213832 | lr:3.9917e-04 | norm 0.2498 | dt 339.42ms | 1544651.66 tokens/sec
Step 8381 | loss: 3.241975 | lr:3.9913e-04 | norm 0.2771 | dt 339.80ms | 1542918.69 tokens/sec
Step 8382 | loss: 3.217388 | lr:3.9908e-04 | norm 0.2650 | dt 338.36ms | 1549479.78 tokens/sec
Step 8383 | loss: 3.230879 | lr:3.9904e-04 | norm 0.2792 | dt 338.76ms | 1547651.00 tokens/sec
Step 8384 | loss: 3.189478 | lr:3.9900e-04 | norm 0.2821 | dt 338.30ms | 1549789.91 tokens/sec
Step 8385 | loss: 3.249363 | lr:3.9895e-04 | norm 0.2805 | dt 337.69ms | 1552581.22 tokens/sec
Step 8386 | loss: 3.310972 | lr:3.9891e-04 | norm 0.2463 | dt 338.77ms | 1547606.34 tokens/sec
Step 8387 | loss: 3.193606 | lr:3.9886e-04 | norm 0.2586 | dt 337.70ms | 1552520.94 tokens/sec
Step 8388 | loss: 3.222280 | lr:3.9882e-04 | norm 0.2535 | dt 339.00ms | 1546577.77 tokens/sec
Step 8389 | loss: 3.220723 | lr:3.9877e-04 | norm 0.2681 | dt 338.61ms | 1548356.05 tokens/sec
Step 8390 | loss: 3.251424 | lr:3.9873e-04 | norm 0.2635 | dt 338.98ms | 1546646.30 tokens/sec
Step 8391 | loss: 3.310712 | lr:3.9868e-04 | norm 0.2720 | dt 338.56ms | 1548577.39 tokens/sec
Step 8392 | loss: 3.165466 | lr:3.9864e-04 | norm 0.2711 | dt 338.05ms | 1550904.79 tokens/sec
Step 8393 | loss: 3.356106 | lr:3.9859e-04 | norm 0.3522 | dt 339.01ms | 1546517.95 tokens/sec
Step 8394 | loss: 3.204330 | lr:3.9855e-04 | norm 0.3518 | dt 338.64ms | 1548204.52 tokens/sec
Step 8395 | loss: 3.232036 | lr:3.9850e-04 | norm 0.2726 | dt 339.11ms | 1546048.24 tokens/sec
Step 8396 | loss: 3.195839 | lr:3.9846e-04 | norm 0.3092 | dt 338.98ms | 1546664.79 tokens/sec
Step 8397 | loss: 3.192969 | lr:3.9841e-04 | norm 0.2722 | dt 338.21ms | 1550203.98 tokens/sec
Step 8398 | loss: 3.238096 | lr:3.9837e-04 | norm 0.2806 | dt 337.99ms | 1551177.20 tokens/sec
Step 8399 | loss: 3.231745 | lr:3.9833e-04 | norm 0.2799 | dt 339.14ms | 1545914.55 tokens/sec
Step 8400 | loss: 3.207026 | lr:3.9828e-04 | norm 0.2846 | dt 338.80ms | 1547462.59 tokens/sec
Step 8401 | loss: 3.265556 | lr:3.9824e-04 | norm 0.2844 | dt 337.84ms | 1551884.37 tokens/sec
Step 8402 | loss: 3.251017 | lr:3.9819e-04 | norm 0.2880 | dt 338.55ms | 1548623.19 tokens/sec
Step 8403 | loss: 3.241173 | lr:3.9815e-04 | norm 0.3071 | dt 338.74ms | 1547766.46 tokens/sec
Step 8404 | loss: 3.265178 | lr:3.9810e-04 | norm 0.2904 | dt 338.51ms | 1548810.80 tokens/sec
Step 8405 | loss: 3.270276 | lr:3.9806e-04 | norm 0.2632 | dt 337.79ms | 1552113.30 tokens/sec
Step 8406 | loss: 3.234896 | lr:3.9801e-04 | norm 0.2632 | dt 338.72ms | 1547851.44 tokens/sec
Step 8407 | loss: 3.336751 | lr:3.9797e-04 | norm 0.2956 | dt 337.91ms | 1551581.06 tokens/sec
Step 8408 | loss: 3.280417 | lr:3.9792e-04 | norm 0.3073 | dt 338.17ms | 1550361.36 tokens/sec
Step 8409 | loss: 3.270898 | lr:3.9788e-04 | norm 0.2482 | dt 337.68ms | 1552609.72 tokens/sec
Step 8410 | loss: 3.306595 | lr:3.9783e-04 | norm 0.3062 | dt 337.74ms | 1552320.38 tokens/sec
Step 8411 | loss: 3.276534 | lr:3.9779e-04 | norm 0.2800 | dt 337.38ms | 1553976.81 tokens/sec
Step 8412 | loss: 3.273904 | lr:3.9774e-04 | norm 0.2620 | dt 338.83ms | 1547364.59 tokens/sec
Step 8413 | loss: 3.204757 | lr:3.9770e-04 | norm 0.2794 | dt 337.90ms | 1551593.10 tokens/sec
Step 8414 | loss: 3.229494 | lr:3.9765e-04 | norm 0.2481 | dt 337.15ms | 1555073.53 tokens/sec
Step 8415 | loss: 3.225674 | lr:3.9761e-04 | norm 0.2937 | dt 338.11ms | 1550640.14 tokens/sec
Step 8416 | loss: 3.206280 | lr:3.9757e-04 | norm 0.2889 | dt 338.10ms | 1550680.60 tokens/sec
Step 8417 | loss: 3.230104 | lr:3.9752e-04 | norm 0.2696 | dt 337.39ms | 1553931.78 tokens/sec
Step 8418 | loss: 3.264668 | lr:3.9748e-04 | norm 0.3102 | dt 337.99ms | 1551178.29 tokens/sec
Step 8419 | loss: 3.203827 | lr:3.9743e-04 | norm 0.2851 | dt 337.85ms | 1551817.57 tokens/sec
Step 8420 | loss: 3.255223 | lr:3.9739e-04 | norm 0.2847 | dt 337.54ms | 1553276.51 tokens/sec
Step 8421 | loss: 3.256127 | lr:3.9734e-04 | norm 0.2941 | dt 337.97ms | 1551292.10 tokens/sec
Step 8422 | loss: 3.190717 | lr:3.9730e-04 | norm 0.2897 | dt 338.02ms | 1551056.85 tokens/sec
Step 8423 | loss: 3.245591 | lr:3.9725e-04 | norm 0.2960 | dt 338.48ms | 1548952.62 tokens/sec
Step 8424 | loss: 3.234047 | lr:3.9721e-04 | norm 0.2669 | dt 337.96ms | 1551317.27 tokens/sec
Step 8425 | loss: 3.251434 | lr:3.9716e-04 | norm 0.2672 | dt 337.90ms | 1551590.91 tokens/sec
Step 8426 | loss: 3.239419 | lr:3.9712e-04 | norm 0.2747 | dt 337.89ms | 1551640.18 tokens/sec
Step 8427 | loss: 3.183904 | lr:3.9707e-04 | norm 0.2756 | dt 338.40ms | 1549294.20 tokens/sec
Step 8428 | loss: 3.196162 | lr:3.9703e-04 | norm 0.3163 | dt 337.84ms | 1551887.66 tokens/sec
Step 8429 | loss: 3.210231 | lr:3.9698e-04 | norm 0.2860 | dt 337.49ms | 1553478.41 tokens/sec
Step 8430 | loss: 3.198975 | lr:3.9694e-04 | norm 0.2915 | dt 337.50ms | 1553434.51 tokens/sec
Step 8431 | loss: 3.238085 | lr:3.9689e-04 | norm 0.2884 | dt 338.03ms | 1550989.02 tokens/sec
Step 8432 | loss: 3.218095 | lr:3.9685e-04 | norm 0.2885 | dt 337.87ms | 1551735.44 tokens/sec
Step 8433 | loss: 3.193437 | lr:3.9680e-04 | norm 0.3056 | dt 337.47ms | 1553564.01 tokens/sec
Step 8434 | loss: 3.271314 | lr:3.9676e-04 | norm 0.2763 | dt 338.50ms | 1548855.53 tokens/sec
Step 8435 | loss: 3.218983 | lr:3.9671e-04 | norm 0.2997 | dt 338.33ms | 1549640.29 tokens/sec
Step 8436 | loss: 3.265995 | lr:3.9667e-04 | norm 0.2914 | dt 337.39ms | 1553952.65 tokens/sec
Step 8437 | loss: 3.257911 | lr:3.9663e-04 | norm 0.2811 | dt 338.43ms | 1549194.87 tokens/sec
Step 8438 | loss: 3.279042 | lr:3.9658e-04 | norm 0.2903 | dt 339.22ms | 1545566.86 tokens/sec
Step 8439 | loss: 3.283490 | lr:3.9654e-04 | norm 0.2953 | dt 338.23ms | 1550078.32 tokens/sec
Step 8440 | loss: 3.243831 | lr:3.9649e-04 | norm 0.2832 | dt 337.28ms | 1554478.82 tokens/sec
Step 8441 | loss: 3.268876 | lr:3.9645e-04 | norm 0.2882 | dt 337.89ms | 1551636.90 tokens/sec
Step 8442 | loss: 3.198512 | lr:3.9640e-04 | norm 0.2705 | dt 338.37ms | 1549464.50 tokens/sec
Step 8443 | loss: 3.239331 | lr:3.9636e-04 | norm 0.2809 | dt 337.96ms | 1551326.02 tokens/sec
Step 8444 | loss: 3.205930 | lr:3.9631e-04 | norm 0.2765 | dt 338.86ms | 1547201.28 tokens/sec
Step 8445 | loss: 3.317319 | lr:3.9627e-04 | norm 0.2809 | dt 337.68ms | 1552629.46 tokens/sec
Step 8446 | loss: 3.253389 | lr:3.9622e-04 | norm 0.2909 | dt 338.56ms | 1548602.47 tokens/sec
Step 8447 | loss: 3.228383 | lr:3.9618e-04 | norm 0.2649 | dt 339.15ms | 1545903.68 tokens/sec
Step 8448 | loss: 3.265568 | lr:3.9613e-04 | norm 0.3001 | dt 339.11ms | 1546048.24 tokens/sec
Step 8449 | loss: 3.226572 | lr:3.9609e-04 | norm 0.2717 | dt 339.17ms | 1545793.93 tokens/sec
Step 8450 | loss: 3.212099 | lr:3.9604e-04 | norm 0.2914 | dt 338.80ms | 1547499.61 tokens/sec
Step 8451 | loss: 3.219294 | lr:3.9600e-04 | norm 0.2920 | dt 338.46ms | 1549061.74 tokens/sec
Step 8452 | loss: 3.207864 | lr:3.9595e-04 | norm 0.2885 | dt 340.12ms | 1541501.82 tokens/sec
Step 8453 | loss: 3.233096 | lr:3.9591e-04 | norm 0.2891 | dt 338.84ms | 1547324.30 tokens/sec
Step 8454 | loss: 3.209518 | lr:3.9586e-04 | norm 0.2753 | dt 338.47ms | 1548997.36 tokens/sec
Step 8455 | loss: 3.249983 | lr:3.9582e-04 | norm 0.3095 | dt 338.78ms | 1547578.02 tokens/sec
Step 8456 | loss: 3.263496 | lr:3.9577e-04 | norm 0.2703 | dt 339.09ms | 1546158.03 tokens/sec
Step 8457 | loss: 3.281199 | lr:3.9573e-04 | norm 0.2901 | dt 338.99ms | 1546611.49 tokens/sec
Step 8458 | loss: 3.209057 | lr:3.9568e-04 | norm 0.2989 | dt 338.88ms | 1547135.97 tokens/sec
Step 8459 | loss: 3.270892 | lr:3.9564e-04 | norm 0.2639 | dt 338.68ms | 1548035.59 tokens/sec
Step 8460 | loss: 3.175653 | lr:3.9559e-04 | norm 0.2719 | dt 339.57ms | 1543985.76 tokens/sec
Step 8461 | loss: 3.208464 | lr:3.9555e-04 | norm 0.2891 | dt 339.57ms | 1543995.51 tokens/sec
Step 8462 | loss: 3.233077 | lr:3.9551e-04 | norm 0.3048 | dt 338.99ms | 1546635.42 tokens/sec
Step 8463 | loss: 3.251586 | lr:3.9546e-04 | norm 0.3382 | dt 338.65ms | 1548162.01 tokens/sec
Step 8464 | loss: 3.255362 | lr:3.9542e-04 | norm 0.3022 | dt 339.67ms | 1543531.66 tokens/sec
Step 8465 | loss: 3.320288 | lr:3.9537e-04 | norm 0.3282 | dt 339.25ms | 1545438.69 tokens/sec
Step 8466 | loss: 3.237395 | lr:3.9533e-04 | norm 0.3523 | dt 339.93ms | 1542339.73 tokens/sec
Step 8467 | loss: 3.256131 | lr:3.9528e-04 | norm 0.3116 | dt 339.17ms | 1545806.97 tokens/sec
Step 8468 | loss: 3.230721 | lr:3.9524e-04 | norm 0.2962 | dt 338.62ms | 1548316.80 tokens/sec
Step 8469 | loss: 3.238107 | lr:3.9519e-04 | norm 0.3117 | dt 340.89ms | 1537991.45 tokens/sec
Step 8470 | loss: 3.202624 | lr:3.9515e-04 | norm 0.2852 | dt 338.77ms | 1547608.52 tokens/sec
Step 8471 | loss: 3.236555 | lr:3.9510e-04 | norm 0.2848 | dt 338.52ms | 1548763.89 tokens/sec
Step 8472 | loss: 3.174159 | lr:3.9506e-04 | norm 0.3150 | dt 340.46ms | 1539960.29 tokens/sec
Step 8473 | loss: 3.254033 | lr:3.9501e-04 | norm 0.2796 | dt 339.34ms | 1545004.37 tokens/sec
Step 8474 | loss: 3.260617 | lr:3.9497e-04 | norm 0.2887 | dt 339.93ms | 1542362.44 tokens/sec
Step 8475 | loss: 3.289614 | lr:3.9492e-04 | norm 0.3097 | dt 338.69ms | 1547981.10 tokens/sec
Step 8476 | loss: 3.224777 | lr:3.9488e-04 | norm 0.2698 | dt 338.82ms | 1547378.74 tokens/sec
Step 8477 | loss: 3.253117 | lr:3.9483e-04 | norm 0.3002 | dt 339.82ms | 1542824.51 tokens/sec
Step 8478 | loss: 3.227711 | lr:3.9479e-04 | norm 0.2613 | dt 340.38ms | 1540296.83 tokens/sec
Step 8479 | loss: 3.343957 | lr:3.9474e-04 | norm 0.2583 | dt 342.22ms | 1532009.36 tokens/sec
Step 8480 | loss: 3.200514 | lr:3.9470e-04 | norm 0.2603 | dt 339.55ms | 1544052.97 tokens/sec
Step 8481 | loss: 3.257744 | lr:3.9465e-04 | norm 0.2796 | dt 338.66ms | 1548134.76 tokens/sec
Step 8482 | loss: 3.268050 | lr:3.9461e-04 | norm 0.2535 | dt 337.66ms | 1552722.64 tokens/sec
Step 8483 | loss: 3.227863 | lr:3.9456e-04 | norm 0.2938 | dt 338.35ms | 1549558.40 tokens/sec
Step 8484 | loss: 3.229240 | lr:3.9452e-04 | norm 0.2631 | dt 338.26ms | 1549939.56 tokens/sec
Step 8485 | loss: 3.285311 | lr:3.9447e-04 | norm 0.2892 | dt 338.34ms | 1549607.53 tokens/sec
Step 8486 | loss: 3.321038 | lr:3.9443e-04 | norm 0.2729 | dt 337.56ms | 1553171.19 tokens/sec
Step 8487 | loss: 3.204718 | lr:3.9438e-04 | norm 0.3209 | dt 337.92ms | 1551509.91 tokens/sec
Step 8488 | loss: 3.256261 | lr:3.9434e-04 | norm 0.2994 | dt 337.91ms | 1551538.37 tokens/sec
Step 8489 | loss: 3.240204 | lr:3.9429e-04 | norm 0.2933 | dt 338.07ms | 1550833.70 tokens/sec
Step 8490 | loss: 3.205690 | lr:3.9425e-04 | norm 0.2767 | dt 338.33ms | 1549649.03 tokens/sec
Step 8491 | loss: 3.187533 | lr:3.9420e-04 | norm 0.2984 | dt 337.78ms | 1552157.12 tokens/sec
Step 8492 | loss: 3.224579 | lr:3.9416e-04 | norm 0.2607 | dt 338.54ms | 1548664.64 tokens/sec
Step 8493 | loss: 3.218134 | lr:3.9411e-04 | norm 0.3080 | dt 338.01ms | 1551114.83 tokens/sec
Step 8494 | loss: 3.216406 | lr:3.9407e-04 | norm 0.2733 | dt 338.07ms | 1550849.01 tokens/sec
Step 8495 | loss: 3.223162 | lr:3.9402e-04 | norm 0.2839 | dt 338.02ms | 1551066.69 tokens/sec
Step 8496 | loss: 3.205587 | lr:3.9398e-04 | norm 0.2715 | dt 338.85ms | 1547264.42 tokens/sec
Step 8497 | loss: 3.223391 | lr:3.9394e-04 | norm 0.2931 | dt 338.90ms | 1547047.80 tokens/sec
Step 8498 | loss: 3.274991 | lr:3.9389e-04 | norm 0.2943 | dt 338.20ms | 1550228.02 tokens/sec
Step 8499 | loss: 3.209598 | lr:3.9385e-04 | norm 0.3273 | dt 338.92ms | 1546933.53 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 8500: 3.2357
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2869/10042=0.2857


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm a programmer in the company of my parents. My mom will, of course, try to teach my family
rank 3 sample 1 >Hello, I'm a language model, and my grammar is very old but I can say how I can do it. I'm not very good at what to
rank 3 sample 2 >Hello, I'm a language model, and you've got us working all day now. We're using the C library and the language it uses to compile this


ddp_rank 5: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 3 sample 3 >Hello, I'm a language model, I hate to write things, and I don't understand anything, because I'm never told when I are wrong.



rank 5 sample 0 >Hello, I'm a language model, but that doesn't really matter. You probably don't have to read English.
When I read Japanese you start thinking
rank 5 sample 1 >Hello, I'm a language model, or what are some basic things we need to cover?
So with the Internet so big and so powerful is it possible
rank 1 sample 0 >Hello, I'm a language model, i hope to use that for my children and to get better quickly.
Hi, thanks for my help and help.
rank 5 sample 2 >Hello, I'm a language model, so I use it sometimes. (My first language, I'm also a bilingual person of mine.)
I'm in
rank 1 sample 1 >Hello, I'm a language model, a computer, a web server, a router, a website, and a computer together
what do I do with the
rank 5 sample 3 >Hello, I'm a language model, with lots of text and animations, but it's the first of my two thoughts here and it may be the first piecerank 1 sample 2 >Hello, I'm a language model, but after reading, I'm not sure what I'm thinking but that I'm not sure I should say, so they





ddp_rank 2: ####### Printing generated samples ####### 

rank 1 sample 3 >Hello, I'm a language model, and I'm the only child of me which I'm now doing I'll know how to run my own program. I


rank 2 sample 0 >Hello, I'm a language model, then my teacher would have to do this for you. You have to decide which word you want to use because you need
rank 2 sample 1 >Hello, I'm a language model, but I couldn't see what I didn't see anyway. So we decided to go to the Web. I'm still
rank 2 sample 2 >Hello, I'm a language model, so I can tell you how this process works. I'm having problems explaining what it is. We use it to describe
rank 2 sample 3 >Hello, I'm a language model, I want to build an app that is readable by machine, that is, what do i make that my website should look




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm going to write a program right now that's actually a big chunk of code. Why don't you take
rank 7 sample 1 >Hello, I'm a language model, have you been to an airport with so many people crossing them? We're going to learn a lot faster. We don
rank 7 sample 2 >Hello, I'm a language model, but I don't agree with why it would be cool to learn about programming.
I'm not sure what's really
rank 7 sample 3 >Hello, I'm a language model, sorry. The first word has the words A is the base, and X is the prefix. X is the prefix if




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I just want to get started with. I'm going to start on the lesson I said, what you are doing
rank 4 sample 1 >Hello, I'm a language model, don't worry when I see some "gravite" language. What is Gravite? The Gravite
rank 4 sample 2 >Hello, I'm a language model, I get questions regarding grammar. I know what grammar is, it is the rules we use to communicate we use grammar,
rank 4 sample 3 >Hello, I'm a language model, and it wasn't very helpful to me in any way to help. She said that I was able to use more words




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I am a teacher! It just requires ICT to be successful. But I don't understand what technology is really
rank 0 sample 1 >Hello, I'm a language model, but for the sake of your knowledge as well. Why you're interested in languages, you'll want to read this article
rank 0 sample 2 >Hello, I'm a language model, so I didn't include one until it was an option. That would be a little confusing, but I'm going to
rank 0 sample 3 >Hello, I'm a language model, but here's my first language. In English only, you can speak and understand an article on a TV show or play




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, I know. But when your machine learns a language from you it goes into some specific language, and you learn it all
rank 6 sample 1 >Hello, I'm a language model, which is very similar to a normal language classroom. So I'm going to make the most important lesson, I'm going
rank 6 sample 2 >Hello, I'm a language model, but this is not going to make sense for everyone.
- Why would people want to learn a language without actually being
rank 6 sample 3 >Hello, I'm a language model, I like to draw on my model, I want to draw on my model to draw a bit more." This kind of


Step 8500 | loss: 3.229661 | lr:3.9380e-04 | norm 0.3017 | dt 18798.45ms | 27889.96 tokens/sec
Step 8501 | loss: 3.232773 | lr:3.9376e-04 | norm 0.2837 | dt 333.86ms | 1570392.14 tokens/sec
Step 8502 | loss: 3.225158 | lr:3.9371e-04 | norm 0.3183 | dt 334.74ms | 1566272.61 tokens/sec
Step 8503 | loss: 3.173790 | lr:3.9367e-04 | norm 0.2937 | dt 336.86ms | 1556404.20 tokens/sec
Step 8504 | loss: 3.240246 | lr:3.9362e-04 | norm 0.3112 | dt 1027.04ms | 510483.07 tokens/sec
Step 8505 | loss: 3.215680 | lr:3.9358e-04 | norm 0.3433 | dt 336.81ms | 1556625.64 tokens/sec
Step 8506 | loss: 3.197969 | lr:3.9353e-04 | norm 0.2803 | dt 335.73ms | 1561641.03 tokens/sec
Step 8507 | loss: 3.222633 | lr:3.9349e-04 | norm 0.3012 | dt 337.60ms | 1552991.30 tokens/sec
Step 8508 | loss: 3.230404 | lr:3.9344e-04 | norm 0.2852 | dt 335.34ms | 1563456.36 tokens/sec
Step 8509 | loss: 3.260890 | lr:3.9340e-04 | norm 0.2788 | dt 335.58ms | 1562315.60 tokens/sec
Step 8510 | loss: 3.258976 | lr:3.9335e-04 | norm 0.3137 | dt 335.66ms | 1561978.24 tokens/sec
Step 8511 | loss: 3.239975 | lr:3.9331e-04 | norm 0.2527 | dt 335.65ms | 1561997.10 tokens/sec
Step 8512 | loss: 3.264567 | lr:3.9326e-04 | norm 0.2877 | dt 336.08ms | 1559998.11 tokens/sec
Step 8513 | loss: 3.203979 | lr:3.9322e-04 | norm 0.2524 | dt 336.11ms | 1559878.60 tokens/sec
Step 8514 | loss: 3.215985 | lr:3.9317e-04 | norm 0.3030 | dt 336.30ms | 1558989.48 tokens/sec
Step 8515 | loss: 3.244752 | lr:3.9313e-04 | norm 0.2591 | dt 336.52ms | 1557972.22 tokens/sec
Step 8516 | loss: 3.217542 | lr:3.9308e-04 | norm 0.2803 | dt 337.43ms | 1553783.56 tokens/sec
Step 8517 | loss: 3.230989 | lr:3.9304e-04 | norm 0.2747 | dt 336.45ms | 1558300.11 tokens/sec
Step 8518 | loss: 3.222706 | lr:3.9299e-04 | norm 0.2699 | dt 337.02ms | 1555635.67 tokens/sec
Step 8519 | loss: 3.230954 | lr:3.9295e-04 | norm 0.2795 | dt 336.36ms | 1558733.11 tokens/sec
Step 8520 | loss: 3.242666 | lr:3.9290e-04 | norm 0.2691 | dt 336.26ms | 1559159.70 tokens/sec
Step 8521 | loss: 3.216933 | lr:3.9286e-04 | norm 0.3207 | dt 336.47ms | 1558184.18 tokens/sec
Step 8522 | loss: 3.325790 | lr:3.9281e-04 | norm 0.2695 | dt 336.56ms | 1557783.49 tokens/sec
Step 8523 | loss: 3.209137 | lr:3.9277e-04 | norm 0.2966 | dt 337.60ms | 1553007.75 tokens/sec
Step 8524 | loss: 3.219177 | lr:3.9272e-04 | norm 0.2862 | dt 336.34ms | 1558800.51 tokens/sec
Step 8525 | loss: 3.228632 | lr:3.9268e-04 | norm 0.2675 | dt 336.80ms | 1556696.17 tokens/sec
Step 8526 | loss: 3.206890 | lr:3.9263e-04 | norm 0.3018 | dt 337.27ms | 1554495.30 tokens/sec
Step 8527 | loss: 3.218749 | lr:3.9259e-04 | norm 0.2726 | dt 336.79ms | 1556739.15 tokens/sec
Step 8528 | loss: 3.211487 | lr:3.9254e-04 | norm 0.2992 | dt 336.87ms | 1556337.00 tokens/sec
Step 8529 | loss: 3.224328 | lr:3.9250e-04 | norm 0.2673 | dt 337.11ms | 1555230.80 tokens/sec
Step 8530 | loss: 3.222315 | lr:3.9245e-04 | norm 0.2774 | dt 337.80ms | 1552074.96 tokens/sec
Step 8531 | loss: 3.237964 | lr:3.9241e-04 | norm 0.2576 | dt 337.43ms | 1553771.48 tokens/sec
Step 8532 | loss: 3.259999 | lr:3.9236e-04 | norm 0.2557 | dt 336.77ms | 1556826.21 tokens/sec
Step 8533 | loss: 3.252125 | lr:3.9232e-04 | norm 0.2585 | dt 338.13ms | 1550557.04 tokens/sec
Step 8534 | loss: 3.341749 | lr:3.9227e-04 | norm 0.2752 | dt 337.20ms | 1554805.25 tokens/sec
Step 8535 | loss: 3.228248 | lr:3.9223e-04 | norm 0.2774 | dt 336.87ms | 1556350.22 tokens/sec
Step 8536 | loss: 3.210890 | lr:3.9218e-04 | norm 0.2757 | dt 339.34ms | 1545029.33 tokens/sec
Step 8537 | loss: 3.261452 | lr:3.9214e-04 | norm 0.2542 | dt 337.83ms | 1551941.32 tokens/sec
Step 8538 | loss: 3.215205 | lr:3.9209e-04 | norm 0.2823 | dt 337.16ms | 1555005.35 tokens/sec
Step 8539 | loss: 3.300416 | lr:3.9205e-04 | norm 0.2869 | dt 338.39ms | 1549338.95 tokens/sec
Step 8540 | loss: 3.210663 | lr:3.9200e-04 | norm 0.2838 | dt 338.01ms | 1551091.86 tokens/sec
Step 8541 | loss: 3.232054 | lr:3.9196e-04 | norm 0.2979 | dt 337.44ms | 1553700.12 tokens/sec
Step 8542 | loss: 3.252823 | lr:3.9191e-04 | norm 0.2861 | dt 337.70ms | 1552517.65 tokens/sec
Step 8543 | loss: 3.240027 | lr:3.9187e-04 | norm 0.2872 | dt 339.16ms | 1545840.65 tokens/sec
Step 8544 | loss: 3.267197 | lr:3.9182e-04 | norm 0.2942 | dt 338.42ms | 1549229.80 tokens/sec
Step 8545 | loss: 3.238794 | lr:3.9178e-04 | norm 0.2756 | dt 338.28ms | 1549847.80 tokens/sec
Step 8546 | loss: 3.272153 | lr:3.9173e-04 | norm 0.2748 | dt 339.39ms | 1544814.43 tokens/sec
Step 8547 | loss: 3.205640 | lr:3.9169e-04 | norm 0.2548 | dt 337.76ms | 1552254.63 tokens/sec
Step 8548 | loss: 3.258811 | lr:3.9164e-04 | norm 0.2789 | dt 338.02ms | 1551056.85 tokens/sec
Step 8549 | loss: 3.232522 | lr:3.9160e-04 | norm 0.2635 | dt 991.18ms | 528951.49 tokens/sec
Step 8550 | loss: 3.235505 | lr:3.9155e-04 | norm 0.2868 | dt 336.20ms | 1559446.08 tokens/sec
Step 8551 | loss: 3.185031 | lr:3.9151e-04 | norm 0.3114 | dt 338.59ms | 1548453.08 tokens/sec
Step 8552 | loss: 3.201718 | lr:3.9146e-04 | norm 0.2989 | dt 338.20ms | 1550217.10 tokens/sec
Step 8553 | loss: 3.223907 | lr:3.9142e-04 | norm 0.3082 | dt 339.23ms | 1545516.89 tokens/sec
Step 8554 | loss: 3.212215 | lr:3.9137e-04 | norm 0.2977 | dt 338.12ms | 1550619.36 tokens/sec
Step 8555 | loss: 3.262613 | lr:3.9133e-04 | norm 0.2906 | dt 339.64ms | 1543658.44 tokens/sec
Step 8556 | loss: 3.198056 | lr:3.9128e-04 | norm 0.2927 | dt 337.53ms | 1553307.23 tokens/sec
Step 8557 | loss: 3.213607 | lr:3.9124e-04 | norm 0.2946 | dt 338.49ms | 1548892.62 tokens/sec
Step 8558 | loss: 3.172533 | lr:3.9119e-04 | norm 0.3087 | dt 339.13ms | 1545999.32 tokens/sec
Step 8559 | loss: 3.228344 | lr:3.9115e-04 | norm 0.3071 | dt 342.08ms | 1532660.70 tokens/sec
Step 8560 | loss: 3.227479 | lr:3.9110e-04 | norm 0.2982 | dt 338.26ms | 1549940.66 tokens/sec
Step 8561 | loss: 3.281704 | lr:3.9106e-04 | norm 0.2748 | dt 337.83ms | 1551915.04 tokens/sec
Step 8562 | loss: 3.211133 | lr:3.9101e-04 | norm 0.2880 | dt 339.22ms | 1545589.67 tokens/sec
Step 8563 | loss: 3.209764 | lr:3.9097e-04 | norm 0.3031 | dt 338.97ms | 1546697.43 tokens/sec
Step 8564 | loss: 3.306745 | lr:3.9092e-04 | norm 0.3101 | dt 339.63ms | 1543684.44 tokens/sec
Step 8565 | loss: 3.240272 | lr:3.9088e-04 | norm 0.2904 | dt 338.59ms | 1548440.00 tokens/sec
Step 8566 | loss: 3.232136 | lr:3.9083e-04 | norm 0.2817 | dt 338.65ms | 1548152.20 tokens/sec
Step 8567 | loss: 3.206140 | lr:3.9079e-04 | norm 0.2619 | dt 338.92ms | 1546919.39 tokens/sec
Step 8568 | loss: 3.272200 | lr:3.9074e-04 | norm 0.2905 | dt 339.27ms | 1545330.09 tokens/sec
Step 8569 | loss: 3.255629 | lr:3.9070e-04 | norm 0.2671 | dt 338.73ms | 1547817.67 tokens/sec
Step 8570 | loss: 3.434031 | lr:3.9065e-04 | norm 0.3945 | dt 338.09ms | 1550744.02 tokens/sec
Step 8571 | loss: 3.287220 | lr:3.9061e-04 | norm 0.4426 | dt 337.49ms | 1553503.65 tokens/sec
Step 8572 | loss: 3.229316 | lr:3.9056e-04 | norm 0.3741 | dt 337.47ms | 1553605.72 tokens/sec
Step 8573 | loss: 3.409843 | lr:3.9052e-04 | norm 0.3731 | dt 338.60ms | 1548382.21 tokens/sec
Step 8574 | loss: 3.282536 | lr:3.9047e-04 | norm 0.3686 | dt 337.60ms | 1552998.98 tokens/sec
Step 8575 | loss: 3.234327 | lr:3.9043e-04 | norm 0.3434 | dt 337.37ms | 1554028.42 tokens/sec
Step 8576 | loss: 3.250154 | lr:3.9038e-04 | norm 0.2811 | dt 338.21ms | 1550165.73 tokens/sec
Step 8577 | loss: 3.255786 | lr:3.9034e-04 | norm 0.3044 | dt 337.97ms | 1551308.51 tokens/sec
Step 8578 | loss: 3.248580 | lr:3.9029e-04 | norm 0.3162 | dt 337.27ms | 1554492.01 tokens/sec
Step 8579 | loss: 3.251808 | lr:3.9025e-04 | norm 0.2782 | dt 338.75ms | 1547723.98 tokens/sec
Step 8580 | loss: 3.229665 | lr:3.9020e-04 | norm 0.2790 | dt 337.47ms | 1553583.77 tokens/sec
Step 8581 | loss: 3.177331 | lr:3.9016e-04 | norm 0.2908 | dt 337.28ms | 1554444.76 tokens/sec
Step 8582 | loss: 3.209216 | lr:3.9011e-04 | norm 0.3026 | dt 338.08ms | 1550780.11 tokens/sec
Step 8583 | loss: 3.236919 | lr:3.9007e-04 | norm 0.3004 | dt 337.48ms | 1553520.11 tokens/sec
Step 8584 | loss: 3.201087 | lr:3.9002e-04 | norm 0.2846 | dt 337.68ms | 1552623.97 tokens/sec
Step 8585 | loss: 3.242846 | lr:3.8998e-04 | norm 0.2893 | dt 338.15ms | 1550481.61 tokens/sec
Step 8586 | loss: 3.249920 | lr:3.8993e-04 | norm 0.2873 | dt 337.84ms | 1551871.23 tokens/sec
Step 8587 | loss: 3.221152 | lr:3.8989e-04 | norm 0.3017 | dt 338.55ms | 1548634.10 tokens/sec
Step 8588 | loss: 3.225419 | lr:3.8984e-04 | norm 0.2609 | dt 338.73ms | 1547816.58 tokens/sec
Step 8589 | loss: 3.328047 | lr:3.8980e-04 | norm 0.2696 | dt 338.78ms | 1547591.09 tokens/sec
Step 8590 | loss: 3.215000 | lr:3.8975e-04 | norm 0.2958 | dt 337.87ms | 1551754.05 tokens/sec
Step 8591 | loss: 3.200785 | lr:3.8971e-04 | norm 0.2621 | dt 339.95ms | 1542264.01 tokens/sec
Step 8592 | loss: 3.143964 | lr:3.8966e-04 | norm 0.3017 | dt 338.79ms | 1547545.35 tokens/sec
Step 8593 | loss: 3.190654 | lr:3.8962e-04 | norm 0.2671 | dt 338.21ms | 1550171.20 tokens/sec
Step 8594 | loss: 3.258625 | lr:3.8957e-04 | norm 0.2890 | dt 337.41ms | 1553856.02 tokens/sec
Step 8595 | loss: 3.190582 | lr:3.8953e-04 | norm 0.2799 | dt 338.60ms | 1548382.21 tokens/sec
Step 8596 | loss: 3.216738 | lr:3.8948e-04 | norm 0.2580 | dt 338.82ms | 1547391.81 tokens/sec
Step 8597 | loss: 3.281947 | lr:3.8944e-04 | norm 0.2813 | dt 339.38ms | 1544826.37 tokens/sec
Step 8598 | loss: 3.231668 | lr:3.8939e-04 | norm 0.2623 | dt 337.96ms | 1551318.36 tokens/sec
Step 8599 | loss: 3.219415 | lr:3.8935e-04 | norm 0.2854 | dt 338.91ms | 1546974.89 tokens/sec
Step 8600 | loss: 3.199733 | lr:3.8930e-04 | norm 0.2715 | dt 338.68ms | 1548012.70 tokens/sec
Step 8601 | loss: 3.241265 | lr:3.8926e-04 | norm 0.2580 | dt 338.85ms | 1547277.49 tokens/sec
Step 8602 | loss: 3.201514 | lr:3.8921e-04 | norm 0.2976 | dt 339.04ms | 1546372.22 tokens/sec
Step 8603 | loss: 3.281331 | lr:3.8917e-04 | norm 0.2849 | dt 339.05ms | 1546351.56 tokens/sec
Step 8604 | loss: 3.257988 | lr:3.8912e-04 | norm 0.2749 | dt 339.91ms | 1542436.01 tokens/sec
Step 8605 | loss: 3.204034 | lr:3.8908e-04 | norm 0.2692 | dt 338.97ms | 1546699.60 tokens/sec
Step 8606 | loss: 3.298882 | lr:3.8903e-04 | norm 0.2736 | dt 338.69ms | 1547983.28 tokens/sec
Step 8607 | loss: 3.242221 | lr:3.8899e-04 | norm 0.2723 | dt 339.03ms | 1546451.61 tokens/sec
Step 8608 | loss: 3.278459 | lr:3.8894e-04 | norm 0.2717 | dt 338.75ms | 1547708.73 tokens/sec
Step 8609 | loss: 3.237410 | lr:3.8890e-04 | norm 0.2607 | dt 339.86ms | 1542638.35 tokens/sec
Step 8610 | loss: 3.251995 | lr:3.8885e-04 | norm 0.2686 | dt 338.97ms | 1546707.22 tokens/sec
Step 8611 | loss: 3.275808 | lr:3.8881e-04 | norm 0.2689 | dt 339.24ms | 1545484.31 tokens/sec
Step 8612 | loss: 3.150963 | lr:3.8876e-04 | norm 0.2716 | dt 338.21ms | 1550170.10 tokens/sec
Step 8613 | loss: 3.245497 | lr:3.8872e-04 | norm 0.2797 | dt 340.10ms | 1541576.38 tokens/sec
Step 8614 | loss: 3.270436 | lr:3.8867e-04 | norm 0.2950 | dt 337.69ms | 1552558.20 tokens/sec
Step 8615 | loss: 3.269008 | lr:3.8863e-04 | norm 0.2929 | dt 338.68ms | 1548015.97 tokens/sec
Step 8616 | loss: 3.230596 | lr:3.8858e-04 | norm 0.2773 | dt 337.63ms | 1552852.02 tokens/sec
Step 8617 | loss: 3.248079 | lr:3.8854e-04 | norm 0.2694 | dt 337.82ms | 1551962.13 tokens/sec
Step 8618 | loss: 3.296451 | lr:3.8849e-04 | norm 0.2900 | dt 340.47ms | 1539880.49 tokens/sec
Step 8619 | loss: 3.208605 | lr:3.8844e-04 | norm 0.2669 | dt 339.26ms | 1545400.68 tokens/sec
Step 8620 | loss: 3.237685 | lr:3.8840e-04 | norm 0.2681 | dt 341.89ms | 1533499.71 tokens/sec
Step 8621 | loss: 3.208793 | lr:3.8835e-04 | norm 0.2943 | dt 337.83ms | 1551930.37 tokens/sec
Step 8622 | loss: 3.196502 | lr:3.8831e-04 | norm 0.2792 | dt 338.13ms | 1550541.74 tokens/sec
Step 8623 | loss: 3.276954 | lr:3.8826e-04 | norm 0.2921 | dt 337.72ms | 1552456.27 tokens/sec
Step 8624 | loss: 3.157724 | lr:3.8822e-04 | norm 0.3044 | dt 337.76ms | 1552245.87 tokens/sec
Step 8625 | loss: 3.242295 | lr:3.8817e-04 | norm 0.2804 | dt 338.41ms | 1549264.73 tokens/sec
Step 8626 | loss: 3.257343 | lr:3.8813e-04 | norm 0.2989 | dt 338.20ms | 1550247.70 tokens/sec
Step 8627 | loss: 3.206157 | lr:3.8808e-04 | norm 0.2961 | dt 337.72ms | 1552440.92 tokens/sec
Step 8628 | loss: 3.244387 | lr:3.8804e-04 | norm 0.2847 | dt 337.59ms | 1553017.62 tokens/sec
Step 8629 | loss: 3.231750 | lr:3.8799e-04 | norm 0.3240 | dt 337.76ms | 1552266.68 tokens/sec
Step 8630 | loss: 3.206022 | lr:3.8795e-04 | norm 0.2951 | dt 338.06ms | 1550873.07 tokens/sec
Step 8631 | loss: 3.257137 | lr:3.8790e-04 | norm 0.2804 | dt 336.98ms | 1555844.80 tokens/sec
Step 8632 | loss: 3.193634 | lr:3.8786e-04 | norm 0.3026 | dt 337.47ms | 1553569.50 tokens/sec
Step 8633 | loss: 3.193315 | lr:3.8781e-04 | norm 0.2678 | dt 338.41ms | 1549247.26 tokens/sec
Step 8634 | loss: 3.258002 | lr:3.8777e-04 | norm 0.3155 | dt 337.79ms | 1552124.25 tokens/sec
Step 8635 | loss: 3.212244 | lr:3.8772e-04 | norm 0.2875 | dt 337.98ms | 1551243.95 tokens/sec
Step 8636 | loss: 3.266072 | lr:3.8768e-04 | norm 0.3142 | dt 337.99ms | 1551197.99 tokens/sec
Step 8637 | loss: 3.203443 | lr:3.8763e-04 | norm 0.3113 | dt 338.57ms | 1548538.13 tokens/sec
Step 8638 | loss: 3.206872 | lr:3.8759e-04 | norm 0.2758 | dt 339.12ms | 1546042.80 tokens/sec
Step 8639 | loss: 3.245040 | lr:3.8754e-04 | norm 0.3568 | dt 337.97ms | 1551282.25 tokens/sec
Step 8640 | loss: 3.245225 | lr:3.8750e-04 | norm 0.3434 | dt 337.75ms | 1552312.71 tokens/sec
Step 8641 | loss: 3.215651 | lr:3.8745e-04 | norm 0.3445 | dt 338.17ms | 1550377.76 tokens/sec
Step 8642 | loss: 3.245999 | lr:3.8741e-04 | norm 0.2998 | dt 337.68ms | 1552634.94 tokens/sec
Step 8643 | loss: 3.283931 | lr:3.8736e-04 | norm 0.3220 | dt 339.04ms | 1546408.11 tokens/sec
Step 8644 | loss: 3.385691 | lr:3.8732e-04 | norm 0.3410 | dt 337.94ms | 1551419.05 tokens/sec
Step 8645 | loss: 3.263766 | lr:3.8727e-04 | norm 0.3191 | dt 338.41ms | 1549276.73 tokens/sec
Step 8646 | loss: 3.256103 | lr:3.8723e-04 | norm 0.3059 | dt 339.40ms | 1544730.87 tokens/sec
Step 8647 | loss: 3.208454 | lr:3.8718e-04 | norm 0.3110 | dt 339.50ms | 1544273.09 tokens/sec
Step 8648 | loss: 3.283845 | lr:3.8714e-04 | norm 0.2984 | dt 338.05ms | 1550921.20 tokens/sec
Step 8649 | loss: 3.254715 | lr:3.8709e-04 | norm 0.2974 | dt 338.67ms | 1548075.91 tokens/sec
Step 8650 | loss: 3.249501 | lr:3.8705e-04 | norm 0.2828 | dt 338.57ms | 1548558.85 tokens/sec
Step 8651 | loss: 3.213675 | lr:3.8700e-04 | norm 0.2869 | dt 339.44ms | 1544565.95 tokens/sec
Step 8652 | loss: 3.222663 | lr:3.8696e-04 | norm 0.2698 | dt 340.48ms | 1539857.85 tokens/sec
Step 8653 | loss: 3.331450 | lr:3.8691e-04 | norm 0.2727 | dt 337.44ms | 1553745.13 tokens/sec
Step 8654 | loss: 3.229525 | lr:3.8687e-04 | norm 0.2736 | dt 337.78ms | 1552179.03 tokens/sec
Step 8655 | loss: 3.193377 | lr:3.8682e-04 | norm 0.2885 | dt 338.80ms | 1547481.10 tokens/sec
Step 8656 | loss: 3.252883 | lr:3.8677e-04 | norm 0.2842 | dt 339.81ms | 1542888.37 tokens/sec
Step 8657 | loss: 3.295321 | lr:3.8673e-04 | norm 0.3343 | dt 339.29ms | 1545248.64 tokens/sec
Step 8658 | loss: 3.241152 | lr:3.8668e-04 | norm 0.3280 | dt 338.72ms | 1547847.08 tokens/sec
Step 8659 | loss: 3.194824 | lr:3.8664e-04 | norm 0.2789 | dt 340.50ms | 1539748.95 tokens/sec
Step 8660 | loss: 3.194563 | lr:3.8659e-04 | norm 0.2937 | dt 338.34ms | 1549587.88 tokens/sec
Step 8661 | loss: 3.283467 | lr:3.8655e-04 | norm 0.2942 | dt 339.32ms | 1545114.01 tokens/sec
Step 8662 | loss: 3.243388 | lr:3.8650e-04 | norm 0.2957 | dt 339.01ms | 1546509.25 tokens/sec
Step 8663 | loss: 3.228252 | lr:3.8646e-04 | norm 0.3057 | dt 337.85ms | 1551825.23 tokens/sec
Step 8664 | loss: 3.197331 | lr:3.8641e-04 | norm 0.2856 | dt 338.08ms | 1550766.99 tokens/sec
Step 8665 | loss: 3.256742 | lr:3.8637e-04 | norm 0.2825 | dt 337.74ms | 1552342.29 tokens/sec
Step 8666 | loss: 3.219149 | lr:3.8632e-04 | norm 0.2500 | dt 338.09ms | 1550740.74 tokens/sec
Step 8667 | loss: 3.196436 | lr:3.8628e-04 | norm 0.2681 | dt 338.44ms | 1549125.03 tokens/sec
Step 8668 | loss: 3.170682 | lr:3.8623e-04 | norm 0.2507 | dt 337.85ms | 1551852.61 tokens/sec
Step 8669 | loss: 3.186326 | lr:3.8619e-04 | norm 0.2592 | dt 337.40ms | 1553912.02 tokens/sec
Step 8670 | loss: 3.213382 | lr:3.8614e-04 | norm 0.2595 | dt 338.10ms | 1550691.53 tokens/sec
Step 8671 | loss: 3.259558 | lr:3.8610e-04 | norm 0.2554 | dt 338.90ms | 1547019.51 tokens/sec
Step 8672 | loss: 3.278981 | lr:3.8605e-04 | norm 0.2781 | dt 337.38ms | 1554014.14 tokens/sec
Step 8673 | loss: 3.228127 | lr:3.8601e-04 | norm 0.2859 | dt 337.71ms | 1552496.82 tokens/sec
Step 8674 | loss: 3.230953 | lr:3.8596e-04 | norm 0.3012 | dt 338.41ms | 1549284.37 tokens/sec
Step 8675 | loss: 3.177356 | lr:3.8592e-04 | norm 0.2804 | dt 338.30ms | 1549771.35 tokens/sec
Step 8676 | loss: 3.252343 | lr:3.8587e-04 | norm 0.2860 | dt 337.75ms | 1552295.17 tokens/sec
Step 8677 | loss: 3.229054 | lr:3.8583e-04 | norm 0.2915 | dt 337.88ms | 1551692.74 tokens/sec
Step 8678 | loss: 3.199306 | lr:3.8578e-04 | norm 0.2762 | dt 338.70ms | 1547924.44 tokens/sec
Step 8679 | loss: 3.237630 | lr:3.8574e-04 | norm 0.2611 | dt 338.29ms | 1549818.31 tokens/sec
Step 8680 | loss: 3.218397 | lr:3.8569e-04 | norm 0.3005 | dt 337.70ms | 1552543.95 tokens/sec
Step 8681 | loss: 3.214180 | lr:3.8564e-04 | norm 0.2670 | dt 338.49ms | 1548903.53 tokens/sec
Step 8682 | loss: 3.205321 | lr:3.8560e-04 | norm 0.2878 | dt 338.41ms | 1549287.65 tokens/sec
Step 8683 | loss: 3.325349 | lr:3.8555e-04 | norm 0.2770 | dt 339.08ms | 1546209.12 tokens/sec
Step 8684 | loss: 3.245469 | lr:3.8551e-04 | norm 0.2839 | dt 339.02ms | 1546499.46 tokens/sec
Step 8685 | loss: 3.220031 | lr:3.8546e-04 | norm 0.2723 | dt 337.78ms | 1552164.79 tokens/sec
Step 8686 | loss: 3.231602 | lr:3.8542e-04 | norm 0.2761 | dt 338.19ms | 1550254.25 tokens/sec
Step 8687 | loss: 3.277292 | lr:3.8537e-04 | norm 0.2706 | dt 337.76ms | 1552269.97 tokens/sec
Step 8688 | loss: 3.243877 | lr:3.8533e-04 | norm 0.2780 | dt 337.90ms | 1551589.82 tokens/sec
Step 8689 | loss: 3.280925 | lr:3.8528e-04 | norm 0.2884 | dt 338.32ms | 1549661.04 tokens/sec
Step 8690 | loss: 3.210227 | lr:3.8524e-04 | norm 0.2777 | dt 337.87ms | 1551744.20 tokens/sec
Step 8691 | loss: 3.231349 | lr:3.8519e-04 | norm 0.2690 | dt 338.95ms | 1546792.08 tokens/sec
Step 8692 | loss: 3.180493 | lr:3.8515e-04 | norm 0.2857 | dt 337.95ms | 1551359.95 tokens/sec
Step 8693 | loss: 3.227823 | lr:3.8510e-04 | norm 0.2781 | dt 1033.88ms | 507107.68 tokens/sec
Step 8694 | loss: 3.184098 | lr:3.8506e-04 | norm 0.2765 | dt 334.67ms | 1566589.51 tokens/sec
Step 8695 | loss: 3.204034 | lr:3.8501e-04 | norm 0.2715 | dt 337.32ms | 1554252.49 tokens/sec
Step 8696 | loss: 3.225616 | lr:3.8497e-04 | norm 0.3205 | dt 338.68ms | 1548035.59 tokens/sec
Step 8697 | loss: 3.197547 | lr:3.8492e-04 | norm 0.2651 | dt 337.57ms | 1553113.05 tokens/sec
Step 8698 | loss: 3.151170 | lr:3.8488e-04 | norm 0.3048 | dt 337.06ms | 1555479.42 tokens/sec
Step 8699 | loss: 3.215809 | lr:3.8483e-04 | norm 0.2628 | dt 337.07ms | 1555444.21 tokens/sec
Step 8700 | loss: 3.165571 | lr:3.8479e-04 | norm 0.3158 | dt 337.59ms | 1553027.49 tokens/sec
Step 8701 | loss: 3.245094 | lr:3.8474e-04 | norm 0.2945 | dt 337.49ms | 1553512.43 tokens/sec
Step 8702 | loss: 3.185000 | lr:3.8470e-04 | norm 0.3148 | dt 337.27ms | 1554489.81 tokens/sec
Step 8703 | loss: 3.179982 | lr:3.8465e-04 | norm 0.2775 | dt 337.13ms | 1555136.21 tokens/sec
Step 8704 | loss: 3.247899 | lr:3.8460e-04 | norm 0.2758 | dt 338.43ms | 1549197.06 tokens/sec
Step 8705 | loss: 3.188089 | lr:3.8456e-04 | norm 0.2968 | dt 339.75ms | 1543150.39 tokens/sec
Step 8706 | loss: 3.317949 | lr:3.8451e-04 | norm 0.3197 | dt 338.72ms | 1547865.60 tokens/sec
Step 8707 | loss: 3.162292 | lr:3.8447e-04 | norm 0.2979 | dt 338.63ms | 1548262.29 tokens/sec
Step 8708 | loss: 3.210468 | lr:3.8442e-04 | norm 0.3423 | dt 339.32ms | 1545100.98 tokens/sec
Step 8709 | loss: 3.202030 | lr:3.8438e-04 | norm 0.3065 | dt 338.95ms | 1546794.26 tokens/sec
Step 8710 | loss: 3.222610 | lr:3.8433e-04 | norm 0.3029 | dt 338.89ms | 1547054.34 tokens/sec
Step 8711 | loss: 3.217502 | lr:3.8429e-04 | norm 0.3075 | dt 339.00ms | 1546582.12 tokens/sec
Step 8712 | loss: 3.191038 | lr:3.8424e-04 | norm 0.2909 | dt 338.78ms | 1547579.11 tokens/sec
Step 8713 | loss: 3.206796 | lr:3.8420e-04 | norm 0.3168 | dt 338.69ms | 1548010.53 tokens/sec
Step 8714 | loss: 3.227708 | lr:3.8415e-04 | norm 0.2890 | dt 338.87ms | 1547143.59 tokens/sec
Step 8715 | loss: 3.193164 | lr:3.8411e-04 | norm 0.3058 | dt 339.50ms | 1544313.21 tokens/sec
Step 8716 | loss: 3.133727 | lr:3.8406e-04 | norm 0.2650 | dt 339.02ms | 1546492.93 tokens/sec
Step 8717 | loss: 3.203886 | lr:3.8402e-04 | norm 0.3249 | dt 339.68ms | 1543461.24 tokens/sec
Step 8718 | loss: 3.192799 | lr:3.8397e-04 | norm 0.2641 | dt 339.22ms | 1545564.69 tokens/sec
Step 8719 | loss: 3.209243 | lr:3.8393e-04 | norm 0.3128 | dt 339.99ms | 1542079.07 tokens/sec
Step 8720 | loss: 3.231196 | lr:3.8388e-04 | norm 0.2717 | dt 339.49ms | 1544360.93 tokens/sec
Step 8721 | loss: 3.232012 | lr:3.8384e-04 | norm 0.2764 | dt 339.00ms | 1546560.37 tokens/sec
Step 8722 | loss: 3.245532 | lr:3.8379e-04 | norm 0.2868 | dt 339.16ms | 1545850.43 tokens/sec
Step 8723 | loss: 3.210540 | lr:3.8374e-04 | norm 0.2743 | dt 339.20ms | 1545642.90 tokens/sec
Step 8724 | loss: 3.233079 | lr:3.8370e-04 | norm 0.3348 | dt 338.30ms | 1549782.27 tokens/sec
Step 8725 | loss: 3.236763 | lr:3.8365e-04 | norm 0.3118 | dt 339.35ms | 1544996.77 tokens/sec
Step 8726 | loss: 3.249470 | lr:3.8361e-04 | norm 0.3050 | dt 339.76ms | 1543122.24 tokens/sec
Step 8727 | loss: 3.211537 | lr:3.8356e-04 | norm 0.3019 | dt 340.54ms | 1539564.61 tokens/sec
Step 8728 | loss: 3.283231 | lr:3.8352e-04 | norm 0.3111 | dt 339.24ms | 1545459.33 tokens/sec
Step 8729 | loss: 3.267911 | lr:3.8347e-04 | norm 0.2872 | dt 338.93ms | 1546901.98 tokens/sec
Step 8730 | loss: 3.237217 | lr:3.8343e-04 | norm 0.2726 | dt 340.02ms | 1541923.36 tokens/sec
Step 8731 | loss: 3.258621 | lr:3.8338e-04 | norm 0.2839 | dt 339.28ms | 1545276.88 tokens/sec
Step 8732 | loss: 3.185953 | lr:3.8334e-04 | norm 0.2589 | dt 339.94ms | 1542304.03 tokens/sec
Step 8733 | loss: 3.293208 | lr:3.8329e-04 | norm 0.2811 | dt 339.16ms | 1545823.27 tokens/sec
Step 8734 | loss: 3.213792 | lr:3.8325e-04 | norm 0.2809 | dt 338.14ms | 1550527.52 tokens/sec
Step 8735 | loss: 3.180195 | lr:3.8320e-04 | norm 0.2734 | dt 338.26ms | 1549949.40 tokens/sec
Step 8736 | loss: 3.262781 | lr:3.8316e-04 | norm 0.2880 | dt 338.64ms | 1548202.34 tokens/sec
Step 8737 | loss: 3.222622 | lr:3.8311e-04 | norm 0.2644 | dt 337.90ms | 1551618.28 tokens/sec
Step 8738 | loss: 3.253333 | lr:3.8307e-04 | norm 0.2956 | dt 337.70ms | 1552545.05 tokens/sec
Step 8739 | loss: 3.181550 | lr:3.8302e-04 | norm 0.2687 | dt 938.80ms | 558464.23 tokens/sec
Step 8740 | loss: 3.140035 | lr:3.8297e-04 | norm 0.2815 | dt 336.57ms | 1557727.21 tokens/sec
Step 8741 | loss: 3.232940 | lr:3.8293e-04 | norm 0.2618 | dt 338.71ms | 1547902.65 tokens/sec
Step 8742 | loss: 3.162573 | lr:3.8288e-04 | norm 0.2653 | dt 337.10ms | 1555286.90 tokens/sec
Step 8743 | loss: 3.198105 | lr:3.8284e-04 | norm 0.2669 | dt 337.56ms | 1553167.90 tokens/sec
Step 8744 | loss: 3.238783 | lr:3.8279e-04 | norm 0.2697 | dt 337.57ms | 1553115.24 tokens/sec
Step 8745 | loss: 3.362818 | lr:3.8275e-04 | norm 0.2867 | dt 337.41ms | 1553863.71 tokens/sec
Step 8746 | loss: 3.199783 | lr:3.8270e-04 | norm 0.2918 | dt 338.02ms | 1551069.98 tokens/sec
Step 8747 | loss: 3.238819 | lr:3.8266e-04 | norm 0.2967 | dt 337.38ms | 1554008.65 tokens/sec
Step 8748 | loss: 3.169886 | lr:3.8261e-04 | norm 0.2603 | dt 337.73ms | 1552406.95 tokens/sec
Step 8749 | loss: 3.332014 | lr:3.8257e-04 | norm 0.2862 | dt 338.06ms | 1550873.07 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 8750: 3.2276
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2899/10042=0.2887


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but that is not what I'm thinking of, or what I was thinking of when I started working on the concept of

rank 5 sample 1 >Hello, I'm a language model, thanks for my time
My book The Language Machine is called, "The Model: Language and Logic (in Part 1
rank 5 sample 2 >Hello, I'm a language model, and it uses the XML syntax of the XML protocol.
There are many ways of reading XML, but the most efficient

rank 5 sample 3 >Hello, I'm a language model, now you know the basics in a language, I need help with your language processing skills to get yours done. I also


ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so the questions are in this section a lot! So in these questions, I'm just going to talk about the basics
rank 1 sample 1 >Hello, I'm a language model, which means that if you want to know more about language there are ways to help. My computer seems like a good idea
rank 1 sample 2 >Hello, I'm a language model, but even with it, I'm not a native speaker or someone that has an accent. So that's what this post
rank 1 sample 3 >Hello, I'm a language model, so I'm gonna use all the language capabilities I've needed. To begin and end when I see the code, I




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to write a program right now, but I'm in the top left corner.
So I wrote
rank 7 sample 1 >Hello, I'm a language model, having lots of different definitions and it says there are just 24 languages spoken in the world, which is an average of a
rank 2 sample 0 >Hello, I'm a language model, do it with you.
- There's no need to write a line to be finished.
- This is how
rank 7 sample 2 >Hello, I'm a language model, so I'll assume you're gonna call this anyway.
So the function is:
get_p = 1,
rank 2 sample 1 >Hello, I'm a language model, but I prefer that there are two choices—one for the sake of an argument, and the other for a sentence.
rank 7 sample 3 >Hello, I'm a language model, using a standard set of tools. Thanks for
i did that, which is great.
b) I use J


rank 2 sample 2 >Hello, I'm a language model, and I have to write a paragraph here;
- I need to go and use the following. My teacher asks you


rank 2 sample 3 >Hello, I'm a language model, but there are a couple of things you should know about. There are a lot of other applications in which a different way
ddp_rank 4: ####### Printing generated samples ####### 



rank 4 sample 0 >Hello, I'm a language model, and I am a person who has a PhD degree in English language and language learning. This semester, I'll be writing
rank 4 sample 1 >Hello, I'm a language model, here I'm happy to tell everyone we're talking about our language model to the rest of us. But that gets a
rank 4 sample 2 >Hello, I'm a language model, I only read two paragraphs here. You have to write a couple of paragraphs here each time. The title is what you
rank 4 sample 3 >Hello, I'm a language model, so it didn't even exist. Here it comes by as some sort of an applet, not really a part of




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know that it's the perfect programming language for this. My second class is the standard compiler. My third was
rank 0 sample 1 >Hello, I'm a language model, and like your book. I love math, so in all of it I love the idea of using variables, variables,
rank 0 sample 2 >Hello, I'm a language model, and I still want to go to another language with a lot that can be done with a few languages.
So I
rank 0 sample 3 >Hello, I'm a language model, so for the first time, we have the exact same vocabulary for words as the grammar is really simple.
The new




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not using that. There are many other languages out there. When I think of things based on languages as
rank 3 sample 1 >Hello, I'm a language model, so this was a good option!
The second was something I wanted to do with my math and physics classes at college


ddp_rank 6: ####### Printing generated samples ####### 

rank 3 sample 2 >Hello, I'm a language model, so my name is Kevin.<|endoftext|>The Internet in Pictures
Internet is the largest platform for storing vast amounts of electronic data
rank 3 sample 3 >Hello, I'm a language model, so the first sentence is "an." I don't really understand that, but I'll say that anyway, so that


rank 6 sample 0 >Hello, I'm a language model, like that. So here's a sample code. We'll call the
string string "P" and we'll go
rank 6 sample 1 >Hello, I'm a language model, not a machine. That is what I learned from teaching my students. In my experience, it is a much more common
rank 6 sample 2 >Hello, I'm a language model, but if I need to use an XML document with my XML documents, it just doesn't work. To see some of
rank 6 sample 3 >Hello, I'm a language model, so let's first try to describe it here, and then I'm going to talk a little bit about some pretty sophisticated


Step 8750 | loss: 3.178521 | lr:3.8252e-04 | norm 0.2830 | dt 18702.23ms | 28033.45 tokens/sec
Step 8751 | loss: 3.221131 | lr:3.8248e-04 | norm 0.2976 | dt 334.43ms | 1567725.34 tokens/sec
Step 8752 | loss: 3.232821 | lr:3.8243e-04 | norm 0.2872 | dt 336.08ms | 1559995.90 tokens/sec
Step 8753 | loss: 3.225467 | lr:3.8239e-04 | norm 0.2856 | dt 336.25ms | 1559199.50 tokens/sec
Step 8754 | loss: 3.282535 | lr:3.8234e-04 | norm 0.3179 | dt 335.18ms | 1564192.57 tokens/sec
Step 8755 | loss: 3.236109 | lr:3.8229e-04 | norm 0.3093 | dt 336.87ms | 1556367.84 tokens/sec
Step 8756 | loss: 3.281822 | lr:3.8225e-04 | norm 0.2700 | dt 336.50ms | 1558046.18 tokens/sec
Step 8757 | loss: 3.177745 | lr:3.8220e-04 | norm 0.2947 | dt 335.94ms | 1560667.93 tokens/sec
Step 8758 | loss: 3.207685 | lr:3.8216e-04 | norm 0.2924 | dt 337.30ms | 1554384.33 tokens/sec
Step 8759 | loss: 3.228186 | lr:3.8211e-04 | norm 0.3112 | dt 336.73ms | 1556978.33 tokens/sec
Step 8760 | loss: 3.208406 | lr:3.8207e-04 | norm 0.2919 | dt 336.85ms | 1556442.75 tokens/sec
Step 8761 | loss: 3.229764 | lr:3.8202e-04 | norm 0.2652 | dt 336.82ms | 1556587.08 tokens/sec
Step 8762 | loss: 3.232323 | lr:3.8198e-04 | norm 0.2936 | dt 336.74ms | 1556958.49 tokens/sec
Step 8763 | loss: 3.198860 | lr:3.8193e-04 | norm 0.2689 | dt 337.96ms | 1551310.70 tokens/sec
Step 8764 | loss: 3.231441 | lr:3.8189e-04 | norm 0.2822 | dt 335.93ms | 1560711.13 tokens/sec
Step 8765 | loss: 3.350601 | lr:3.8184e-04 | norm 0.3004 | dt 337.23ms | 1554710.71 tokens/sec
Step 8766 | loss: 3.241930 | lr:3.8180e-04 | norm 0.3017 | dt 337.05ms | 1555534.44 tokens/sec
Step 8767 | loss: 3.197021 | lr:3.8175e-04 | norm 0.2893 | dt 337.22ms | 1554733.80 tokens/sec
Step 8768 | loss: 3.228540 | lr:3.8171e-04 | norm 0.3080 | dt 336.48ms | 1558138.91 tokens/sec
Step 8769 | loss: 3.161550 | lr:3.8166e-04 | norm 0.2663 | dt 336.47ms | 1558215.09 tokens/sec
Step 8770 | loss: 3.230872 | lr:3.8161e-04 | norm 0.2824 | dt 338.10ms | 1550691.53 tokens/sec
Step 8771 | loss: 3.277622 | lr:3.8157e-04 | norm 0.2740 | dt 338.50ms | 1548854.43 tokens/sec
Step 8772 | loss: 3.181361 | lr:3.8152e-04 | norm 0.2861 | dt 336.64ms | 1557432.65 tokens/sec
Step 8773 | loss: 3.157522 | lr:3.8148e-04 | norm 0.2693 | dt 337.02ms | 1555652.18 tokens/sec
Step 8774 | loss: 3.245095 | lr:3.8143e-04 | norm 0.2977 | dt 337.73ms | 1552379.55 tokens/sec
Step 8775 | loss: 3.223828 | lr:3.8139e-04 | norm 0.2827 | dt 337.43ms | 1553773.68 tokens/sec
Step 8776 | loss: 3.196505 | lr:3.8134e-04 | norm 0.2805 | dt 337.45ms | 1553695.73 tokens/sec
Step 8777 | loss: 3.203942 | lr:3.8130e-04 | norm 0.2593 | dt 337.05ms | 1555535.54 tokens/sec
Step 8778 | loss: 3.235320 | lr:3.8125e-04 | norm 0.2805 | dt 337.87ms | 1551762.81 tokens/sec
Step 8779 | loss: 3.246918 | lr:3.8121e-04 | norm 0.2619 | dt 338.06ms | 1550864.32 tokens/sec
Step 8780 | loss: 3.183124 | lr:3.8116e-04 | norm 0.2688 | dt 336.97ms | 1555875.62 tokens/sec
Step 8781 | loss: 3.271175 | lr:3.8112e-04 | norm 0.2538 | dt 336.91ms | 1556146.47 tokens/sec
Step 8782 | loss: 3.232617 | lr:3.8107e-04 | norm 0.2663 | dt 337.46ms | 1553611.21 tokens/sec
Step 8783 | loss: 3.273614 | lr:3.8102e-04 | norm 0.2601 | dt 337.81ms | 1552022.38 tokens/sec
Step 8784 | loss: 3.226022 | lr:3.8098e-04 | norm 0.2720 | dt 337.41ms | 1553841.75 tokens/sec
Step 8785 | loss: 3.197599 | lr:3.8093e-04 | norm 0.2600 | dt 337.69ms | 1552576.84 tokens/sec
Step 8786 | loss: 3.216188 | lr:3.8089e-04 | norm 0.2513 | dt 337.52ms | 1553365.38 tokens/sec
Step 8787 | loss: 3.223380 | lr:3.8084e-04 | norm 0.2696 | dt 337.15ms | 1555044.94 tokens/sec
Step 8788 | loss: 3.213340 | lr:3.8080e-04 | norm 0.2651 | dt 337.70ms | 1552522.03 tokens/sec
Step 8789 | loss: 3.276047 | lr:3.8075e-04 | norm 0.2740 | dt 337.69ms | 1552584.51 tokens/sec
Step 8790 | loss: 3.242486 | lr:3.8071e-04 | norm 0.2563 | dt 337.50ms | 1553437.81 tokens/sec
Step 8791 | loss: 3.321030 | lr:3.8066e-04 | norm 0.2855 | dt 337.72ms | 1552420.10 tokens/sec
Step 8792 | loss: 3.232955 | lr:3.8062e-04 | norm 0.2831 | dt 337.15ms | 1555049.34 tokens/sec
Step 8793 | loss: 3.295141 | lr:3.8057e-04 | norm 0.2967 | dt 338.19ms | 1550268.46 tokens/sec
Step 8794 | loss: 3.260249 | lr:3.8053e-04 | norm 0.3242 | dt 338.10ms | 1550710.12 tokens/sec
Step 8795 | loss: 3.272837 | lr:3.8048e-04 | norm 0.3204 | dt 337.89ms | 1551639.09 tokens/sec
Step 8796 | loss: 3.238454 | lr:3.8044e-04 | norm 0.2606 | dt 337.77ms | 1552218.47 tokens/sec
Step 8797 | loss: 3.240913 | lr:3.8039e-04 | norm 0.3120 | dt 338.24ms | 1550045.54 tokens/sec
Step 8798 | loss: 3.184406 | lr:3.8034e-04 | norm 0.2730 | dt 337.37ms | 1554040.50 tokens/sec
Step 8799 | loss: 3.169318 | lr:3.8030e-04 | norm 0.2679 | dt 338.09ms | 1550727.62 tokens/sec
Step 8800 | loss: 3.274292 | lr:3.8025e-04 | norm 0.2791 | dt 337.58ms | 1553059.30 tokens/sec
Step 8801 | loss: 3.271537 | lr:3.8021e-04 | norm 0.2708 | dt 337.85ms | 1551839.47 tokens/sec
Step 8802 | loss: 3.285298 | lr:3.8016e-04 | norm 0.2904 | dt 337.65ms | 1552762.11 tokens/sec
Step 8803 | loss: 3.211719 | lr:3.8012e-04 | norm 0.2948 | dt 338.43ms | 1549195.97 tokens/sec
Step 8804 | loss: 3.132743 | lr:3.8007e-04 | norm 0.2853 | dt 338.57ms | 1548533.77 tokens/sec
Step 8805 | loss: 3.218518 | lr:3.8003e-04 | norm 0.2977 | dt 338.03ms | 1550992.30 tokens/sec
Step 8806 | loss: 3.228041 | lr:3.7998e-04 | norm 0.2681 | dt 336.91ms | 1556171.80 tokens/sec
Step 8807 | loss: 3.205624 | lr:3.7994e-04 | norm 0.2782 | dt 337.73ms | 1552404.76 tokens/sec
Step 8808 | loss: 3.180155 | lr:3.7989e-04 | norm 0.2745 | dt 338.00ms | 1551136.72 tokens/sec
Step 8809 | loss: 3.197936 | lr:3.7984e-04 | norm 0.2636 | dt 337.80ms | 1552064.00 tokens/sec
Step 8810 | loss: 3.268419 | lr:3.7980e-04 | norm 0.2977 | dt 338.13ms | 1550543.92 tokens/sec
Step 8811 | loss: 3.156396 | lr:3.7975e-04 | norm 0.2805 | dt 337.89ms | 1551646.75 tokens/sec
Step 8812 | loss: 3.226277 | lr:3.7971e-04 | norm 0.2815 | dt 338.01ms | 1551118.12 tokens/sec
Step 8813 | loss: 3.157644 | lr:3.7966e-04 | norm 0.2937 | dt 338.73ms | 1547794.79 tokens/sec
Step 8814 | loss: 3.249386 | lr:3.7962e-04 | norm 0.2791 | dt 338.08ms | 1550762.61 tokens/sec
Step 8815 | loss: 3.307992 | lr:3.7957e-04 | norm 0.3170 | dt 338.12ms | 1550618.27 tokens/sec
Step 8816 | loss: 3.213621 | lr:3.7953e-04 | norm 0.3114 | dt 338.39ms | 1549354.24 tokens/sec
Step 8817 | loss: 3.264126 | lr:3.7948e-04 | norm 0.2935 | dt 338.43ms | 1549155.59 tokens/sec
Step 8818 | loss: 3.197610 | lr:3.7944e-04 | norm 0.2904 | dt 338.06ms | 1550850.11 tokens/sec
Step 8819 | loss: 3.293095 | lr:3.7939e-04 | norm 0.2933 | dt 338.70ms | 1547957.13 tokens/sec
Step 8820 | loss: 3.207941 | lr:3.7935e-04 | norm 0.2874 | dt 337.72ms | 1552426.68 tokens/sec
Step 8821 | loss: 3.180290 | lr:3.7930e-04 | norm 0.2957 | dt 337.73ms | 1552400.37 tokens/sec
Step 8822 | loss: 3.236283 | lr:3.7925e-04 | norm 0.2869 | dt 339.06ms | 1546278.71 tokens/sec
Step 8823 | loss: 3.208448 | lr:3.7921e-04 | norm 0.2950 | dt 338.68ms | 1548018.15 tokens/sec
Step 8824 | loss: 3.283227 | lr:3.7916e-04 | norm 0.3591 | dt 338.23ms | 1550072.85 tokens/sec
Step 8825 | loss: 3.225199 | lr:3.7912e-04 | norm 0.3453 | dt 339.20ms | 1545653.77 tokens/sec
Step 8826 | loss: 3.222831 | lr:3.7907e-04 | norm 0.3181 | dt 338.40ms | 1549316.03 tokens/sec
Step 8827 | loss: 3.291211 | lr:3.7903e-04 | norm 0.3140 | dt 338.88ms | 1547100.05 tokens/sec
Step 8828 | loss: 3.231926 | lr:3.7898e-04 | norm 0.3164 | dt 338.90ms | 1547034.74 tokens/sec
Step 8829 | loss: 3.196479 | lr:3.7894e-04 | norm 0.2906 | dt 338.91ms | 1546994.48 tokens/sec
Step 8830 | loss: 3.241770 | lr:3.7889e-04 | norm 0.3253 | dt 338.33ms | 1549623.91 tokens/sec
Step 8831 | loss: 3.232617 | lr:3.7885e-04 | norm 0.3147 | dt 339.32ms | 1545117.27 tokens/sec
Step 8832 | loss: 3.210921 | lr:3.7880e-04 | norm 0.2669 | dt 339.18ms | 1545743.95 tokens/sec
Step 8833 | loss: 3.179098 | lr:3.7875e-04 | norm 0.2938 | dt 338.55ms | 1548627.56 tokens/sec
Step 8834 | loss: 3.243965 | lr:3.7871e-04 | norm 0.2568 | dt 338.41ms | 1549261.45 tokens/sec
Step 8835 | loss: 3.244894 | lr:3.7866e-04 | norm 0.3052 | dt 338.55ms | 1548614.47 tokens/sec
Step 8836 | loss: 3.203528 | lr:3.7862e-04 | norm 0.2746 | dt 339.37ms | 1544891.48 tokens/sec
Step 8837 | loss: 3.193305 | lr:3.7857e-04 | norm 0.3118 | dt 341.59ms | 1534862.26 tokens/sec
Step 8838 | loss: 3.199194 | lr:3.7853e-04 | norm 0.3226 | dt 338.39ms | 1549337.86 tokens/sec
Step 8839 | loss: 3.197889 | lr:3.7848e-04 | norm 0.2806 | dt 339.87ms | 1542605.89 tokens/sec
Step 8840 | loss: 3.174824 | lr:3.7844e-04 | norm 0.2874 | dt 338.95ms | 1546793.17 tokens/sec
Step 8841 | loss: 3.209238 | lr:3.7839e-04 | norm 0.2868 | dt 337.96ms | 1551336.97 tokens/sec
Step 8842 | loss: 3.199389 | lr:3.7835e-04 | norm 0.3046 | dt 338.38ms | 1549386.98 tokens/sec
Step 8843 | loss: 3.233335 | lr:3.7830e-04 | norm 0.2800 | dt 339.07ms | 1546259.14 tokens/sec
Step 8844 | loss: 3.209519 | lr:3.7825e-04 | norm 0.2893 | dt 338.88ms | 1547115.29 tokens/sec
Step 8845 | loss: 3.166514 | lr:3.7821e-04 | norm 0.2626 | dt 338.12ms | 1550593.12 tokens/sec
Step 8846 | loss: 3.174525 | lr:3.7816e-04 | norm 0.2578 | dt 337.71ms | 1552493.53 tokens/sec
Step 8847 | loss: 3.183877 | lr:3.7812e-04 | norm 0.2613 | dt 338.16ms | 1550433.51 tokens/sec
Step 8848 | loss: 3.275756 | lr:3.7807e-04 | norm 0.2716 | dt 338.46ms | 1549059.55 tokens/sec
Step 8849 | loss: 3.235103 | lr:3.7803e-04 | norm 0.2701 | dt 337.85ms | 1551849.32 tokens/sec
Step 8850 | loss: 3.212811 | lr:3.7798e-04 | norm 0.2825 | dt 337.93ms | 1551488.01 tokens/sec
Step 8851 | loss: 3.186004 | lr:3.7794e-04 | norm 0.2856 | dt 337.48ms | 1553537.67 tokens/sec
Step 8852 | loss: 3.232012 | lr:3.7789e-04 | norm 0.2934 | dt 338.67ms | 1548062.83 tokens/sec
Step 8853 | loss: 3.246442 | lr:3.7785e-04 | norm 0.3062 | dt 337.61ms | 1552951.82 tokens/sec
Step 8854 | loss: 3.141706 | lr:3.7780e-04 | norm 0.2894 | dt 338.88ms | 1547106.58 tokens/sec
Step 8855 | loss: 3.200514 | lr:3.7775e-04 | norm 0.2859 | dt 337.64ms | 1552800.49 tokens/sec
Step 8856 | loss: 3.301389 | lr:3.7771e-04 | norm 0.3081 | dt 339.43ms | 1544620.20 tokens/sec
Step 8857 | loss: 3.299188 | lr:3.7766e-04 | norm 0.2831 | dt 338.55ms | 1548650.46 tokens/sec
Step 8858 | loss: 3.277794 | lr:3.7762e-04 | norm 0.2884 | dt 337.23ms | 1554679.94 tokens/sec
Step 8859 | loss: 3.324996 | lr:3.7757e-04 | norm 0.3454 | dt 338.05ms | 1550928.86 tokens/sec
Step 8860 | loss: 3.249470 | lr:3.7753e-04 | norm 0.4100 | dt 338.85ms | 1547235.03 tokens/sec
Step 8861 | loss: 3.257422 | lr:3.7748e-04 | norm 0.3733 | dt 338.18ms | 1550305.62 tokens/sec
Step 8862 | loss: 3.218831 | lr:3.7744e-04 | norm 0.3060 | dt 337.94ms | 1551432.19 tokens/sec
Step 8863 | loss: 3.203117 | lr:3.7739e-04 | norm 0.3792 | dt 337.57ms | 1553144.86 tokens/sec
Step 8864 | loss: 3.163820 | lr:3.7735e-04 | norm 0.3147 | dt 338.58ms | 1548487.97 tokens/sec
Step 8865 | loss: 3.297057 | lr:3.7730e-04 | norm 0.3731 | dt 338.78ms | 1547592.18 tokens/sec
Step 8866 | loss: 3.278833 | lr:3.7725e-04 | norm 0.3210 | dt 337.87ms | 1551747.48 tokens/sec
Step 8867 | loss: 3.281451 | lr:3.7721e-04 | norm 0.3278 | dt 337.78ms | 1552158.22 tokens/sec
Step 8868 | loss: 3.216350 | lr:3.7716e-04 | norm 0.3219 | dt 337.62ms | 1552916.72 tokens/sec
Step 8869 | loss: 3.243834 | lr:3.7712e-04 | norm 0.2983 | dt 338.19ms | 1550282.67 tokens/sec
Step 8870 | loss: 3.297037 | lr:3.7707e-04 | norm 0.3241 | dt 343.02ms | 1528461.30 tokens/sec
Step 8871 | loss: 3.278616 | lr:3.7703e-04 | norm 0.2920 | dt 338.49ms | 1548913.35 tokens/sec
Step 8872 | loss: 3.263902 | lr:3.7698e-04 | norm 0.3082 | dt 337.89ms | 1551635.80 tokens/sec
Step 8873 | loss: 3.245417 | lr:3.7694e-04 | norm 0.2938 | dt 338.97ms | 1546705.04 tokens/sec
Step 8874 | loss: 3.202347 | lr:3.7689e-04 | norm 0.2872 | dt 338.07ms | 1550843.54 tokens/sec
Step 8875 | loss: 3.246733 | lr:3.7684e-04 | norm 0.2769 | dt 338.99ms | 1546603.88 tokens/sec
Step 8876 | loss: 3.225542 | lr:3.7680e-04 | norm 0.2719 | dt 338.61ms | 1548349.50 tokens/sec
Step 8877 | loss: 3.210789 | lr:3.7675e-04 | norm 0.2613 | dt 338.10ms | 1550691.53 tokens/sec
Step 8878 | loss: 3.247532 | lr:3.7671e-04 | norm 0.2759 | dt 338.84ms | 1547321.03 tokens/sec
Step 8879 | loss: 3.218501 | lr:3.7666e-04 | norm 0.2699 | dt 338.62ms | 1548296.09 tokens/sec
Step 8880 | loss: 3.193640 | lr:3.7662e-04 | norm 0.2695 | dt 338.49ms | 1548887.16 tokens/sec
Step 8881 | loss: 3.205130 | lr:3.7657e-04 | norm 0.2717 | dt 339.73ms | 1543264.10 tokens/sec
Step 8882 | loss: 3.259102 | lr:3.7653e-04 | norm 0.2847 | dt 1037.77ms | 505206.22 tokens/sec
Step 8883 | loss: 3.214541 | lr:3.7648e-04 | norm 0.2734 | dt 337.50ms | 1553465.24 tokens/sec
Step 8884 | loss: 3.195587 | lr:3.7643e-04 | norm 0.2825 | dt 339.03ms | 1546421.16 tokens/sec
Step 8885 | loss: 3.218179 | lr:3.7639e-04 | norm 0.2733 | dt 338.98ms | 1546644.12 tokens/sec
Step 8886 | loss: 3.218086 | lr:3.7634e-04 | norm 0.2774 | dt 337.95ms | 1551376.37 tokens/sec
Step 8887 | loss: 3.214464 | lr:3.7630e-04 | norm 0.2662 | dt 339.07ms | 1546241.74 tokens/sec
Step 8888 | loss: 3.174806 | lr:3.7625e-04 | norm 0.2697 | dt 337.94ms | 1551400.45 tokens/sec
Step 8889 | loss: 3.288183 | lr:3.7621e-04 | norm 0.2700 | dt 337.28ms | 1554463.44 tokens/sec
Step 8890 | loss: 3.263530 | lr:3.7616e-04 | norm 0.3486 | dt 338.20ms | 1550223.65 tokens/sec
Step 8891 | loss: 3.232170 | lr:3.7612e-04 | norm 0.2930 | dt 338.48ms | 1548965.72 tokens/sec
Step 8892 | loss: 3.206864 | lr:3.7607e-04 | norm 0.3082 | dt 338.07ms | 1550818.39 tokens/sec
Step 8893 | loss: 3.176796 | lr:3.7603e-04 | norm 0.2689 | dt 337.69ms | 1552553.82 tokens/sec
Step 8894 | loss: 3.295984 | lr:3.7598e-04 | norm 0.2854 | dt 338.99ms | 1546607.14 tokens/sec
Step 8895 | loss: 3.247100 | lr:3.7593e-04 | norm 0.2810 | dt 337.87ms | 1551757.34 tokens/sec
Step 8896 | loss: 3.189296 | lr:3.7589e-04 | norm 0.2877 | dt 339.25ms | 1545422.40 tokens/sec
Step 8897 | loss: 3.297940 | lr:3.7584e-04 | norm 0.3021 | dt 338.29ms | 1549811.76 tokens/sec
Step 8898 | loss: 3.249446 | lr:3.7580e-04 | norm 0.2834 | dt 337.71ms | 1552502.30 tokens/sec
Step 8899 | loss: 3.231126 | lr:3.7575e-04 | norm 0.2594 | dt 337.93ms | 1551471.59 tokens/sec
Step 8900 | loss: 3.212787 | lr:3.7571e-04 | norm 0.2759 | dt 338.12ms | 1550587.66 tokens/sec
Step 8901 | loss: 3.239499 | lr:3.7566e-04 | norm 0.2707 | dt 339.60ms | 1543837.25 tokens/sec
Step 8902 | loss: 3.239034 | lr:3.7562e-04 | norm 0.2896 | dt 338.59ms | 1548435.64 tokens/sec
Step 8903 | loss: 3.183393 | lr:3.7557e-04 | norm 0.2946 | dt 338.11ms | 1550637.95 tokens/sec
Step 8904 | loss: 3.239295 | lr:3.7552e-04 | norm 0.2965 | dt 337.68ms | 1552598.76 tokens/sec
Step 8905 | loss: 3.283866 | lr:3.7548e-04 | norm 0.2683 | dt 338.61ms | 1548350.59 tokens/sec
Step 8906 | loss: 3.247272 | lr:3.7543e-04 | norm 0.3096 | dt 339.41ms | 1544686.38 tokens/sec
Step 8907 | loss: 3.169361 | lr:3.7539e-04 | norm 0.2706 | dt 338.81ms | 1547437.54 tokens/sec
Step 8908 | loss: 3.187508 | lr:3.7534e-04 | norm 0.2730 | dt 338.21ms | 1550167.92 tokens/sec
Step 8909 | loss: 3.193033 | lr:3.7530e-04 | norm 0.2683 | dt 338.44ms | 1549144.67 tokens/sec
Step 8910 | loss: 3.208190 | lr:3.7525e-04 | norm 0.2817 | dt 337.84ms | 1551882.18 tokens/sec
Step 8911 | loss: 3.192641 | lr:3.7521e-04 | norm 0.2754 | dt 339.42ms | 1544642.98 tokens/sec
Step 8912 | loss: 3.172238 | lr:3.7516e-04 | norm 0.2754 | dt 338.35ms | 1549542.02 tokens/sec
Step 8913 | loss: 3.161493 | lr:3.7511e-04 | norm 0.3079 | dt 338.00ms | 1551166.26 tokens/sec
Step 8914 | loss: 3.249355 | lr:3.7507e-04 | norm 0.2672 | dt 338.31ms | 1549740.77 tokens/sec
Step 8915 | loss: 3.226746 | lr:3.7502e-04 | norm 0.3161 | dt 338.66ms | 1548116.24 tokens/sec
Step 8916 | loss: 3.227265 | lr:3.7498e-04 | norm 0.3152 | dt 337.59ms | 1553051.62 tokens/sec
Step 8917 | loss: 3.205605 | lr:3.7493e-04 | norm 0.3219 | dt 338.44ms | 1549130.49 tokens/sec
Step 8918 | loss: 3.191695 | lr:3.7489e-04 | norm 0.3014 | dt 338.82ms | 1547399.43 tokens/sec
Step 8919 | loss: 3.254577 | lr:3.7484e-04 | norm 0.2709 | dt 338.02ms | 1551052.47 tokens/sec
Step 8920 | loss: 3.217988 | lr:3.7480e-04 | norm 0.2814 | dt 338.07ms | 1550829.33 tokens/sec
Step 8921 | loss: 3.210809 | lr:3.7475e-04 | norm 0.3172 | dt 338.26ms | 1549951.58 tokens/sec
Step 8922 | loss: 3.247921 | lr:3.7470e-04 | norm 0.2493 | dt 337.98ms | 1551237.38 tokens/sec
Step 8923 | loss: 3.177496 | lr:3.7466e-04 | norm 0.3210 | dt 339.02ms | 1546489.67 tokens/sec
Step 8924 | loss: 3.198120 | lr:3.7461e-04 | norm 0.2872 | dt 338.01ms | 1551124.68 tokens/sec
Step 8925 | loss: 3.231486 | lr:3.7457e-04 | norm 0.2814 | dt 337.45ms | 1553677.07 tokens/sec
Step 8926 | loss: 3.272958 | lr:3.7452e-04 | norm 0.2833 | dt 339.03ms | 1546416.81 tokens/sec
Step 8927 | loss: 3.204805 | lr:3.7448e-04 | norm 0.2681 | dt 338.38ms | 1549419.74 tokens/sec
Step 8928 | loss: 3.247164 | lr:3.7443e-04 | norm 0.2979 | dt 337.41ms | 1553840.65 tokens/sec
Step 8929 | loss: 3.219364 | lr:3.7439e-04 | norm 0.2603 | dt 1008.02ms | 520115.23 tokens/sec
Step 8930 | loss: 3.187801 | lr:3.7434e-04 | norm 0.2896 | dt 336.69ms | 1557186.71 tokens/sec
Step 8931 | loss: 3.416237 | lr:3.7429e-04 | norm 0.3473 | dt 339.78ms | 1543011.79 tokens/sec
Step 8932 | loss: 3.271370 | lr:3.7425e-04 | norm 0.3271 | dt 337.96ms | 1551310.70 tokens/sec
Step 8933 | loss: 3.309151 | lr:3.7420e-04 | norm 0.3257 | dt 337.96ms | 1551319.46 tokens/sec
Step 8934 | loss: 3.196806 | lr:3.7416e-04 | norm 0.3225 | dt 337.73ms | 1552376.27 tokens/sec
Step 8935 | loss: 3.237435 | lr:3.7411e-04 | norm 0.3165 | dt 337.91ms | 1551540.56 tokens/sec
Step 8936 | loss: 3.299154 | lr:3.7407e-04 | norm 0.3138 | dt 338.17ms | 1550372.29 tokens/sec
Step 8937 | loss: 3.191214 | lr:3.7402e-04 | norm 0.3328 | dt 338.33ms | 1549651.21 tokens/sec
Step 8938 | loss: 3.284556 | lr:3.7398e-04 | norm 0.3089 | dt 336.89ms | 1556278.63 tokens/sec
Step 8939 | loss: 3.247255 | lr:3.7393e-04 | norm 0.3339 | dt 338.32ms | 1549702.54 tokens/sec
Step 8940 | loss: 3.259603 | lr:3.7388e-04 | norm 0.3085 | dt 337.45ms | 1553672.68 tokens/sec
Step 8941 | loss: 3.283776 | lr:3.7384e-04 | norm 0.3056 | dt 337.92ms | 1551506.62 tokens/sec
Step 8942 | loss: 3.222411 | lr:3.7379e-04 | norm 0.2938 | dt 337.64ms | 1552804.87 tokens/sec
Step 8943 | loss: 3.213166 | lr:3.7375e-04 | norm 0.2917 | dt 338.70ms | 1547950.59 tokens/sec
Step 8944 | loss: 3.228070 | lr:3.7370e-04 | norm 0.2876 | dt 338.07ms | 1550811.83 tokens/sec
Step 8945 | loss: 3.220498 | lr:3.7366e-04 | norm 0.2863 | dt 337.90ms | 1551599.67 tokens/sec
Step 8946 | loss: 3.187087 | lr:3.7361e-04 | norm 0.2717 | dt 337.95ms | 1551399.35 tokens/sec
Step 8947 | loss: 3.229086 | lr:3.7356e-04 | norm 0.2839 | dt 338.06ms | 1550853.39 tokens/sec
Step 8948 | loss: 3.208536 | lr:3.7352e-04 | norm 0.2766 | dt 338.41ms | 1549260.36 tokens/sec
Step 8949 | loss: 3.252091 | lr:3.7347e-04 | norm 0.3047 | dt 336.74ms | 1556936.44 tokens/sec
Step 8950 | loss: 3.255735 | lr:3.7343e-04 | norm 0.3037 | dt 337.99ms | 1551216.59 tokens/sec
Step 8951 | loss: 3.254947 | lr:3.7338e-04 | norm 0.3348 | dt 338.17ms | 1550351.53 tokens/sec
Step 8952 | loss: 3.202394 | lr:3.7334e-04 | norm 0.3086 | dt 337.97ms | 1551294.29 tokens/sec
Step 8953 | loss: 3.245881 | lr:3.7329e-04 | norm 0.2914 | dt 338.36ms | 1549484.15 tokens/sec
Step 8954 | loss: 3.200855 | lr:3.7325e-04 | norm 0.2869 | dt 340.36ms | 1540398.25 tokens/sec
Step 8955 | loss: 3.181296 | lr:3.7320e-04 | norm 0.3356 | dt 338.40ms | 1549298.56 tokens/sec
Step 8956 | loss: 3.217940 | lr:3.7315e-04 | norm 0.2891 | dt 338.33ms | 1549615.18 tokens/sec
Step 8957 | loss: 3.241187 | lr:3.7311e-04 | norm 0.2951 | dt 338.58ms | 1548499.97 tokens/sec
Step 8958 | loss: 3.185843 | lr:3.7306e-04 | norm 0.3114 | dt 338.36ms | 1549513.63 tokens/sec
Step 8959 | loss: 3.227259 | lr:3.7302e-04 | norm 0.2688 | dt 337.96ms | 1551345.72 tokens/sec
Step 8960 | loss: 3.218179 | lr:3.7297e-04 | norm 0.2990 | dt 338.60ms | 1548392.02 tokens/sec
Step 8961 | loss: 3.248891 | lr:3.7293e-04 | norm 0.3005 | dt 337.67ms | 1552641.51 tokens/sec
Step 8962 | loss: 3.270575 | lr:3.7288e-04 | norm 0.3078 | dt 338.22ms | 1550148.25 tokens/sec
Step 8963 | loss: 3.204366 | lr:3.7284e-04 | norm 0.2829 | dt 338.20ms | 1550211.63 tokens/sec
Step 8964 | loss: 3.158676 | lr:3.7279e-04 | norm 0.2894 | dt 337.92ms | 1551505.53 tokens/sec
Step 8965 | loss: 3.211641 | lr:3.7274e-04 | norm 0.2650 | dt 337.18ms | 1554933.88 tokens/sec
Step 8966 | loss: 3.247277 | lr:3.7270e-04 | norm 0.2810 | dt 338.32ms | 1549698.17 tokens/sec
Step 8967 | loss: 3.230660 | lr:3.7265e-04 | norm 0.2740 | dt 338.62ms | 1548288.46 tokens/sec
Step 8968 | loss: 3.225350 | lr:3.7261e-04 | norm 0.2765 | dt 337.66ms | 1552690.85 tokens/sec
Step 8969 | loss: 3.196453 | lr:3.7256e-04 | norm 0.2643 | dt 337.96ms | 1551327.12 tokens/sec
Step 8970 | loss: 3.266565 | lr:3.7252e-04 | norm 0.2895 | dt 338.04ms | 1550980.27 tokens/sec
Step 8971 | loss: 3.202651 | lr:3.7247e-04 | norm 0.2632 | dt 339.06ms | 1546278.71 tokens/sec
Step 8972 | loss: 3.263399 | lr:3.7242e-04 | norm 0.2955 | dt 338.36ms | 1549502.71 tokens/sec
Step 8973 | loss: 3.215046 | lr:3.7238e-04 | norm 0.2775 | dt 337.76ms | 1552268.88 tokens/sec
Step 8974 | loss: 3.196267 | lr:3.7233e-04 | norm 0.2813 | dt 337.72ms | 1552432.16 tokens/sec
Step 8975 | loss: 3.201973 | lr:3.7229e-04 | norm 0.3030 | dt 338.02ms | 1551071.07 tokens/sec
Step 8976 | loss: 3.227616 | lr:3.7224e-04 | norm 0.2531 | dt 338.19ms | 1550293.60 tokens/sec
Step 8977 | loss: 3.243753 | lr:3.7220e-04 | norm 0.2842 | dt 337.96ms | 1551339.16 tokens/sec
Step 8978 | loss: 3.232174 | lr:3.7215e-04 | norm 0.2532 | dt 337.84ms | 1551883.27 tokens/sec
Step 8979 | loss: 3.197648 | lr:3.7210e-04 | norm 0.2574 | dt 337.48ms | 1553517.92 tokens/sec
Step 8980 | loss: 3.227416 | lr:3.7206e-04 | norm 0.2717 | dt 338.51ms | 1548833.71 tokens/sec
Step 8981 | loss: 3.176682 | lr:3.7201e-04 | norm 0.2934 | dt 342.42ms | 1531117.60 tokens/sec
Step 8982 | loss: 3.156648 | lr:3.7197e-04 | norm 0.2577 | dt 337.53ms | 1553318.20 tokens/sec
Step 8983 | loss: 3.197846 | lr:3.7192e-04 | norm 0.2737 | dt 338.20ms | 1550241.14 tokens/sec
Step 8984 | loss: 3.208869 | lr:3.7188e-04 | norm 0.2858 | dt 337.43ms | 1553784.66 tokens/sec
Step 8985 | loss: 3.179006 | lr:3.7183e-04 | norm 0.2905 | dt 337.55ms | 1553208.49 tokens/sec
Step 8986 | loss: 3.228085 | lr:3.7179e-04 | norm 0.2853 | dt 337.75ms | 1552312.71 tokens/sec
Step 8987 | loss: 3.252515 | lr:3.7174e-04 | norm 0.2931 | dt 337.36ms | 1554100.91 tokens/sec
Step 8988 | loss: 3.210439 | lr:3.7169e-04 | norm 0.3086 | dt 337.69ms | 1552553.82 tokens/sec
Step 8989 | loss: 3.228129 | lr:3.7165e-04 | norm 0.2761 | dt 338.27ms | 1549916.62 tokens/sec
Step 8990 | loss: 3.275682 | lr:3.7160e-04 | norm 0.2817 | dt 337.78ms | 1552138.49 tokens/sec
Step 8991 | loss: 3.184458 | lr:3.7156e-04 | norm 0.3137 | dt 337.20ms | 1554844.83 tokens/sec
Step 8992 | loss: 3.225638 | lr:3.7151e-04 | norm 0.3119 | dt 338.50ms | 1548876.25 tokens/sec
Step 8993 | loss: 3.235121 | lr:3.7147e-04 | norm 0.2796 | dt 338.48ms | 1548949.35 tokens/sec
Step 8994 | loss: 3.285512 | lr:3.7142e-04 | norm 0.3174 | dt 337.60ms | 1552993.49 tokens/sec
Step 8995 | loss: 3.241674 | lr:3.7137e-04 | norm 0.3246 | dt 337.92ms | 1551496.77 tokens/sec
Step 8996 | loss: 3.200943 | lr:3.7133e-04 | norm 0.3027 | dt 338.86ms | 1547219.79 tokens/sec
Step 8997 | loss: 3.239222 | lr:3.7128e-04 | norm 0.2926 | dt 338.87ms | 1547163.18 tokens/sec
Step 8998 | loss: 3.232430 | lr:3.7124e-04 | norm 0.3142 | dt 337.98ms | 1551260.36 tokens/sec
Step 8999 | loss: 3.291434 | lr:3.7119e-04 | norm 0.2886 | dt 337.89ms | 1551636.90 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 9000: 3.2223
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2889/10042=0.2877


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, and my job is to create a language tool for myself, and in my case it's a program that's in a
rank 5 sample 1 >Hello, I'm a language model, please try our new lesson! Thanks.
Please write your thoughts in the comment box below and share them via the social
rank 5 sample 2 >Hello, I'm a language model, I've read about Python, I wrote a script to make it behave like this, let's see how to do that
rank 5 sample 3 >Hello, I'm a language model, as it's called. This is a language model which can be understood by everybody except myself, we're all the same




ddp_rank 1: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 



ddp_rank 3: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so I'll be giving a simple outline. Let's assume you're writing a program for this program. If you're
rank 6 sample 0 >Hello, I'm a language model, meaning to learn about things such as how they are pronounced and the spelling patterns, or how the words are pronounced. To
rank 1 sample 1 >Hello, I'm a language model, a person with multiple languages. I know all the languages for my child, even though our age group has been studying languages
rank 3 sample 0 >Hello, I'm a language model, so I'm not just making it better and more consistent. To me who loves a job as a trainer and to manyrank 6 sample 1 >Hello, I'm a language model, you know how to program. So I went to see the source code first, I wanted the program, then I sent

rank 1 sample 2 >Hello, I'm a language model, and therefore the way I talk to my students is much the same, I do not have the power and I know what
rank 6 sample 2 >Hello, I'm a language model, and this is my language modeling project. I love how I use math to explain things. I love how people come to
rank 3 sample 1 >Hello, I'm a language model, so my model is a Python file, so this is my first Python file.
import model python def model: python
rank 1 sample 3 >Hello, I'm a language model, so I'm interested to tell you, since I'm getting confused how to deal with many characters of the language.



rank 6 sample 3 >Hello, I'm a language model, so I have a book, it's about how to write a sentence. So, it used to be in Arabic,


rank 3 sample 2 >Hello, I'm a language model, so this one is almost for myself. Anyway, I am glad to have been asked to come as soon as. The
rank 3 sample 3 >Hello, I'm a language model, so that's my theory. You're a person that likes this language and I would like to get someone's ideas about




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I would like to make a more explicit version. But I'm more interested in making models here because it's my
rank 2 sample 0 >Hello, I'm a language model, here. I use the programming language to write code and then we're done with all the code in the process. And
rank 7 sample 1 >Hello, I'm a language model, of course, but I'll say another thing. Thanks, for my reply. There are two ways to solve it in
rank 2 sample 1 >Hello, I'm a language model, and I won't get it down anyhow. . . I just wrote a program that could run into the datafile
rank 7 sample 2 >Hello, I'm a language model, and I've taught, I hope to work with one of the teachers you've been teaching. I think I am really
rank 2 sample 2 >Hello, I'm a language model, I'm not sure if I could translate as a language model, so in a sentence, there'd be two lines to
rank 7 sample 3 >Hello, I'm a language model, trying to do something for your application/service)
- Is the data type (byte, numeric, etc) supported


rank 2 sample 3 >Hello, I'm a language model, and if I'm a language model, like, as far as I can tell, you mean someone is in the position




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, I'm looking for the basics.
Hello, I'm trying to build a library which implements the C# library,
rank 4 sample 1 >Hello, I'm a language model, because that's their favorite way of getting their knowledge from this book. There is something I like about these books — it
rank 4 sample 2 >Hello, I'm a language model, but I'm the model builder. The reason I'm looking is to do something simple like reading out this book... I
rank 4 sample 3 >Hello, I'm a language model, so it did that with a lot of people coming with those models,
And I'm a human, so I can




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know that it's really interesting since I'm at a point during that time and I just can't figure out
rank 0 sample 1 >Hello, I'm a language model, and as a language model, I wanted to create such a model for us and to see how it affects our lives.
rank 0 sample 2 >Hello, I'm a language model, I'm one of the world's great language educators. I like learning languages, I'm a language model, and,
rank 0 sample 3 >Hello, I'm a language model, and am sure I can say I have some type of a real name (or what you'll call something like a verb


Step 9000 | loss: 3.255170 | lr:3.7115e-04 | norm 0.3188 | dt 12272.14ms | 42721.81 tokens/sec
Step 9001 | loss: 3.264057 | lr:3.7110e-04 | norm 0.2985 | dt 334.67ms | 1566561.61 tokens/sec
Step 9002 | loss: 3.202038 | lr:3.7105e-04 | norm 0.2830 | dt 335.98ms | 1560481.87 tokens/sec
Step 9003 | loss: 3.221909 | lr:3.7101e-04 | norm 0.2893 | dt 337.31ms | 1554304.12 tokens/sec
Step 9004 | loss: 3.196290 | lr:3.7096e-04 | norm 0.2893 | dt 336.22ms | 1559342.13 tokens/sec
Step 9005 | loss: 3.244724 | lr:3.7092e-04 | norm 0.2917 | dt 336.37ms | 1558661.29 tokens/sec
Step 9006 | loss: 3.252169 | lr:3.7087e-04 | norm 0.2800 | dt 336.11ms | 1559859.79 tokens/sec
Step 9007 | loss: 3.218710 | lr:3.7083e-04 | norm 0.2603 | dt 336.68ms | 1557225.30 tokens/sec
Step 9008 | loss: 3.213801 | lr:3.7078e-04 | norm 0.2815 | dt 335.95ms | 1560599.26 tokens/sec
Step 9009 | loss: 3.190134 | lr:3.7074e-04 | norm 0.2698 | dt 337.88ms | 1551699.30 tokens/sec
Step 9010 | loss: 3.218729 | lr:3.7069e-04 | norm 0.2755 | dt 336.50ms | 1558080.40 tokens/sec
Step 9011 | loss: 3.157372 | lr:3.7064e-04 | norm 0.2908 | dt 336.07ms | 1560050.13 tokens/sec
Step 9012 | loss: 3.197130 | lr:3.7060e-04 | norm 0.2483 | dt 337.20ms | 1554845.92 tokens/sec
Step 9013 | loss: 3.228904 | lr:3.7055e-04 | norm 0.2950 | dt 336.40ms | 1558548.61 tokens/sec
Step 9014 | loss: 3.193211 | lr:3.7051e-04 | norm 0.2811 | dt 337.00ms | 1555758.94 tokens/sec
Step 9015 | loss: 3.170614 | lr:3.7046e-04 | norm 0.2742 | dt 337.28ms | 1554468.93 tokens/sec
Step 9016 | loss: 3.189843 | lr:3.7042e-04 | norm 0.2517 | dt 337.65ms | 1552759.92 tokens/sec
Step 9017 | loss: 3.233652 | lr:3.7037e-04 | norm 0.2842 | dt 337.99ms | 1551216.59 tokens/sec
Step 9018 | loss: 3.238574 | lr:3.7032e-04 | norm 0.2903 | dt 337.88ms | 1551719.01 tokens/sec
Step 9019 | loss: 3.173065 | lr:3.7028e-04 | norm 0.2717 | dt 337.91ms | 1551581.06 tokens/sec
Step 9020 | loss: 3.192750 | lr:3.7023e-04 | norm 0.2583 | dt 338.47ms | 1549010.45 tokens/sec
Step 9021 | loss: 3.215331 | lr:3.7019e-04 | norm 0.2830 | dt 337.53ms | 1553325.88 tokens/sec
Step 9022 | loss: 3.207278 | lr:3.7014e-04 | norm 0.2989 | dt 337.33ms | 1554211.85 tokens/sec
Step 9023 | loss: 3.203089 | lr:3.7010e-04 | norm 0.2734 | dt 337.57ms | 1553144.86 tokens/sec
Step 9024 | loss: 3.205417 | lr:3.7005e-04 | norm 0.3008 | dt 337.69ms | 1552568.07 tokens/sec
Step 9025 | loss: 3.221320 | lr:3.7000e-04 | norm 0.2833 | dt 337.66ms | 1552695.23 tokens/sec
Step 9026 | loss: 3.177835 | lr:3.6996e-04 | norm 0.2971 | dt 338.02ms | 1551033.87 tokens/sec
Step 9027 | loss: 3.261438 | lr:3.6991e-04 | norm 0.2965 | dt 337.85ms | 1551839.47 tokens/sec
Step 9028 | loss: 3.241565 | lr:3.6987e-04 | norm 0.2936 | dt 337.76ms | 1552259.01 tokens/sec
Step 9029 | loss: 3.224409 | lr:3.6982e-04 | norm 0.3003 | dt 338.09ms | 1550756.05 tokens/sec
Step 9030 | loss: 3.199203 | lr:3.6978e-04 | norm 0.2770 | dt 338.03ms | 1551012.00 tokens/sec
Step 9031 | loss: 3.276594 | lr:3.6973e-04 | norm 0.2842 | dt 338.08ms | 1550783.39 tokens/sec
Step 9032 | loss: 3.340282 | lr:3.6968e-04 | norm 0.3239 | dt 337.51ms | 1553421.34 tokens/sec
Step 9033 | loss: 3.228827 | lr:3.6964e-04 | norm 0.3039 | dt 337.61ms | 1552923.30 tokens/sec
Step 9034 | loss: 3.233797 | lr:3.6959e-04 | norm 0.3248 | dt 339.00ms | 1546570.16 tokens/sec
Step 9035 | loss: 3.238367 | lr:3.6955e-04 | norm 0.2805 | dt 337.57ms | 1553136.08 tokens/sec
Step 9036 | loss: 3.239681 | lr:3.6950e-04 | norm 0.3040 | dt 338.40ms | 1549322.58 tokens/sec
Step 9037 | loss: 3.286269 | lr:3.6946e-04 | norm 0.3336 | dt 338.30ms | 1549751.69 tokens/sec
Step 9038 | loss: 3.266563 | lr:3.6941e-04 | norm 0.3303 | dt 338.29ms | 1549801.93 tokens/sec
Step 9039 | loss: 3.196941 | lr:3.6936e-04 | norm 0.3011 | dt 337.71ms | 1552502.30 tokens/sec
Step 9040 | loss: 3.258653 | lr:3.6932e-04 | norm 0.3251 | dt 337.23ms | 1554681.04 tokens/sec
Step 9041 | loss: 3.192743 | lr:3.6927e-04 | norm 0.3038 | dt 337.44ms | 1553704.51 tokens/sec
Step 9042 | loss: 3.197067 | lr:3.6923e-04 | norm 0.3037 | dt 338.61ms | 1548374.58 tokens/sec
Step 9043 | loss: 3.219983 | lr:3.6918e-04 | norm 0.3042 | dt 338.49ms | 1548917.71 tokens/sec
Step 9044 | loss: 3.203887 | lr:3.6914e-04 | norm 0.3075 | dt 337.95ms | 1551376.37 tokens/sec
Step 9045 | loss: 3.298842 | lr:3.6909e-04 | norm 0.2921 | dt 338.93ms | 1546872.60 tokens/sec
Step 9046 | loss: 3.211846 | lr:3.6904e-04 | norm 0.3173 | dt 337.66ms | 1552714.97 tokens/sec
Step 9047 | loss: 3.257520 | lr:3.6900e-04 | norm 0.2722 | dt 337.45ms | 1553654.02 tokens/sec
Step 9048 | loss: 3.211746 | lr:3.6895e-04 | norm 0.2895 | dt 338.39ms | 1549377.16 tokens/sec
Step 9049 | loss: 3.185365 | lr:3.6891e-04 | norm 0.2618 | dt 338.28ms | 1549868.56 tokens/sec
Step 9050 | loss: 3.224593 | lr:3.6886e-04 | norm 0.2858 | dt 338.46ms | 1549051.92 tokens/sec
Step 9051 | loss: 3.342788 | lr:3.6882e-04 | norm 0.3592 | dt 337.48ms | 1553547.55 tokens/sec
Step 9052 | loss: 3.322184 | lr:3.6877e-04 | norm 0.3466 | dt 337.90ms | 1551617.19 tokens/sec
Step 9053 | loss: 3.229697 | lr:3.6872e-04 | norm 0.3170 | dt 338.95ms | 1546805.14 tokens/sec
Step 9054 | loss: 3.174073 | lr:3.6868e-04 | norm 0.3130 | dt 339.16ms | 1545833.05 tokens/sec
Step 9055 | loss: 3.214966 | lr:3.6863e-04 | norm 0.2783 | dt 338.02ms | 1551047.00 tokens/sec
Step 9056 | loss: 3.210332 | lr:3.6859e-04 | norm 0.3023 | dt 339.32ms | 1545102.07 tokens/sec
Step 9057 | loss: 3.257488 | lr:3.6854e-04 | norm 0.2686 | dt 338.60ms | 1548409.47 tokens/sec
Step 9058 | loss: 3.192724 | lr:3.6850e-04 | norm 0.2920 | dt 338.60ms | 1548376.76 tokens/sec
Step 9059 | loss: 3.247161 | lr:3.6845e-04 | norm 0.2968 | dt 338.72ms | 1547854.71 tokens/sec
Step 9060 | loss: 3.270739 | lr:3.6840e-04 | norm 0.2693 | dt 338.65ms | 1548183.81 tokens/sec
Step 9061 | loss: 3.210168 | lr:3.6836e-04 | norm 0.2964 | dt 338.61ms | 1548365.86 tokens/sec
Step 9062 | loss: 3.200470 | lr:3.6831e-04 | norm 0.2856 | dt 337.99ms | 1551210.03 tokens/sec
Step 9063 | loss: 3.271451 | lr:3.6827e-04 | norm 0.3096 | dt 338.46ms | 1549027.91 tokens/sec
Step 9064 | loss: 3.267493 | lr:3.6822e-04 | norm 0.3166 | dt 338.42ms | 1549206.88 tokens/sec
Step 9065 | loss: 3.243646 | lr:3.6818e-04 | norm 0.2945 | dt 341.34ms | 1535948.25 tokens/sec
Step 9066 | loss: 3.257857 | lr:3.6813e-04 | norm 0.2753 | dt 339.00ms | 1546566.89 tokens/sec
Step 9067 | loss: 3.221910 | lr:3.6808e-04 | norm 0.2824 | dt 339.64ms | 1543664.94 tokens/sec
Step 9068 | loss: 3.226146 | lr:3.6804e-04 | norm 0.2858 | dt 337.90ms | 1551592.01 tokens/sec
Step 9069 | loss: 3.245498 | lr:3.6799e-04 | norm 0.2719 | dt 338.04ms | 1550945.27 tokens/sec
Step 9070 | loss: 3.248851 | lr:3.6795e-04 | norm 0.3040 | dt 337.94ms | 1551443.13 tokens/sec
Step 9071 | loss: 3.210660 | lr:3.6790e-04 | norm 0.2726 | dt 1024.77ms | 511614.80 tokens/sec
Step 9072 | loss: 3.344060 | lr:3.6786e-04 | norm 0.3251 | dt 337.13ms | 1555159.31 tokens/sec
Step 9073 | loss: 3.208049 | lr:3.6781e-04 | norm 0.3371 | dt 336.77ms | 1556825.11 tokens/sec
Step 9074 | loss: 3.241934 | lr:3.6776e-04 | norm 0.2799 | dt 339.86ms | 1542671.90 tokens/sec
Step 9075 | loss: 3.275861 | lr:3.6772e-04 | norm 0.2940 | dt 337.77ms | 1552219.57 tokens/sec
Step 9076 | loss: 3.308357 | lr:3.6767e-04 | norm 0.3128 | dt 338.01ms | 1551113.74 tokens/sec
Step 9077 | loss: 3.299201 | lr:3.6763e-04 | norm 0.3389 | dt 337.46ms | 1553646.34 tokens/sec
Step 9078 | loss: 3.208306 | lr:3.6758e-04 | norm 0.2874 | dt 337.24ms | 1554624.98 tokens/sec
Step 9079 | loss: 3.222879 | lr:3.6754e-04 | norm 0.2898 | dt 337.68ms | 1552626.17 tokens/sec
Step 9080 | loss: 3.248038 | lr:3.6749e-04 | norm 0.2974 | dt 337.71ms | 1552471.61 tokens/sec
Step 9081 | loss: 3.187948 | lr:3.6744e-04 | norm 0.2727 | dt 337.89ms | 1551662.08 tokens/sec
Step 9082 | loss: 3.238683 | lr:3.6740e-04 | norm 0.3182 | dt 338.12ms | 1550584.38 tokens/sec
Step 9083 | loss: 3.212995 | lr:3.6735e-04 | norm 0.2739 | dt 337.55ms | 1553216.17 tokens/sec
Step 9084 | loss: 3.191932 | lr:3.6731e-04 | norm 0.2692 | dt 339.12ms | 1546047.15 tokens/sec
Step 9085 | loss: 3.180799 | lr:3.6726e-04 | norm 0.2673 | dt 337.56ms | 1553164.61 tokens/sec
Step 9086 | loss: 3.154304 | lr:3.6721e-04 | norm 0.3013 | dt 338.21ms | 1550175.57 tokens/sec
Step 9087 | loss: 3.182196 | lr:3.6717e-04 | norm 0.2865 | dt 338.13ms | 1550558.14 tokens/sec
Step 9088 | loss: 3.189822 | lr:3.6712e-04 | norm 0.3153 | dt 337.79ms | 1552108.92 tokens/sec
Step 9089 | loss: 3.195668 | lr:3.6708e-04 | norm 0.3106 | dt 337.88ms | 1551682.88 tokens/sec
Step 9090 | loss: 3.229890 | lr:3.6703e-04 | norm 0.2658 | dt 337.96ms | 1551349.01 tokens/sec
Step 9091 | loss: 3.216399 | lr:3.6699e-04 | norm 0.2745 | dt 337.66ms | 1552723.74 tokens/sec
Step 9092 | loss: 3.169042 | lr:3.6694e-04 | norm 0.2825 | dt 338.73ms | 1547808.95 tokens/sec
Step 9093 | loss: 3.204693 | lr:3.6689e-04 | norm 0.2630 | dt 338.00ms | 1551135.62 tokens/sec
Step 9094 | loss: 3.188806 | lr:3.6685e-04 | norm 0.2832 | dt 337.51ms | 1553410.37 tokens/sec
Step 9095 | loss: 3.193789 | lr:3.6680e-04 | norm 0.2793 | dt 339.01ms | 1546546.23 tokens/sec
Step 9096 | loss: 3.203701 | lr:3.6676e-04 | norm 0.2868 | dt 338.21ms | 1550205.07 tokens/sec
Step 9097 | loss: 3.229693 | lr:3.6671e-04 | norm 0.2990 | dt 337.93ms | 1551452.99 tokens/sec
Step 9098 | loss: 3.252533 | lr:3.6667e-04 | norm 0.2892 | dt 338.14ms | 1550488.17 tokens/sec
Step 9099 | loss: 3.324389 | lr:3.6662e-04 | norm 0.3449 | dt 337.80ms | 1552048.67 tokens/sec
Step 9100 | loss: 3.224719 | lr:3.6657e-04 | norm 0.3146 | dt 338.63ms | 1548266.65 tokens/sec
Step 9101 | loss: 3.212547 | lr:3.6653e-04 | norm 0.3028 | dt 340.34ms | 1540496.45 tokens/sec
Step 9102 | loss: 3.201193 | lr:3.6648e-04 | norm 0.2861 | dt 339.03ms | 1546438.56 tokens/sec
Step 9103 | loss: 3.242507 | lr:3.6644e-04 | norm 0.3139 | dt 338.47ms | 1549009.36 tokens/sec
Step 9104 | loss: 3.231081 | lr:3.6639e-04 | norm 0.3050 | dt 338.13ms | 1550535.18 tokens/sec
Step 9105 | loss: 3.203712 | lr:3.6635e-04 | norm 0.3610 | dt 338.10ms | 1550700.28 tokens/sec
Step 9106 | loss: 3.295138 | lr:3.6630e-04 | norm 0.3067 | dt 337.97ms | 1551283.34 tokens/sec
Step 9107 | loss: 3.257538 | lr:3.6625e-04 | norm 0.3054 | dt 337.99ms | 1551192.52 tokens/sec
Step 9108 | loss: 3.182482 | lr:3.6621e-04 | norm 0.2912 | dt 337.64ms | 1552814.74 tokens/sec
Step 9109 | loss: 3.193926 | lr:3.6616e-04 | norm 0.3147 | dt 337.72ms | 1552452.98 tokens/sec
Step 9110 | loss: 3.188560 | lr:3.6612e-04 | norm 0.3013 | dt 337.97ms | 1551303.04 tokens/sec
Step 9111 | loss: 3.211985 | lr:3.6607e-04 | norm 0.2950 | dt 337.92ms | 1551494.58 tokens/sec
Step 9112 | loss: 3.219861 | lr:3.6602e-04 | norm 0.2792 | dt 337.63ms | 1552838.87 tokens/sec
Step 9113 | loss: 3.229219 | lr:3.6598e-04 | norm 0.3027 | dt 337.44ms | 1553708.91 tokens/sec
Step 9114 | loss: 3.266664 | lr:3.6593e-04 | norm 0.2989 | dt 338.00ms | 1551135.62 tokens/sec
Step 9115 | loss: 3.200387 | lr:3.6589e-04 | norm 0.3038 | dt 338.71ms | 1547902.65 tokens/sec
Step 9116 | loss: 3.225369 | lr:3.6584e-04 | norm 0.3352 | dt 338.28ms | 1549875.11 tokens/sec
Step 9117 | loss: 3.213293 | lr:3.6580e-04 | norm 0.2789 | dt 337.66ms | 1552714.97 tokens/sec
Step 9118 | loss: 3.234976 | lr:3.6575e-04 | norm 0.2798 | dt 338.39ms | 1549354.24 tokens/sec
Step 9119 | loss: 3.246945 | lr:3.6570e-04 | norm 0.2855 | dt 1057.02ms | 496007.23 tokens/sec
Step 9120 | loss: 3.242582 | lr:3.6566e-04 | norm 0.2975 | dt 336.84ms | 1556503.34 tokens/sec
Step 9121 | loss: 3.220588 | lr:3.6561e-04 | norm 0.2867 | dt 337.34ms | 1554187.68 tokens/sec
Step 9122 | loss: 3.148182 | lr:3.6557e-04 | norm 0.2885 | dt 337.43ms | 1553772.58 tokens/sec
Step 9123 | loss: 3.304118 | lr:3.6552e-04 | norm 0.2937 | dt 336.97ms | 1555874.52 tokens/sec
Step 9124 | loss: 3.233833 | lr:3.6547e-04 | norm 0.2694 | dt 337.13ms | 1555134.01 tokens/sec
Step 9125 | loss: 3.236141 | lr:3.6543e-04 | norm 0.2748 | dt 337.26ms | 1554546.95 tokens/sec
Step 9126 | loss: 3.215992 | lr:3.6538e-04 | norm 0.3054 | dt 337.44ms | 1553745.13 tokens/sec
Step 9127 | loss: 3.186621 | lr:3.6534e-04 | norm 0.2706 | dt 336.99ms | 1555800.77 tokens/sec
Step 9128 | loss: 3.203140 | lr:3.6529e-04 | norm 0.2978 | dt 337.03ms | 1555634.57 tokens/sec
Step 9129 | loss: 3.210690 | lr:3.6525e-04 | norm 0.2621 | dt 337.73ms | 1552367.50 tokens/sec
Step 9130 | loss: 3.249719 | lr:3.6520e-04 | norm 0.2857 | dt 338.32ms | 1549690.53 tokens/sec
Step 9131 | loss: 3.158461 | lr:3.6515e-04 | norm 0.2606 | dt 338.24ms | 1550069.58 tokens/sec
Step 9132 | loss: 3.186380 | lr:3.6511e-04 | norm 0.2891 | dt 337.46ms | 1553622.19 tokens/sec
Step 9133 | loss: 3.171835 | lr:3.6506e-04 | norm 0.2832 | dt 338.63ms | 1548284.10 tokens/sec
Step 9134 | loss: 3.142264 | lr:3.6502e-04 | norm 0.2616 | dt 338.35ms | 1549521.27 tokens/sec
Step 9135 | loss: 3.172211 | lr:3.6497e-04 | norm 0.2985 | dt 337.53ms | 1553287.48 tokens/sec
Step 9136 | loss: 3.150327 | lr:3.6493e-04 | norm 0.3212 | dt 337.06ms | 1555469.52 tokens/sec
Step 9137 | loss: 3.188145 | lr:3.6488e-04 | norm 0.2807 | dt 338.42ms | 1549200.33 tokens/sec
Step 9138 | loss: 3.297526 | lr:3.6483e-04 | norm 0.3016 | dt 338.10ms | 1550710.12 tokens/sec
Step 9139 | loss: 3.209602 | lr:3.6479e-04 | norm 0.2937 | dt 338.01ms | 1551083.11 tokens/sec
Step 9140 | loss: 3.240255 | lr:3.6474e-04 | norm 0.2913 | dt 338.05ms | 1550931.05 tokens/sec
Step 9141 | loss: 3.199228 | lr:3.6470e-04 | norm 0.2845 | dt 338.01ms | 1551119.21 tokens/sec
Step 9142 | loss: 3.223615 | lr:3.6465e-04 | norm 0.3054 | dt 338.39ms | 1549356.42 tokens/sec
Step 9143 | loss: 3.179812 | lr:3.6460e-04 | norm 0.2747 | dt 338.04ms | 1550974.80 tokens/sec
Step 9144 | loss: 3.209461 | lr:3.6456e-04 | norm 0.3048 | dt 338.53ms | 1548720.26 tokens/sec
Step 9145 | loss: 3.226723 | lr:3.6451e-04 | norm 0.2692 | dt 338.27ms | 1549924.27 tokens/sec
Step 9146 | loss: 3.241006 | lr:3.6447e-04 | norm 0.2829 | dt 338.03ms | 1551026.22 tokens/sec
Step 9147 | loss: 3.254412 | lr:3.6442e-04 | norm 0.3154 | dt 338.54ms | 1548688.63 tokens/sec
Step 9148 | loss: 3.191860 | lr:3.6438e-04 | norm 0.2913 | dt 337.75ms | 1552296.27 tokens/sec
Step 9149 | loss: 3.224291 | lr:3.6433e-04 | norm 0.2745 | dt 338.17ms | 1550373.39 tokens/sec
Step 9150 | loss: 3.249123 | lr:3.6428e-04 | norm 0.3065 | dt 337.63ms | 1552829.00 tokens/sec
Step 9151 | loss: 3.182314 | lr:3.6424e-04 | norm 0.2609 | dt 337.91ms | 1551560.26 tokens/sec
Step 9152 | loss: 3.182830 | lr:3.6419e-04 | norm 0.2899 | dt 338.15ms | 1550466.30 tokens/sec
Step 9153 | loss: 3.223468 | lr:3.6415e-04 | norm 0.2794 | dt 337.80ms | 1552055.24 tokens/sec
Step 9154 | loss: 3.238468 | lr:3.6410e-04 | norm 0.2715 | dt 338.30ms | 1549754.96 tokens/sec
Step 9155 | loss: 3.154274 | lr:3.6405e-04 | norm 0.2726 | dt 337.80ms | 1552081.53 tokens/sec
Step 9156 | loss: 3.208225 | lr:3.6401e-04 | norm 0.2968 | dt 337.48ms | 1553516.82 tokens/sec
Step 9157 | loss: 3.208951 | lr:3.6396e-04 | norm 0.2718 | dt 338.01ms | 1551108.27 tokens/sec
Step 9158 | loss: 3.186928 | lr:3.6392e-04 | norm 0.2807 | dt 338.53ms | 1548721.35 tokens/sec
Step 9159 | loss: 3.213321 | lr:3.6387e-04 | norm 0.2746 | dt 341.34ms | 1535986.87 tokens/sec
Step 9160 | loss: 3.172338 | lr:3.6383e-04 | norm 0.2700 | dt 337.45ms | 1553693.54 tokens/sec
Step 9161 | loss: 3.214054 | lr:3.6378e-04 | norm 0.2974 | dt 339.11ms | 1546053.67 tokens/sec
Step 9162 | loss: 3.217455 | lr:3.6373e-04 | norm 0.2668 | dt 338.77ms | 1547640.11 tokens/sec
Step 9163 | loss: 3.247600 | lr:3.6369e-04 | norm 0.2845 | dt 339.04ms | 1546368.96 tokens/sec
Step 9164 | loss: 3.255696 | lr:3.6364e-04 | norm 0.3007 | dt 339.77ms | 1543073.51 tokens/sec
Step 9165 | loss: 3.257694 | lr:3.6360e-04 | norm 0.2783 | dt 338.42ms | 1549240.71 tokens/sec
Step 9166 | loss: 3.190843 | lr:3.6355e-04 | norm 0.2815 | dt 338.64ms | 1548199.07 tokens/sec
Step 9167 | loss: 3.209437 | lr:3.6350e-04 | norm 0.2931 | dt 338.47ms | 1549001.72 tokens/sec
Step 9168 | loss: 3.197020 | lr:3.6346e-04 | norm 0.2843 | dt 338.64ms | 1548201.25 tokens/sec
Step 9169 | loss: 3.228146 | lr:3.6341e-04 | norm 0.2944 | dt 339.31ms | 1545163.95 tokens/sec
Step 9170 | loss: 3.191280 | lr:3.6337e-04 | norm 0.2875 | dt 338.92ms | 1546944.42 tokens/sec
Step 9171 | loss: 3.209528 | lr:3.6332e-04 | norm 0.2696 | dt 337.85ms | 1551837.28 tokens/sec
Step 9172 | loss: 3.137909 | lr:3.6328e-04 | norm 0.2895 | dt 338.37ms | 1549430.65 tokens/sec
Step 9173 | loss: 3.209237 | lr:3.6323e-04 | norm 0.2710 | dt 337.99ms | 1551185.95 tokens/sec
Step 9174 | loss: 3.172643 | lr:3.6318e-04 | norm 0.2664 | dt 339.08ms | 1546217.82 tokens/sec
Step 9175 | loss: 3.244019 | lr:3.6314e-04 | norm 0.2848 | dt 337.86ms | 1551812.09 tokens/sec
Step 9176 | loss: 3.277126 | lr:3.6309e-04 | norm 0.2703 | dt 338.19ms | 1550261.90 tokens/sec
Step 9177 | loss: 3.286939 | lr:3.6305e-04 | norm 0.3162 | dt 338.53ms | 1548713.72 tokens/sec
Step 9178 | loss: 3.157749 | lr:3.6300e-04 | norm 0.2709 | dt 338.13ms | 1550554.86 tokens/sec
Step 9179 | loss: 3.197672 | lr:3.6295e-04 | norm 0.2855 | dt 337.95ms | 1551374.18 tokens/sec
Step 9180 | loss: 3.207950 | lr:3.6291e-04 | norm 0.2744 | dt 338.29ms | 1549797.56 tokens/sec
Step 9181 | loss: 3.262465 | lr:3.6286e-04 | norm 0.3854 | dt 337.84ms | 1551869.04 tokens/sec
Step 9182 | loss: 3.218505 | lr:3.6282e-04 | norm 0.3128 | dt 337.80ms | 1552078.24 tokens/sec
Step 9183 | loss: 3.229037 | lr:3.6277e-04 | norm 0.3135 | dt 338.13ms | 1550547.20 tokens/sec
Step 9184 | loss: 3.243443 | lr:3.6272e-04 | norm 0.3061 | dt 337.94ms | 1551421.24 tokens/sec
Step 9185 | loss: 3.264604 | lr:3.6268e-04 | norm 0.2900 | dt 338.00ms | 1551159.69 tokens/sec
Step 9186 | loss: 3.188356 | lr:3.6263e-04 | norm 0.2939 | dt 337.85ms | 1551840.56 tokens/sec
Step 9187 | loss: 3.203953 | lr:3.6259e-04 | norm 0.2728 | dt 338.18ms | 1550340.60 tokens/sec
Step 9188 | loss: 3.265240 | lr:3.6254e-04 | norm 0.2870 | dt 337.82ms | 1551975.28 tokens/sec
Step 9189 | loss: 3.159930 | lr:3.6250e-04 | norm 0.2751 | dt 338.31ms | 1549725.47 tokens/sec
Step 9190 | loss: 3.218577 | lr:3.6245e-04 | norm 0.2892 | dt 338.06ms | 1550866.51 tokens/sec
Step 9191 | loss: 3.226021 | lr:3.6240e-04 | norm 0.2925 | dt 337.33ms | 1554242.60 tokens/sec
Step 9192 | loss: 3.182996 | lr:3.6236e-04 | norm 0.2625 | dt 338.85ms | 1547244.83 tokens/sec
Step 9193 | loss: 3.206313 | lr:3.6231e-04 | norm 0.2990 | dt 337.64ms | 1552807.07 tokens/sec
Step 9194 | loss: 3.373993 | lr:3.6227e-04 | norm 0.3250 | dt 337.27ms | 1554511.79 tokens/sec
Step 9195 | loss: 3.218865 | lr:3.6222e-04 | norm 0.3681 | dt 338.23ms | 1550095.80 tokens/sec
Step 9196 | loss: 3.225949 | lr:3.6217e-04 | norm 0.3046 | dt 337.67ms | 1552659.05 tokens/sec
Step 9197 | loss: 3.197105 | lr:3.6213e-04 | norm 0.3091 | dt 337.42ms | 1553792.34 tokens/sec
Step 9198 | loss: 3.289083 | lr:3.6208e-04 | norm 0.3515 | dt 338.63ms | 1548259.02 tokens/sec
Step 9199 | loss: 3.172674 | lr:3.6204e-04 | norm 0.3084 | dt 337.75ms | 1552318.19 tokens/sec
Step 9200 | loss: 3.205267 | lr:3.6199e-04 | norm 0.3240 | dt 337.18ms | 1554919.59 tokens/sec
Step 9201 | loss: 3.184978 | lr:3.6195e-04 | norm 0.3178 | dt 338.27ms | 1549907.88 tokens/sec
Step 9202 | loss: 3.195245 | lr:3.6190e-04 | norm 0.3025 | dt 337.87ms | 1551758.43 tokens/sec
Step 9203 | loss: 3.225478 | lr:3.6185e-04 | norm 0.2900 | dt 339.84ms | 1542754.15 tokens/sec
Step 9204 | loss: 3.172575 | lr:3.6181e-04 | norm 0.3220 | dt 338.08ms | 1550758.24 tokens/sec
Step 9205 | loss: 3.255674 | lr:3.6176e-04 | norm 0.3060 | dt 337.81ms | 1552022.38 tokens/sec
Step 9206 | loss: 3.177068 | lr:3.6172e-04 | norm 0.2931 | dt 337.79ms | 1552094.67 tokens/sec
Step 9207 | loss: 3.214728 | lr:3.6167e-04 | norm 0.2771 | dt 338.55ms | 1548611.20 tokens/sec
Step 9208 | loss: 3.179404 | lr:3.6162e-04 | norm 0.2750 | dt 337.62ms | 1552915.63 tokens/sec
Step 9209 | loss: 3.251748 | lr:3.6158e-04 | norm 0.2832 | dt 337.49ms | 1553479.51 tokens/sec
Step 9210 | loss: 3.193338 | lr:3.6153e-04 | norm 0.2869 | dt 337.63ms | 1552830.09 tokens/sec
Step 9211 | loss: 3.171365 | lr:3.6149e-04 | norm 0.2956 | dt 340.97ms | 1537649.47 tokens/sec
Step 9212 | loss: 3.216666 | lr:3.6144e-04 | norm 0.2846 | dt 337.60ms | 1553007.75 tokens/sec
Step 9213 | loss: 3.231154 | lr:3.6139e-04 | norm 0.2901 | dt 338.10ms | 1550703.56 tokens/sec
Step 9214 | loss: 3.233661 | lr:3.6135e-04 | norm 0.3137 | dt 338.35ms | 1549545.29 tokens/sec
Step 9215 | loss: 3.227768 | lr:3.6130e-04 | norm 0.2731 | dt 338.00ms | 1551146.56 tokens/sec
Step 9216 | loss: 3.216038 | lr:3.6126e-04 | norm 0.3024 | dt 338.76ms | 1547666.25 tokens/sec
Step 9217 | loss: 3.166378 | lr:3.6121e-04 | norm 0.2699 | dt 338.07ms | 1550818.39 tokens/sec
Step 9218 | loss: 3.210349 | lr:3.6116e-04 | norm 0.2762 | dt 338.71ms | 1547895.02 tokens/sec
Step 9219 | loss: 3.197307 | lr:3.6112e-04 | norm 0.2703 | dt 338.74ms | 1547749.03 tokens/sec
Step 9220 | loss: 3.214040 | lr:3.6107e-04 | norm 0.2756 | dt 338.63ms | 1548240.49 tokens/sec
Step 9221 | loss: 3.141116 | lr:3.6103e-04 | norm 0.2678 | dt 338.55ms | 1548628.65 tokens/sec
Step 9222 | loss: 3.234666 | lr:3.6098e-04 | norm 0.2706 | dt 338.30ms | 1549793.19 tokens/sec
Step 9223 | loss: 3.171340 | lr:3.6094e-04 | norm 0.2985 | dt 338.13ms | 1550555.95 tokens/sec
Step 9224 | loss: 3.223232 | lr:3.6089e-04 | norm 0.3053 | dt 339.01ms | 1546526.65 tokens/sec
Step 9225 | loss: 3.217901 | lr:3.6084e-04 | norm 0.2723 | dt 338.36ms | 1549515.81 tokens/sec
Step 9226 | loss: 3.170890 | lr:3.6080e-04 | norm 0.2960 | dt 338.18ms | 1550301.25 tokens/sec
Step 9227 | loss: 3.192364 | lr:3.6075e-04 | norm 0.2857 | dt 337.93ms | 1551482.54 tokens/sec
Step 9228 | loss: 3.217621 | lr:3.6071e-04 | norm 0.2948 | dt 338.82ms | 1547396.16 tokens/sec
Step 9229 | loss: 3.243422 | lr:3.6066e-04 | norm 0.2688 | dt 338.78ms | 1547554.06 tokens/sec
Step 9230 | loss: 3.211809 | lr:3.6061e-04 | norm 0.2740 | dt 339.14ms | 1545926.51 tokens/sec
Step 9231 | loss: 3.244498 | lr:3.6057e-04 | norm 0.2701 | dt 338.48ms | 1548946.08 tokens/sec
Step 9232 | loss: 3.188982 | lr:3.6052e-04 | norm 0.2712 | dt 338.78ms | 1547569.31 tokens/sec
Step 9233 | loss: 3.242373 | lr:3.6048e-04 | norm 0.2510 | dt 337.95ms | 1551393.88 tokens/sec
Step 9234 | loss: 3.139830 | lr:3.6043e-04 | norm 0.2742 | dt 337.66ms | 1552725.93 tokens/sec
Step 9235 | loss: 3.218458 | lr:3.6038e-04 | norm 0.2625 | dt 338.39ms | 1549374.98 tokens/sec
Step 9236 | loss: 3.151711 | lr:3.6034e-04 | norm 0.2653 | dt 338.26ms | 1549948.30 tokens/sec
Step 9237 | loss: 3.214578 | lr:3.6029e-04 | norm 0.2661 | dt 338.20ms | 1550208.35 tokens/sec
Step 9238 | loss: 3.167313 | lr:3.6025e-04 | norm 0.2786 | dt 338.10ms | 1550693.72 tokens/sec
Step 9239 | loss: 3.219974 | lr:3.6020e-04 | norm 0.3030 | dt 337.93ms | 1551458.46 tokens/sec
Step 9240 | loss: 3.244614 | lr:3.6015e-04 | norm 0.2881 | dt 338.47ms | 1549014.82 tokens/sec
Step 9241 | loss: 3.101022 | lr:3.6011e-04 | norm 0.3430 | dt 337.82ms | 1551959.94 tokens/sec
Step 9242 | loss: 3.256228 | lr:3.6006e-04 | norm 0.3378 | dt 337.57ms | 1553128.41 tokens/sec
Step 9243 | loss: 3.214641 | lr:3.6002e-04 | norm 0.2968 | dt 337.73ms | 1552377.36 tokens/sec
Step 9244 | loss: 3.234949 | lr:3.5997e-04 | norm 0.2841 | dt 338.74ms | 1547767.55 tokens/sec
Step 9245 | loss: 3.227673 | lr:3.5993e-04 | norm 0.2951 | dt 337.58ms | 1553080.14 tokens/sec
Step 9246 | loss: 3.221567 | lr:3.5988e-04 | norm 0.2721 | dt 337.61ms | 1552940.85 tokens/sec
Step 9247 | loss: 3.175037 | lr:3.5983e-04 | norm 0.2950 | dt 338.33ms | 1549634.83 tokens/sec
Step 9248 | loss: 3.222785 | lr:3.5979e-04 | norm 0.2957 | dt 338.58ms | 1548489.06 tokens/sec
Step 9249 | loss: 3.192091 | lr:3.5974e-04 | norm 0.3152 | dt 337.52ms | 1553336.85 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 9250: 3.2175
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2910/10042=0.2898


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not interested if you guys go out there and call it different characters and try to give everyone the same syntax
rank 3 sample 1 >Hello, I'm a language model, so you know what I'm supposed to be doing. But I think I'm going to be able to do just what
rank 3 sample 2 >Hello, I'm a language model, so this post is aimed at introducing you to another language model in the same sentence.
What will I learn this lesson
rank 3 sample 3 >Hello, I'm a language model, so the first two bits are like the first bits and last two are called the “bits” bits. The




ddp_rank 5: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, and if it's useful, I'd write it a lot more correctly and in a lot more accurate. So this could
rank 2 sample 0 >Hello, I'm a language model, yet you're so confused. I am not going to teach you how to do an array. You're supposed to start
rank 5 sample 1 >Hello, I'm a language model, now this is just fun... and so is going to get more practice with the more advanced concepts. You've just got
rank 2 sample 1 >Hello, I'm a language model, and I find this to be a particularly weird thing to do with language in general.
- I was just wondering about
rank 5 sample 2 >Hello, I'm a language model, but it might not use the old adage of "we're good at using it"; it's not really good if
rank 2 sample 2 >Hello, I'm a language model, but I can't really explain what "sounds" mean. The best answer for me, which I've gotten at
rank 5 sample 3 >Hello, I'm a language model, an interface designer who is using a language model. In a language model, all components can interact so they can be translated


rank 2 sample 3 >Hello, I'm a language model, and that you're gonna use to teach languages. When I think about it, I'd get all of that out of




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I am going to start with some exercises. But there are some tricks I hope will help me to get started with
rank 1 sample 0 >Hello, I'm a language model, and this is my only way to actually get there! :-)<|endoftext|>The term "Tensile Serum" is used
rank 7 sample 1 >Hello, I'm a language model, someone didn't know that language! "And he has spoken it so well," when I heard him pronounce it "H
rank 1 sample 1 >Hello, I'm a language model, not an artist. I'm going to say that there could be things that the programmer of this writing could do that would
rank 7 sample 2 >Hello, I'm a language model, and I'll probably like to explain its value. :)
- In Python, you can use the C library as your
rank 1 sample 2 >Hello, I'm a language model, and, if you're interested in learning a language, this will give a good overview of what is being done and how
rank 7 sample 3 >Hello, I'm a language model, but it's not easy to put me around that. I started thinking about how to make a basic app, and have




ddp_rank 6: ####### Printing generated samples ####### 

rank 1 sample 3 >Hello, I'm a language model, so I'm interested to run this app all the time ... :)
Now with the next line:
There are a


rank 6 sample 0 >Hello, I'm a language model, that doesn't give it everything, that doesn't help it, but I do a lot of things in the language environment
rank 6 sample 1 >Hello, I'm a language model, which is what I do with all of C. But I'm not even a programmer at the moment, just a researcher
rank 6 sample 2 >Hello, I'm a language model, and this is the easiest language I've ever run. I can learn to be a writer, but I think of it

rank 6 sample 3 >Hello, I'm a language model, so the goal of this post is to create a language model for one of those language families at the beginning stages of reading



ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, but I have a new question. I hope it will help me. This is a way to take away your anger and
rank 4 sample 1 >Hello, I'm a language model, trying to make decisions. I was very upset. But no one could agree that language teaching was a better choice.

rank 4 sample 2 >Hello, I'm a language model, I never understand or appreciate anything. The language model is all there.
I feel this is not bad. When you
rank 4 sample 3 >Hello, I'm a language model, so it starts on the second line without saying the number. How to fix it?
I was thinking about doing the




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I want to know which is how? Right, I'm really asking my students. (They don't know which
rank 0 sample 1 >Hello, I'm a language model, and for the first time I have asked you what happens when a user comes in and tells you that something has happened.
rank 0 sample 2 >Hello, I'm a language model, but I wanted to look through it all. How do I find an answer to my problem?
I'm going to
rank 0 sample 3 >Hello, I'm a language model, and for the first time I've made the real world connection between grammar and the process of producing the next sentence in time


Step 9250 | loss: 3.204040 | lr:3.5970e-04 | norm 0.3213 | dt 12421.66ms | 42207.57 tokens/sec
Step 9251 | loss: 3.193809 | lr:3.5965e-04 | norm 0.2972 | dt 335.96ms | 1560579.33 tokens/sec
Step 9252 | loss: 3.228230 | lr:3.5960e-04 | norm 0.2886 | dt 336.56ms | 1557797.84 tokens/sec
Step 9253 | loss: 3.225427 | lr:3.5956e-04 | norm 0.2804 | dt 335.91ms | 1560796.43 tokens/sec
Step 9254 | loss: 3.190560 | lr:3.5951e-04 | norm 0.2858 | dt 336.06ms | 1560109.89 tokens/sec
Step 9255 | loss: 3.165756 | lr:3.5947e-04 | norm 0.2874 | dt 336.59ms | 1557654.39 tokens/sec
Step 9256 | loss: 3.195942 | lr:3.5942e-04 | norm 0.2898 | dt 335.54ms | 1562532.07 tokens/sec
Step 9257 | loss: 3.213152 | lr:3.5937e-04 | norm 0.2546 | dt 336.84ms | 1556476.90 tokens/sec
Step 9258 | loss: 3.234769 | lr:3.5933e-04 | norm 0.3045 | dt 335.94ms | 1560667.93 tokens/sec
Step 9259 | loss: 3.200584 | lr:3.5928e-04 | norm 0.2528 | dt 335.90ms | 1560830.77 tokens/sec
Step 9260 | loss: 3.230442 | lr:3.5924e-04 | norm 0.2838 | dt 896.57ms | 584772.11 tokens/sec
Step 9261 | loss: 3.196345 | lr:3.5919e-04 | norm 0.2620 | dt 334.66ms | 1566639.73 tokens/sec
Step 9262 | loss: 3.207004 | lr:3.5914e-04 | norm 0.2644 | dt 336.20ms | 1559472.62 tokens/sec
Step 9263 | loss: 3.170263 | lr:3.5910e-04 | norm 0.2724 | dt 337.93ms | 1551484.73 tokens/sec
Step 9264 | loss: 3.217792 | lr:3.5905e-04 | norm 0.2584 | dt 336.16ms | 1559646.27 tokens/sec
Step 9265 | loss: 3.189814 | lr:3.5901e-04 | norm 0.2712 | dt 336.09ms | 1559942.78 tokens/sec
Step 9266 | loss: 3.194410 | lr:3.5896e-04 | norm 0.2704 | dt 337.15ms | 1555063.63 tokens/sec
Step 9267 | loss: 3.215287 | lr:3.5891e-04 | norm 0.2922 | dt 336.47ms | 1558195.22 tokens/sec
Step 9268 | loss: 3.219367 | lr:3.5887e-04 | norm 0.2652 | dt 337.43ms | 1553790.14 tokens/sec
Step 9269 | loss: 3.175285 | lr:3.5882e-04 | norm 0.2622 | dt 337.31ms | 1554339.28 tokens/sec
Step 9270 | loss: 3.230074 | lr:3.5878e-04 | norm 0.2949 | dt 335.94ms | 1560679.01 tokens/sec
Step 9271 | loss: 3.196840 | lr:3.5873e-04 | norm 0.2636 | dt 336.85ms | 1556420.72 tokens/sec
Step 9272 | loss: 3.182747 | lr:3.5869e-04 | norm 0.2624 | dt 337.81ms | 1552013.61 tokens/sec
Step 9273 | loss: 3.208649 | lr:3.5864e-04 | norm 0.2733 | dt 336.16ms | 1559638.52 tokens/sec
Step 9274 | loss: 3.244475 | lr:3.5859e-04 | norm 0.2738 | dt 337.20ms | 1554826.14 tokens/sec
Step 9275 | loss: 3.159990 | lr:3.5855e-04 | norm 0.2722 | dt 337.52ms | 1553366.48 tokens/sec
Step 9276 | loss: 3.226082 | lr:3.5850e-04 | norm 0.2880 | dt 337.41ms | 1553872.49 tokens/sec
Step 9277 | loss: 3.259066 | lr:3.5846e-04 | norm 0.3007 | dt 337.16ms | 1554994.35 tokens/sec
Step 9278 | loss: 3.229323 | lr:3.5841e-04 | norm 0.2693 | dt 338.06ms | 1550887.29 tokens/sec
Step 9279 | loss: 3.190622 | lr:3.5836e-04 | norm 0.2844 | dt 337.49ms | 1553506.94 tokens/sec
Step 9280 | loss: 3.252104 | lr:3.5832e-04 | norm 0.2785 | dt 337.47ms | 1553588.16 tokens/sec
Step 9281 | loss: 3.199847 | lr:3.5827e-04 | norm 0.2744 | dt 337.78ms | 1552176.84 tokens/sec
Step 9282 | loss: 3.207072 | lr:3.5823e-04 | norm 0.2749 | dt 338.14ms | 1550527.52 tokens/sec
Step 9283 | loss: 3.207873 | lr:3.5818e-04 | norm 0.2647 | dt 337.92ms | 1551520.85 tokens/sec
Step 9284 | loss: 3.281657 | lr:3.5813e-04 | norm 0.2976 | dt 337.53ms | 1553296.26 tokens/sec
Step 9285 | loss: 3.185398 | lr:3.5809e-04 | norm 0.2808 | dt 338.76ms | 1547649.91 tokens/sec
Step 9286 | loss: 3.382068 | lr:3.5804e-04 | norm 0.3345 | dt 337.99ms | 1551201.27 tokens/sec
Step 9287 | loss: 3.208400 | lr:3.5800e-04 | norm 0.3338 | dt 338.31ms | 1549717.83 tokens/sec
Step 9288 | loss: 3.191244 | lr:3.5795e-04 | norm 0.2885 | dt 337.91ms | 1551551.50 tokens/sec
Step 9289 | loss: 3.200521 | lr:3.5790e-04 | norm 0.3282 | dt 337.95ms | 1551374.18 tokens/sec
Step 9290 | loss: 3.241712 | lr:3.5786e-04 | norm 0.3113 | dt 338.36ms | 1549507.08 tokens/sec
Step 9291 | loss: 3.182034 | lr:3.5781e-04 | norm 0.2694 | dt 338.50ms | 1548852.25 tokens/sec
Step 9292 | loss: 3.276675 | lr:3.5777e-04 | norm 0.2861 | dt 337.98ms | 1551248.33 tokens/sec
Step 9293 | loss: 3.300911 | lr:3.5772e-04 | norm 0.2686 | dt 338.13ms | 1550541.74 tokens/sec
Step 9294 | loss: 3.245795 | lr:3.5767e-04 | norm 0.2586 | dt 338.37ms | 1549451.40 tokens/sec
Step 9295 | loss: 3.257931 | lr:3.5763e-04 | norm 0.2700 | dt 337.69ms | 1552565.88 tokens/sec
Step 9296 | loss: 3.215097 | lr:3.5758e-04 | norm 0.2599 | dt 337.98ms | 1551226.44 tokens/sec
Step 9297 | loss: 3.199136 | lr:3.5754e-04 | norm 0.2672 | dt 337.71ms | 1552465.04 tokens/sec
Step 9298 | loss: 3.200170 | lr:3.5749e-04 | norm 0.2778 | dt 338.44ms | 1549144.67 tokens/sec
Step 9299 | loss: 3.245800 | lr:3.5744e-04 | norm 0.2598 | dt 337.58ms | 1553096.59 tokens/sec
Step 9300 | loss: 3.236921 | lr:3.5740e-04 | norm 0.2734 | dt 337.87ms | 1551734.34 tokens/sec
Step 9301 | loss: 3.149181 | lr:3.5735e-04 | norm 0.2615 | dt 338.13ms | 1550536.27 tokens/sec
Step 9302 | loss: 3.169923 | lr:3.5731e-04 | norm 0.2985 | dt 339.04ms | 1546397.23 tokens/sec
Step 9303 | loss: 3.213854 | lr:3.5726e-04 | norm 0.2598 | dt 338.82ms | 1547402.70 tokens/sec
Step 9304 | loss: 3.221747 | lr:3.5721e-04 | norm 0.2750 | dt 338.95ms | 1546805.14 tokens/sec
Step 9305 | loss: 3.190936 | lr:3.5717e-04 | norm 0.2918 | dt 338.76ms | 1547658.62 tokens/sec
Step 9306 | loss: 3.175363 | lr:3.5712e-04 | norm 0.2718 | dt 338.40ms | 1549295.29 tokens/sec
Step 9307 | loss: 3.192805 | lr:3.5708e-04 | norm 0.2631 | dt 338.44ms | 1549150.13 tokens/sec
Step 9308 | loss: 3.202374 | lr:3.5703e-04 | norm 0.2873 | dt 337.70ms | 1552522.03 tokens/sec
Step 9309 | loss: 3.221429 | lr:3.5698e-04 | norm 0.2614 | dt 1036.82ms | 505671.15 tokens/sec
Step 9310 | loss: 3.205547 | lr:3.5694e-04 | norm 0.2865 | dt 337.81ms | 1552017.99 tokens/sec
Step 9311 | loss: 3.247899 | lr:3.5689e-04 | norm 0.2863 | dt 338.34ms | 1549567.13 tokens/sec
Step 9312 | loss: 3.298267 | lr:3.5685e-04 | norm 0.2888 | dt 337.88ms | 1551711.35 tokens/sec
Step 9313 | loss: 3.194445 | lr:3.5680e-04 | norm 0.2746 | dt 339.04ms | 1546380.92 tokens/sec
Step 9314 | loss: 3.181589 | lr:3.5675e-04 | norm 0.2780 | dt 338.06ms | 1550894.95 tokens/sec
Step 9315 | loss: 3.223635 | lr:3.5671e-04 | norm 0.2968 | dt 337.80ms | 1552060.72 tokens/sec
Step 9316 | loss: 3.194910 | lr:3.5666e-04 | norm 0.2828 | dt 338.53ms | 1548725.72 tokens/sec
Step 9317 | loss: 3.245874 | lr:3.5662e-04 | norm 0.2839 | dt 338.49ms | 1548904.62 tokens/sec
Step 9318 | loss: 3.207748 | lr:3.5657e-04 | norm 0.2677 | dt 339.29ms | 1545264.93 tokens/sec
Step 9319 | loss: 3.138302 | lr:3.5652e-04 | norm 0.2725 | dt 338.01ms | 1551122.49 tokens/sec
Step 9320 | loss: 3.172777 | lr:3.5648e-04 | norm 0.2754 | dt 338.39ms | 1549366.24 tokens/sec
Step 9321 | loss: 3.227679 | lr:3.5643e-04 | norm 0.2741 | dt 338.91ms | 1546996.65 tokens/sec
Step 9322 | loss: 3.184485 | lr:3.5639e-04 | norm 0.3016 | dt 338.90ms | 1547038.01 tokens/sec
Step 9323 | loss: 3.235805 | lr:3.5634e-04 | norm 0.2716 | dt 338.49ms | 1548892.62 tokens/sec
Step 9324 | loss: 3.242443 | lr:3.5630e-04 | norm 0.2606 | dt 338.87ms | 1547180.60 tokens/sec
Step 9325 | loss: 3.281135 | lr:3.5625e-04 | norm 0.2599 | dt 338.04ms | 1550975.90 tokens/sec
Step 9326 | loss: 3.293599 | lr:3.5620e-04 | norm 0.2929 | dt 339.02ms | 1546469.01 tokens/sec
Step 9327 | loss: 3.283964 | lr:3.5616e-04 | norm 0.2884 | dt 337.63ms | 1552861.89 tokens/sec
Step 9328 | loss: 3.206891 | lr:3.5611e-04 | norm 0.2938 | dt 337.60ms | 1552981.43 tokens/sec
Step 9329 | loss: 3.227511 | lr:3.5607e-04 | norm 0.2971 | dt 337.69ms | 1552549.43 tokens/sec
Step 9330 | loss: 3.219676 | lr:3.5602e-04 | norm 0.2660 | dt 338.15ms | 1550460.84 tokens/sec
Step 9331 | loss: 3.260950 | lr:3.5597e-04 | norm 0.2720 | dt 337.60ms | 1552988.01 tokens/sec
Step 9332 | loss: 3.221098 | lr:3.5593e-04 | norm 0.2863 | dt 338.40ms | 1549314.94 tokens/sec
Step 9333 | loss: 3.173180 | lr:3.5588e-04 | norm 0.2564 | dt 338.29ms | 1549807.39 tokens/sec
Step 9334 | loss: 3.203314 | lr:3.5584e-04 | norm 0.2721 | dt 337.82ms | 1551957.75 tokens/sec
Step 9335 | loss: 3.198207 | lr:3.5579e-04 | norm 0.2765 | dt 337.49ms | 1553505.85 tokens/sec
Step 9336 | loss: 3.246360 | lr:3.5574e-04 | norm 0.3368 | dt 338.33ms | 1549643.57 tokens/sec
Step 9337 | loss: 3.168699 | lr:3.5570e-04 | norm 0.2948 | dt 337.48ms | 1553559.62 tokens/sec
Step 9338 | loss: 3.148673 | lr:3.5565e-04 | norm 0.2918 | dt 339.53ms | 1544146.21 tokens/sec
Step 9339 | loss: 3.191467 | lr:3.5561e-04 | norm 0.2956 | dt 337.41ms | 1553860.41 tokens/sec
Step 9340 | loss: 3.256857 | lr:3.5556e-04 | norm 0.3142 | dt 337.55ms | 1553208.49 tokens/sec
Step 9341 | loss: 3.170438 | lr:3.5551e-04 | norm 0.3051 | dt 337.71ms | 1552457.36 tokens/sec
Step 9342 | loss: 3.169542 | lr:3.5547e-04 | norm 0.2870 | dt 341.55ms | 1535010.12 tokens/sec
Step 9343 | loss: 3.222797 | lr:3.5542e-04 | norm 0.3423 | dt 338.80ms | 1547483.28 tokens/sec
Step 9344 | loss: 3.177020 | lr:3.5538e-04 | norm 0.3181 | dt 337.90ms | 1551629.23 tokens/sec
Step 9345 | loss: 3.202133 | lr:3.5533e-04 | norm 0.3332 | dt 337.78ms | 1552173.55 tokens/sec
Step 9346 | loss: 3.220093 | lr:3.5528e-04 | norm 0.2688 | dt 339.37ms | 1544892.57 tokens/sec
Step 9347 | loss: 3.255204 | lr:3.5524e-04 | norm 0.3074 | dt 338.07ms | 1550816.20 tokens/sec
Step 9348 | loss: 3.237323 | lr:3.5519e-04 | norm 0.3035 | dt 337.80ms | 1552051.95 tokens/sec
Step 9349 | loss: 3.175576 | lr:3.5515e-04 | norm 0.2934 | dt 338.79ms | 1547533.37 tokens/sec
Step 9350 | loss: 3.204976 | lr:3.5510e-04 | norm 0.2886 | dt 337.53ms | 1553322.59 tokens/sec
Step 9351 | loss: 3.178731 | lr:3.5505e-04 | norm 0.3026 | dt 338.09ms | 1550738.55 tokens/sec
Step 9352 | loss: 3.222723 | lr:3.5501e-04 | norm 0.2791 | dt 338.34ms | 1549584.60 tokens/sec
Step 9353 | loss: 3.214930 | lr:3.5496e-04 | norm 0.2682 | dt 337.81ms | 1551999.37 tokens/sec
Step 9354 | loss: 3.226220 | lr:3.5492e-04 | norm 0.2798 | dt 338.31ms | 1549737.49 tokens/sec
Step 9355 | loss: 3.211929 | lr:3.5487e-04 | norm 0.2772 | dt 338.77ms | 1547632.48 tokens/sec
Step 9356 | loss: 3.192053 | lr:3.5482e-04 | norm 0.2549 | dt 338.03ms | 1551013.09 tokens/sec
Step 9357 | loss: 3.189928 | lr:3.5478e-04 | norm 0.2665 | dt 338.68ms | 1548020.33 tokens/sec
Step 9358 | loss: 3.396424 | lr:3.5473e-04 | norm 0.3318 | dt 337.82ms | 1551974.18 tokens/sec
Step 9359 | loss: 3.225572 | lr:3.5469e-04 | norm 0.2928 | dt 338.45ms | 1549078.10 tokens/sec
Step 9360 | loss: 3.266976 | lr:3.5464e-04 | norm 0.2816 | dt 338.76ms | 1547658.62 tokens/sec
Step 9361 | loss: 3.203919 | lr:3.5459e-04 | norm 0.2658 | dt 337.49ms | 1553471.82 tokens/sec
Step 9362 | loss: 3.184327 | lr:3.5455e-04 | norm 0.2835 | dt 337.59ms | 1553053.82 tokens/sec
Step 9363 | loss: 3.171122 | lr:3.5450e-04 | norm 0.2807 | dt 338.83ms | 1547334.10 tokens/sec
Step 9364 | loss: 3.231665 | lr:3.5446e-04 | norm 0.2770 | dt 337.66ms | 1552720.45 tokens/sec
Step 9365 | loss: 3.184265 | lr:3.5441e-04 | norm 0.2566 | dt 338.04ms | 1550948.55 tokens/sec
Step 9366 | loss: 3.236339 | lr:3.5436e-04 | norm 0.2672 | dt 338.05ms | 1550906.98 tokens/sec
Step 9367 | loss: 3.184480 | lr:3.5432e-04 | norm 0.2697 | dt 337.86ms | 1551795.66 tokens/sec
Step 9368 | loss: 3.211683 | lr:3.5427e-04 | norm 0.2841 | dt 337.58ms | 1553061.49 tokens/sec
Step 9369 | loss: 3.221652 | lr:3.5422e-04 | norm 0.3009 | dt 338.67ms | 1548084.63 tokens/sec
Step 9370 | loss: 3.216064 | lr:3.5418e-04 | norm 0.3163 | dt 337.63ms | 1552869.57 tokens/sec
Step 9371 | loss: 3.217797 | lr:3.5413e-04 | norm 0.2794 | dt 337.87ms | 1551742.01 tokens/sec
Step 9372 | loss: 3.221912 | lr:3.5409e-04 | norm 0.2918 | dt 337.79ms | 1552110.01 tokens/sec
Step 9373 | loss: 3.207825 | lr:3.5404e-04 | norm 0.2804 | dt 338.43ms | 1549155.59 tokens/sec
Step 9374 | loss: 3.218936 | lr:3.5399e-04 | norm 0.2706 | dt 338.51ms | 1548803.16 tokens/sec
Step 9375 | loss: 3.281922 | lr:3.5395e-04 | norm 0.3045 | dt 338.73ms | 1547791.52 tokens/sec
Step 9376 | loss: 3.227554 | lr:3.5390e-04 | norm 0.2947 | dt 338.55ms | 1548645.01 tokens/sec
Step 9377 | loss: 3.335020 | lr:3.5386e-04 | norm 0.2938 | dt 338.50ms | 1548875.16 tokens/sec
Step 9378 | loss: 3.240319 | lr:3.5381e-04 | norm 0.3460 | dt 339.13ms | 1545979.76 tokens/sec
Step 9379 | loss: 3.194838 | lr:3.5376e-04 | norm 0.2906 | dt 338.29ms | 1549803.02 tokens/sec
Step 9380 | loss: 3.255115 | lr:3.5372e-04 | norm 0.3151 | dt 338.87ms | 1547155.56 tokens/sec
Step 9381 | loss: 3.229386 | lr:3.5367e-04 | norm 0.2868 | dt 339.13ms | 1545960.20 tokens/sec
Step 9382 | loss: 3.226934 | lr:3.5363e-04 | norm 0.3093 | dt 338.66ms | 1548132.58 tokens/sec
Step 9383 | loss: 3.210271 | lr:3.5358e-04 | norm 0.2777 | dt 338.27ms | 1549916.62 tokens/sec
Step 9384 | loss: 3.232251 | lr:3.5353e-04 | norm 0.2949 | dt 337.65ms | 1552757.73 tokens/sec
Step 9385 | loss: 3.274379 | lr:3.5349e-04 | norm 0.3129 | dt 337.99ms | 1551196.90 tokens/sec
Step 9386 | loss: 3.220985 | lr:3.5344e-04 | norm 0.3044 | dt 338.79ms | 1547543.17 tokens/sec
Step 9387 | loss: 3.221422 | lr:3.5340e-04 | norm 0.3012 | dt 338.08ms | 1550800.89 tokens/sec
Step 9388 | loss: 3.204065 | lr:3.5335e-04 | norm 0.3002 | dt 336.69ms | 1557204.35 tokens/sec
Step 9389 | loss: 3.269309 | lr:3.5330e-04 | norm 0.2919 | dt 338.76ms | 1547658.62 tokens/sec
Step 9390 | loss: 3.225746 | lr:3.5326e-04 | norm 0.2877 | dt 338.25ms | 1549979.99 tokens/sec
Step 9391 | loss: 3.230438 | lr:3.5321e-04 | norm 0.2992 | dt 337.68ms | 1552599.86 tokens/sec
Step 9392 | loss: 3.173309 | lr:3.5317e-04 | norm 0.2716 | dt 338.06ms | 1550853.39 tokens/sec
Step 9393 | loss: 3.249551 | lr:3.5312e-04 | norm 0.2901 | dt 338.51ms | 1548823.89 tokens/sec
Step 9394 | loss: 3.175693 | lr:3.5307e-04 | norm 0.3036 | dt 337.84ms | 1551882.18 tokens/sec
Step 9395 | loss: 3.192620 | lr:3.5303e-04 | norm 0.2972 | dt 337.76ms | 1552238.20 tokens/sec
Step 9396 | loss: 3.232892 | lr:3.5298e-04 | norm 0.2821 | dt 337.78ms | 1552170.27 tokens/sec
Step 9397 | loss: 3.211211 | lr:3.5294e-04 | norm 0.2876 | dt 338.66ms | 1548135.85 tokens/sec
Step 9398 | loss: 3.230547 | lr:3.5289e-04 | norm 0.2776 | dt 337.74ms | 1552348.87 tokens/sec
Step 9399 | loss: 3.166389 | lr:3.5284e-04 | norm 0.2838 | dt 338.07ms | 1550840.26 tokens/sec
Step 9400 | loss: 3.241285 | lr:3.5280e-04 | norm 0.2819 | dt 337.84ms | 1551881.08 tokens/sec
Step 9401 | loss: 3.190047 | lr:3.5275e-04 | norm 0.3091 | dt 338.52ms | 1548767.17 tokens/sec
Step 9402 | loss: 3.231554 | lr:3.5271e-04 | norm 0.2983 | dt 337.69ms | 1552569.16 tokens/sec
Step 9403 | loss: 3.216391 | lr:3.5266e-04 | norm 0.2955 | dt 337.43ms | 1553753.92 tokens/sec
Step 9404 | loss: 3.150891 | lr:3.5261e-04 | norm 0.3039 | dt 337.87ms | 1551738.72 tokens/sec
Step 9405 | loss: 3.196406 | lr:3.5257e-04 | norm 0.3023 | dt 337.91ms | 1551573.40 tokens/sec
Step 9406 | loss: 3.214006 | lr:3.5252e-04 | norm 0.2821 | dt 337.78ms | 1552136.30 tokens/sec
Step 9407 | loss: 3.205104 | lr:3.5248e-04 | norm 0.3231 | dt 336.95ms | 1555978.00 tokens/sec
Step 9408 | loss: 3.198697 | lr:3.5243e-04 | norm 0.3299 | dt 337.80ms | 1552045.38 tokens/sec
Step 9409 | loss: 3.174193 | lr:3.5238e-04 | norm 0.2609 | dt 338.29ms | 1549799.74 tokens/sec
Step 9410 | loss: 3.171007 | lr:3.5234e-04 | norm 0.2886 | dt 338.37ms | 1549447.03 tokens/sec
Step 9411 | loss: 3.206386 | lr:3.5229e-04 | norm 0.2686 | dt 337.68ms | 1552600.95 tokens/sec
Step 9412 | loss: 3.271712 | lr:3.5225e-04 | norm 0.3062 | dt 337.65ms | 1552739.09 tokens/sec
Step 9413 | loss: 3.194514 | lr:3.5220e-04 | norm 0.2713 | dt 338.59ms | 1548451.99 tokens/sec
Step 9414 | loss: 3.172817 | lr:3.5215e-04 | norm 0.2870 | dt 337.08ms | 1555377.10 tokens/sec
Step 9415 | loss: 3.237631 | lr:3.5211e-04 | norm 0.2662 | dt 337.27ms | 1554486.51 tokens/sec
Step 9416 | loss: 3.287030 | lr:3.5206e-04 | norm 0.2994 | dt 338.23ms | 1550071.76 tokens/sec
Step 9417 | loss: 3.229859 | lr:3.5202e-04 | norm 0.2895 | dt 337.40ms | 1553883.47 tokens/sec
Step 9418 | loss: 3.218617 | lr:3.5197e-04 | norm 0.2860 | dt 336.98ms | 1555834.89 tokens/sec
Step 9419 | loss: 3.220363 | lr:3.5192e-04 | norm 0.3028 | dt 338.49ms | 1548886.07 tokens/sec
Step 9420 | loss: 3.221997 | lr:3.5188e-04 | norm 0.2902 | dt 337.75ms | 1552290.79 tokens/sec
Step 9421 | loss: 3.228527 | lr:3.5183e-04 | norm 0.2890 | dt 337.67ms | 1552652.48 tokens/sec
Step 9422 | loss: 3.169322 | lr:3.5179e-04 | norm 0.2654 | dt 338.46ms | 1549042.09 tokens/sec
Step 9423 | loss: 3.193774 | lr:3.5174e-04 | norm 0.2839 | dt 338.26ms | 1549950.49 tokens/sec
Step 9424 | loss: 3.245301 | lr:3.5169e-04 | norm 0.2784 | dt 337.08ms | 1555391.40 tokens/sec
Step 9425 | loss: 3.235694 | lr:3.5165e-04 | norm 0.2664 | dt 338.08ms | 1550794.33 tokens/sec
Step 9426 | loss: 3.201700 | lr:3.5160e-04 | norm 0.2712 | dt 337.90ms | 1551585.44 tokens/sec
Step 9427 | loss: 3.199331 | lr:3.5155e-04 | norm 0.2718 | dt 337.45ms | 1553661.70 tokens/sec
Step 9428 | loss: 3.202148 | lr:3.5151e-04 | norm 0.3176 | dt 337.42ms | 1553793.44 tokens/sec
Step 9429 | loss: 3.223909 | lr:3.5146e-04 | norm 0.2771 | dt 337.66ms | 1552698.52 tokens/sec
Step 9430 | loss: 3.176874 | lr:3.5142e-04 | norm 0.2695 | dt 337.31ms | 1554304.12 tokens/sec
Step 9431 | loss: 3.209962 | lr:3.5137e-04 | norm 0.2852 | dt 338.22ms | 1550139.51 tokens/sec
Step 9432 | loss: 3.224425 | lr:3.5132e-04 | norm 0.2808 | dt 337.55ms | 1553201.90 tokens/sec
Step 9433 | loss: 3.191296 | lr:3.5128e-04 | norm 0.3219 | dt 338.02ms | 1551075.45 tokens/sec
Step 9434 | loss: 3.208342 | lr:3.5123e-04 | norm 0.2606 | dt 338.48ms | 1548960.26 tokens/sec
Step 9435 | loss: 3.234135 | lr:3.5119e-04 | norm 0.3188 | dt 338.22ms | 1550124.21 tokens/sec
Step 9436 | loss: 3.174864 | lr:3.5114e-04 | norm 0.3205 | dt 337.57ms | 1553111.95 tokens/sec
Step 9437 | loss: 3.220789 | lr:3.5109e-04 | norm 0.2768 | dt 338.31ms | 1549738.58 tokens/sec
Step 9438 | loss: 3.210405 | lr:3.5105e-04 | norm 0.3089 | dt 338.25ms | 1550012.76 tokens/sec
Step 9439 | loss: 3.263086 | lr:3.5100e-04 | norm 0.2630 | dt 338.24ms | 1550033.52 tokens/sec
Step 9440 | loss: 3.188668 | lr:3.5096e-04 | norm 0.2709 | dt 338.40ms | 1549324.76 tokens/sec
Step 9441 | loss: 3.280785 | lr:3.5091e-04 | norm 0.2913 | dt 338.71ms | 1547889.57 tokens/sec
Step 9442 | loss: 3.150524 | lr:3.5086e-04 | norm 0.2626 | dt 338.12ms | 1550583.28 tokens/sec
Step 9443 | loss: 3.258552 | lr:3.5082e-04 | norm 0.2889 | dt 338.20ms | 1550247.70 tokens/sec
Step 9444 | loss: 3.215440 | lr:3.5077e-04 | norm 0.2906 | dt 338.20ms | 1550246.60 tokens/sec
Step 9445 | loss: 3.208641 | lr:3.5073e-04 | norm 0.2783 | dt 338.88ms | 1547108.76 tokens/sec
Step 9446 | loss: 3.237185 | lr:3.5068e-04 | norm 0.2896 | dt 337.70ms | 1552508.88 tokens/sec
Step 9447 | loss: 3.216374 | lr:3.5063e-04 | norm 0.3064 | dt 338.45ms | 1549062.83 tokens/sec
Step 9448 | loss: 3.211220 | lr:3.5059e-04 | norm 0.2817 | dt 338.94ms | 1546862.80 tokens/sec
Step 9449 | loss: 3.223400 | lr:3.5054e-04 | norm 0.3171 | dt 1033.34ms | 507372.34 tokens/sec
Step 9450 | loss: 3.196734 | lr:3.5050e-04 | norm 0.3143 | dt 336.36ms | 1558709.90 tokens/sec
Step 9451 | loss: 3.187376 | lr:3.5045e-04 | norm 0.3006 | dt 337.60ms | 1552969.36 tokens/sec
Step 9452 | loss: 3.232049 | lr:3.5040e-04 | norm 0.3122 | dt 339.34ms | 1545041.27 tokens/sec
Step 9453 | loss: 3.168737 | lr:3.5036e-04 | norm 0.2884 | dt 338.11ms | 1550658.73 tokens/sec
Step 9454 | loss: 3.219986 | lr:3.5031e-04 | norm 0.3032 | dt 338.30ms | 1549777.90 tokens/sec
Step 9455 | loss: 3.147575 | lr:3.5026e-04 | norm 0.2822 | dt 338.01ms | 1551123.59 tokens/sec
Step 9456 | loss: 3.155258 | lr:3.5022e-04 | norm 0.2926 | dt 338.00ms | 1551132.34 tokens/sec
Step 9457 | loss: 3.197713 | lr:3.5017e-04 | norm 0.2715 | dt 337.72ms | 1552449.69 tokens/sec
Step 9458 | loss: 3.174400 | lr:3.5013e-04 | norm 0.2820 | dt 338.07ms | 1550812.92 tokens/sec
Step 9459 | loss: 3.187987 | lr:3.5008e-04 | norm 0.2839 | dt 341.44ms | 1535537.47 tokens/sec
Step 9460 | loss: 3.193961 | lr:3.5003e-04 | norm 0.2879 | dt 339.91ms | 1542448.99 tokens/sec
Step 9461 | loss: 3.147328 | lr:3.4999e-04 | norm 0.2898 | dt 338.97ms | 1546686.55 tokens/sec
Step 9462 | loss: 3.224283 | lr:3.4994e-04 | norm 0.2683 | dt 338.26ms | 1549949.40 tokens/sec
Step 9463 | loss: 3.215322 | lr:3.4990e-04 | norm 0.3024 | dt 339.31ms | 1545157.44 tokens/sec
Step 9464 | loss: 3.230846 | lr:3.4985e-04 | norm 0.2634 | dt 338.68ms | 1548018.15 tokens/sec
Step 9465 | loss: 3.208030 | lr:3.4980e-04 | norm 0.3245 | dt 339.41ms | 1544704.83 tokens/sec
Step 9466 | loss: 3.233766 | lr:3.4976e-04 | norm 0.2625 | dt 338.99ms | 1546637.60 tokens/sec
Step 9467 | loss: 3.170807 | lr:3.4971e-04 | norm 0.2788 | dt 339.35ms | 1544960.95 tokens/sec
Step 9468 | loss: 3.240097 | lr:3.4967e-04 | norm 0.2822 | dt 338.90ms | 1547029.30 tokens/sec
Step 9469 | loss: 3.157590 | lr:3.4962e-04 | norm 0.2703 | dt 339.48ms | 1544366.36 tokens/sec
Step 9470 | loss: 3.213706 | lr:3.4957e-04 | norm 0.2635 | dt 339.08ms | 1546227.61 tokens/sec
Step 9471 | loss: 3.206857 | lr:3.4953e-04 | norm 0.2553 | dt 339.20ms | 1545674.41 tokens/sec
Step 9472 | loss: 3.222293 | lr:3.4948e-04 | norm 0.2652 | dt 338.88ms | 1547135.97 tokens/sec
Step 9473 | loss: 3.236113 | lr:3.4944e-04 | norm 0.2491 | dt 338.71ms | 1547904.83 tokens/sec
Step 9474 | loss: 3.209488 | lr:3.4939e-04 | norm 0.2819 | dt 339.60ms | 1543843.76 tokens/sec
Step 9475 | loss: 3.195479 | lr:3.4934e-04 | norm 0.2811 | dt 338.74ms | 1547751.21 tokens/sec
Step 9476 | loss: 3.254221 | lr:3.4930e-04 | norm 0.2896 | dt 339.32ms | 1545114.01 tokens/sec
Step 9477 | loss: 3.244352 | lr:3.4925e-04 | norm 0.2714 | dt 338.95ms | 1546795.34 tokens/sec
Step 9478 | loss: 3.159646 | lr:3.4921e-04 | norm 0.2709 | dt 338.95ms | 1546784.46 tokens/sec
Step 9479 | loss: 3.190557 | lr:3.4916e-04 | norm 0.2912 | dt 339.67ms | 1543503.50 tokens/sec
Step 9480 | loss: 3.198137 | lr:3.4911e-04 | norm 0.2651 | dt 339.23ms | 1545545.13 tokens/sec
Step 9481 | loss: 3.101763 | lr:3.4907e-04 | norm 0.2669 | dt 338.48ms | 1548947.17 tokens/sec
Step 9482 | loss: 3.202849 | lr:3.4902e-04 | norm 0.2859 | dt 339.15ms | 1545878.69 tokens/sec
Step 9483 | loss: 3.155180 | lr:3.4897e-04 | norm 0.2699 | dt 339.14ms | 1545950.42 tokens/sec
Step 9484 | loss: 3.161545 | lr:3.4893e-04 | norm 0.2899 | dt 338.48ms | 1548962.44 tokens/sec
Step 9485 | loss: 3.182804 | lr:3.4888e-04 | norm 0.2616 | dt 338.92ms | 1546920.48 tokens/sec
Step 9486 | loss: 3.144401 | lr:3.4884e-04 | norm 0.2946 | dt 338.89ms | 1547088.08 tokens/sec
Step 9487 | loss: 3.254578 | lr:3.4879e-04 | norm 0.3004 | dt 338.47ms | 1548978.81 tokens/sec
Step 9488 | loss: 3.292911 | lr:3.4874e-04 | norm 0.3073 | dt 338.84ms | 1547300.35 tokens/sec
Step 9489 | loss: 3.158537 | lr:3.4870e-04 | norm 0.3124 | dt 338.81ms | 1547434.27 tokens/sec
Step 9490 | loss: 3.262602 | lr:3.4865e-04 | norm 0.3091 | dt 338.27ms | 1549898.05 tokens/sec
Step 9491 | loss: 3.246801 | lr:3.4861e-04 | norm 0.2883 | dt 338.10ms | 1550702.47 tokens/sec
Step 9492 | loss: 3.191407 | lr:3.4856e-04 | norm 0.3078 | dt 338.30ms | 1549759.33 tokens/sec
Step 9493 | loss: 3.217171 | lr:3.4851e-04 | norm 0.2812 | dt 339.69ms | 1543412.50 tokens/sec
Step 9494 | loss: 3.202058 | lr:3.4847e-04 | norm 0.2926 | dt 337.63ms | 1552847.64 tokens/sec
Step 9495 | loss: 3.168875 | lr:3.4842e-04 | norm 0.2700 | dt 338.87ms | 1547178.42 tokens/sec
Step 9496 | loss: 3.234020 | lr:3.4838e-04 | norm 0.2737 | dt 338.62ms | 1548292.82 tokens/sec
Step 9497 | loss: 3.192060 | lr:3.4833e-04 | norm 0.2897 | dt 339.42ms | 1544658.17 tokens/sec
Step 9498 | loss: 3.194304 | lr:3.4828e-04 | norm 0.2840 | dt 339.55ms | 1544074.65 tokens/sec
Step 9499 | loss: 3.212770 | lr:3.4824e-04 | norm 0.2706 | dt 1053.37ms | 497723.09 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 9500: 3.2101
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2890/10042=0.2878


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what do I need to do with that language? I'm wondering about the difference between the verb and the participle
rank 5 sample 1 >Hello, I'm a language model, for those of you who like to be more adventurous, but also enjoy learning to work on a computer, with a little
rank 5 sample 2 >Hello, I'm a language model, and I need a text editor for my computer, but I also have a small keyboard attached to my computer. I also
rank 5 sample 3 >Hello, I'm a language model, if you need some help... I'm a member of an international group of scholars based out of Portugal, who are engaged




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, learning is a breeze. We can make it happen for the sake of the children, but it is not going to stop
rank 2 sample 1 >Hello, I'm a language model, but I read about it in a lecture called "New York City, People Speak English." The idea behind this is that
rank 2 sample 2 >Hello, I'm a language model, and I'll talk about the language concept a little more in a future topic. Here's the part for a lecture in
rank 2 sample 3 >Hello, I'm a language model, but in what ways is that possible? That's pretty easy if you're already conversant like most of us, but




ddp_rank 3: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not an architect, and I don't even like the thing itself. In the diagram below I want,
rank 1 sample 0 >Hello, I'm a language model, so there's a lot of ways that we can create new verbs.
To create the first of these verbs, we
rank 3 sample 1 >Hello, I'm a language model, so let us write a report with a sentence:
Here's a sentence:
I started this essay with what's
rank 1 sample 1 >Hello, I'm a language model, a person that takes us to the heart of the word-processing systems that work with different languages to make them work together
rank 3 sample 2 >Hello, I'm a language model, so it has to write a file that works at the level of the user:
- the browser's default default page
rank 1 sample 2 >Hello, I'm a language model, but could it do something about it?
I'm looking at it as a model of a web.
In this
rank 3 sample 3 >Hello, I'm a language model, so i'm trying to draw something from my imagination so we'll get a little space, that's part of what I


rank 1 sample 3 >Hello, I'm a language model, so I'm doing this when I come anywhere. I found a link where links to our "The Language" web site




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I wanted to share one of my newest books with you. I wrote the basic structure of languages for kids, but
rank 7 sample 1 >Hello, I'm a language model, using real language. I'll teach at a middle school course, but I'm happy to have a job as a model
rank 7 sample 2 >Hello, I'm a language model, so I'll say two words or paragraphs and then to the left, one word or two sentences. And I have the
rank 7 sample 3 >Hello, I'm a language model, and it's a way of describing different people in different ways, so to speak, so I thought, I was wrong




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, that you can get through many if not most of the time and can't use anything else to get the same thing or
rank 6 sample 1 >Hello, I'm a language model, which means I'm learning how to write any language I want to learn from.
This means that if my vocabulary is
rank 6 sample 2 >Hello, I'm a language model, but I don't need any other. I just want to be able to make a quick, easy reading that goes a
rank 6 sample 3 >Hello, I'm a language model, so here we're in a situation where we can't get the numbers to their correct order — we need something smaller.




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I am a beginner learner, since it's a lot of the time I just do...
You may be
rank 4 sample 1 >Hello, I'm a language model, who knows how powerful that's a technology. What does the word technology mean in these two words? In other words,
rank 4 sample 2 >Hello, I'm a language model, I understand that word and sentence structure of words and sentences. It helps me to recognize how this language looks and looks like
rank 4 sample 3 >Hello, I'm a language model, so you take that language into account while we create models, they are essentially a framework. So there's a certain language




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I think I'm gonna make one even simpler. I've had tons of experience and all my classes have been very
rank 0 sample 1 >Hello, I'm a language model, and in my opinion it's one of the easiest people to understand. With my understanding and research on L1, I
rank 0 sample 2 >Hello, I'm a language model, and I wish I could talk to anyone who says, "We'll do it all with our own language." I also
rank 0 sample 3 >Hello, I'm a language model, and what I have to say about it. All of my experiences I've been a native speaker of German, so why


Step 9500 | loss: 3.188400 | lr:3.4819e-04 | norm 0.2917 | dt 18839.84ms | 27828.68 tokens/sec
Step 9501 | loss: 3.224983 | lr:3.4814e-04 | norm 0.2625 | dt 335.31ms | 1563599.77 tokens/sec
Step 9502 | loss: 3.309536 | lr:3.4810e-04 | norm 0.3468 | dt 335.44ms | 1562998.52 tokens/sec
Step 9503 | loss: 3.235255 | lr:3.4805e-04 | norm 0.3257 | dt 336.37ms | 1558673.44 tokens/sec
Step 9504 | loss: 3.158301 | lr:3.4801e-04 | norm 0.3148 | dt 336.00ms | 1560382.22 tokens/sec
Step 9505 | loss: 3.183307 | lr:3.4796e-04 | norm 0.3040 | dt 337.82ms | 1551993.90 tokens/sec
Step 9506 | loss: 3.264869 | lr:3.4791e-04 | norm 0.3097 | dt 336.91ms | 1556158.58 tokens/sec
Step 9507 | loss: 3.248994 | lr:3.4787e-04 | norm 0.3031 | dt 337.27ms | 1554515.08 tokens/sec
Step 9508 | loss: 3.209247 | lr:3.4782e-04 | norm 0.3154 | dt 337.01ms | 1555681.90 tokens/sec
Step 9509 | loss: 3.161722 | lr:3.4778e-04 | norm 0.3006 | dt 336.03ms | 1560247.15 tokens/sec
Step 9510 | loss: 3.188254 | lr:3.4773e-04 | norm 0.2671 | dt 337.26ms | 1554540.36 tokens/sec
Step 9511 | loss: 3.248810 | lr:3.4768e-04 | norm 0.2804 | dt 338.40ms | 1549329.13 tokens/sec
Step 9512 | loss: 3.180801 | lr:3.4764e-04 | norm 0.2713 | dt 337.39ms | 1553930.69 tokens/sec
Step 9513 | loss: 3.172042 | lr:3.4759e-04 | norm 0.2825 | dt 342.07ms | 1532681.00 tokens/sec
Step 9514 | loss: 3.168162 | lr:3.4755e-04 | norm 0.2695 | dt 336.69ms | 1557172.37 tokens/sec
Step 9515 | loss: 3.216218 | lr:3.4750e-04 | norm 0.2885 | dt 337.06ms | 1555460.72 tokens/sec
Step 9516 | loss: 3.185291 | lr:3.4745e-04 | norm 0.2896 | dt 337.48ms | 1553532.19 tokens/sec
Step 9517 | loss: 3.196403 | lr:3.4741e-04 | norm 0.3043 | dt 336.91ms | 1556176.20 tokens/sec
Step 9518 | loss: 3.202612 | lr:3.4736e-04 | norm 0.3005 | dt 336.83ms | 1556514.36 tokens/sec
Step 9519 | loss: 3.203433 | lr:3.4731e-04 | norm 0.2981 | dt 336.21ms | 1559428.38 tokens/sec
Step 9520 | loss: 3.165937 | lr:3.4727e-04 | norm 0.3389 | dt 336.33ms | 1558848.02 tokens/sec
Step 9521 | loss: 3.241284 | lr:3.4722e-04 | norm 0.2761 | dt 336.50ms | 1558045.07 tokens/sec
Step 9522 | loss: 3.234606 | lr:3.4718e-04 | norm 0.3008 | dt 336.32ms | 1558890.01 tokens/sec
Step 9523 | loss: 3.171529 | lr:3.4713e-04 | norm 0.2968 | dt 336.72ms | 1557041.17 tokens/sec
Step 9524 | loss: 3.217708 | lr:3.4708e-04 | norm 0.2917 | dt 337.09ms | 1555333.10 tokens/sec
Step 9525 | loss: 3.245254 | lr:3.4704e-04 | norm 0.2632 | dt 335.95ms | 1560626.95 tokens/sec
Step 9526 | loss: 3.205015 | lr:3.4699e-04 | norm 0.2998 | dt 337.00ms | 1555769.95 tokens/sec
Step 9527 | loss: 3.335617 | lr:3.4695e-04 | norm 0.3172 | dt 337.59ms | 1553014.33 tokens/sec
Step 9528 | loss: 3.203946 | lr:3.4690e-04 | norm 0.3083 | dt 338.41ms | 1549287.65 tokens/sec
Step 9529 | loss: 3.173562 | lr:3.4685e-04 | norm 0.2891 | dt 338.26ms | 1549947.21 tokens/sec
Step 9530 | loss: 3.277083 | lr:3.4681e-04 | norm 0.3014 | dt 339.12ms | 1546026.50 tokens/sec
Step 9531 | loss: 3.242603 | lr:3.4676e-04 | norm 0.2841 | dt 338.79ms | 1547521.39 tokens/sec
Step 9532 | loss: 3.280748 | lr:3.4672e-04 | norm 0.3041 | dt 338.59ms | 1548425.82 tokens/sec
Step 9533 | loss: 3.197580 | lr:3.4667e-04 | norm 0.2909 | dt 338.52ms | 1548750.80 tokens/sec
Step 9534 | loss: 3.189769 | lr:3.4662e-04 | norm 0.2708 | dt 338.82ms | 1547376.56 tokens/sec
Step 9535 | loss: 3.207600 | lr:3.4658e-04 | norm 0.2808 | dt 338.90ms | 1547019.51 tokens/sec
Step 9536 | loss: 3.174024 | lr:3.4653e-04 | norm 0.2729 | dt 339.02ms | 1546467.92 tokens/sec
Step 9537 | loss: 3.140966 | lr:3.4648e-04 | norm 0.3015 | dt 339.31ms | 1545172.64 tokens/sec
Step 9538 | loss: 3.185699 | lr:3.4644e-04 | norm 0.2590 | dt 339.70ms | 1543393.00 tokens/sec
Step 9539 | loss: 3.215158 | lr:3.4639e-04 | norm 0.2833 | dt 339.00ms | 1546563.63 tokens/sec
Step 9540 | loss: 3.194619 | lr:3.4635e-04 | norm 0.2559 | dt 338.30ms | 1549765.89 tokens/sec
Step 9541 | loss: 3.161635 | lr:3.4630e-04 | norm 0.2645 | dt 339.43ms | 1544606.09 tokens/sec
Step 9542 | loss: 3.174104 | lr:3.4625e-04 | norm 0.2499 | dt 338.42ms | 1549209.06 tokens/sec
Step 9543 | loss: 3.146680 | lr:3.4621e-04 | norm 0.2581 | dt 339.05ms | 1546342.86 tokens/sec
Step 9544 | loss: 3.137651 | lr:3.4616e-04 | norm 0.2510 | dt 338.63ms | 1548265.56 tokens/sec
Step 9545 | loss: 3.183981 | lr:3.4612e-04 | norm 0.2506 | dt 338.20ms | 1550224.75 tokens/sec
Step 9546 | loss: 3.283525 | lr:3.4607e-04 | norm 0.2878 | dt 338.37ms | 1549464.50 tokens/sec
Step 9547 | loss: 3.258671 | lr:3.4602e-04 | norm 0.2616 | dt 338.38ms | 1549413.18 tokens/sec
Step 9548 | loss: 3.250213 | lr:3.4598e-04 | norm 0.2944 | dt 337.92ms | 1551497.86 tokens/sec
Step 9549 | loss: 3.221576 | lr:3.4593e-04 | norm 0.2916 | dt 338.59ms | 1548440.00 tokens/sec
Step 9550 | loss: 3.139573 | lr:3.4589e-04 | norm 0.2835 | dt 339.14ms | 1545923.25 tokens/sec
Step 9551 | loss: 3.172431 | lr:3.4584e-04 | norm 0.2759 | dt 338.23ms | 1550080.50 tokens/sec
Step 9552 | loss: 3.276633 | lr:3.4579e-04 | norm 0.2771 | dt 338.51ms | 1548831.53 tokens/sec
Step 9553 | loss: 3.177437 | lr:3.4575e-04 | norm 0.2779 | dt 339.25ms | 1545443.03 tokens/sec
Step 9554 | loss: 3.164578 | lr:3.4570e-04 | norm 0.2465 | dt 338.56ms | 1548604.65 tokens/sec
Step 9555 | loss: 3.289662 | lr:3.4565e-04 | norm 0.3122 | dt 339.11ms | 1546087.37 tokens/sec
Step 9556 | loss: 3.212085 | lr:3.4561e-04 | norm 0.2749 | dt 338.69ms | 1547965.85 tokens/sec
Step 9557 | loss: 3.243137 | lr:3.4556e-04 | norm 0.2910 | dt 339.03ms | 1546451.61 tokens/sec
Step 9558 | loss: 3.193263 | lr:3.4552e-04 | norm 0.2719 | dt 338.69ms | 1547984.37 tokens/sec
Step 9559 | loss: 3.226938 | lr:3.4547e-04 | norm 0.3077 | dt 339.41ms | 1544704.83 tokens/sec
Step 9560 | loss: 3.214087 | lr:3.4542e-04 | norm 0.2832 | dt 339.70ms | 1543397.33 tokens/sec
Step 9561 | loss: 3.245814 | lr:3.4538e-04 | norm 0.2880 | dt 337.36ms | 1554075.65 tokens/sec
Step 9562 | loss: 3.275587 | lr:3.4533e-04 | norm 0.2986 | dt 337.80ms | 1552069.48 tokens/sec
Step 9563 | loss: 3.157405 | lr:3.4529e-04 | norm 0.3078 | dt 338.30ms | 1549794.28 tokens/sec
Step 9564 | loss: 3.198023 | lr:3.4524e-04 | norm 0.2701 | dt 338.11ms | 1550652.17 tokens/sec
Step 9565 | loss: 3.134742 | lr:3.4519e-04 | norm 0.3292 | dt 338.20ms | 1550229.12 tokens/sec
Step 9566 | loss: 3.173202 | lr:3.4515e-04 | norm 0.2864 | dt 337.66ms | 1552704.00 tokens/sec
Step 9567 | loss: 3.207556 | lr:3.4510e-04 | norm 0.3001 | dt 339.20ms | 1545680.93 tokens/sec
Step 9568 | loss: 3.236997 | lr:3.4505e-04 | norm 0.2877 | dt 338.29ms | 1549801.93 tokens/sec
Step 9569 | loss: 3.199075 | lr:3.4501e-04 | norm 0.3063 | dt 340.43ms | 1540084.32 tokens/sec
Step 9570 | loss: 3.221784 | lr:3.4496e-04 | norm 0.2712 | dt 339.15ms | 1545885.21 tokens/sec
Step 9571 | loss: 3.200403 | lr:3.4492e-04 | norm 0.2981 | dt 338.08ms | 1550785.58 tokens/sec
Step 9572 | loss: 3.204011 | lr:3.4487e-04 | norm 0.2727 | dt 338.26ms | 1549942.84 tokens/sec
Step 9573 | loss: 3.132198 | lr:3.4482e-04 | norm 0.2841 | dt 338.96ms | 1546775.76 tokens/sec
Step 9574 | loss: 3.206867 | lr:3.4478e-04 | norm 0.2791 | dt 339.18ms | 1545753.72 tokens/sec
Step 9575 | loss: 3.190801 | lr:3.4473e-04 | norm 0.2969 | dt 338.63ms | 1548275.37 tokens/sec
Step 9576 | loss: 3.176277 | lr:3.4469e-04 | norm 0.2841 | dt 338.92ms | 1546921.56 tokens/sec
Step 9577 | loss: 3.192372 | lr:3.4464e-04 | norm 0.3030 | dt 338.96ms | 1546754.00 tokens/sec
Step 9578 | loss: 3.137490 | lr:3.4459e-04 | norm 0.3049 | dt 339.46ms | 1544480.25 tokens/sec
Step 9579 | loss: 3.159791 | lr:3.4455e-04 | norm 0.3017 | dt 339.27ms | 1545334.43 tokens/sec
Step 9580 | loss: 3.157699 | lr:3.4450e-04 | norm 0.2898 | dt 338.92ms | 1546934.62 tokens/sec
Step 9581 | loss: 3.172127 | lr:3.4446e-04 | norm 0.3180 | dt 339.48ms | 1544393.47 tokens/sec
Step 9582 | loss: 3.143674 | lr:3.4441e-04 | norm 0.2703 | dt 339.80ms | 1542934.92 tokens/sec
Step 9583 | loss: 3.194314 | lr:3.4436e-04 | norm 0.2923 | dt 338.99ms | 1546597.35 tokens/sec
Step 9584 | loss: 3.203818 | lr:3.4432e-04 | norm 0.3049 | dt 341.19ms | 1536653.41 tokens/sec
Step 9585 | loss: 3.178328 | lr:3.4427e-04 | norm 0.2845 | dt 339.51ms | 1544240.55 tokens/sec
Step 9586 | loss: 3.164482 | lr:3.4422e-04 | norm 0.2658 | dt 338.62ms | 1548295.00 tokens/sec
Step 9587 | loss: 3.201696 | lr:3.4418e-04 | norm 0.2900 | dt 338.00ms | 1551132.34 tokens/sec
Step 9588 | loss: 3.134127 | lr:3.4413e-04 | norm 0.2773 | dt 338.25ms | 1550012.76 tokens/sec
Step 9589 | loss: 3.234587 | lr:3.4409e-04 | norm 0.2868 | dt 338.58ms | 1548505.42 tokens/sec
Step 9590 | loss: 3.271279 | lr:3.4404e-04 | norm 0.3204 | dt 338.42ms | 1549204.70 tokens/sec
Step 9591 | loss: 3.176498 | lr:3.4399e-04 | norm 0.2998 | dt 337.88ms | 1551694.93 tokens/sec
Step 9592 | loss: 3.158001 | lr:3.4395e-04 | norm 0.3056 | dt 337.95ms | 1551373.09 tokens/sec
Step 9593 | loss: 3.240354 | lr:3.4390e-04 | norm 0.3435 | dt 338.13ms | 1550561.42 tokens/sec
Step 9594 | loss: 3.179060 | lr:3.4386e-04 | norm 0.3049 | dt 338.82ms | 1547408.14 tokens/sec
Step 9595 | loss: 3.105008 | lr:3.4381e-04 | norm 0.3318 | dt 338.60ms | 1548395.29 tokens/sec
Step 9596 | loss: 3.180697 | lr:3.4376e-04 | norm 0.3142 | dt 338.27ms | 1549913.35 tokens/sec
Step 9597 | loss: 3.156081 | lr:3.4372e-04 | norm 0.2986 | dt 338.70ms | 1547964.76 tokens/sec
Step 9598 | loss: 3.191106 | lr:3.4367e-04 | norm 0.3268 | dt 339.04ms | 1546379.83 tokens/sec
Step 9599 | loss: 3.185314 | lr:3.4362e-04 | norm 0.3024 | dt 338.11ms | 1550653.26 tokens/sec
Step 9600 | loss: 3.160537 | lr:3.4358e-04 | norm 0.2735 | dt 337.27ms | 1554501.90 tokens/sec
Step 9601 | loss: 3.184030 | lr:3.4353e-04 | norm 0.2905 | dt 337.90ms | 1551616.09 tokens/sec
Step 9602 | loss: 3.173161 | lr:3.4349e-04 | norm 0.2969 | dt 337.76ms | 1552257.92 tokens/sec
Step 9603 | loss: 3.183751 | lr:3.4344e-04 | norm 0.2730 | dt 338.20ms | 1550245.51 tokens/sec
Step 9604 | loss: 3.187010 | lr:3.4339e-04 | norm 0.2796 | dt 338.27ms | 1549901.33 tokens/sec
Step 9605 | loss: 3.190860 | lr:3.4335e-04 | norm 0.2821 | dt 337.80ms | 1552083.72 tokens/sec
Step 9606 | loss: 3.196673 | lr:3.4330e-04 | norm 0.2596 | dt 337.94ms | 1551444.23 tokens/sec
Step 9607 | loss: 3.202155 | lr:3.4326e-04 | norm 0.2873 | dt 338.35ms | 1549526.73 tokens/sec
Step 9608 | loss: 3.154123 | lr:3.4321e-04 | norm 0.2741 | dt 337.14ms | 1555093.32 tokens/sec
Step 9609 | loss: 3.139811 | lr:3.4316e-04 | norm 0.2809 | dt 338.50ms | 1548877.34 tokens/sec
Step 9610 | loss: 3.195877 | lr:3.4312e-04 | norm 0.2565 | dt 338.81ms | 1547453.87 tokens/sec
Step 9611 | loss: 3.148594 | lr:3.4307e-04 | norm 0.2792 | dt 338.06ms | 1550892.76 tokens/sec
Step 9612 | loss: 3.188645 | lr:3.4302e-04 | norm 0.3059 | dt 337.65ms | 1552774.17 tokens/sec
Step 9613 | loss: 3.194162 | lr:3.4298e-04 | norm 0.2645 | dt 338.40ms | 1549320.40 tokens/sec
Step 9614 | loss: 3.159751 | lr:3.4293e-04 | norm 0.2831 | dt 337.49ms | 1553493.77 tokens/sec
Step 9615 | loss: 3.217854 | lr:3.4289e-04 | norm 0.2640 | dt 338.02ms | 1551069.98 tokens/sec
Step 9616 | loss: 3.160141 | lr:3.4284e-04 | norm 0.2711 | dt 337.65ms | 1552758.82 tokens/sec
Step 9617 | loss: 3.134235 | lr:3.4279e-04 | norm 0.2635 | dt 338.78ms | 1547561.69 tokens/sec
Step 9618 | loss: 3.196289 | lr:3.4275e-04 | norm 0.2647 | dt 338.63ms | 1548254.66 tokens/sec
Step 9619 | loss: 3.229477 | lr:3.4270e-04 | norm 0.2819 | dt 337.86ms | 1551804.42 tokens/sec
Step 9620 | loss: 3.175348 | lr:3.4266e-04 | norm 0.2943 | dt 337.58ms | 1553062.59 tokens/sec
Step 9621 | loss: 3.158856 | lr:3.4261e-04 | norm 0.2658 | dt 338.73ms | 1547802.42 tokens/sec
Step 9622 | loss: 3.156512 | lr:3.4256e-04 | norm 0.2913 | dt 338.13ms | 1550560.32 tokens/sec
Step 9623 | loss: 3.247450 | lr:3.4252e-04 | norm 0.2668 | dt 338.26ms | 1549975.62 tokens/sec
Step 9624 | loss: 3.187629 | lr:3.4247e-04 | norm 0.2777 | dt 337.63ms | 1552857.51 tokens/sec
Step 9625 | loss: 3.190655 | lr:3.4242e-04 | norm 0.2877 | dt 338.96ms | 1546771.41 tokens/sec
Step 9626 | loss: 3.184245 | lr:3.4238e-04 | norm 0.2661 | dt 338.22ms | 1550152.62 tokens/sec
Step 9627 | loss: 3.243057 | lr:3.4233e-04 | norm 0.3223 | dt 338.82ms | 1547387.45 tokens/sec
Step 9628 | loss: 3.176670 | lr:3.4229e-04 | norm 0.2984 | dt 337.64ms | 1552782.94 tokens/sec
Step 9629 | loss: 3.206180 | lr:3.4224e-04 | norm 0.2888 | dt 338.49ms | 1548890.44 tokens/sec
Step 9630 | loss: 3.186435 | lr:3.4219e-04 | norm 0.2798 | dt 339.20ms | 1545667.89 tokens/sec
Step 9631 | loss: 3.162530 | lr:3.4215e-04 | norm 0.2639 | dt 337.49ms | 1553509.14 tokens/sec
Step 9632 | loss: 3.172480 | lr:3.4210e-04 | norm 0.2688 | dt 339.19ms | 1545690.71 tokens/sec
Step 9633 | loss: 3.167035 | lr:3.4206e-04 | norm 0.2739 | dt 338.79ms | 1547542.08 tokens/sec
Step 9634 | loss: 3.219281 | lr:3.4201e-04 | norm 0.2794 | dt 338.57ms | 1548554.49 tokens/sec
Step 9635 | loss: 3.171546 | lr:3.4196e-04 | norm 0.2777 | dt 339.68ms | 1543461.24 tokens/sec
Step 9636 | loss: 3.200335 | lr:3.4192e-04 | norm 0.2955 | dt 338.26ms | 1549962.51 tokens/sec
Step 9637 | loss: 3.179355 | lr:3.4187e-04 | norm 0.2862 | dt 338.99ms | 1546634.33 tokens/sec
Step 9638 | loss: 3.198508 | lr:3.4182e-04 | norm 0.2732 | dt 913.64ms | 573842.27 tokens/sec
Step 9639 | loss: 3.168601 | lr:3.4178e-04 | norm 0.2870 | dt 335.33ms | 1563489.71 tokens/sec
Step 9640 | loss: 3.255141 | lr:3.4173e-04 | norm 0.2899 | dt 336.89ms | 1556235.67 tokens/sec
Step 9641 | loss: 3.195232 | lr:3.4169e-04 | norm 0.2818 | dt 338.38ms | 1549419.74 tokens/sec
Step 9642 | loss: 3.165946 | lr:3.4164e-04 | norm 0.3308 | dt 337.94ms | 1551427.81 tokens/sec
Step 9643 | loss: 3.220975 | lr:3.4159e-04 | norm 0.2755 | dt 337.61ms | 1552922.21 tokens/sec
Step 9644 | loss: 3.215326 | lr:3.4155e-04 | norm 0.3071 | dt 337.97ms | 1551287.72 tokens/sec
Step 9645 | loss: 3.217207 | lr:3.4150e-04 | norm 0.2686 | dt 338.08ms | 1550775.74 tokens/sec
Step 9646 | loss: 3.189279 | lr:3.4146e-04 | norm 0.2872 | dt 337.34ms | 1554161.32 tokens/sec
Step 9647 | loss: 3.215230 | lr:3.4141e-04 | norm 0.2699 | dt 338.87ms | 1547163.18 tokens/sec
Step 9648 | loss: 3.248743 | lr:3.4136e-04 | norm 0.2814 | dt 339.00ms | 1546556.02 tokens/sec
Step 9649 | loss: 3.187047 | lr:3.4132e-04 | norm 0.2626 | dt 339.47ms | 1544428.18 tokens/sec
Step 9650 | loss: 3.165012 | lr:3.4127e-04 | norm 0.2613 | dt 338.57ms | 1548529.41 tokens/sec
Step 9651 | loss: 3.190496 | lr:3.4122e-04 | norm 0.2680 | dt 339.37ms | 1544866.52 tokens/sec
Step 9652 | loss: 3.197986 | lr:3.4118e-04 | norm 0.2838 | dt 339.06ms | 1546311.33 tokens/sec
Step 9653 | loss: 3.175225 | lr:3.4113e-04 | norm 0.2762 | dt 339.22ms | 1545562.52 tokens/sec
Step 9654 | loss: 3.143581 | lr:3.4109e-04 | norm 0.2963 | dt 338.99ms | 1546603.88 tokens/sec
Step 9655 | loss: 3.241323 | lr:3.4104e-04 | norm 0.2678 | dt 340.43ms | 1540068.14 tokens/sec
Step 9656 | loss: 3.225408 | lr:3.4099e-04 | norm 0.3111 | dt 338.75ms | 1547693.48 tokens/sec
Step 9657 | loss: 3.185238 | lr:3.4095e-04 | norm 0.2664 | dt 338.71ms | 1547892.84 tokens/sec
Step 9658 | loss: 3.175337 | lr:3.4090e-04 | norm 0.2593 | dt 340.33ms | 1540506.17 tokens/sec
Step 9659 | loss: 3.172366 | lr:3.4086e-04 | norm 0.2811 | dt 339.19ms | 1545695.05 tokens/sec
Step 9660 | loss: 3.127087 | lr:3.4081e-04 | norm 0.2525 | dt 338.37ms | 1549449.21 tokens/sec
Step 9661 | loss: 3.167002 | lr:3.4076e-04 | norm 0.2710 | dt 339.95ms | 1542254.27 tokens/sec
Step 9662 | loss: 3.190780 | lr:3.4072e-04 | norm 0.3032 | dt 339.88ms | 1542588.57 tokens/sec
Step 9663 | loss: 3.174882 | lr:3.4067e-04 | norm 0.2813 | dt 340.28ms | 1540733.91 tokens/sec
Step 9664 | loss: 3.214813 | lr:3.4062e-04 | norm 0.2941 | dt 339.30ms | 1545188.92 tokens/sec
Step 9665 | loss: 3.201238 | lr:3.4058e-04 | norm 0.3339 | dt 338.90ms | 1547031.48 tokens/sec
Step 9666 | loss: 3.197747 | lr:3.4053e-04 | norm 0.2909 | dt 341.32ms | 1536039.45 tokens/sec
Step 9667 | loss: 3.175817 | lr:3.4049e-04 | norm 0.2892 | dt 339.31ms | 1545180.24 tokens/sec
Step 9668 | loss: 3.148992 | lr:3.4044e-04 | norm 0.2924 | dt 341.88ms | 1533524.31 tokens/sec
Step 9669 | loss: 3.251269 | lr:3.4039e-04 | norm 0.2862 | dt 338.84ms | 1547288.37 tokens/sec
Step 9670 | loss: 3.211210 | lr:3.4035e-04 | norm 0.2906 | dt 341.10ms | 1537043.30 tokens/sec
Step 9671 | loss: 3.160571 | lr:3.4030e-04 | norm 0.2762 | dt 339.22ms | 1545547.31 tokens/sec
Step 9672 | loss: 3.174687 | lr:3.4026e-04 | norm 0.2737 | dt 339.19ms | 1545712.44 tokens/sec
Step 9673 | loss: 3.175849 | lr:3.4021e-04 | norm 0.2811 | dt 339.63ms | 1543712.62 tokens/sec
Step 9674 | loss: 3.201443 | lr:3.4016e-04 | norm 0.2674 | dt 338.88ms | 1547100.05 tokens/sec
Step 9675 | loss: 3.294219 | lr:3.4012e-04 | norm 0.3009 | dt 339.83ms | 1542774.72 tokens/sec
Step 9676 | loss: 3.141535 | lr:3.4007e-04 | norm 0.2985 | dt 340.21ms | 1541078.35 tokens/sec
Step 9677 | loss: 3.190681 | lr:3.4002e-04 | norm 0.2712 | dt 339.39ms | 1544774.28 tokens/sec
Step 9678 | loss: 3.202169 | lr:3.3998e-04 | norm 0.3054 | dt 338.64ms | 1548232.86 tokens/sec
Step 9679 | loss: 3.186970 | lr:3.3993e-04 | norm 0.2667 | dt 338.05ms | 1550929.95 tokens/sec
Step 9680 | loss: 3.195009 | lr:3.3989e-04 | norm 0.2941 | dt 339.20ms | 1545663.55 tokens/sec
Step 9681 | loss: 3.188179 | lr:3.3984e-04 | norm 0.2646 | dt 338.52ms | 1548783.53 tokens/sec
Step 9682 | loss: 3.252083 | lr:3.3979e-04 | norm 0.2634 | dt 338.54ms | 1548651.55 tokens/sec
Step 9683 | loss: 3.168311 | lr:3.3975e-04 | norm 0.2736 | dt 338.60ms | 1548389.84 tokens/sec
Step 9684 | loss: 3.242035 | lr:3.3970e-04 | norm 0.2611 | dt 338.09ms | 1550726.52 tokens/sec
Step 9685 | loss: 3.212606 | lr:3.3965e-04 | norm 0.2909 | dt 338.25ms | 1550020.41 tokens/sec
Step 9686 | loss: 3.193118 | lr:3.3961e-04 | norm 0.2583 | dt 338.32ms | 1549676.33 tokens/sec
Step 9687 | loss: 3.191578 | lr:3.3956e-04 | norm 0.2782 | dt 337.95ms | 1551389.50 tokens/sec
Step 9688 | loss: 3.143458 | lr:3.3952e-04 | norm 0.2670 | dt 338.21ms | 1550207.26 tokens/sec
Step 9689 | loss: 3.186056 | lr:3.3947e-04 | norm 0.2562 | dt 1038.65ms | 504776.91 tokens/sec
Step 9690 | loss: 3.204034 | lr:3.3942e-04 | norm 0.2681 | dt 336.13ms | 1559794.51 tokens/sec
Step 9691 | loss: 3.256496 | lr:3.3938e-04 | norm 0.2627 | dt 337.54ms | 1553275.41 tokens/sec
Step 9692 | loss: 3.166805 | lr:3.3933e-04 | norm 0.3012 | dt 338.67ms | 1548095.53 tokens/sec
Step 9693 | loss: 3.229067 | lr:3.3929e-04 | norm 0.2473 | dt 337.57ms | 1553107.56 tokens/sec
Step 9694 | loss: 3.197093 | lr:3.3924e-04 | norm 0.2771 | dt 337.14ms | 1555084.52 tokens/sec
Step 9695 | loss: 3.115653 | lr:3.3919e-04 | norm 0.3034 | dt 338.72ms | 1547867.78 tokens/sec
Step 9696 | loss: 3.157304 | lr:3.3915e-04 | norm 0.2904 | dt 339.63ms | 1543699.61 tokens/sec
Step 9697 | loss: 3.233519 | lr:3.3910e-04 | norm 0.2991 | dt 338.01ms | 1551100.61 tokens/sec
Step 9698 | loss: 3.183403 | lr:3.3905e-04 | norm 0.3273 | dt 339.87ms | 1542610.21 tokens/sec
Step 9699 | loss: 3.175160 | lr:3.3901e-04 | norm 0.3176 | dt 338.87ms | 1547175.16 tokens/sec
Step 9700 | loss: 3.179271 | lr:3.3896e-04 | norm 0.2884 | dt 338.57ms | 1548518.50 tokens/sec
Step 9701 | loss: 3.196441 | lr:3.3892e-04 | norm 0.2763 | dt 338.19ms | 1550272.83 tokens/sec
Step 9702 | loss: 3.220922 | lr:3.3887e-04 | norm 0.2854 | dt 337.66ms | 1552702.91 tokens/sec
Step 9703 | loss: 3.182192 | lr:3.3882e-04 | norm 0.2685 | dt 338.29ms | 1549795.38 tokens/sec
Step 9704 | loss: 3.189278 | lr:3.3878e-04 | norm 0.3044 | dt 338.80ms | 1547462.59 tokens/sec
Step 9705 | loss: 3.200708 | lr:3.3873e-04 | norm 0.2888 | dt 339.09ms | 1546152.59 tokens/sec
Step 9706 | loss: 3.239963 | lr:3.3869e-04 | norm 0.3051 | dt 338.42ms | 1549217.79 tokens/sec
Step 9707 | loss: 3.191092 | lr:3.3864e-04 | norm 0.2869 | dt 338.16ms | 1550421.48 tokens/sec
Step 9708 | loss: 3.249876 | lr:3.3859e-04 | norm 0.2878 | dt 337.48ms | 1553549.75 tokens/sec
Step 9709 | loss: 3.156884 | lr:3.3855e-04 | norm 0.3067 | dt 338.52ms | 1548759.53 tokens/sec
Step 9710 | loss: 3.271370 | lr:3.3850e-04 | norm 0.3073 | dt 338.33ms | 1549635.92 tokens/sec
Step 9711 | loss: 3.123067 | lr:3.3845e-04 | norm 0.3002 | dt 337.71ms | 1552495.73 tokens/sec
Step 9712 | loss: 3.200251 | lr:3.3841e-04 | norm 0.3087 | dt 338.50ms | 1548862.07 tokens/sec
Step 9713 | loss: 3.177490 | lr:3.3836e-04 | norm 0.3178 | dt 342.25ms | 1531866.35 tokens/sec
Step 9714 | loss: 3.173903 | lr:3.3832e-04 | norm 0.2868 | dt 338.13ms | 1550537.36 tokens/sec
Step 9715 | loss: 3.130839 | lr:3.3827e-04 | norm 0.3103 | dt 339.56ms | 1544008.52 tokens/sec
Step 9716 | loss: 3.214842 | lr:3.3822e-04 | norm 0.3073 | dt 339.68ms | 1543489.41 tokens/sec
Step 9717 | loss: 3.180820 | lr:3.3818e-04 | norm 0.3017 | dt 338.87ms | 1547163.18 tokens/sec
Step 9718 | loss: 3.182877 | lr:3.3813e-04 | norm 0.2799 | dt 339.69ms | 1543436.33 tokens/sec
Step 9719 | loss: 3.194826 | lr:3.3808e-04 | norm 0.2931 | dt 338.92ms | 1546943.33 tokens/sec
Step 9720 | loss: 3.278493 | lr:3.3804e-04 | norm 0.2542 | dt 338.80ms | 1547463.68 tokens/sec
Step 9721 | loss: 3.234602 | lr:3.3799e-04 | norm 0.2744 | dt 337.83ms | 1551915.04 tokens/sec
Step 9722 | loss: 3.158197 | lr:3.3795e-04 | norm 0.2606 | dt 337.68ms | 1552604.24 tokens/sec
Step 9723 | loss: 3.139449 | lr:3.3790e-04 | norm 0.2561 | dt 338.27ms | 1549913.35 tokens/sec
Step 9724 | loss: 3.189897 | lr:3.3785e-04 | norm 0.2541 | dt 338.40ms | 1549309.48 tokens/sec
Step 9725 | loss: 3.223074 | lr:3.3781e-04 | norm 0.2676 | dt 338.35ms | 1549525.64 tokens/sec
Step 9726 | loss: 3.312479 | lr:3.3776e-04 | norm 0.3047 | dt 339.13ms | 1545983.02 tokens/sec
Step 9727 | loss: 3.229659 | lr:3.3772e-04 | norm 0.2806 | dt 338.10ms | 1550693.72 tokens/sec
Step 9728 | loss: 3.219452 | lr:3.3767e-04 | norm 0.2841 | dt 337.34ms | 1554181.09 tokens/sec
Step 9729 | loss: 3.193449 | lr:3.3762e-04 | norm 0.2736 | dt 338.55ms | 1548606.84 tokens/sec
Step 9730 | loss: 3.208845 | lr:3.3758e-04 | norm 0.2755 | dt 338.86ms | 1547229.58 tokens/sec
Step 9731 | loss: 3.156209 | lr:3.3753e-04 | norm 0.2691 | dt 338.00ms | 1551165.16 tokens/sec
Step 9732 | loss: 3.156457 | lr:3.3748e-04 | norm 0.3243 | dt 338.18ms | 1550324.20 tokens/sec
Step 9733 | loss: 3.326804 | lr:3.3744e-04 | norm 0.3338 | dt 338.97ms | 1546703.96 tokens/sec
Step 9734 | loss: 3.193416 | lr:3.3739e-04 | norm 0.3969 | dt 337.87ms | 1551731.06 tokens/sec
Step 9735 | loss: 3.216793 | lr:3.3735e-04 | norm 0.3060 | dt 338.87ms | 1547161.00 tokens/sec
Step 9736 | loss: 3.178400 | lr:3.3730e-04 | norm 0.3340 | dt 337.51ms | 1553398.30 tokens/sec
Step 9737 | loss: 3.210518 | lr:3.3725e-04 | norm 0.3224 | dt 338.72ms | 1547845.99 tokens/sec
Step 9738 | loss: 3.166291 | lr:3.3721e-04 | norm 0.2961 | dt 337.54ms | 1553255.66 tokens/sec
Step 9739 | loss: 3.166168 | lr:3.3716e-04 | norm 0.3044 | dt 337.80ms | 1552059.62 tokens/sec
Step 9740 | loss: 3.214173 | lr:3.3711e-04 | norm 0.3021 | dt 338.98ms | 1546644.12 tokens/sec
Step 9741 | loss: 3.160313 | lr:3.3707e-04 | norm 0.2699 | dt 338.02ms | 1551059.04 tokens/sec
Step 9742 | loss: 3.161139 | lr:3.3702e-04 | norm 0.3004 | dt 338.20ms | 1550229.12 tokens/sec
Step 9743 | loss: 3.210821 | lr:3.3698e-04 | norm 0.2938 | dt 336.91ms | 1556182.81 tokens/sec
Step 9744 | loss: 3.240755 | lr:3.3693e-04 | norm 0.2938 | dt 338.26ms | 1549971.25 tokens/sec
Step 9745 | loss: 3.260415 | lr:3.3688e-04 | norm 0.2954 | dt 337.61ms | 1552930.98 tokens/sec
Step 9746 | loss: 3.195043 | lr:3.3684e-04 | norm 0.2708 | dt 338.24ms | 1550053.19 tokens/sec
Step 9747 | loss: 3.174535 | lr:3.3679e-04 | norm 0.2860 | dt 337.91ms | 1551572.30 tokens/sec
Step 9748 | loss: 3.210992 | lr:3.3675e-04 | norm 0.2621 | dt 337.89ms | 1551648.94 tokens/sec
Step 9749 | loss: 3.211877 | lr:3.3670e-04 | norm 0.2998 | dt 337.60ms | 1553006.65 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 9750: 3.2038
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2920/10042=0.2908


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I just don't have a command line I'm working on. The reason I have to do that is I've a
rank 5 sample 1 >Hello, I'm a language model, am using a web class in an email, the computer, I did not live there and it's still here, I
rank 5 sample 2 >Hello, I'm a language model, and I do not expect to ever know how to do it. And in the meantime, I'm not going to learn
rank 5 sample 3 >Hello, I'm a language model, one of the leading models today, and it's one thing to make a little history in a specific way, and also





ddp_rank 3: ####### Printing generated samples ####### 


ddp_rank 2: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not an optimist at teaching.
If you want to introduce the notion of a fixed point value into
rank 2 sample 0 >Hello, I'm a language model, who are all so different.
This post was originally published on the University Of Thessaloniki. Reposted
rank 3 sample 1 >Hello, I'm a language model, so what I'm doing is going to make an assumption. I'll want to make a premise for my question that it
rank 2 sample 1 >Hello, I'm a language model, just like any good software. I mean most people take it seriously. We're all very different, so if you've
rank 3 sample 2 >Hello, I'm a language model, so this one is one of the best you can do with all the different dialects. If you want to create it
rank 3 sample 3 >Hello, I'm a language model, so don't bother me, and I'll tell you a story about it.
This story is full of language and


rank 2 sample 2 >Hello, I'm a language model, and I have an old friend that reads what I have for me to think is what I'm for the first time!
rank 2 sample 3 >Hello, I'm a language model, I need a lot of practice. I will give out my lessons and I like what the books look like.
A




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I was wondering how much I could achieve. I guess I could spend 15 on the site each month.
You
rank 7 sample 1 >Hello, I'm a language model, we get lots of errors. You write down the keywords about the sentence and then sort the text into those. You also
rank 7 sample 2 >Hello, I'm a language model, meaning I can see I'm here right.
P.S. To be honest, I'm not sure of this
rank 7 sample 3 >Hello, I'm a language model, or I'm a programming tutor, don't tell me how I'm done. I'm not doing the same thing because




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, i've been talking on twitter about that, but there is nothing for you and i'm thinking about it, i was
rank 6 sample 1 >Hello, I'm a language model, which is what I mean. In my program, there is a single object, a single word.
Now, this
rank 6 sample 2 >Hello, I'm a language model, I want to explain something. It's all my own. It's not hard to follow.
To do anything in
rank 6 sample 3 >Hello, I'm a language model, but what's it to me? I was wondering if I could even write my own code where I read them again like




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, with all my grammar and grammar patterns thrown together at me. Can you help me? Thanks for a great idea.

rank 1 sample 1 >Hello, I'm a language model, a computer science writer, and a teacher. I'm fascinated by everything I did in mathematics and chemistry, and I'm
rank 1 sample 2 >Hello, I'm a language model, I did my work in the past. I'm a teacher myself and my style is the same way that you use English
rank 1 sample 3 >Hello, I'm a language model, so I'm doing the basic question here…
...Can a class or do a web lab work with a single class




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know what it's supposed to be in.
- When one language is introduced, that's how it gets
rank 0 sample 1 >Hello, I'm a language model, and how do I know what's missing?<|endoftext|>To see a sample of your paper click here
There are many things
rank 0 sample 2 >Hello, I'm a language model, and I feel it's cool to use it all.
"You don't even have to be a native English speaker
rank 0 sample 3 >Hello, I'm a language model, I wrote a blog about it, I have friends who are already in college. Then you write about some of these books




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I love being a good teacher. What a different way to teach grammar like this with young children. And, of
rank 4 sample 1 >Hello, I'm a language model, please let me know, if i'd you help with a question,please, you have a question to answer below and
rank 4 sample 2 >Hello, I'm a language model, but did you realize you don't even understand it?
You have to learn grammar for math problems without knowing all of
rank 4 sample 3 >Hello, I'm a language model, so it made you really, really wonder, about some thing to do around it.
As long as I did,


Step 9750 | loss: 3.196226 | lr:3.3665e-04 | norm 0.2697 | dt 12575.27ms | 41691.99 tokens/sec
Step 9751 | loss: 3.227694 | lr:3.3661e-04 | norm 0.2675 | dt 334.41ms | 1567808.05 tokens/sec
Step 9752 | loss: 3.143686 | lr:3.3656e-04 | norm 0.2719 | dt 336.89ms | 1556272.02 tokens/sec
Step 9753 | loss: 3.249133 | lr:3.3651e-04 | norm 0.2819 | dt 336.03ms | 1560246.04 tokens/sec
Step 9754 | loss: 3.156612 | lr:3.3647e-04 | norm 0.2926 | dt 336.46ms | 1558238.28 tokens/sec
Step 9755 | loss: 3.179607 | lr:3.3642e-04 | norm 0.2709 | dt 335.89ms | 1560889.49 tokens/sec
Step 9756 | loss: 3.269912 | lr:3.3638e-04 | norm 0.2820 | dt 335.69ms | 1561831.80 tokens/sec
Step 9757 | loss: 3.178788 | lr:3.3633e-04 | norm 0.2695 | dt 336.44ms | 1558353.12 tokens/sec
Step 9758 | loss: 3.195990 | lr:3.3628e-04 | norm 0.2758 | dt 338.15ms | 1550457.56 tokens/sec
Step 9759 | loss: 3.216845 | lr:3.3624e-04 | norm 0.2681 | dt 336.05ms | 1560139.78 tokens/sec
Step 9760 | loss: 3.188158 | lr:3.3619e-04 | norm 0.2775 | dt 336.91ms | 1556169.60 tokens/sec
Step 9761 | loss: 3.182576 | lr:3.3614e-04 | norm 0.2557 | dt 337.58ms | 1553064.78 tokens/sec
Step 9762 | loss: 3.182668 | lr:3.3610e-04 | norm 0.2654 | dt 336.37ms | 1558685.60 tokens/sec
Step 9763 | loss: 3.178597 | lr:3.3605e-04 | norm 0.2736 | dt 336.29ms | 1559052.48 tokens/sec
Step 9764 | loss: 3.204320 | lr:3.3601e-04 | norm 0.2890 | dt 338.25ms | 1549984.36 tokens/sec
Step 9765 | loss: 3.208844 | lr:3.3596e-04 | norm 0.2643 | dt 337.49ms | 1553512.43 tokens/sec
Step 9766 | loss: 3.217200 | lr:3.3591e-04 | norm 0.2763 | dt 336.81ms | 1556611.32 tokens/sec
Step 9767 | loss: 3.238943 | lr:3.3587e-04 | norm 0.2632 | dt 337.52ms | 1553339.05 tokens/sec
Step 9768 | loss: 3.186143 | lr:3.3582e-04 | norm 0.2954 | dt 336.85ms | 1556439.45 tokens/sec
Step 9769 | loss: 3.166625 | lr:3.3578e-04 | norm 0.2835 | dt 337.28ms | 1554478.82 tokens/sec
Step 9770 | loss: 3.164272 | lr:3.3573e-04 | norm 0.2645 | dt 337.86ms | 1551795.66 tokens/sec
Step 9771 | loss: 3.282905 | lr:3.3568e-04 | norm 0.2577 | dt 337.85ms | 1551852.61 tokens/sec
Step 9772 | loss: 3.156738 | lr:3.3564e-04 | norm 0.2728 | dt 337.26ms | 1554550.25 tokens/sec
Step 9773 | loss: 3.154604 | lr:3.3559e-04 | norm 0.2781 | dt 337.71ms | 1552474.90 tokens/sec
Step 9774 | loss: 3.223404 | lr:3.3554e-04 | norm 0.3163 | dt 337.86ms | 1551789.09 tokens/sec
Step 9775 | loss: 3.192121 | lr:3.3550e-04 | norm 0.3301 | dt 336.92ms | 1556104.62 tokens/sec
Step 9776 | loss: 3.265133 | lr:3.3545e-04 | norm 0.3216 | dt 338.66ms | 1548135.85 tokens/sec
Step 9777 | loss: 3.251174 | lr:3.3541e-04 | norm 0.2973 | dt 337.53ms | 1553329.17 tokens/sec
Step 9778 | loss: 3.221102 | lr:3.3536e-04 | norm 0.2974 | dt 337.07ms | 1555429.91 tokens/sec
Step 9779 | loss: 3.199776 | lr:3.3531e-04 | norm 0.3006 | dt 337.83ms | 1551930.37 tokens/sec
Step 9780 | loss: 3.217325 | lr:3.3527e-04 | norm 0.2846 | dt 338.34ms | 1549597.70 tokens/sec
Step 9781 | loss: 3.237350 | lr:3.3522e-04 | norm 0.2841 | dt 337.33ms | 1554248.10 tokens/sec
Step 9782 | loss: 3.193907 | lr:3.3517e-04 | norm 0.2963 | dt 337.38ms | 1554015.24 tokens/sec
Step 9783 | loss: 3.181157 | lr:3.3513e-04 | norm 0.2561 | dt 337.49ms | 1553469.63 tokens/sec
Step 9784 | loss: 3.228479 | lr:3.3508e-04 | norm 0.2909 | dt 337.58ms | 1553092.21 tokens/sec
Step 9785 | loss: 3.193692 | lr:3.3504e-04 | norm 0.2786 | dt 337.30ms | 1554389.82 tokens/sec
Step 9786 | loss: 3.201701 | lr:3.3499e-04 | norm 0.2878 | dt 337.93ms | 1551447.51 tokens/sec
Step 9787 | loss: 3.219909 | lr:3.3494e-04 | norm 0.3709 | dt 337.79ms | 1552105.63 tokens/sec
Step 9788 | loss: 3.185412 | lr:3.3490e-04 | norm 0.4201 | dt 337.37ms | 1554026.22 tokens/sec
Step 9789 | loss: 3.203275 | lr:3.3485e-04 | norm 0.3142 | dt 338.37ms | 1549439.39 tokens/sec
Step 9790 | loss: 3.184527 | lr:3.3481e-04 | norm 0.3436 | dt 337.46ms | 1553630.97 tokens/sec
Step 9791 | loss: 3.198798 | lr:3.3476e-04 | norm 0.3285 | dt 337.32ms | 1554265.67 tokens/sec
Step 9792 | loss: 3.241412 | lr:3.3471e-04 | norm 0.2934 | dt 338.00ms | 1551170.64 tokens/sec
Step 9793 | loss: 3.171705 | lr:3.3467e-04 | norm 0.3320 | dt 337.55ms | 1553206.29 tokens/sec
Step 9794 | loss: 3.169332 | lr:3.3462e-04 | norm 0.2749 | dt 338.03ms | 1551010.90 tokens/sec
Step 9795 | loss: 3.184967 | lr:3.3457e-04 | norm 0.3016 | dt 337.64ms | 1552790.62 tokens/sec
Step 9796 | loss: 3.210503 | lr:3.3453e-04 | norm 0.2652 | dt 337.93ms | 1551472.69 tokens/sec
Step 9797 | loss: 3.210913 | lr:3.3448e-04 | norm 0.3061 | dt 337.63ms | 1552853.12 tokens/sec
Step 9798 | loss: 3.186385 | lr:3.3444e-04 | norm 0.2773 | dt 338.21ms | 1550195.24 tokens/sec
Step 9799 | loss: 3.212180 | lr:3.3439e-04 | norm 0.3051 | dt 337.74ms | 1552353.25 tokens/sec
Step 9800 | loss: 3.169247 | lr:3.3434e-04 | norm 0.2671 | dt 337.66ms | 1552725.93 tokens/sec
Step 9801 | loss: 3.227824 | lr:3.3430e-04 | norm 0.2711 | dt 338.14ms | 1550523.15 tokens/sec
Step 9802 | loss: 3.247691 | lr:3.3425e-04 | norm 0.3616 | dt 338.08ms | 1550762.61 tokens/sec
Step 9803 | loss: 3.157632 | lr:3.3420e-04 | norm 0.3249 | dt 338.64ms | 1548229.59 tokens/sec
Step 9804 | loss: 3.177703 | lr:3.3416e-04 | norm 0.2924 | dt 338.63ms | 1548274.28 tokens/sec
Step 9805 | loss: 3.136857 | lr:3.3411e-04 | norm 0.3134 | dt 338.05ms | 1550910.26 tokens/sec
Step 9806 | loss: 3.159924 | lr:3.3407e-04 | norm 0.2968 | dt 338.10ms | 1550667.47 tokens/sec
Step 9807 | loss: 3.227184 | lr:3.3402e-04 | norm 0.3175 | dt 338.23ms | 1550072.85 tokens/sec
Step 9808 | loss: 3.186138 | lr:3.3397e-04 | norm 0.2917 | dt 337.85ms | 1551841.66 tokens/sec
Step 9809 | loss: 3.225026 | lr:3.3393e-04 | norm 0.2829 | dt 338.84ms | 1547288.37 tokens/sec
Step 9810 | loss: 3.218710 | lr:3.3388e-04 | norm 0.2908 | dt 337.54ms | 1553273.22 tokens/sec
Step 9811 | loss: 3.173358 | lr:3.3383e-04 | norm 0.2695 | dt 337.60ms | 1553000.07 tokens/sec
Step 9812 | loss: 3.186095 | lr:3.3379e-04 | norm 0.2739 | dt 338.36ms | 1549516.90 tokens/sec
Step 9813 | loss: 3.166472 | lr:3.3374e-04 | norm 0.2762 | dt 337.68ms | 1552638.23 tokens/sec
Step 9814 | loss: 3.281325 | lr:3.3370e-04 | norm 0.3235 | dt 337.75ms | 1552274.35 tokens/sec
Step 9815 | loss: 3.228823 | lr:3.3365e-04 | norm 0.3022 | dt 337.49ms | 1553488.29 tokens/sec
Step 9816 | loss: 3.193494 | lr:3.3360e-04 | norm 0.2994 | dt 338.02ms | 1551054.66 tokens/sec
Step 9817 | loss: 3.233643 | lr:3.3356e-04 | norm 0.2831 | dt 338.87ms | 1547175.16 tokens/sec
Step 9818 | loss: 3.214658 | lr:3.3351e-04 | norm 0.3026 | dt 338.10ms | 1550667.47 tokens/sec
Step 9819 | loss: 3.158308 | lr:3.3347e-04 | norm 0.2857 | dt 337.47ms | 1553567.31 tokens/sec
Step 9820 | loss: 3.199954 | lr:3.3342e-04 | norm 0.2867 | dt 338.91ms | 1546962.92 tokens/sec
Step 9821 | loss: 3.190363 | lr:3.3337e-04 | norm 0.2787 | dt 338.54ms | 1548670.09 tokens/sec
Step 9822 | loss: 3.193573 | lr:3.3333e-04 | norm 0.2973 | dt 338.11ms | 1550621.55 tokens/sec
Step 9823 | loss: 3.209334 | lr:3.3328e-04 | norm 0.2899 | dt 337.47ms | 1553595.84 tokens/sec
Step 9824 | loss: 3.198691 | lr:3.3323e-04 | norm 0.2857 | dt 338.44ms | 1549122.85 tokens/sec
Step 9825 | loss: 3.162528 | lr:3.3319e-04 | norm 0.2877 | dt 337.31ms | 1554314.01 tokens/sec
Step 9826 | loss: 3.194800 | lr:3.3314e-04 | norm 0.2770 | dt 337.77ms | 1552211.90 tokens/sec
Step 9827 | loss: 3.243651 | lr:3.3310e-04 | norm 0.2680 | dt 901.08ms | 581841.00 tokens/sec
Step 9828 | loss: 3.177397 | lr:3.3305e-04 | norm 0.2827 | dt 334.00ms | 1569730.75 tokens/sec
Step 9829 | loss: 3.228257 | lr:3.3300e-04 | norm 0.2580 | dt 338.95ms | 1546802.96 tokens/sec
Step 9830 | loss: 3.166752 | lr:3.3296e-04 | norm 0.2670 | dt 338.12ms | 1550609.52 tokens/sec
Step 9831 | loss: 3.159928 | lr:3.3291e-04 | norm 0.2770 | dt 338.85ms | 1547270.95 tokens/sec
Step 9832 | loss: 3.203732 | lr:3.3286e-04 | norm 0.2863 | dt 337.28ms | 1554453.55 tokens/sec
Step 9833 | loss: 3.218246 | lr:3.3282e-04 | norm 0.2883 | dt 337.52ms | 1553339.05 tokens/sec
Step 9834 | loss: 3.102380 | lr:3.3277e-04 | norm 0.3114 | dt 337.95ms | 1551394.97 tokens/sec
Step 9835 | loss: 3.244575 | lr:3.3273e-04 | norm 0.3111 | dt 337.03ms | 1555604.86 tokens/sec
Step 9836 | loss: 3.231396 | lr:3.3268e-04 | norm 0.3290 | dt 337.69ms | 1552549.43 tokens/sec
Step 9837 | loss: 3.169359 | lr:3.3263e-04 | norm 0.2779 | dt 337.37ms | 1554041.60 tokens/sec
Step 9838 | loss: 3.163543 | lr:3.3259e-04 | norm 0.3046 | dt 337.27ms | 1554489.81 tokens/sec
Step 9839 | loss: 3.157440 | lr:3.3254e-04 | norm 0.2899 | dt 338.06ms | 1550857.76 tokens/sec
Step 9840 | loss: 3.266006 | lr:3.3250e-04 | norm 0.3152 | dt 338.67ms | 1548086.81 tokens/sec
Step 9841 | loss: 3.202366 | lr:3.3245e-04 | norm 0.2691 | dt 337.41ms | 1553879.08 tokens/sec
Step 9842 | loss: 3.158427 | lr:3.3240e-04 | norm 0.2687 | dt 337.83ms | 1551947.89 tokens/sec
Step 9843 | loss: 3.200640 | lr:3.3236e-04 | norm 0.2725 | dt 338.22ms | 1550139.51 tokens/sec
Step 9844 | loss: 3.231359 | lr:3.3231e-04 | norm 0.2732 | dt 337.74ms | 1552332.43 tokens/sec
Step 9845 | loss: 3.271694 | lr:3.3226e-04 | norm 0.2881 | dt 337.82ms | 1551988.42 tokens/sec
Step 9846 | loss: 3.142648 | lr:3.3222e-04 | norm 0.2971 | dt 338.70ms | 1547930.98 tokens/sec
Step 9847 | loss: 3.249690 | lr:3.3217e-04 | norm 0.2851 | dt 337.86ms | 1551781.43 tokens/sec
Step 9848 | loss: 3.194013 | lr:3.3213e-04 | norm 0.2960 | dt 337.65ms | 1552755.53 tokens/sec
Step 9849 | loss: 3.204423 | lr:3.3208e-04 | norm 0.2946 | dt 337.68ms | 1552605.34 tokens/sec
Step 9850 | loss: 3.138981 | lr:3.3203e-04 | norm 0.2782 | dt 338.34ms | 1549582.42 tokens/sec
Step 9851 | loss: 3.172855 | lr:3.3199e-04 | norm 0.3074 | dt 338.17ms | 1550358.08 tokens/sec
Step 9852 | loss: 3.170315 | lr:3.3194e-04 | norm 0.2965 | dt 337.62ms | 1552910.14 tokens/sec
Step 9853 | loss: 3.199103 | lr:3.3189e-04 | norm 0.2765 | dt 337.92ms | 1551514.28 tokens/sec
Step 9854 | loss: 3.209845 | lr:3.3185e-04 | norm 0.2962 | dt 337.70ms | 1552520.94 tokens/sec
Step 9855 | loss: 3.133737 | lr:3.3180e-04 | norm 0.2730 | dt 338.68ms | 1548035.59 tokens/sec
Step 9856 | loss: 3.150329 | lr:3.3176e-04 | norm 0.2810 | dt 338.62ms | 1548321.16 tokens/sec
Step 9857 | loss: 3.201568 | lr:3.3171e-04 | norm 0.2819 | dt 337.87ms | 1551748.58 tokens/sec
Step 9858 | loss: 3.179381 | lr:3.3166e-04 | norm 0.2718 | dt 338.50ms | 1548856.62 tokens/sec
Step 9859 | loss: 3.139010 | lr:3.3162e-04 | norm 0.2652 | dt 337.62ms | 1552915.63 tokens/sec
Step 9860 | loss: 3.220439 | lr:3.3157e-04 | norm 0.2886 | dt 338.30ms | 1549760.42 tokens/sec
Step 9861 | loss: 3.189671 | lr:3.3152e-04 | norm 0.2498 | dt 338.54ms | 1548693.00 tokens/sec
Step 9862 | loss: 3.160331 | lr:3.3148e-04 | norm 0.2732 | dt 337.69ms | 1552573.55 tokens/sec
Step 9863 | loss: 3.137286 | lr:3.3143e-04 | norm 0.2527 | dt 337.76ms | 1552262.30 tokens/sec
Step 9864 | loss: 3.186908 | lr:3.3139e-04 | norm 0.2789 | dt 337.88ms | 1551678.50 tokens/sec
Step 9865 | loss: 3.145543 | lr:3.3134e-04 | norm 0.3090 | dt 338.86ms | 1547233.94 tokens/sec
Step 9866 | loss: 3.219792 | lr:3.3129e-04 | norm 0.2867 | dt 338.99ms | 1546638.69 tokens/sec
Step 9867 | loss: 3.184323 | lr:3.3125e-04 | norm 0.2874 | dt 339.06ms | 1546301.54 tokens/sec
Step 9868 | loss: 3.185343 | lr:3.3120e-04 | norm 0.2806 | dt 338.42ms | 1549219.98 tokens/sec
Step 9869 | loss: 3.179426 | lr:3.3116e-04 | norm 0.2814 | dt 338.58ms | 1548509.78 tokens/sec
Step 9870 | loss: 3.189288 | lr:3.3111e-04 | norm 0.3165 | dt 338.90ms | 1547020.60 tokens/sec
Step 9871 | loss: 3.196194 | lr:3.3106e-04 | norm 0.2701 | dt 338.96ms | 1546744.21 tokens/sec
Step 9872 | loss: 3.134058 | lr:3.3102e-04 | norm 0.2866 | dt 338.39ms | 1549361.88 tokens/sec
Step 9873 | loss: 3.265074 | lr:3.3097e-04 | norm 0.2764 | dt 337.94ms | 1551428.90 tokens/sec
Step 9874 | loss: 3.198307 | lr:3.3092e-04 | norm 0.2774 | dt 338.54ms | 1548659.18 tokens/sec
Step 9875 | loss: 3.177801 | lr:3.3088e-04 | norm 0.2842 | dt 338.13ms | 1550532.99 tokens/sec
Step 9876 | loss: 3.163102 | lr:3.3083e-04 | norm 0.2826 | dt 338.26ms | 1549943.93 tokens/sec
Step 9877 | loss: 3.150455 | lr:3.3079e-04 | norm 0.3033 | dt 338.31ms | 1549739.67 tokens/sec
Step 9878 | loss: 3.167190 | lr:3.3074e-04 | norm 0.2865 | dt 338.01ms | 1551086.39 tokens/sec
Step 9879 | loss: 3.204719 | lr:3.3069e-04 | norm 0.3342 | dt 1033.77ms | 507160.07 tokens/sec
Step 9880 | loss: 3.142911 | lr:3.3065e-04 | norm 0.2749 | dt 337.28ms | 1554477.72 tokens/sec
Step 9881 | loss: 3.159365 | lr:3.3060e-04 | norm 0.2666 | dt 337.51ms | 1553410.37 tokens/sec
Step 9882 | loss: 3.219327 | lr:3.3055e-04 | norm 0.2720 | dt 338.52ms | 1548768.26 tokens/sec
Step 9883 | loss: 3.224229 | lr:3.3051e-04 | norm 0.2722 | dt 337.24ms | 1554642.57 tokens/sec
Step 9884 | loss: 3.150287 | lr:3.3046e-04 | norm 0.2627 | dt 338.62ms | 1548309.17 tokens/sec
Step 9885 | loss: 3.219126 | lr:3.3042e-04 | norm 0.2661 | dt 338.30ms | 1549787.73 tokens/sec
Step 9886 | loss: 3.241167 | lr:3.3037e-04 | norm 0.2870 | dt 338.37ms | 1549465.59 tokens/sec
Step 9887 | loss: 3.176842 | lr:3.3032e-04 | norm 0.2624 | dt 337.70ms | 1552534.09 tokens/sec
Step 9888 | loss: 3.174817 | lr:3.3028e-04 | norm 0.2725 | dt 337.52ms | 1553355.51 tokens/sec
Step 9889 | loss: 3.205971 | lr:3.3023e-04 | norm 0.3123 | dt 337.93ms | 1551467.21 tokens/sec
Step 9890 | loss: 3.222125 | lr:3.3018e-04 | norm 0.3071 | dt 338.18ms | 1550343.87 tokens/sec
Step 9891 | loss: 3.174052 | lr:3.3014e-04 | norm 0.2743 | dt 338.63ms | 1548248.12 tokens/sec
Step 9892 | loss: 3.195958 | lr:3.3009e-04 | norm 0.3212 | dt 338.05ms | 1550932.14 tokens/sec
Step 9893 | loss: 3.163647 | lr:3.3005e-04 | norm 0.2759 | dt 338.33ms | 1549646.84 tokens/sec
Step 9894 | loss: 3.175717 | lr:3.3000e-04 | norm 0.2674 | dt 338.13ms | 1550554.86 tokens/sec
Step 9895 | loss: 3.184161 | lr:3.2995e-04 | norm 0.2853 | dt 337.59ms | 1553047.24 tokens/sec
Step 9896 | loss: 3.156328 | lr:3.2991e-04 | norm 0.2652 | dt 338.40ms | 1549324.76 tokens/sec
Step 9897 | loss: 3.154584 | lr:3.2986e-04 | norm 0.2698 | dt 338.24ms | 1550045.54 tokens/sec
Step 9898 | loss: 3.124774 | lr:3.2982e-04 | norm 0.3026 | dt 337.54ms | 1553279.80 tokens/sec
Step 9899 | loss: 3.231825 | lr:3.2977e-04 | norm 0.2921 | dt 338.30ms | 1549762.61 tokens/sec
Step 9900 | loss: 3.194814 | lr:3.2972e-04 | norm 0.3023 | dt 338.53ms | 1548714.81 tokens/sec
Step 9901 | loss: 3.211194 | lr:3.2968e-04 | norm 0.2897 | dt 339.07ms | 1546261.31 tokens/sec
Step 9902 | loss: 3.229724 | lr:3.2963e-04 | norm 0.3046 | dt 338.91ms | 1546977.06 tokens/sec
Step 9903 | loss: 3.241126 | lr:3.2958e-04 | norm 0.2883 | dt 338.78ms | 1547578.02 tokens/sec
Step 9904 | loss: 3.173237 | lr:3.2954e-04 | norm 0.3124 | dt 338.86ms | 1547229.58 tokens/sec
Step 9905 | loss: 3.155157 | lr:3.2949e-04 | norm 0.2856 | dt 338.28ms | 1549879.48 tokens/sec
Step 9906 | loss: 3.253693 | lr:3.2945e-04 | norm 0.4449 | dt 337.93ms | 1551475.97 tokens/sec
Step 9907 | loss: 3.204167 | lr:3.2940e-04 | norm 0.3624 | dt 337.97ms | 1551301.95 tokens/sec
Step 9908 | loss: 3.184606 | lr:3.2935e-04 | norm 0.3274 | dt 337.82ms | 1551957.75 tokens/sec
Step 9909 | loss: 3.240852 | lr:3.2931e-04 | norm 0.3281 | dt 338.14ms | 1550495.82 tokens/sec
Step 9910 | loss: 3.207911 | lr:3.2926e-04 | norm 0.3056 | dt 338.18ms | 1550303.43 tokens/sec
Step 9911 | loss: 3.205932 | lr:3.2921e-04 | norm 0.3007 | dt 337.74ms | 1552319.28 tokens/sec
Step 9912 | loss: 3.202081 | lr:3.2917e-04 | norm 0.3005 | dt 337.41ms | 1553880.18 tokens/sec
Step 9913 | loss: 3.190139 | lr:3.2912e-04 | norm 0.2764 | dt 338.24ms | 1550042.26 tokens/sec
Step 9914 | loss: 3.187905 | lr:3.2908e-04 | norm 0.3136 | dt 338.10ms | 1550692.63 tokens/sec
Step 9915 | loss: 3.197726 | lr:3.2903e-04 | norm 0.2749 | dt 337.96ms | 1551319.46 tokens/sec
Step 9916 | loss: 3.232657 | lr:3.2898e-04 | norm 0.3149 | dt 337.49ms | 1553511.33 tokens/sec
Step 9917 | loss: 3.204501 | lr:3.2894e-04 | norm 0.2969 | dt 338.22ms | 1550118.75 tokens/sec
Step 9918 | loss: 3.220704 | lr:3.2889e-04 | norm 0.2658 | dt 337.52ms | 1553332.46 tokens/sec
Step 9919 | loss: 3.213172 | lr:3.2884e-04 | norm 0.2795 | dt 337.71ms | 1552461.75 tokens/sec
Step 9920 | loss: 3.185688 | lr:3.2880e-04 | norm 0.2739 | dt 337.52ms | 1553362.09 tokens/sec
Step 9921 | loss: 3.134746 | lr:3.2875e-04 | norm 0.2494 | dt 338.53ms | 1548731.17 tokens/sec
Step 9922 | loss: 3.212184 | lr:3.2871e-04 | norm 0.2731 | dt 338.01ms | 1551079.82 tokens/sec
Step 9923 | loss: 3.179134 | lr:3.2866e-04 | norm 0.2585 | dt 337.60ms | 1553007.75 tokens/sec
Step 9924 | loss: 3.178094 | lr:3.2861e-04 | norm 0.2679 | dt 338.30ms | 1549776.81 tokens/sec
Step 9925 | loss: 3.205684 | lr:3.2857e-04 | norm 0.2729 | dt 337.31ms | 1554310.71 tokens/sec
Step 9926 | loss: 3.183318 | lr:3.2852e-04 | norm 0.2974 | dt 337.54ms | 1553246.88 tokens/sec
Step 9927 | loss: 3.129818 | lr:3.2848e-04 | norm 0.2651 | dt 337.76ms | 1552254.63 tokens/sec
Step 9928 | loss: 3.138652 | lr:3.2843e-04 | norm 0.2702 | dt 338.20ms | 1550223.65 tokens/sec
Step 9929 | loss: 3.178795 | lr:3.2838e-04 | norm 0.2977 | dt 337.76ms | 1552234.91 tokens/sec
Step 9930 | loss: 3.163889 | lr:3.2834e-04 | norm 0.2576 | dt 337.72ms | 1552416.81 tokens/sec
Step 9931 | loss: 3.136069 | lr:3.2829e-04 | norm 0.2985 | dt 337.88ms | 1551712.44 tokens/sec
Step 9932 | loss: 3.142334 | lr:3.2824e-04 | norm 0.2595 | dt 337.65ms | 1552768.69 tokens/sec
Step 9933 | loss: 3.166539 | lr:3.2820e-04 | norm 0.2693 | dt 337.67ms | 1552672.21 tokens/sec
Step 9934 | loss: 3.195370 | lr:3.2815e-04 | norm 0.2694 | dt 337.09ms | 1555337.50 tokens/sec
Step 9935 | loss: 3.175679 | lr:3.2811e-04 | norm 0.2671 | dt 338.40ms | 1549335.68 tokens/sec
Step 9936 | loss: 3.237068 | lr:3.2806e-04 | norm 0.2769 | dt 338.21ms | 1550183.22 tokens/sec
Step 9937 | loss: 3.205260 | lr:3.2801e-04 | norm 0.2878 | dt 337.66ms | 1552699.62 tokens/sec
Step 9938 | loss: 3.187939 | lr:3.2797e-04 | norm 0.3046 | dt 337.42ms | 1553828.57 tokens/sec
Step 9939 | loss: 3.202118 | lr:3.2792e-04 | norm 0.2687 | dt 338.34ms | 1549592.24 tokens/sec
Step 9940 | loss: 3.195340 | lr:3.2787e-04 | norm 0.2908 | dt 337.04ms | 1555568.55 tokens/sec
Step 9941 | loss: 3.174562 | lr:3.2783e-04 | norm 0.2774 | dt 337.80ms | 1552055.24 tokens/sec
Step 9942 | loss: 3.151779 | lr:3.2778e-04 | norm 0.2971 | dt 338.57ms | 1548557.76 tokens/sec
Step 9943 | loss: 3.120328 | lr:3.2774e-04 | norm 0.3139 | dt 337.59ms | 1553019.82 tokens/sec
Step 9944 | loss: 3.303407 | lr:3.2769e-04 | norm 0.3050 | dt 337.47ms | 1553583.77 tokens/sec
Step 9945 | loss: 3.176510 | lr:3.2764e-04 | norm 0.2925 | dt 337.71ms | 1552484.77 tokens/sec
Step 9946 | loss: 3.156729 | lr:3.2760e-04 | norm 0.2807 | dt 337.98ms | 1551254.89 tokens/sec
Step 9947 | loss: 3.196233 | lr:3.2755e-04 | norm 0.2741 | dt 337.02ms | 1555676.39 tokens/sec
Step 9948 | loss: 3.189327 | lr:3.2750e-04 | norm 0.2790 | dt 337.46ms | 1553640.85 tokens/sec
Step 9949 | loss: 3.161155 | lr:3.2746e-04 | norm 0.2970 | dt 337.95ms | 1551394.97 tokens/sec
Step 9950 | loss: 3.226919 | lr:3.2741e-04 | norm 0.3198 | dt 338.00ms | 1551144.38 tokens/sec
Step 9951 | loss: 3.196187 | lr:3.2737e-04 | norm 0.2692 | dt 337.30ms | 1554367.84 tokens/sec
Step 9952 | loss: 3.193112 | lr:3.2732e-04 | norm 0.3100 | dt 338.38ms | 1549428.47 tokens/sec
Step 9953 | loss: 3.179334 | lr:3.2727e-04 | norm 0.2921 | dt 337.02ms | 1555660.99 tokens/sec
Step 9954 | loss: 3.239390 | lr:3.2723e-04 | norm 0.3061 | dt 337.40ms | 1553913.12 tokens/sec
Step 9955 | loss: 3.215342 | lr:3.2718e-04 | norm 0.2889 | dt 338.41ms | 1549257.09 tokens/sec
Step 9956 | loss: 3.152239 | lr:3.2714e-04 | norm 0.2605 | dt 337.44ms | 1553719.88 tokens/sec
Step 9957 | loss: 3.178493 | lr:3.2709e-04 | norm 0.2978 | dt 337.69ms | 1552572.45 tokens/sec
Step 9958 | loss: 3.171245 | lr:3.2704e-04 | norm 0.2854 | dt 338.08ms | 1550769.17 tokens/sec
Step 9959 | loss: 3.200778 | lr:3.2700e-04 | norm 0.2822 | dt 337.96ms | 1551343.54 tokens/sec
Step 9960 | loss: 3.154066 | lr:3.2695e-04 | norm 0.3025 | dt 337.62ms | 1552905.76 tokens/sec
Step 9961 | loss: 3.121469 | lr:3.2690e-04 | norm 0.2694 | dt 337.87ms | 1551755.15 tokens/sec
Step 9962 | loss: 3.250613 | lr:3.2686e-04 | norm 0.2976 | dt 337.89ms | 1551658.79 tokens/sec
Step 9963 | loss: 3.208680 | lr:3.2681e-04 | norm 0.2834 | dt 337.53ms | 1553328.07 tokens/sec
Step 9964 | loss: 3.136389 | lr:3.2677e-04 | norm 0.2744 | dt 338.52ms | 1548743.17 tokens/sec
Step 9965 | loss: 3.212522 | lr:3.2672e-04 | norm 0.2710 | dt 338.40ms | 1549317.12 tokens/sec
Step 9966 | loss: 3.191230 | lr:3.2667e-04 | norm 0.3049 | dt 336.75ms | 1556923.21 tokens/sec
Step 9967 | loss: 3.167761 | lr:3.2663e-04 | norm 0.2700 | dt 338.65ms | 1548148.93 tokens/sec
Step 9968 | loss: 3.204499 | lr:3.2658e-04 | norm 0.2705 | dt 337.35ms | 1554126.17 tokens/sec
Step 9969 | loss: 3.244417 | lr:3.2653e-04 | norm 0.3187 | dt 337.59ms | 1553016.52 tokens/sec
Step 9970 | loss: 3.220661 | lr:3.2649e-04 | norm 0.2923 | dt 338.48ms | 1548932.98 tokens/sec
Step 9971 | loss: 3.225166 | lr:3.2644e-04 | norm 0.2823 | dt 337.97ms | 1551276.78 tokens/sec
Step 9972 | loss: 3.194217 | lr:3.2640e-04 | norm 0.3071 | dt 337.42ms | 1553803.32 tokens/sec
Step 9973 | loss: 3.167516 | lr:3.2635e-04 | norm 0.3018 | dt 338.30ms | 1549789.91 tokens/sec
Step 9974 | loss: 3.192671 | lr:3.2630e-04 | norm 0.2916 | dt 337.62ms | 1552875.05 tokens/sec
Step 9975 | loss: 3.218266 | lr:3.2626e-04 | norm 0.3063 | dt 337.40ms | 1553926.29 tokens/sec
Step 9976 | loss: 3.205802 | lr:3.2621e-04 | norm 0.2970 | dt 338.95ms | 1546821.46 tokens/sec
Step 9977 | loss: 3.237711 | lr:3.2617e-04 | norm 0.2868 | dt 337.92ms | 1551521.95 tokens/sec
Step 9978 | loss: 3.219283 | lr:3.2612e-04 | norm 0.3450 | dt 337.68ms | 1552619.59 tokens/sec
Step 9979 | loss: 3.197436 | lr:3.2607e-04 | norm 0.2738 | dt 337.80ms | 1552055.24 tokens/sec
Step 9980 | loss: 3.237655 | lr:3.2603e-04 | norm 0.2962 | dt 338.13ms | 1550572.35 tokens/sec
Step 9981 | loss: 3.181384 | lr:3.2598e-04 | norm 0.2946 | dt 337.90ms | 1551604.05 tokens/sec
Step 9982 | loss: 3.248047 | lr:3.2593e-04 | norm 0.2685 | dt 336.88ms | 1556302.86 tokens/sec
Step 9983 | loss: 3.159188 | lr:3.2589e-04 | norm 0.2908 | dt 338.30ms | 1549782.27 tokens/sec
Step 9984 | loss: 3.165447 | lr:3.2584e-04 | norm 0.3120 | dt 337.96ms | 1551309.61 tokens/sec
Step 9985 | loss: 3.199932 | lr:3.2580e-04 | norm 0.2869 | dt 338.15ms | 1550448.81 tokens/sec
Step 9986 | loss: 3.277522 | lr:3.2575e-04 | norm 0.2985 | dt 337.90ms | 1551592.01 tokens/sec
Step 9987 | loss: 3.222843 | lr:3.2570e-04 | norm 0.2948 | dt 338.10ms | 1550702.47 tokens/sec
Step 9988 | loss: 3.186945 | lr:3.2566e-04 | norm 0.2790 | dt 338.11ms | 1550658.73 tokens/sec
Step 9989 | loss: 3.167581 | lr:3.2561e-04 | norm 0.2681 | dt 338.03ms | 1551005.43 tokens/sec
Step 9990 | loss: 3.240678 | lr:3.2556e-04 | norm 0.3050 | dt 337.72ms | 1552424.49 tokens/sec
Step 9991 | loss: 3.203918 | lr:3.2552e-04 | norm 0.2859 | dt 338.34ms | 1549596.61 tokens/sec
Step 9992 | loss: 3.145039 | lr:3.2547e-04 | norm 0.2821 | dt 337.75ms | 1552295.17 tokens/sec
Step 9993 | loss: 3.172422 | lr:3.2543e-04 | norm 0.2686 | dt 337.39ms | 1553973.51 tokens/sec
Step 9994 | loss: 3.174177 | lr:3.2538e-04 | norm 0.2686 | dt 338.09ms | 1550741.83 tokens/sec
Step 9995 | loss: 3.146876 | lr:3.2533e-04 | norm 0.2692 | dt 337.41ms | 1553871.39 tokens/sec
Step 9996 | loss: 3.193243 | lr:3.2529e-04 | norm 0.2811 | dt 338.27ms | 1549912.25 tokens/sec
Step 9997 | loss: 3.131508 | lr:3.2524e-04 | norm 0.2782 | dt 338.04ms | 1550948.55 tokens/sec
Step 9998 | loss: 3.137862 | lr:3.2519e-04 | norm 0.2723 | dt 338.09ms | 1550739.65 tokens/sec
Step 9999 | loss: 3.152169 | lr:3.2515e-04 | norm 0.2693 | dt 337.94ms | 1551403.73 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 10000: 3.2001
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2899/10042=0.2887


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but that is not my thing. I see it now as a kind of a 'super' machine. It's the
rank 5 sample 1 >Hello, I'm a language model, one whose vocabulary has nothing to do with language that is an entirely different way to the English. You just are speaking English
rank 5 sample 2 >Hello, I'm a language model, and you'll find good examples. My favorite thing is that you can choose how to get the best out of it to
rank 5 sample 3 >Hello, I'm a language model, thanks.<|endoftext|>Laws to be adopted in 2015 are to be updated to meet consumer requirements and encourage children to learn a




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I can't see any.
That is a good example of the many issues, too. But there's also
rank 4 sample 1 >Hello, I'm a language model, using all the technologies available and in touch and communication rather than the typical traditional methods such as writing, talking or computer.
rank 4 sample 2 >Hello, I'm a language model, I always wonder exactly when i was writing the same thing. If you're writing or listening to something while listening (like
rank 4 sample 3 >Hello, I'm a language model, so this analogy should be applied to many aspects of what you actually use: the concept of the sentence.
As I




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I guess I mean "language model"—in the sense that I was interested in using one without having a language.
rank 7 sample 1 >Hello, I'm a language model, well with English. I had my say as a third-year student, and one day as a child in my neighborhood
rank 7 sample 2 >Hello, I'm a language model, but I don't want to. (Hey, hey, I'm so confused, but I'm pretty sure my language
rank 7 sample 3 >Hello, I'm a language model, am you?
What did you make last?
What do I get?
Hi, if I'm really into




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, someone else, we're really just gonna talk to you and we're gonna be happy.
So, here's your
rank 2 sample 1 >Hello, I'm a language model, but I wasn't thinking about it myself since I understood it in an educational context. I thought that there were some issues
rank 2 sample 2 >Hello, I'm a language model, and I've had a lot of feedback on how I actually felt and looked like myself.
"Hello, I want
rank 2 sample 3 >Hello, I'm a language model, but a grammar teacher and I'm in Year 4 when we do this. Thank you, Emma, for giving us an




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, that I remember from my early childhood writing lessons on how to code. I'd like to see some of the following code
rank 1 sample 1 >Hello, I'm a language model, a person that works with a language to produce a speech sounds, i.e., voice. Sounds can be produced by
rank 1 sample 2 >Hello, I'm a language model, but just want to make sure that I'm using the most useful languages for my career. So what's a Language model
rank 1 sample 3 >Hello, I'm a language model, so I'm really trying to help you create a strong syntax with how to find the exact end points of the lines.




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I am going to start a project but want to look at another object or a series of lines and draw a circle
rank 0 sample 1 >Hello, I'm a language model, and when I'm done, the picture is like. "And it means a little, little bit; that's it
rank 0 sample 2 >Hello, I'm a language model, and I believe you already know that many of what I'm thinking and talking about is a little bit different from the idea
rank 0 sample 3 >Hello, I'm a language model, but no one else. I've read many on the web, but I don't like all of them. So for




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not familiar or comfortable with all of them. To make things comfortable for your student, take the opportunity to
rank 3 sample 1 >Hello, I'm a language model, so if someone asks me to show you a sentence, it's not OK. I'm a good English teacher with lots
rank 3 sample 2 >Hello, I'm a language model, so it doesn't count as grammar. As grammar is the process of creating new patterns, and each pattern has unique parts
rank 3 sample 3 >Hello, I'm a language model, so now I can look at an example. I want this example to look like, but then I make it the same




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model,
- I'm the program editor
- I'm the compiler
- I'm the editor
- The compiler makes
rank 6 sample 1 >Hello, I'm a language model, not a philosophy. You have to be open minded in your day to go to the next stage of the project.

rank 6 sample 2 >Hello, I'm a language model, but this time, I'll go a little farther.
The following table displays the contents of the class (see Figure
rank 6 sample 3 >Hello, I'm a language model, so what I want to do is make sure you understand the language you're speaking, but only if you speak it first


Step 10000 | loss: 3.147463 | lr:3.2510e-04 | norm 0.2823 | dt 18221.22ms | 28773.49 tokens/sec
Step 10001 | loss: 3.140687 | lr:3.2506e-04 | norm 0.2867 | dt 335.28ms | 1563747.65 tokens/sec
Step 10002 | loss: 3.181844 | lr:3.2501e-04 | norm 0.3015 | dt 334.85ms | 1565754.04 tokens/sec
Step 10003 | loss: 3.242470 | lr:3.2496e-04 | norm 0.2875 | dt 336.40ms | 1558546.40 tokens/sec
Step 10004 | loss: 3.206005 | lr:3.2492e-04 | norm 0.3116 | dt 337.04ms | 1555580.65 tokens/sec
Step 10005 | loss: 3.207323 | lr:3.2487e-04 | norm 0.3061 | dt 336.40ms | 1558547.51 tokens/sec
Step 10006 | loss: 3.160261 | lr:3.2483e-04 | norm 0.4275 | dt 336.04ms | 1560206.19 tokens/sec
Step 10007 | loss: 3.170962 | lr:3.2478e-04 | norm 0.4116 | dt 335.60ms | 1562226.80 tokens/sec
Step 10008 | loss: 3.132353 | lr:3.2473e-04 | norm 0.3171 | dt 335.93ms | 1560718.89 tokens/sec
Step 10009 | loss: 3.175043 | lr:3.2469e-04 | norm 0.3381 | dt 335.65ms | 1561994.88 tokens/sec
Step 10010 | loss: 3.204991 | lr:3.2464e-04 | norm 0.3073 | dt 334.97ms | 1565192.35 tokens/sec
Step 10011 | loss: 3.198721 | lr:3.2459e-04 | norm 0.3052 | dt 336.24ms | 1559256.99 tokens/sec
Step 10012 | loss: 3.218546 | lr:3.2455e-04 | norm 0.2838 | dt 335.56ms | 1562434.37 tokens/sec
Step 10013 | loss: 3.236151 | lr:3.2450e-04 | norm 0.3068 | dt 335.04ms | 1564830.37 tokens/sec
Step 10014 | loss: 3.218507 | lr:3.2446e-04 | norm 0.3012 | dt 336.78ms | 1556764.50 tokens/sec
Step 10015 | loss: 3.198702 | lr:3.2441e-04 | norm 0.2635 | dt 336.67ms | 1557279.34 tokens/sec
Step 10016 | loss: 3.224808 | lr:3.2436e-04 | norm 0.2813 | dt 902.23ms | 581104.67 tokens/sec
Step 10017 | loss: 3.179878 | lr:3.2432e-04 | norm 0.2737 | dt 333.78ms | 1570774.65 tokens/sec
Step 10018 | loss: 3.221852 | lr:3.2427e-04 | norm 0.2836 | dt 336.28ms | 1559095.59 tokens/sec
Step 10019 | loss: 3.126086 | lr:3.2422e-04 | norm 0.2585 | dt 338.23ms | 1550093.61 tokens/sec
Step 10020 | loss: 3.141319 | lr:3.2418e-04 | norm 0.2778 | dt 337.67ms | 1552675.50 tokens/sec
Step 10021 | loss: 3.174489 | lr:3.2413e-04 | norm 0.3310 | dt 336.34ms | 1558796.09 tokens/sec
Step 10022 | loss: 3.176436 | lr:3.2409e-04 | norm 0.2750 | dt 337.30ms | 1554387.62 tokens/sec
Step 10023 | loss: 3.156779 | lr:3.2404e-04 | norm 0.2712 | dt 336.99ms | 1555794.16 tokens/sec
Step 10024 | loss: 3.194062 | lr:3.2399e-04 | norm 0.2881 | dt 336.57ms | 1557751.49 tokens/sec
Step 10025 | loss: 3.225652 | lr:3.2395e-04 | norm 0.2634 | dt 338.80ms | 1547470.21 tokens/sec
Step 10026 | loss: 3.199853 | lr:3.2390e-04 | norm 0.2892 | dt 337.26ms | 1554530.47 tokens/sec
Step 10027 | loss: 3.209094 | lr:3.2386e-04 | norm 0.2663 | dt 337.15ms | 1555063.63 tokens/sec
Step 10028 | loss: 3.173975 | lr:3.2381e-04 | norm 0.2595 | dt 338.54ms | 1548652.64 tokens/sec
Step 10029 | loss: 3.161609 | lr:3.2376e-04 | norm 0.2724 | dt 338.07ms | 1550809.64 tokens/sec
Step 10030 | loss: 3.210671 | lr:3.2372e-04 | norm 0.2708 | dt 337.16ms | 1555024.04 tokens/sec
Step 10031 | loss: 3.132574 | lr:3.2367e-04 | norm 0.2650 | dt 338.04ms | 1550971.52 tokens/sec
Step 10032 | loss: 3.167808 | lr:3.2362e-04 | norm 0.2717 | dt 338.62ms | 1548323.34 tokens/sec
Step 10033 | loss: 3.192869 | lr:3.2358e-04 | norm 0.2525 | dt 337.88ms | 1551719.01 tokens/sec
Step 10034 | loss: 3.123286 | lr:3.2353e-04 | norm 0.2591 | dt 338.93ms | 1546909.59 tokens/sec
Step 10035 | loss: 3.164887 | lr:3.2349e-04 | norm 0.2710 | dt 339.52ms | 1544188.50 tokens/sec
Step 10036 | loss: 3.185889 | lr:3.2344e-04 | norm 0.2800 | dt 338.19ms | 1550279.39 tokens/sec
Step 10037 | loss: 3.248972 | lr:3.2339e-04 | norm 0.2929 | dt 338.20ms | 1550246.60 tokens/sec
Step 10038 | loss: 3.305203 | lr:3.2335e-04 | norm 0.3094 | dt 337.90ms | 1551605.15 tokens/sec
Step 10039 | loss: 3.223161 | lr:3.2330e-04 | norm 0.2900 | dt 338.90ms | 1547018.42 tokens/sec
Step 10040 | loss: 3.152961 | lr:3.2325e-04 | norm 0.2946 | dt 339.43ms | 1544603.92 tokens/sec
Step 10041 | loss: 3.169999 | lr:3.2321e-04 | norm 0.2805 | dt 338.24ms | 1550046.63 tokens/sec
Step 10042 | loss: 3.217155 | lr:3.2316e-04 | norm 0.3010 | dt 338.33ms | 1549641.38 tokens/sec
Step 10043 | loss: 3.178872 | lr:3.2312e-04 | norm 0.3042 | dt 337.88ms | 1551714.63 tokens/sec
Step 10044 | loss: 3.175068 | lr:3.2307e-04 | norm 0.2996 | dt 337.46ms | 1553621.09 tokens/sec
Step 10045 | loss: 3.195998 | lr:3.2302e-04 | norm 0.2929 | dt 340.99ms | 1537551.63 tokens/sec
Step 10046 | loss: 3.177271 | lr:3.2298e-04 | norm 0.3097 | dt 337.39ms | 1553961.43 tokens/sec
Step 10047 | loss: 3.209195 | lr:3.2293e-04 | norm 0.2980 | dt 338.26ms | 1549936.29 tokens/sec
Step 10048 | loss: 3.257977 | lr:3.2289e-04 | norm 0.2917 | dt 339.65ms | 1543627.01 tokens/sec
Step 10049 | loss: 3.210649 | lr:3.2284e-04 | norm 0.3072 | dt 338.72ms | 1547852.53 tokens/sec
Step 10050 | loss: 3.144813 | lr:3.2279e-04 | norm 0.2719 | dt 338.87ms | 1547157.74 tokens/sec
Step 10051 | loss: 3.189501 | lr:3.2275e-04 | norm 0.3065 | dt 339.23ms | 1545525.58 tokens/sec
Step 10052 | loss: 3.153507 | lr:3.2270e-04 | norm 0.2819 | dt 338.58ms | 1548470.53 tokens/sec
Step 10053 | loss: 3.151179 | lr:3.2265e-04 | norm 0.2690 | dt 339.48ms | 1544405.40 tokens/sec
Step 10054 | loss: 3.155088 | lr:3.2261e-04 | norm 0.2738 | dt 337.93ms | 1551447.51 tokens/sec
Step 10055 | loss: 3.214736 | lr:3.2256e-04 | norm 0.3013 | dt 337.94ms | 1551442.04 tokens/sec
Step 10056 | loss: 3.205816 | lr:3.2252e-04 | norm 0.2969 | dt 338.62ms | 1548305.90 tokens/sec
Step 10057 | loss: 3.187303 | lr:3.2247e-04 | norm 0.2680 | dt 338.22ms | 1550132.95 tokens/sec
Step 10058 | loss: 3.177455 | lr:3.2242e-04 | norm 0.2626 | dt 337.86ms | 1551796.76 tokens/sec
Step 10059 | loss: 3.121602 | lr:3.2238e-04 | norm 0.2784 | dt 336.54ms | 1557880.61 tokens/sec
Step 10060 | loss: 3.132331 | lr:3.2233e-04 | norm 0.3222 | dt 339.11ms | 1546054.76 tokens/sec
Step 10061 | loss: 3.210598 | lr:3.2228e-04 | norm 0.2978 | dt 338.48ms | 1548927.53 tokens/sec
Step 10062 | loss: 3.135100 | lr:3.2224e-04 | norm 0.2812 | dt 337.42ms | 1553832.96 tokens/sec
Step 10063 | loss: 3.128813 | lr:3.2219e-04 | norm 0.2931 | dt 337.61ms | 1552935.37 tokens/sec
Step 10064 | loss: 3.180513 | lr:3.2215e-04 | norm 0.2628 | dt 338.47ms | 1548979.90 tokens/sec
Step 10065 | loss: 3.197745 | lr:3.2210e-04 | norm 0.2763 | dt 337.95ms | 1551397.16 tokens/sec
Step 10066 | loss: 3.136251 | lr:3.2205e-04 | norm 0.2696 | dt 338.60ms | 1548387.66 tokens/sec
Step 10067 | loss: 3.173886 | lr:3.2201e-04 | norm 0.3110 | dt 338.31ms | 1549727.66 tokens/sec
Step 10068 | loss: 3.194946 | lr:3.2196e-04 | norm 0.2884 | dt 338.33ms | 1549642.48 tokens/sec
Step 10069 | loss: 3.136375 | lr:3.2192e-04 | norm 0.2910 | dt 951.06ms | 551264.76 tokens/sec
Step 10070 | loss: 3.067421 | lr:3.2187e-04 | norm 0.2520 | dt 337.02ms | 1555678.60 tokens/sec
Step 10071 | loss: 3.116854 | lr:3.2182e-04 | norm 0.3065 | dt 338.82ms | 1547376.56 tokens/sec
Step 10072 | loss: 3.204023 | lr:3.2178e-04 | norm 0.2649 | dt 337.31ms | 1554327.19 tokens/sec
Step 10073 | loss: 3.125181 | lr:3.2173e-04 | norm 0.3137 | dt 337.40ms | 1553912.02 tokens/sec
Step 10074 | loss: 3.182832 | lr:3.2168e-04 | norm 0.2707 | dt 337.42ms | 1553801.12 tokens/sec
Step 10075 | loss: 3.239441 | lr:3.2164e-04 | norm 0.3514 | dt 336.58ms | 1557693.01 tokens/sec
Step 10076 | loss: 3.130695 | lr:3.2159e-04 | norm 0.3320 | dt 336.79ms | 1556699.47 tokens/sec
Step 10077 | loss: 3.188291 | lr:3.2155e-04 | norm 0.2955 | dt 337.77ms | 1552194.37 tokens/sec
Step 10078 | loss: 3.176206 | lr:3.2150e-04 | norm 0.3185 | dt 337.95ms | 1551367.61 tokens/sec
Step 10079 | loss: 3.190597 | lr:3.2145e-04 | norm 0.3313 | dt 337.29ms | 1554416.19 tokens/sec
Step 10080 | loss: 3.133439 | lr:3.2141e-04 | norm 0.2982 | dt 337.03ms | 1555613.67 tokens/sec
Step 10081 | loss: 3.200398 | lr:3.2136e-04 | norm 0.3190 | dt 337.78ms | 1552172.46 tokens/sec
Step 10082 | loss: 3.271325 | lr:3.2131e-04 | norm 0.3159 | dt 337.38ms | 1554016.34 tokens/sec
Step 10083 | loss: 3.215274 | lr:3.2127e-04 | norm 0.3187 | dt 337.36ms | 1554099.81 tokens/sec
Step 10084 | loss: 3.172629 | lr:3.2122e-04 | norm 0.3170 | dt 338.41ms | 1549276.73 tokens/sec
Step 10085 | loss: 3.189055 | lr:3.2118e-04 | norm 0.3260 | dt 337.64ms | 1552809.26 tokens/sec
Step 10086 | loss: 3.177530 | lr:3.2113e-04 | norm 0.3095 | dt 337.30ms | 1554348.07 tokens/sec
Step 10087 | loss: 3.249534 | lr:3.2108e-04 | norm 0.2942 | dt 338.30ms | 1549760.42 tokens/sec
Step 10088 | loss: 3.214059 | lr:3.2104e-04 | norm 0.3239 | dt 337.83ms | 1551939.13 tokens/sec
Step 10089 | loss: 3.247353 | lr:3.2099e-04 | norm 0.3136 | dt 337.63ms | 1552845.44 tokens/sec
Step 10090 | loss: 3.207486 | lr:3.2095e-04 | norm 0.2926 | dt 338.59ms | 1548448.72 tokens/sec
Step 10091 | loss: 3.289583 | lr:3.2090e-04 | norm 0.3137 | dt 338.10ms | 1550689.34 tokens/sec
Step 10092 | loss: 3.278752 | lr:3.2085e-04 | norm 0.2988 | dt 337.27ms | 1554524.97 tokens/sec
Step 10093 | loss: 3.256791 | lr:3.2081e-04 | norm 0.3046 | dt 338.72ms | 1547836.19 tokens/sec
Step 10094 | loss: 3.181803 | lr:3.2076e-04 | norm 0.2999 | dt 337.52ms | 1553369.77 tokens/sec
Step 10095 | loss: 3.197510 | lr:3.2071e-04 | norm 0.2792 | dt 337.74ms | 1552340.10 tokens/sec
Step 10096 | loss: 3.189458 | lr:3.2067e-04 | norm 0.2880 | dt 339.14ms | 1545930.85 tokens/sec
Step 10097 | loss: 3.171089 | lr:3.2062e-04 | norm 0.2814 | dt 338.67ms | 1548075.91 tokens/sec
Step 10098 | loss: 3.141992 | lr:3.2058e-04 | norm 0.2809 | dt 337.69ms | 1552584.51 tokens/sec
Step 10099 | loss: 3.153701 | lr:3.2053e-04 | norm 0.2727 | dt 338.55ms | 1548640.64 tokens/sec
Step 10100 | loss: 3.269073 | lr:3.2048e-04 | norm 0.2840 | dt 338.54ms | 1548688.63 tokens/sec
Step 10101 | loss: 3.167624 | lr:3.2044e-04 | norm 0.2640 | dt 338.41ms | 1549282.19 tokens/sec
Step 10102 | loss: 3.228145 | lr:3.2039e-04 | norm 0.2789 | dt 338.25ms | 1550009.48 tokens/sec
Step 10103 | loss: 3.153267 | lr:3.2035e-04 | norm 0.2625 | dt 338.04ms | 1550974.80 tokens/sec
Step 10104 | loss: 3.181416 | lr:3.2030e-04 | norm 0.2705 | dt 338.88ms | 1547117.46 tokens/sec
Step 10105 | loss: 3.160765 | lr:3.2025e-04 | norm 0.2610 | dt 336.76ms | 1556855.97 tokens/sec
Step 10106 | loss: 3.194025 | lr:3.2021e-04 | norm 0.2888 | dt 338.33ms | 1549618.45 tokens/sec
Step 10107 | loss: 3.165823 | lr:3.2016e-04 | norm 0.2643 | dt 338.76ms | 1547685.85 tokens/sec
Step 10108 | loss: 3.204965 | lr:3.2011e-04 | norm 0.2555 | dt 337.61ms | 1552943.04 tokens/sec
Step 10109 | loss: 3.189987 | lr:3.2007e-04 | norm 0.2726 | dt 338.06ms | 1550861.04 tokens/sec
Step 10110 | loss: 3.216378 | lr:3.2002e-04 | norm 0.2697 | dt 337.57ms | 1553142.67 tokens/sec
Step 10111 | loss: 3.140936 | lr:3.1998e-04 | norm 0.2568 | dt 337.64ms | 1552793.91 tokens/sec
Step 10112 | loss: 3.169413 | lr:3.1993e-04 | norm 0.2658 | dt 338.29ms | 1549810.67 tokens/sec
Step 10113 | loss: 3.163014 | lr:3.1988e-04 | norm 0.2828 | dt 337.50ms | 1553447.68 tokens/sec
Step 10114 | loss: 3.202515 | lr:3.1984e-04 | norm 0.2796 | dt 337.11ms | 1555260.50 tokens/sec
Step 10115 | loss: 3.196795 | lr:3.1979e-04 | norm 0.2966 | dt 338.33ms | 1549628.28 tokens/sec
Step 10116 | loss: 3.099235 | lr:3.1974e-04 | norm 0.2836 | dt 337.89ms | 1551635.80 tokens/sec
Step 10117 | loss: 3.154263 | lr:3.1970e-04 | norm 0.3025 | dt 337.52ms | 1553333.56 tokens/sec
Step 10118 | loss: 3.210013 | lr:3.1965e-04 | norm 0.2752 | dt 338.33ms | 1549638.11 tokens/sec
Step 10119 | loss: 3.141486 | lr:3.1961e-04 | norm 0.3044 | dt 338.30ms | 1549760.42 tokens/sec
Step 10120 | loss: 3.212330 | lr:3.1956e-04 | norm 0.2734 | dt 340.73ms | 1538722.18 tokens/sec
Step 10121 | loss: 3.192099 | lr:3.1951e-04 | norm 0.2801 | dt 337.60ms | 1552983.62 tokens/sec
Step 10122 | loss: 3.202163 | lr:3.1947e-04 | norm 0.3019 | dt 337.76ms | 1552245.87 tokens/sec
Step 10123 | loss: 3.186962 | lr:3.1942e-04 | norm 0.3120 | dt 337.50ms | 1553448.78 tokens/sec
Step 10124 | loss: 3.189932 | lr:3.1938e-04 | norm 0.3376 | dt 338.57ms | 1548555.58 tokens/sec
Step 10125 | loss: 3.178103 | lr:3.1933e-04 | norm 0.2938 | dt 337.51ms | 1553391.72 tokens/sec
Step 10126 | loss: 3.143772 | lr:3.1928e-04 | norm 0.3129 | dt 338.68ms | 1548013.79 tokens/sec
Step 10127 | loss: 3.235622 | lr:3.1924e-04 | norm 0.2957 | dt 337.71ms | 1552476.00 tokens/sec
Step 10128 | loss: 3.230244 | lr:3.1919e-04 | norm 0.2849 | dt 339.28ms | 1545305.11 tokens/sec
Step 10129 | loss: 3.129371 | lr:3.1914e-04 | norm 0.3325 | dt 337.81ms | 1552010.33 tokens/sec
Step 10130 | loss: 3.188520 | lr:3.1910e-04 | norm 0.2675 | dt 338.73ms | 1547816.58 tokens/sec
Step 10131 | loss: 3.180149 | lr:3.1905e-04 | norm 0.2881 | dt 337.55ms | 1553221.65 tokens/sec
Step 10132 | loss: 3.184631 | lr:3.1901e-04 | norm 0.2780 | dt 338.78ms | 1547555.15 tokens/sec
Step 10133 | loss: 3.180873 | lr:3.1896e-04 | norm 0.2743 | dt 338.08ms | 1550792.14 tokens/sec
Step 10134 | loss: 3.094302 | lr:3.1891e-04 | norm 0.2621 | dt 337.60ms | 1553000.07 tokens/sec
Step 10135 | loss: 3.219375 | lr:3.1887e-04 | norm 0.2790 | dt 338.29ms | 1549815.04 tokens/sec
Step 10136 | loss: 3.151212 | lr:3.1882e-04 | norm 0.2606 | dt 338.37ms | 1549456.85 tokens/sec
Step 10137 | loss: 3.208891 | lr:3.1878e-04 | norm 0.2846 | dt 338.49ms | 1548887.16 tokens/sec
Step 10138 | loss: 3.158530 | lr:3.1873e-04 | norm 0.2596 | dt 338.01ms | 1551085.29 tokens/sec
Step 10139 | loss: 3.185637 | lr:3.1868e-04 | norm 0.2657 | dt 338.24ms | 1550055.37 tokens/sec
Step 10140 | loss: 3.130212 | lr:3.1864e-04 | norm 0.2655 | dt 338.06ms | 1550850.11 tokens/sec
Step 10141 | loss: 3.149629 | lr:3.1859e-04 | norm 0.2786 | dt 337.78ms | 1552139.59 tokens/sec
Step 10142 | loss: 3.181854 | lr:3.1854e-04 | norm 0.2877 | dt 337.93ms | 1551488.01 tokens/sec
Step 10143 | loss: 3.256171 | lr:3.1850e-04 | norm 0.2836 | dt 338.16ms | 1550423.67 tokens/sec
Step 10144 | loss: 3.194255 | lr:3.1845e-04 | norm 0.2870 | dt 337.87ms | 1551766.10 tokens/sec
Step 10145 | loss: 3.171954 | lr:3.1841e-04 | norm 0.2869 | dt 337.46ms | 1553645.24 tokens/sec
Step 10146 | loss: 3.212368 | lr:3.1836e-04 | norm 0.3111 | dt 338.92ms | 1546948.77 tokens/sec
Step 10147 | loss: 3.165887 | lr:3.1831e-04 | norm 0.2879 | dt 338.39ms | 1549379.34 tokens/sec
Step 10148 | loss: 3.182127 | lr:3.1827e-04 | norm 0.2934 | dt 337.90ms | 1551608.43 tokens/sec
Step 10149 | loss: 3.204352 | lr:3.1822e-04 | norm 0.2764 | dt 338.03ms | 1550995.59 tokens/sec
Step 10150 | loss: 3.189592 | lr:3.1818e-04 | norm 0.3032 | dt 337.88ms | 1551717.92 tokens/sec
Step 10151 | loss: 3.165902 | lr:3.1813e-04 | norm 0.2805 | dt 338.31ms | 1549715.65 tokens/sec
Step 10152 | loss: 3.186350 | lr:3.1808e-04 | norm 0.3091 | dt 338.29ms | 1549800.84 tokens/sec
Step 10153 | loss: 3.204894 | lr:3.1804e-04 | norm 0.2754 | dt 338.00ms | 1551154.22 tokens/sec
Step 10154 | loss: 3.244114 | lr:3.1799e-04 | norm 0.3408 | dt 338.60ms | 1548380.03 tokens/sec
Step 10155 | loss: 3.204449 | lr:3.1794e-04 | norm 0.3505 | dt 338.24ms | 1550066.30 tokens/sec
Step 10156 | loss: 3.176744 | lr:3.1790e-04 | norm 0.3229 | dt 337.81ms | 1552022.38 tokens/sec
Step 10157 | loss: 3.252315 | lr:3.1785e-04 | norm 0.3312 | dt 338.22ms | 1550152.62 tokens/sec
Step 10158 | loss: 3.212541 | lr:3.1781e-04 | norm 0.3260 | dt 338.27ms | 1549893.68 tokens/sec
Step 10159 | loss: 3.242213 | lr:3.1776e-04 | norm 0.3177 | dt 338.05ms | 1550924.48 tokens/sec
Step 10160 | loss: 3.199631 | lr:3.1771e-04 | norm 0.3455 | dt 337.95ms | 1551397.16 tokens/sec
Step 10161 | loss: 3.264640 | lr:3.1767e-04 | norm 0.3071 | dt 337.82ms | 1551964.32 tokens/sec
Step 10162 | loss: 3.194402 | lr:3.1762e-04 | norm 0.3071 | dt 338.04ms | 1550943.08 tokens/sec
Step 10163 | loss: 3.226514 | lr:3.1758e-04 | norm 0.3024 | dt 337.55ms | 1553229.33 tokens/sec
Step 10164 | loss: 3.252870 | lr:3.1753e-04 | norm 0.3167 | dt 337.62ms | 1552910.14 tokens/sec
Step 10165 | loss: 3.142321 | lr:3.1748e-04 | norm 0.2852 | dt 338.19ms | 1550287.04 tokens/sec
Step 10166 | loss: 3.133575 | lr:3.1744e-04 | norm 0.2858 | dt 337.69ms | 1552563.68 tokens/sec
Step 10167 | loss: 3.220857 | lr:3.1739e-04 | norm 0.2941 | dt 337.98ms | 1551239.57 tokens/sec
Step 10168 | loss: 3.194449 | lr:3.1734e-04 | norm 0.2912 | dt 338.94ms | 1546845.39 tokens/sec
Step 10169 | loss: 3.187942 | lr:3.1730e-04 | norm 0.2975 | dt 337.10ms | 1555296.80 tokens/sec
Step 10170 | loss: 3.175081 | lr:3.1725e-04 | norm 0.2822 | dt 337.75ms | 1552283.12 tokens/sec
Step 10171 | loss: 3.201301 | lr:3.1721e-04 | norm 0.2654 | dt 338.75ms | 1547720.71 tokens/sec
Step 10172 | loss: 3.154338 | lr:3.1716e-04 | norm 0.2841 | dt 338.15ms | 1550453.18 tokens/sec
Step 10173 | loss: 3.199753 | lr:3.1711e-04 | norm 0.2635 | dt 338.70ms | 1547946.23 tokens/sec
Step 10174 | loss: 3.208979 | lr:3.1707e-04 | norm 0.2606 | dt 337.13ms | 1555150.51 tokens/sec
Step 10175 | loss: 3.168212 | lr:3.1702e-04 | norm 0.2792 | dt 338.50ms | 1548876.25 tokens/sec
Step 10176 | loss: 3.217126 | lr:3.1698e-04 | norm 0.2533 | dt 337.98ms | 1551223.16 tokens/sec
Step 10177 | loss: 3.197365 | lr:3.1693e-04 | norm 0.2928 | dt 338.72ms | 1547856.89 tokens/sec
Step 10178 | loss: 3.193536 | lr:3.1688e-04 | norm 0.3040 | dt 337.59ms | 1553022.01 tokens/sec
Step 10179 | loss: 3.197933 | lr:3.1684e-04 | norm 0.2650 | dt 337.90ms | 1551617.19 tokens/sec
Step 10180 | loss: 3.135149 | lr:3.1679e-04 | norm 0.2870 | dt 338.40ms | 1549298.56 tokens/sec
Step 10181 | loss: 3.166407 | lr:3.1674e-04 | norm 0.2869 | dt 338.05ms | 1550915.73 tokens/sec
Step 10182 | loss: 3.190704 | lr:3.1670e-04 | norm 0.2992 | dt 338.01ms | 1551098.42 tokens/sec
Step 10183 | loss: 3.187619 | lr:3.1665e-04 | norm 0.2670 | dt 337.69ms | 1552583.41 tokens/sec
Step 10184 | loss: 3.084630 | lr:3.1661e-04 | norm 0.5767 | dt 338.60ms | 1548411.65 tokens/sec
Step 10185 | loss: 3.184091 | lr:3.1656e-04 | norm 0.3164 | dt 338.45ms | 1549091.20 tokens/sec
Step 10186 | loss: 3.189951 | lr:3.1651e-04 | norm 0.2950 | dt 338.31ms | 1549747.32 tokens/sec
Step 10187 | loss: 3.229662 | lr:3.1647e-04 | norm 0.3030 | dt 338.54ms | 1548652.64 tokens/sec
Step 10188 | loss: 3.180473 | lr:3.1642e-04 | norm 0.2919 | dt 338.01ms | 1551091.86 tokens/sec
Step 10189 | loss: 3.178928 | lr:3.1638e-04 | norm 0.2834 | dt 338.70ms | 1547948.41 tokens/sec
Step 10190 | loss: 3.207636 | lr:3.1633e-04 | norm 0.2638 | dt 338.10ms | 1550701.37 tokens/sec
Step 10191 | loss: 3.221174 | lr:3.1628e-04 | norm 0.2738 | dt 339.67ms | 1543528.41 tokens/sec
Step 10192 | loss: 3.225713 | lr:3.1624e-04 | norm 0.2835 | dt 338.07ms | 1550838.08 tokens/sec
Step 10193 | loss: 3.194557 | lr:3.1619e-04 | norm 0.3582 | dt 339.14ms | 1545942.81 tokens/sec
Step 10194 | loss: 3.190976 | lr:3.1614e-04 | norm 0.2986 | dt 338.82ms | 1547402.70 tokens/sec
Step 10195 | loss: 3.237211 | lr:3.1610e-04 | norm 0.2973 | dt 338.69ms | 1547981.10 tokens/sec
Step 10196 | loss: 3.185002 | lr:3.1605e-04 | norm 0.5575 | dt 338.79ms | 1547531.19 tokens/sec
Step 10197 | loss: 3.244807 | lr:3.1601e-04 | norm 0.3282 | dt 337.98ms | 1551218.78 tokens/sec
Step 10198 | loss: 3.199572 | lr:3.1596e-04 | norm 0.3085 | dt 338.89ms | 1547081.55 tokens/sec
Step 10199 | loss: 3.173135 | lr:3.1591e-04 | norm 0.3135 | dt 338.07ms | 1550822.76 tokens/sec
Step 10200 | loss: 3.178025 | lr:3.1587e-04 | norm 0.2852 | dt 338.43ms | 1549167.59 tokens/sec
Step 10201 | loss: 3.146021 | lr:3.1582e-04 | norm 0.2900 | dt 339.23ms | 1545503.86 tokens/sec
Step 10202 | loss: 3.170919 | lr:3.1578e-04 | norm 0.2915 | dt 337.82ms | 1551961.04 tokens/sec
Step 10203 | loss: 3.166976 | lr:3.1573e-04 | norm 0.2861 | dt 339.71ms | 1543349.67 tokens/sec
Step 10204 | loss: 3.165912 | lr:3.1568e-04 | norm 0.3016 | dt 337.80ms | 1552072.76 tokens/sec
Step 10205 | loss: 3.159572 | lr:3.1564e-04 | norm 0.3177 | dt 899.05ms | 583155.92 tokens/sec
Step 10206 | loss: 3.143393 | lr:3.1559e-04 | norm 0.2722 | dt 336.48ms | 1558172.03 tokens/sec
Step 10207 | loss: 3.181662 | lr:3.1554e-04 | norm 0.2856 | dt 339.60ms | 1543834.00 tokens/sec
Step 10208 | loss: 3.177315 | lr:3.1550e-04 | norm 0.2610 | dt 338.89ms | 1547081.55 tokens/sec
Step 10209 | loss: 3.218832 | lr:3.1545e-04 | norm 0.2894 | dt 337.68ms | 1552598.76 tokens/sec
Step 10210 | loss: 3.180803 | lr:3.1541e-04 | norm 0.2694 | dt 338.80ms | 1547462.59 tokens/sec
Step 10211 | loss: 3.161026 | lr:3.1536e-04 | norm 0.2522 | dt 338.66ms | 1548118.42 tokens/sec
Step 10212 | loss: 3.173017 | lr:3.1531e-04 | norm 0.2893 | dt 337.69ms | 1552587.80 tokens/sec
Step 10213 | loss: 3.241026 | lr:3.1527e-04 | norm 0.2705 | dt 337.81ms | 1552014.71 tokens/sec
Step 10214 | loss: 3.251488 | lr:3.1522e-04 | norm 0.3106 | dt 338.12ms | 1550583.28 tokens/sec
Step 10215 | loss: 3.177145 | lr:3.1518e-04 | norm 0.2646 | dt 338.37ms | 1549452.49 tokens/sec
Step 10216 | loss: 3.133571 | lr:3.1513e-04 | norm 0.2621 | dt 337.79ms | 1552115.49 tokens/sec
Step 10217 | loss: 3.183428 | lr:3.1508e-04 | norm 0.2794 | dt 338.22ms | 1550125.30 tokens/sec
Step 10218 | loss: 3.215920 | lr:3.1504e-04 | norm 0.3324 | dt 338.87ms | 1547169.71 tokens/sec
Step 10219 | loss: 3.151590 | lr:3.1499e-04 | norm 0.2924 | dt 339.03ms | 1546430.94 tokens/sec
Step 10220 | loss: 3.214402 | lr:3.1495e-04 | norm 0.2743 | dt 338.15ms | 1550471.77 tokens/sec
Step 10221 | loss: 3.135187 | lr:3.1490e-04 | norm 0.2620 | dt 338.35ms | 1549562.76 tokens/sec
Step 10222 | loss: 3.134893 | lr:3.1485e-04 | norm 0.2724 | dt 338.75ms | 1547721.80 tokens/sec
Step 10223 | loss: 3.201435 | lr:3.1481e-04 | norm 0.2708 | dt 338.71ms | 1547887.40 tokens/sec
Step 10224 | loss: 3.200453 | lr:3.1476e-04 | norm 0.2839 | dt 338.82ms | 1547402.70 tokens/sec
Step 10225 | loss: 3.235687 | lr:3.1471e-04 | norm 0.2745 | dt 338.77ms | 1547618.32 tokens/sec
Step 10226 | loss: 3.226848 | lr:3.1467e-04 | norm 0.3124 | dt 340.01ms | 1541965.53 tokens/sec
Step 10227 | loss: 3.196134 | lr:3.1462e-04 | norm 0.3045 | dt 339.47ms | 1544452.05 tokens/sec
Step 10228 | loss: 3.219581 | lr:3.1458e-04 | norm 0.2815 | dt 338.72ms | 1547853.62 tokens/sec
Step 10229 | loss: 3.197051 | lr:3.1453e-04 | norm 0.2909 | dt 338.86ms | 1547213.26 tokens/sec
Step 10230 | loss: 3.203366 | lr:3.1448e-04 | norm 0.2701 | dt 339.05ms | 1546325.46 tokens/sec
Step 10231 | loss: 3.235965 | lr:3.1444e-04 | norm 0.2770 | dt 339.58ms | 1543919.63 tokens/sec
Step 10232 | loss: 3.192900 | lr:3.1439e-04 | norm 0.2594 | dt 338.66ms | 1548129.31 tokens/sec
Step 10233 | loss: 3.208318 | lr:3.1435e-04 | norm 0.2621 | dt 339.40ms | 1544763.42 tokens/sec
Step 10234 | loss: 3.181694 | lr:3.1430e-04 | norm 0.2814 | dt 338.03ms | 1550987.93 tokens/sec
Step 10235 | loss: 3.224159 | lr:3.1425e-04 | norm 0.2715 | dt 337.54ms | 1553257.86 tokens/sec
Step 10236 | loss: 3.200052 | lr:3.1421e-04 | norm 0.2883 | dt 338.70ms | 1547941.88 tokens/sec
Step 10237 | loss: 3.135569 | lr:3.1416e-04 | norm 0.2889 | dt 338.02ms | 1551045.91 tokens/sec
Step 10238 | loss: 3.192580 | lr:3.1411e-04 | norm 0.2722 | dt 338.98ms | 1546675.67 tokens/sec
Step 10239 | loss: 3.218764 | lr:3.1407e-04 | norm 0.2849 | dt 337.86ms | 1551800.04 tokens/sec
Step 10240 | loss: 3.123647 | lr:3.1402e-04 | norm 0.2786 | dt 338.53ms | 1548707.17 tokens/sec
Step 10241 | loss: 3.176725 | lr:3.1398e-04 | norm 0.2653 | dt 337.92ms | 1551503.34 tokens/sec
Step 10242 | loss: 3.183223 | lr:3.1393e-04 | norm 0.2633 | dt 339.50ms | 1544302.37 tokens/sec
Step 10243 | loss: 3.187425 | lr:3.1388e-04 | norm 0.2572 | dt 339.44ms | 1544551.85 tokens/sec
Step 10244 | loss: 3.197664 | lr:3.1384e-04 | norm 0.2686 | dt 338.82ms | 1547398.34 tokens/sec
Step 10245 | loss: 3.197218 | lr:3.1379e-04 | norm 0.2663 | dt 339.74ms | 1543225.11 tokens/sec
Step 10246 | loss: 3.179660 | lr:3.1375e-04 | norm 0.2477 | dt 338.39ms | 1549369.52 tokens/sec
Step 10247 | loss: 3.171192 | lr:3.1370e-04 | norm 0.3080 | dt 338.00ms | 1551134.53 tokens/sec
Step 10248 | loss: 3.181441 | lr:3.1365e-04 | norm 0.2834 | dt 337.70ms | 1552530.80 tokens/sec
Step 10249 | loss: 3.211697 | lr:3.1361e-04 | norm 0.3101 | dt 338.60ms | 1548407.29 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 10250: 3.1935
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2889/10042=0.2877


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but not as good as a child's mind, nor is it so bad as the kid's.
So far this
rank 5 sample 1 >Hello, I'm a language model, is really not the whole picture here, I wanted to use some techniques. So the next time you try writing something like
rank 5 sample 2 >Hello, I'm a language model, and it says, Hello, the compiler is to be able to handle all of that!
And it's a way
rank 5 sample 3 >Hello, I'm a language model, in the sense that I understand how to write and speak in a natural language. However in the US there is no native




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, no doubt, in my opinion. A good way to learn it is by doing your own research and writing exercises. This
rank 2 sample 1 >Hello, I'm a language model, but I used this model in my project here. "Hello," they wrote, "Hello, how to use this file


ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 2 >Hello, I'm a language model, and I've never had to think very hard about it. If you say 'this is really complicated', then you know
rank 2 sample 3 >Hello, I'm a language model, but that still leaves a lot of room for newbie programmers!
I recently came up with two new, powerful software


rank 7 sample 0 >Hello, I'm a language model, so I wanted to show them how to construct a class and how to apply it effectively. We wrote this program in a
rank 7 sample 1 >Hello, I'm a language model, well--what's my job to give you? -by:<|endoftext|>The History of the Jews
On June 10,
rank 7 sample 2 >Hello, I'm a language model, but I don't want to start on that. Here's the gist of the problem.
I am trying to solve
rank 7 sample 3 >Hello, I'm a language model, and it's a bit intimidating to come up with a basic "Hello, World!" statement. Let's say you are




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where all the languages work perfectly. You have to write your code quickly, with only a few lines of input. Once
rank 6 sample 1 >Hello, I'm a language model, so I need to learn it a bit too. If you're doing any of my other projects or doing something, I
rank 6 sample 2 >Hello, I'm a language model, but this isn't one I can learn myself...
I can also use another language model, but here comes out a
rank 6 sample 3 >Hello, I'm a language model, so if you want to know which languages are most useful for you, you have to look first at a lot of documents




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I love it!
I love what it looks like!
Here's a fun math games board:
I
rank 4 sample 1 >Hello, I'm a language model, can we say all about it as one and if I didn't, and what did I say? (We always said
rank 4 sample 2 >Hello, I'm a language model, I didn't talk about all the words and then I did an analysis of the differences between the English sentence structure versus the
rank 4 sample 3 >Hello, I'm a language model, so here guys and everyone, I try to understand me correctly--the English-speaking world. There are a lot of




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I think it is worth getting in tune with you. I love helping students understand and connect language, and I hope
rank 0 sample 1 >Hello, I'm a language model, and have only a few lines left, so you shouldn't be worried. You're a very nice kid!
The
rank 0 sample 2 >Hello, I'm a language model, and I find it too obvious that we need one that is as intuitive as possible. So I'm going to try and
rank 0 sample 3 >Hello, I'm a language model, but here's what I mean: we have so many different rules that we need to use, so a lot of work




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not the person I say who you say "Yeah, why?". But this sentence is one that's in
rank 3 sample 1 >Hello, I'm a language model, so it was a great course that I got into. It's really interesting to me that I have a certain language.
rank 3 sample 2 >Hello, I'm a language model, so you'll need a set of tools to program your code:
# language -- language library and language environment- language
rank 3 sample 3 >Hello, I'm a language model, so if you can't take care of the mechanics then there are lots of other projects like that. Now I know what




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so if I'm getting the message we need (the email's message) - I can use -u.
I
rank 1 sample 1 >Hello, I'm a language model, a computer scientist and a computer scientist. Since I'm my first language model, it works best if I'm a computer
rank 1 sample 2 >Hello, I'm a language model, but sometimes, in the real world, I'm not so quick, but I'm a bit rusty and a little geek
rank 1 sample 3 >Hello, I'm a language model, so I'm thinking of building a framework inside my head first.<|endoftext|>By Laura Karpes
The question: How


Step 10250 | loss: 3.160627 | lr:3.1356e-04 | norm 0.3124 | dt 18586.40ms | 28208.15 tokens/sec
Step 10251 | loss: 3.154562 | lr:3.1352e-04 | norm 0.3509 | dt 334.63ms | 1566791.54 tokens/sec
Step 10252 | loss: 3.250058 | lr:3.1347e-04 | norm 0.3499 | dt 335.36ms | 1563347.43 tokens/sec
Step 10253 | loss: 3.150159 | lr:3.1342e-04 | norm 0.3206 | dt 336.13ms | 1559765.74 tokens/sec
Step 10254 | loss: 3.171956 | lr:3.1338e-04 | norm 0.3025 | dt 337.14ms | 1555087.82 tokens/sec
Step 10255 | loss: 3.204403 | lr:3.1333e-04 | norm 0.2675 | dt 335.69ms | 1561807.40 tokens/sec
Step 10256 | loss: 3.184535 | lr:3.1328e-04 | norm 0.2979 | dt 336.41ms | 1558498.91 tokens/sec
Step 10257 | loss: 3.177428 | lr:3.1324e-04 | norm 0.2903 | dt 337.48ms | 1553549.75 tokens/sec
Step 10258 | loss: 3.186254 | lr:3.1319e-04 | norm 0.2993 | dt 336.93ms | 1556053.97 tokens/sec
Step 10259 | loss: 3.211730 | lr:3.1315e-04 | norm 0.3143 | dt 998.78ms | 524929.91 tokens/sec
Step 10260 | loss: 3.176299 | lr:3.1310e-04 | norm 0.2836 | dt 335.95ms | 1560624.74 tokens/sec
Step 10261 | loss: 3.168700 | lr:3.1305e-04 | norm 0.2945 | dt 337.59ms | 1553019.82 tokens/sec
Step 10262 | loss: 3.224988 | lr:3.1301e-04 | norm 0.3015 | dt 337.57ms | 1553140.47 tokens/sec
Step 10263 | loss: 3.175027 | lr:3.1296e-04 | norm 0.2906 | dt 337.09ms | 1555354.00 tokens/sec
Step 10264 | loss: 3.226418 | lr:3.1292e-04 | norm 0.2741 | dt 337.32ms | 1554265.67 tokens/sec
Step 10265 | loss: 3.188344 | lr:3.1287e-04 | norm 0.2811 | dt 337.82ms | 1551957.75 tokens/sec
Step 10266 | loss: 3.268365 | lr:3.1282e-04 | norm 0.2648 | dt 336.47ms | 1558201.84 tokens/sec
Step 10267 | loss: 3.170058 | lr:3.1278e-04 | norm 0.2983 | dt 335.73ms | 1561644.36 tokens/sec
Step 10268 | loss: 3.239079 | lr:3.1273e-04 | norm 0.2773 | dt 336.84ms | 1556489.02 tokens/sec
Step 10269 | loss: 3.279936 | lr:3.1269e-04 | norm 0.3051 | dt 337.36ms | 1554093.22 tokens/sec
Step 10270 | loss: 3.187178 | lr:3.1264e-04 | norm 0.2911 | dt 335.85ms | 1561056.80 tokens/sec
Step 10271 | loss: 3.195801 | lr:3.1259e-04 | norm 0.3094 | dt 336.42ms | 1558428.22 tokens/sec
Step 10272 | loss: 3.153395 | lr:3.1255e-04 | norm 0.2898 | dt 336.48ms | 1558177.55 tokens/sec
Step 10273 | loss: 3.190753 | lr:3.1250e-04 | norm 0.2968 | dt 336.35ms | 1558767.36 tokens/sec
Step 10274 | loss: 3.166460 | lr:3.1245e-04 | norm 0.2717 | dt 336.75ms | 1556916.60 tokens/sec
Step 10275 | loss: 3.188919 | lr:3.1241e-04 | norm 0.2812 | dt 336.29ms | 1559037.01 tokens/sec
Step 10276 | loss: 3.169420 | lr:3.1236e-04 | norm 0.2801 | dt 336.85ms | 1556451.57 tokens/sec
Step 10277 | loss: 3.136253 | lr:3.1232e-04 | norm 0.2604 | dt 337.01ms | 1555692.90 tokens/sec
Step 10278 | loss: 3.181728 | lr:3.1227e-04 | norm 0.2827 | dt 336.52ms | 1557966.70 tokens/sec
Step 10279 | loss: 3.184334 | lr:3.1222e-04 | norm 0.2654 | dt 337.21ms | 1554794.26 tokens/sec
Step 10280 | loss: 3.181417 | lr:3.1218e-04 | norm 0.2794 | dt 336.10ms | 1559894.09 tokens/sec
Step 10281 | loss: 3.100878 | lr:3.1213e-04 | norm 0.2676 | dt 337.68ms | 1552614.11 tokens/sec
Step 10282 | loss: 3.213794 | lr:3.1209e-04 | norm 0.3020 | dt 336.80ms | 1556681.84 tokens/sec
Step 10283 | loss: 3.213798 | lr:3.1204e-04 | norm 0.3054 | dt 339.16ms | 1545843.91 tokens/sec
Step 10284 | loss: 3.199709 | lr:3.1199e-04 | norm 0.3002 | dt 337.69ms | 1552549.43 tokens/sec
Step 10285 | loss: 3.157922 | lr:3.1195e-04 | norm 0.3018 | dt 338.12ms | 1550617.18 tokens/sec
Step 10286 | loss: 3.206408 | lr:3.1190e-04 | norm 0.2732 | dt 338.19ms | 1550297.97 tokens/sec
Step 10287 | loss: 3.127669 | lr:3.1186e-04 | norm 0.3130 | dt 338.18ms | 1550303.43 tokens/sec
Step 10288 | loss: 3.166939 | lr:3.1181e-04 | norm 0.2833 | dt 337.91ms | 1551540.56 tokens/sec
Step 10289 | loss: 3.176695 | lr:3.1176e-04 | norm 0.2815 | dt 338.65ms | 1548171.82 tokens/sec
Step 10290 | loss: 3.176117 | lr:3.1172e-04 | norm 0.2792 | dt 338.50ms | 1548851.16 tokens/sec
Step 10291 | loss: 3.200397 | lr:3.1167e-04 | norm 0.3483 | dt 338.59ms | 1548451.99 tokens/sec
Step 10292 | loss: 3.155235 | lr:3.1162e-04 | norm 0.2845 | dt 338.37ms | 1549445.94 tokens/sec
Step 10293 | loss: 3.223295 | lr:3.1158e-04 | norm 0.3444 | dt 338.22ms | 1550127.49 tokens/sec
Step 10294 | loss: 3.191256 | lr:3.1153e-04 | norm 0.2806 | dt 338.55ms | 1548647.19 tokens/sec
Step 10295 | loss: 3.213783 | lr:3.1149e-04 | norm 0.3277 | dt 338.77ms | 1547621.59 tokens/sec
Step 10296 | loss: 3.163990 | lr:3.1144e-04 | norm 0.2826 | dt 338.86ms | 1547227.41 tokens/sec
Step 10297 | loss: 3.198397 | lr:3.1139e-04 | norm 0.2933 | dt 338.69ms | 1547965.85 tokens/sec
Step 10298 | loss: 3.132646 | lr:3.1135e-04 | norm 0.3077 | dt 338.20ms | 1550216.00 tokens/sec
Step 10299 | loss: 3.297686 | lr:3.1130e-04 | norm 0.2854 | dt 338.11ms | 1550624.83 tokens/sec
Step 10300 | loss: 3.233472 | lr:3.1126e-04 | norm 0.2876 | dt 339.47ms | 1544441.20 tokens/sec
Step 10301 | loss: 3.241987 | lr:3.1121e-04 | norm 0.2865 | dt 338.77ms | 1547618.32 tokens/sec
Step 10302 | loss: 3.218798 | lr:3.1116e-04 | norm 0.2933 | dt 338.62ms | 1548297.18 tokens/sec
Step 10303 | loss: 3.210056 | lr:3.1112e-04 | norm 0.2875 | dt 338.99ms | 1546628.89 tokens/sec
Step 10304 | loss: 3.156709 | lr:3.1107e-04 | norm 0.2847 | dt 338.69ms | 1547974.57 tokens/sec
Step 10305 | loss: 3.138095 | lr:3.1103e-04 | norm 0.2876 | dt 338.04ms | 1550974.80 tokens/sec
Step 10306 | loss: 3.150765 | lr:3.1098e-04 | norm 0.2919 | dt 339.33ms | 1545060.81 tokens/sec
Step 10307 | loss: 3.153746 | lr:3.1093e-04 | norm 0.3507 | dt 339.30ms | 1545188.92 tokens/sec
Step 10308 | loss: 3.143749 | lr:3.1089e-04 | norm 0.2818 | dt 338.28ms | 1549863.10 tokens/sec
Step 10309 | loss: 3.177090 | lr:3.1084e-04 | norm 0.3140 | dt 338.23ms | 1550095.80 tokens/sec
Step 10310 | loss: 3.137936 | lr:3.1079e-04 | norm 0.3013 | dt 341.10ms | 1537065.86 tokens/sec
Step 10311 | loss: 3.193207 | lr:3.1075e-04 | norm 0.3329 | dt 339.15ms | 1545893.90 tokens/sec
Step 10312 | loss: 3.184075 | lr:3.1070e-04 | norm 0.2793 | dt 338.73ms | 1547784.98 tokens/sec
Step 10313 | loss: 3.180319 | lr:3.1066e-04 | norm 0.3300 | dt 338.27ms | 1549892.59 tokens/sec
Step 10314 | loss: 3.174959 | lr:3.1061e-04 | norm 0.2696 | dt 338.68ms | 1548015.97 tokens/sec
Step 10315 | loss: 3.196666 | lr:3.1056e-04 | norm 0.2843 | dt 338.36ms | 1549501.62 tokens/sec
Step 10316 | loss: 3.162897 | lr:3.1052e-04 | norm 0.2780 | dt 338.31ms | 1549708.00 tokens/sec
Step 10317 | loss: 3.207718 | lr:3.1047e-04 | norm 0.3053 | dt 339.33ms | 1545066.24 tokens/sec
Step 10318 | loss: 3.185903 | lr:3.1043e-04 | norm 0.2805 | dt 338.91ms | 1546978.15 tokens/sec
Step 10319 | loss: 3.198022 | lr:3.1038e-04 | norm 0.2735 | dt 338.70ms | 1547932.07 tokens/sec
Step 10320 | loss: 3.275068 | lr:3.1033e-04 | norm 0.2951 | dt 338.52ms | 1548771.53 tokens/sec
Step 10321 | loss: 3.231753 | lr:3.1029e-04 | norm 0.2805 | dt 339.24ms | 1545489.74 tokens/sec
Step 10322 | loss: 3.142360 | lr:3.1024e-04 | norm 0.2800 | dt 339.09ms | 1546163.46 tokens/sec
Step 10323 | loss: 3.273390 | lr:3.1020e-04 | norm 0.3389 | dt 340.38ms | 1540280.65 tokens/sec
Step 10324 | loss: 3.149383 | lr:3.1015e-04 | norm 0.3248 | dt 338.14ms | 1550494.73 tokens/sec
Step 10325 | loss: 3.232077 | lr:3.1010e-04 | norm 0.3168 | dt 339.62ms | 1543752.72 tokens/sec
Step 10326 | loss: 3.198700 | lr:3.1006e-04 | norm 0.3645 | dt 338.86ms | 1547221.96 tokens/sec
Step 10327 | loss: 3.162866 | lr:3.1001e-04 | norm 0.3492 | dt 338.63ms | 1548252.48 tokens/sec
Step 10328 | loss: 3.206107 | lr:3.0997e-04 | norm 0.2970 | dt 339.38ms | 1544854.58 tokens/sec
Step 10329 | loss: 3.190042 | lr:3.0992e-04 | norm 0.3091 | dt 338.32ms | 1549686.16 tokens/sec
Step 10330 | loss: 3.236170 | lr:3.0987e-04 | norm 0.2916 | dt 338.42ms | 1549211.25 tokens/sec
Step 10331 | loss: 3.241395 | lr:3.0983e-04 | norm 0.2955 | dt 338.22ms | 1550125.30 tokens/sec
Step 10332 | loss: 3.236525 | lr:3.0978e-04 | norm 0.2697 | dt 337.76ms | 1552262.30 tokens/sec
Step 10333 | loss: 3.175400 | lr:3.0974e-04 | norm 0.2810 | dt 337.73ms | 1552378.46 tokens/sec
Step 10334 | loss: 3.181817 | lr:3.0969e-04 | norm 0.2602 | dt 337.94ms | 1551439.85 tokens/sec
Step 10335 | loss: 3.232232 | lr:3.0964e-04 | norm 0.3024 | dt 338.40ms | 1549307.30 tokens/sec
Step 10336 | loss: 3.190544 | lr:3.0960e-04 | norm 0.2800 | dt 338.41ms | 1549253.81 tokens/sec
Step 10337 | loss: 3.269811 | lr:3.0955e-04 | norm 0.3188 | dt 338.85ms | 1547261.16 tokens/sec
Step 10338 | loss: 3.171998 | lr:3.0950e-04 | norm 0.2961 | dt 338.63ms | 1548284.10 tokens/sec
Step 10339 | loss: 3.213996 | lr:3.0946e-04 | norm 0.3378 | dt 338.73ms | 1547787.16 tokens/sec
Step 10340 | loss: 3.209756 | lr:3.0941e-04 | norm 0.3324 | dt 338.53ms | 1548736.62 tokens/sec
Step 10341 | loss: 3.291110 | lr:3.0937e-04 | norm 0.3393 | dt 339.05ms | 1546324.38 tokens/sec
Step 10342 | loss: 3.173211 | lr:3.0932e-04 | norm 0.3001 | dt 338.98ms | 1546662.62 tokens/sec
Step 10343 | loss: 3.165893 | lr:3.0927e-04 | norm 0.3674 | dt 339.17ms | 1545811.31 tokens/sec
Step 10344 | loss: 3.101769 | lr:3.0923e-04 | norm 0.4852 | dt 338.20ms | 1550243.32 tokens/sec
Step 10345 | loss: 3.147990 | lr:3.0918e-04 | norm 0.3210 | dt 338.41ms | 1549253.81 tokens/sec
Step 10346 | loss: 3.253083 | lr:3.0914e-04 | norm 0.3268 | dt 339.35ms | 1544981.57 tokens/sec
Step 10347 | loss: 3.143234 | lr:3.0909e-04 | norm 0.3086 | dt 339.36ms | 1544924.04 tokens/sec
Step 10348 | loss: 3.195545 | lr:3.0904e-04 | norm 0.3052 | dt 338.68ms | 1548012.70 tokens/sec
Step 10349 | loss: 3.183875 | lr:3.0900e-04 | norm 0.3163 | dt 337.95ms | 1551356.67 tokens/sec
Step 10350 | loss: 3.132437 | lr:3.0895e-04 | norm 0.3210 | dt 338.77ms | 1547623.77 tokens/sec
Step 10351 | loss: 3.168319 | lr:3.0891e-04 | norm 0.3042 | dt 338.89ms | 1547057.60 tokens/sec
Step 10352 | loss: 3.176084 | lr:3.0886e-04 | norm 0.2998 | dt 338.95ms | 1546814.93 tokens/sec
Step 10353 | loss: 3.216026 | lr:3.0881e-04 | norm 0.3081 | dt 338.82ms | 1547413.58 tokens/sec
Step 10354 | loss: 3.206007 | lr:3.0877e-04 | norm 0.3162 | dt 338.40ms | 1549323.67 tokens/sec
Step 10355 | loss: 3.142588 | lr:3.0872e-04 | norm 0.2753 | dt 338.26ms | 1549961.41 tokens/sec
Step 10356 | loss: 3.173759 | lr:3.0868e-04 | norm 0.3107 | dt 339.34ms | 1545032.59 tokens/sec
Step 10357 | loss: 3.167329 | lr:3.0863e-04 | norm 0.2896 | dt 338.05ms | 1550898.23 tokens/sec
Step 10358 | loss: 3.172152 | lr:3.0858e-04 | norm 0.2853 | dt 337.73ms | 1552366.40 tokens/sec
Step 10359 | loss: 3.200907 | lr:3.0854e-04 | norm 0.2909 | dt 338.68ms | 1548029.05 tokens/sec
Step 10360 | loss: 3.257719 | lr:3.0849e-04 | norm 0.3054 | dt 338.14ms | 1550492.54 tokens/sec
Step 10361 | loss: 3.166383 | lr:3.0845e-04 | norm 0.2940 | dt 337.36ms | 1554104.20 tokens/sec
Step 10362 | loss: 3.212128 | lr:3.0840e-04 | norm 0.2990 | dt 338.08ms | 1550768.08 tokens/sec
Step 10363 | loss: 3.159859 | lr:3.0835e-04 | norm 0.2802 | dt 337.88ms | 1551677.41 tokens/sec
Step 10364 | loss: 3.153815 | lr:3.0831e-04 | norm 0.2947 | dt 338.04ms | 1550963.86 tokens/sec
Step 10365 | loss: 3.177264 | lr:3.0826e-04 | norm 0.2520 | dt 337.79ms | 1552124.25 tokens/sec
Step 10366 | loss: 3.199740 | lr:3.0821e-04 | norm 0.2766 | dt 337.47ms | 1553604.63 tokens/sec
Step 10367 | loss: 3.187489 | lr:3.0817e-04 | norm 0.2591 | dt 337.94ms | 1551422.34 tokens/sec
Step 10368 | loss: 3.228136 | lr:3.0812e-04 | norm 0.2781 | dt 338.22ms | 1550123.12 tokens/sec
Step 10369 | loss: 3.211211 | lr:3.0808e-04 | norm 0.2737 | dt 337.46ms | 1553633.16 tokens/sec
Step 10370 | loss: 3.206209 | lr:3.0803e-04 | norm 0.2715 | dt 337.78ms | 1552136.30 tokens/sec
Step 10371 | loss: 3.230434 | lr:3.0798e-04 | norm 0.2832 | dt 338.19ms | 1550288.13 tokens/sec
Step 10372 | loss: 3.205228 | lr:3.0794e-04 | norm 0.2869 | dt 338.33ms | 1549638.11 tokens/sec
Step 10373 | loss: 3.246281 | lr:3.0789e-04 | norm 0.2712 | dt 337.70ms | 1552506.69 tokens/sec
Step 10374 | loss: 3.246343 | lr:3.0785e-04 | norm 0.2904 | dt 338.02ms | 1551056.85 tokens/sec
Step 10375 | loss: 3.206400 | lr:3.0780e-04 | norm 0.2791 | dt 338.50ms | 1548841.34 tokens/sec
Step 10376 | loss: 3.104978 | lr:3.0775e-04 | norm 0.3056 | dt 339.64ms | 1543671.44 tokens/sec
Step 10377 | loss: 3.159829 | lr:3.0771e-04 | norm 0.2569 | dt 338.47ms | 1549014.82 tokens/sec
Step 10378 | loss: 3.165509 | lr:3.0766e-04 | norm 0.2855 | dt 338.43ms | 1549194.87 tokens/sec
Step 10379 | loss: 3.179151 | lr:3.0762e-04 | norm 0.2710 | dt 338.56ms | 1548595.93 tokens/sec
Step 10380 | loss: 3.138367 | lr:3.0757e-04 | norm 0.2776 | dt 338.99ms | 1546602.79 tokens/sec
Step 10381 | loss: 3.173815 | lr:3.0752e-04 | norm 0.2995 | dt 338.52ms | 1548759.53 tokens/sec
Step 10382 | loss: 3.216569 | lr:3.0748e-04 | norm 0.2783 | dt 338.60ms | 1548411.65 tokens/sec
Step 10383 | loss: 3.121572 | lr:3.0743e-04 | norm 0.3072 | dt 338.24ms | 1550049.91 tokens/sec
Step 10384 | loss: 3.132481 | lr:3.0739e-04 | norm 0.2681 | dt 338.20ms | 1550253.16 tokens/sec
Step 10385 | loss: 3.207754 | lr:3.0734e-04 | norm 0.2800 | dt 338.93ms | 1546879.13 tokens/sec
Step 10386 | loss: 3.191421 | lr:3.0729e-04 | norm 0.2845 | dt 338.52ms | 1548758.44 tokens/sec
Step 10387 | loss: 3.185379 | lr:3.0725e-04 | norm 0.3375 | dt 339.21ms | 1545608.14 tokens/sec
Step 10388 | loss: 3.213544 | lr:3.0720e-04 | norm 0.3255 | dt 338.50ms | 1548879.53 tokens/sec
Step 10389 | loss: 3.252298 | lr:3.0716e-04 | norm 0.3487 | dt 338.43ms | 1549169.77 tokens/sec
Step 10390 | loss: 3.174994 | lr:3.0711e-04 | norm 0.3512 | dt 339.37ms | 1544865.44 tokens/sec
Step 10391 | loss: 3.155428 | lr:3.0706e-04 | norm 0.2885 | dt 339.48ms | 1544392.39 tokens/sec
Step 10392 | loss: 3.188220 | lr:3.0702e-04 | norm 0.3335 | dt 338.46ms | 1549041.00 tokens/sec
Step 10393 | loss: 3.165400 | lr:3.0697e-04 | norm 0.2992 | dt 338.62ms | 1548329.88 tokens/sec
Step 10394 | loss: 3.175951 | lr:3.0693e-04 | norm 0.2986 | dt 896.22ms | 584996.13 tokens/sec
Step 10395 | loss: 3.199117 | lr:3.0688e-04 | norm 0.2854 | dt 338.25ms | 1550004.02 tokens/sec
Step 10396 | loss: 3.151275 | lr:3.0683e-04 | norm 0.2731 | dt 337.60ms | 1552993.49 tokens/sec
Step 10397 | loss: 3.201796 | lr:3.0679e-04 | norm 0.2723 | dt 338.95ms | 1546784.46 tokens/sec
Step 10398 | loss: 3.225665 | lr:3.0674e-04 | norm 0.3034 | dt 337.76ms | 1552260.11 tokens/sec
Step 10399 | loss: 3.219947 | lr:3.0670e-04 | norm 0.2834 | dt 338.35ms | 1549527.82 tokens/sec
Step 10400 | loss: 3.161148 | lr:3.0665e-04 | norm 0.2731 | dt 338.21ms | 1550178.85 tokens/sec
Step 10401 | loss: 3.224404 | lr:3.0660e-04 | norm 0.2743 | dt 337.95ms | 1551363.23 tokens/sec
Step 10402 | loss: 3.138041 | lr:3.0656e-04 | norm 0.2708 | dt 338.08ms | 1550759.33 tokens/sec
Step 10403 | loss: 3.173481 | lr:3.0651e-04 | norm 0.2695 | dt 338.52ms | 1548773.71 tokens/sec
Step 10404 | loss: 3.174228 | lr:3.0647e-04 | norm 0.2665 | dt 336.89ms | 1556275.32 tokens/sec
Step 10405 | loss: 3.216817 | lr:3.0642e-04 | norm 0.2774 | dt 338.18ms | 1550313.27 tokens/sec
Step 10406 | loss: 3.191176 | lr:3.0637e-04 | norm 0.2821 | dt 338.45ms | 1549096.66 tokens/sec
Step 10407 | loss: 3.230974 | lr:3.0633e-04 | norm 0.2846 | dt 338.77ms | 1547619.41 tokens/sec
Step 10408 | loss: 3.254127 | lr:3.0628e-04 | norm 0.2990 | dt 338.04ms | 1550973.71 tokens/sec
Step 10409 | loss: 3.194968 | lr:3.0624e-04 | norm 0.2690 | dt 338.75ms | 1547709.82 tokens/sec
Step 10410 | loss: 3.120554 | lr:3.0619e-04 | norm 0.2668 | dt 337.67ms | 1552682.08 tokens/sec
Step 10411 | loss: 3.175343 | lr:3.0614e-04 | norm 0.2916 | dt 339.28ms | 1545314.88 tokens/sec
Step 10412 | loss: 3.200669 | lr:3.0610e-04 | norm 0.2689 | dt 338.16ms | 1550400.71 tokens/sec
Step 10413 | loss: 3.235679 | lr:3.0605e-04 | norm 0.2670 | dt 337.72ms | 1552426.68 tokens/sec
Step 10414 | loss: 3.155767 | lr:3.0601e-04 | norm 0.2577 | dt 337.91ms | 1551564.64 tokens/sec
Step 10415 | loss: 3.232026 | lr:3.0596e-04 | norm 0.2707 | dt 337.33ms | 1554218.44 tokens/sec
Step 10416 | loss: 3.138022 | lr:3.0591e-04 | norm 0.2489 | dt 337.65ms | 1552770.88 tokens/sec
Step 10417 | loss: 3.168851 | lr:3.0587e-04 | norm 0.2652 | dt 338.10ms | 1550695.91 tokens/sec
Step 10418 | loss: 3.167914 | lr:3.0582e-04 | norm 0.2708 | dt 337.97ms | 1551305.23 tokens/sec
Step 10419 | loss: 3.209245 | lr:3.0578e-04 | norm 0.2694 | dt 338.62ms | 1548302.63 tokens/sec
Step 10420 | loss: 3.193017 | lr:3.0573e-04 | norm 0.2848 | dt 338.97ms | 1546687.64 tokens/sec
Step 10421 | loss: 3.185407 | lr:3.0568e-04 | norm 0.2708 | dt 339.01ms | 1546544.05 tokens/sec
Step 10422 | loss: 3.212123 | lr:3.0564e-04 | norm 0.3171 | dt 339.09ms | 1546149.33 tokens/sec
Step 10423 | loss: 3.136649 | lr:3.0559e-04 | norm 0.2811 | dt 338.91ms | 1546966.18 tokens/sec
Step 10424 | loss: 3.168222 | lr:3.0554e-04 | norm 0.3010 | dt 339.33ms | 1545048.87 tokens/sec
Step 10425 | loss: 3.199122 | lr:3.0550e-04 | norm 0.2667 | dt 338.95ms | 1546783.38 tokens/sec
Step 10426 | loss: 3.264478 | lr:3.0545e-04 | norm 0.3042 | dt 338.16ms | 1550408.37 tokens/sec
Step 10427 | loss: 3.155598 | lr:3.0541e-04 | norm 0.2915 | dt 338.77ms | 1547601.99 tokens/sec
Step 10428 | loss: 3.128708 | lr:3.0536e-04 | norm 0.2780 | dt 338.98ms | 1546670.23 tokens/sec
Step 10429 | loss: 3.158400 | lr:3.0531e-04 | norm 0.2965 | dt 338.53ms | 1548736.62 tokens/sec
Step 10430 | loss: 3.136224 | lr:3.0527e-04 | norm 0.3060 | dt 338.53ms | 1548709.36 tokens/sec
Step 10431 | loss: 3.205040 | lr:3.0522e-04 | norm 0.3016 | dt 337.79ms | 1552108.92 tokens/sec
Step 10432 | loss: 3.153110 | lr:3.0518e-04 | norm 0.3000 | dt 337.62ms | 1552894.79 tokens/sec
Step 10433 | loss: 3.274009 | lr:3.0513e-04 | norm 0.3185 | dt 337.42ms | 1553792.34 tokens/sec
Step 10434 | loss: 3.142503 | lr:3.0508e-04 | norm 0.3284 | dt 338.04ms | 1550969.33 tokens/sec
Step 10435 | loss: 3.248461 | lr:3.0504e-04 | norm 0.2799 | dt 337.69ms | 1552582.32 tokens/sec
Step 10436 | loss: 3.142120 | lr:3.0499e-04 | norm 0.3099 | dt 338.04ms | 1550959.49 tokens/sec
Step 10437 | loss: 3.214173 | lr:3.0495e-04 | norm 0.2909 | dt 337.73ms | 1552372.98 tokens/sec
Step 10438 | loss: 3.174716 | lr:3.0490e-04 | norm 0.3025 | dt 338.35ms | 1549552.94 tokens/sec
Step 10439 | loss: 3.170524 | lr:3.0485e-04 | norm 0.2779 | dt 338.09ms | 1550748.40 tokens/sec
Step 10440 | loss: 3.154949 | lr:3.0481e-04 | norm 0.2806 | dt 337.55ms | 1553224.94 tokens/sec
Step 10441 | loss: 3.192867 | lr:3.0476e-04 | norm 0.3024 | dt 337.43ms | 1553768.19 tokens/sec
Step 10442 | loss: 3.187019 | lr:3.0472e-04 | norm 0.2702 | dt 337.79ms | 1552115.49 tokens/sec
Step 10443 | loss: 3.196885 | lr:3.0467e-04 | norm 0.2745 | dt 337.74ms | 1552334.62 tokens/sec
Step 10444 | loss: 3.161631 | lr:3.0462e-04 | norm 0.2633 | dt 338.15ms | 1550446.63 tokens/sec
Step 10445 | loss: 3.197673 | lr:3.0458e-04 | norm 0.2716 | dt 338.09ms | 1550737.46 tokens/sec
Step 10446 | loss: 3.116390 | lr:3.0453e-04 | norm 0.2692 | dt 337.52ms | 1553353.31 tokens/sec
Step 10447 | loss: 3.208923 | lr:3.0449e-04 | norm 0.2953 | dt 337.38ms | 1554011.95 tokens/sec
Step 10448 | loss: 3.173124 | lr:3.0444e-04 | norm 0.2703 | dt 337.66ms | 1552711.68 tokens/sec
Step 10449 | loss: 3.200235 | lr:3.0439e-04 | norm 0.3094 | dt 921.23ms | 569117.18 tokens/sec
Step 10450 | loss: 3.174599 | lr:3.0435e-04 | norm 0.2712 | dt 337.27ms | 1554503.00 tokens/sec
Step 10451 | loss: 3.211272 | lr:3.0430e-04 | norm 0.3026 | dt 337.66ms | 1552699.62 tokens/sec
Step 10452 | loss: 3.127965 | lr:3.0426e-04 | norm 0.2848 | dt 341.11ms | 1537023.96 tokens/sec
Step 10453 | loss: 3.170691 | lr:3.0421e-04 | norm 0.2769 | dt 337.42ms | 1553804.42 tokens/sec
Step 10454 | loss: 3.175750 | lr:3.0416e-04 | norm 0.2745 | dt 338.41ms | 1549259.27 tokens/sec
Step 10455 | loss: 3.170261 | lr:3.0412e-04 | norm 0.2626 | dt 337.69ms | 1552571.36 tokens/sec
Step 10456 | loss: 3.161258 | lr:3.0407e-04 | norm 0.3016 | dt 337.74ms | 1552353.25 tokens/sec
Step 10457 | loss: 3.124161 | lr:3.0403e-04 | norm 0.3047 | dt 337.54ms | 1553279.80 tokens/sec
Step 10458 | loss: 3.139713 | lr:3.0398e-04 | norm 0.2686 | dt 337.72ms | 1552443.12 tokens/sec
Step 10459 | loss: 3.153449 | lr:3.0393e-04 | norm 0.2953 | dt 337.77ms | 1552195.47 tokens/sec
Step 10460 | loss: 3.142093 | lr:3.0389e-04 | norm 0.2749 | dt 337.97ms | 1551303.04 tokens/sec
Step 10461 | loss: 3.183078 | lr:3.0384e-04 | norm 0.2823 | dt 338.06ms | 1550884.01 tokens/sec
Step 10462 | loss: 3.099589 | lr:3.0380e-04 | norm 0.2675 | dt 338.42ms | 1549210.15 tokens/sec
Step 10463 | loss: 3.181756 | lr:3.0375e-04 | norm 0.2770 | dt 338.76ms | 1547665.16 tokens/sec
Step 10464 | loss: 3.204720 | lr:3.0370e-04 | norm 0.2672 | dt 337.78ms | 1552146.16 tokens/sec
Step 10465 | loss: 3.143020 | lr:3.0366e-04 | norm 0.2768 | dt 337.88ms | 1551693.83 tokens/sec
Step 10466 | loss: 3.171717 | lr:3.0361e-04 | norm 0.2789 | dt 337.89ms | 1551674.12 tokens/sec
Step 10467 | loss: 3.304623 | lr:3.0357e-04 | norm 0.2936 | dt 337.93ms | 1551483.63 tokens/sec
Step 10468 | loss: 3.225484 | lr:3.0352e-04 | norm 0.3049 | dt 338.37ms | 1549462.31 tokens/sec
Step 10469 | loss: 3.226144 | lr:3.0348e-04 | norm 0.3017 | dt 337.46ms | 1553629.87 tokens/sec
Step 10470 | loss: 3.204979 | lr:3.0343e-04 | norm 0.3136 | dt 337.87ms | 1551763.91 tokens/sec
Step 10471 | loss: 3.209589 | lr:3.0338e-04 | norm 0.3252 | dt 337.76ms | 1552231.62 tokens/sec
Step 10472 | loss: 3.190437 | lr:3.0334e-04 | norm 0.3094 | dt 337.88ms | 1551712.44 tokens/sec
Step 10473 | loss: 3.241950 | lr:3.0329e-04 | norm 0.3231 | dt 342.11ms | 1532516.50 tokens/sec
Step 10474 | loss: 3.263454 | lr:3.0325e-04 | norm 0.3221 | dt 339.24ms | 1545459.33 tokens/sec
Step 10475 | loss: 3.224380 | lr:3.0320e-04 | norm 0.3031 | dt 338.93ms | 1546883.48 tokens/sec
Step 10476 | loss: 3.252401 | lr:3.0315e-04 | norm 0.3220 | dt 338.13ms | 1550572.35 tokens/sec
Step 10477 | loss: 3.182678 | lr:3.0311e-04 | norm 0.2705 | dt 338.05ms | 1550909.17 tokens/sec
Step 10478 | loss: 3.176212 | lr:3.0306e-04 | norm 0.2860 | dt 338.80ms | 1547481.10 tokens/sec
Step 10479 | loss: 3.150902 | lr:3.0302e-04 | norm 0.2777 | dt 337.98ms | 1551228.63 tokens/sec
Step 10480 | loss: 3.141160 | lr:3.0297e-04 | norm 0.2640 | dt 337.79ms | 1552104.53 tokens/sec
Step 10481 | loss: 3.184743 | lr:3.0292e-04 | norm 0.2800 | dt 338.42ms | 1549217.79 tokens/sec
Step 10482 | loss: 3.184298 | lr:3.0288e-04 | norm 0.2724 | dt 339.10ms | 1546131.94 tokens/sec
Step 10483 | loss: 3.171894 | lr:3.0283e-04 | norm 0.3058 | dt 338.11ms | 1550639.05 tokens/sec
Step 10484 | loss: 3.161206 | lr:3.0279e-04 | norm 0.2675 | dt 338.39ms | 1549381.53 tokens/sec
Step 10485 | loss: 3.180805 | lr:3.0274e-04 | norm 0.2751 | dt 338.14ms | 1550498.01 tokens/sec
Step 10486 | loss: 3.096262 | lr:3.0269e-04 | norm 0.2774 | dt 337.93ms | 1551472.69 tokens/sec
Step 10487 | loss: 3.175121 | lr:3.0265e-04 | norm 0.2696 | dt 338.17ms | 1550374.48 tokens/sec
Step 10488 | loss: 3.164647 | lr:3.0260e-04 | norm 0.2663 | dt 337.86ms | 1551807.71 tokens/sec
Step 10489 | loss: 3.170647 | lr:3.0256e-04 | norm 0.2938 | dt 338.18ms | 1550328.57 tokens/sec
Step 10490 | loss: 3.153624 | lr:3.0251e-04 | norm 0.3185 | dt 338.47ms | 1548998.45 tokens/sec
Step 10491 | loss: 3.115632 | lr:3.0246e-04 | norm 0.3099 | dt 337.75ms | 1552294.08 tokens/sec
Step 10492 | loss: 3.138237 | lr:3.0242e-04 | norm 0.3102 | dt 338.36ms | 1549475.41 tokens/sec
Step 10493 | loss: 3.170593 | lr:3.0237e-04 | norm 0.2733 | dt 338.10ms | 1550680.60 tokens/sec
Step 10494 | loss: 3.162037 | lr:3.0233e-04 | norm 0.3190 | dt 338.31ms | 1549735.30 tokens/sec
Step 10495 | loss: 3.159595 | lr:3.0228e-04 | norm 0.2802 | dt 338.15ms | 1550442.25 tokens/sec
Step 10496 | loss: 3.187563 | lr:3.0223e-04 | norm 0.2970 | dt 337.93ms | 1551479.26 tokens/sec
Step 10497 | loss: 3.216928 | lr:3.0219e-04 | norm 0.3079 | dt 337.90ms | 1551595.29 tokens/sec
Step 10498 | loss: 3.219948 | lr:3.0214e-04 | norm 0.2958 | dt 338.08ms | 1550765.89 tokens/sec
Step 10499 | loss: 3.215464 | lr:3.0210e-04 | norm 0.2932 | dt 338.38ms | 1549413.18 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 10500: 3.1892
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2931/10042=0.2919


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not so quick to fix up a program in Excel, unless I can explain them here.)
And what
rank 3 sample 1 >Hello, I'm a language model, so you have to know as I say. If you read the book, you can see that they're basically pretty straightforward
rank 3 sample 2 >Hello, I'm a language model, so this means that everyone can make a point of telling people, "Hey, do you understand," and that will allow
rank 3 sample 3 >Hello, I'm a language model, so don't bother talking about languages. I can understand what I say and I can make me understand which words to use




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this isn't an easy one. Because of the complexity of writing code, you have to make sure your code is
rank 5 sample 1 >Hello, I'm a language model, for one, so don't think it's worth doing a translation/translator analysis for this site, please, to


ddp_rank 7: ####### Printing generated samples ####### 

rank 5 sample 2 >Hello, I'm a language model, and it'll help but I am stuck. You can't teach my kids any more because they're just learning how to
rank 5 sample 3 >Hello, I'm a language model, don't know.
"The most common reason we see this error is
using "The world's largest number of


rank 7 sample 0 >Hello, I'm a language model, so I think it's worth a little credit. You're gonna be amazed at other people's enthusiasm about the language,
rank 7 sample 1 >Hello, I'm a language model, do I know what you mean?, Do you mean so to say 'I'm in a class?' Is there an object
rank 7 sample 2 >Hello, I'm a language model, so I'll get ready for real world situations. As for me, how do you know if you're using one,
rank 7 sample 3 >Hello, I'm a language model, and it's a great introduction to grammar. Well, I appreciate the language model. And I understand that it's too




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, to me it isn't about language-related, but the idea is to develop some ways of expressing the relationship between the
rank 2 sample 1 >Hello, I'm a language model, but I appreciate your ideas and are extremely keen to discuss with your fellow colleagues.
I've always found the book on
rank 2 sample 2 >Hello, I'm a language model, and I want to use the right-hand side of the arrow in between each character. I could easily get into some
rank 2 sample 3 >Hello, I'm a language model, but in today's post, I want to use R as I'm writing because that is just for me to use because




ddp_rank 0: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I am going through that part for learning all languages. I also feel a little nervous about what is on the web
rank 0 sample 1 >Hello, I'm a language model, and am also a teacher. And because I've developed over two hundred essays for the past three months, I've never
rank 6 sample 0 >Hello, I'm a language model, just want to know, this seems to me to be an example, and if you're curious, I've got something
rank 0 sample 2 >Hello, I'm a language model, and I thought, that when you start a software program, all it needs is a connection, and that's the function
rank 6 sample 1 >Hello, I'm a language model, a lot like a Java app and it shows how much the language can accomplish. I can see that the most important thing
rank 0 sample 3 >Hello, I'm a language model, so in this lesson, we'll review some code examples.
So, let's do that, as I've heard


rank 6 sample 2 >Hello, I'm a language model, but my teacher says: 'This is an awesome book. This is the only book I have ever read so far,
rank 6 sample 3 >Hello, I'm a language model, so that's the only way I know what language model to use.
1) What function is shown at the image





ddp_rank 4: ####### Printing generated samples ####### 


ddp_rank 1: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I like to keep trying to understand something better than I could. This is a way of not thinking and trying something
rank 1 sample 0 >Hello, I'm a language model, so i would like to see how exactly the structure can be read.
Can you help me figure out which structure is
rank 4 sample 1 >Hello, I'm a language model, there isn't any need to teach language model, just ask me. My teacher taught me how to read, although it
rank 1 sample 1 >Hello, I'm a language model, not an algorithm. I'm just like everybody else. (I haven't yet added what goes)<|endoftext|>The United States
rank 4 sample 2 >Hello, I'm a language model, I created a set of applications that work for me. I can have a look at some of those here. Now I
rank 1 sample 2 >Hello, I'm a language model, but most people would say that it's a good idea for kids and teenagers to learn a new language in elementary, and
rank 4 sample 3 >Hello, I'm a language model, so it goes one step at a time...I will work really well...but I'm a good programmer. This book


rank 1 sample 3 >Hello, I'm a language model, so I'm just trying to keep this quick. Thanks by jeech I'm now interested in the semantics of the


Step 10500 | loss: 3.235965 | lr:3.0205e-04 | norm 0.2958 | dt 18784.10ms | 27911.27 tokens/sec
Step 10501 | loss: 3.242648 | lr:3.0200e-04 | norm 0.2996 | dt 334.94ms | 1565328.28 tokens/sec
Step 10502 | loss: 3.236191 | lr:3.0196e-04 | norm 0.3098 | dt 334.79ms | 1566002.69 tokens/sec
Step 10503 | loss: 3.217742 | lr:3.0191e-04 | norm 0.3167 | dt 337.63ms | 1552830.09 tokens/sec
Step 10504 | loss: 3.207375 | lr:3.0187e-04 | norm 0.2800 | dt 336.49ms | 1558102.48 tokens/sec
Step 10505 | loss: 3.182284 | lr:3.0182e-04 | norm 0.3059 | dt 335.62ms | 1562153.56 tokens/sec
Step 10506 | loss: 3.167778 | lr:3.0177e-04 | norm 0.2807 | dt 335.61ms | 1562187.96 tokens/sec
Step 10507 | loss: 3.237582 | lr:3.0173e-04 | norm 0.3028 | dt 336.22ms | 1559337.71 tokens/sec
Step 10508 | loss: 3.207409 | lr:3.0168e-04 | norm 0.2961 | dt 335.92ms | 1560766.52 tokens/sec
Step 10509 | loss: 3.231618 | lr:3.0164e-04 | norm 0.2750 | dt 336.58ms | 1557702.94 tokens/sec
Step 10510 | loss: 3.217435 | lr:3.0159e-04 | norm 0.2919 | dt 336.26ms | 1559178.50 tokens/sec
Step 10511 | loss: 3.237986 | lr:3.0154e-04 | norm 0.2875 | dt 336.50ms | 1558045.07 tokens/sec
Step 10512 | loss: 3.166378 | lr:3.0150e-04 | norm 0.2804 | dt 336.26ms | 1559182.92 tokens/sec
Step 10513 | loss: 3.192190 | lr:3.0145e-04 | norm 0.2773 | dt 337.35ms | 1554123.97 tokens/sec
Step 10514 | loss: 3.171828 | lr:3.0141e-04 | norm 0.2757 | dt 335.89ms | 1560893.92 tokens/sec
Step 10515 | loss: 3.154450 | lr:3.0136e-04 | norm 0.2789 | dt 337.15ms | 1555058.13 tokens/sec
Step 10516 | loss: 3.181181 | lr:3.0131e-04 | norm 0.3051 | dt 337.34ms | 1554192.07 tokens/sec
Step 10517 | loss: 3.170192 | lr:3.0127e-04 | norm 0.2710 | dt 336.10ms | 1559928.39 tokens/sec
Step 10518 | loss: 3.170027 | lr:3.0122e-04 | norm 0.2914 | dt 336.87ms | 1556372.25 tokens/sec
Step 10519 | loss: 3.156708 | lr:3.0118e-04 | norm 0.2675 | dt 337.54ms | 1553277.60 tokens/sec
Step 10520 | loss: 3.179402 | lr:3.0113e-04 | norm 0.2852 | dt 336.59ms | 1557641.15 tokens/sec
Step 10521 | loss: 3.173463 | lr:3.0109e-04 | norm 0.2800 | dt 336.91ms | 1556156.38 tokens/sec
Step 10522 | loss: 3.214088 | lr:3.0104e-04 | norm 0.2727 | dt 336.04ms | 1560205.08 tokens/sec
Step 10523 | loss: 3.196722 | lr:3.0099e-04 | norm 0.2830 | dt 335.95ms | 1560590.40 tokens/sec
Step 10524 | loss: 3.194484 | lr:3.0095e-04 | norm 0.2832 | dt 337.14ms | 1555119.72 tokens/sec
Step 10525 | loss: 3.202591 | lr:3.0090e-04 | norm 0.3023 | dt 336.90ms | 1556204.84 tokens/sec
Step 10526 | loss: 3.155865 | lr:3.0086e-04 | norm 0.3057 | dt 338.83ms | 1547338.46 tokens/sec
Step 10527 | loss: 3.161484 | lr:3.0081e-04 | norm 0.2811 | dt 337.40ms | 1553928.49 tokens/sec
Step 10528 | loss: 3.159615 | lr:3.0076e-04 | norm 0.2960 | dt 336.99ms | 1555806.27 tokens/sec
Step 10529 | loss: 3.229907 | lr:3.0072e-04 | norm 0.2924 | dt 338.72ms | 1547863.43 tokens/sec
Step 10530 | loss: 3.137516 | lr:3.0067e-04 | norm 0.2962 | dt 337.56ms | 1553179.96 tokens/sec
Step 10531 | loss: 3.164317 | lr:3.0063e-04 | norm 0.2644 | dt 338.35ms | 1549563.85 tokens/sec
Step 10532 | loss: 3.160157 | lr:3.0058e-04 | norm 0.2900 | dt 338.52ms | 1548771.53 tokens/sec
Step 10533 | loss: 3.203700 | lr:3.0053e-04 | norm 0.2875 | dt 336.97ms | 1555893.23 tokens/sec
Step 10534 | loss: 3.162436 | lr:3.0049e-04 | norm 0.2789 | dt 337.16ms | 1555003.15 tokens/sec
Step 10535 | loss: 3.172102 | lr:3.0044e-04 | norm 0.3385 | dt 338.45ms | 1549097.75 tokens/sec
Step 10536 | loss: 3.130137 | lr:3.0040e-04 | norm 0.2959 | dt 337.44ms | 1553700.12 tokens/sec
Step 10537 | loss: 3.196919 | lr:3.0035e-04 | norm 0.3284 | dt 337.04ms | 1555553.14 tokens/sec
Step 10538 | loss: 3.173834 | lr:3.0030e-04 | norm 0.3111 | dt 337.60ms | 1553005.56 tokens/sec
Step 10539 | loss: 3.205775 | lr:3.0026e-04 | norm 0.2921 | dt 338.08ms | 1550770.27 tokens/sec
Step 10540 | loss: 3.229828 | lr:3.0021e-04 | norm 0.3037 | dt 337.76ms | 1552240.39 tokens/sec
Step 10541 | loss: 3.197870 | lr:3.0017e-04 | norm 0.2790 | dt 337.46ms | 1553611.21 tokens/sec
Step 10542 | loss: 3.153216 | lr:3.0012e-04 | norm 0.2702 | dt 337.67ms | 1552679.88 tokens/sec
Step 10543 | loss: 3.225977 | lr:3.0007e-04 | norm 0.2734 | dt 337.46ms | 1553627.68 tokens/sec
Step 10544 | loss: 3.178410 | lr:3.0003e-04 | norm 0.2739 | dt 337.60ms | 1552995.69 tokens/sec
Step 10545 | loss: 3.194365 | lr:2.9998e-04 | norm 0.2799 | dt 337.81ms | 1552035.52 tokens/sec
Step 10546 | loss: 3.228725 | lr:2.9994e-04 | norm 0.2787 | dt 338.43ms | 1549190.51 tokens/sec
Step 10547 | loss: 3.205247 | lr:2.9989e-04 | norm 0.2821 | dt 337.54ms | 1553258.95 tokens/sec
Step 10548 | loss: 3.256704 | lr:2.9985e-04 | norm 0.3096 | dt 337.77ms | 1552220.67 tokens/sec
Step 10549 | loss: 3.199774 | lr:2.9980e-04 | norm 0.2765 | dt 337.34ms | 1554194.27 tokens/sec
Step 10550 | loss: 3.176887 | lr:2.9975e-04 | norm 0.2732 | dt 338.68ms | 1548014.88 tokens/sec
Step 10551 | loss: 3.224147 | lr:2.9971e-04 | norm 0.2914 | dt 337.39ms | 1553970.22 tokens/sec
Step 10552 | loss: 3.145574 | lr:2.9966e-04 | norm 0.2815 | dt 337.75ms | 1552310.51 tokens/sec
Step 10553 | loss: 3.164101 | lr:2.9962e-04 | norm 0.3212 | dt 337.71ms | 1552474.90 tokens/sec
Step 10554 | loss: 3.189876 | lr:2.9957e-04 | norm 0.3029 | dt 338.33ms | 1549640.29 tokens/sec
Step 10555 | loss: 3.259794 | lr:2.9952e-04 | norm 0.4000 | dt 338.28ms | 1549886.04 tokens/sec
Step 10556 | loss: 3.190609 | lr:2.9948e-04 | norm 0.3285 | dt 337.35ms | 1554117.38 tokens/sec
Step 10557 | loss: 3.187470 | lr:2.9943e-04 | norm 0.3150 | dt 337.68ms | 1552637.13 tokens/sec
Step 10558 | loss: 3.166287 | lr:2.9939e-04 | norm 0.2922 | dt 338.56ms | 1548592.66 tokens/sec
Step 10559 | loss: 3.194366 | lr:2.9934e-04 | norm 0.3141 | dt 337.31ms | 1554299.73 tokens/sec
Step 10560 | loss: 3.178614 | lr:2.9929e-04 | norm 0.2849 | dt 337.72ms | 1552455.17 tokens/sec
Step 10561 | loss: 3.150784 | lr:2.9925e-04 | norm 0.3107 | dt 337.84ms | 1551872.32 tokens/sec
Step 10562 | loss: 3.179473 | lr:2.9920e-04 | norm 0.3015 | dt 338.82ms | 1547409.23 tokens/sec
Step 10563 | loss: 3.171518 | lr:2.9916e-04 | norm 0.2843 | dt 338.37ms | 1549448.12 tokens/sec
Step 10564 | loss: 3.101841 | lr:2.9911e-04 | norm 0.2717 | dt 337.87ms | 1551732.15 tokens/sec
Step 10565 | loss: 3.174725 | lr:2.9906e-04 | norm 0.2740 | dt 339.10ms | 1546112.37 tokens/sec
Step 10566 | loss: 3.128747 | lr:2.9902e-04 | norm 0.2725 | dt 338.71ms | 1547897.20 tokens/sec
Step 10567 | loss: 3.180622 | lr:2.9897e-04 | norm 0.2883 | dt 338.40ms | 1549311.66 tokens/sec
Step 10568 | loss: 3.113459 | lr:2.9893e-04 | norm 0.2689 | dt 338.50ms | 1548877.34 tokens/sec
Step 10569 | loss: 3.113804 | lr:2.9888e-04 | norm 0.2905 | dt 337.27ms | 1554512.89 tokens/sec
Step 10570 | loss: 3.139236 | lr:2.9884e-04 | norm 0.2799 | dt 338.83ms | 1547338.46 tokens/sec
Step 10571 | loss: 3.217560 | lr:2.9879e-04 | norm 0.2776 | dt 338.87ms | 1547147.94 tokens/sec
Step 10572 | loss: 3.164831 | lr:2.9874e-04 | norm 0.2894 | dt 337.65ms | 1552764.30 tokens/sec
Step 10573 | loss: 3.219460 | lr:2.9870e-04 | norm 0.2999 | dt 336.99ms | 1555802.97 tokens/sec
Step 10574 | loss: 3.203548 | lr:2.9865e-04 | norm 0.2811 | dt 339.70ms | 1543397.33 tokens/sec
Step 10575 | loss: 3.193156 | lr:2.9861e-04 | norm 0.2770 | dt 338.85ms | 1547249.18 tokens/sec
Step 10576 | loss: 3.231504 | lr:2.9856e-04 | norm 0.2927 | dt 338.26ms | 1549977.80 tokens/sec
Step 10577 | loss: 3.188041 | lr:2.9851e-04 | norm 0.2888 | dt 338.95ms | 1546818.19 tokens/sec
Step 10578 | loss: 3.232191 | lr:2.9847e-04 | norm 0.3124 | dt 338.09ms | 1550745.12 tokens/sec
Step 10579 | loss: 3.175199 | lr:2.9842e-04 | norm 0.2769 | dt 338.18ms | 1550307.81 tokens/sec
Step 10580 | loss: 3.221033 | lr:2.9838e-04 | norm 0.2677 | dt 339.41ms | 1544684.21 tokens/sec
Step 10581 | loss: 3.258654 | lr:2.9833e-04 | norm 0.2907 | dt 342.95ms | 1528774.77 tokens/sec
Step 10582 | loss: 3.186176 | lr:2.9828e-04 | norm 0.2913 | dt 337.83ms | 1551911.75 tokens/sec
Step 10583 | loss: 3.336367 | lr:2.9824e-04 | norm 0.3110 | dt 903.52ms | 580274.94 tokens/sec
Step 10584 | loss: 3.166003 | lr:2.9819e-04 | norm 0.2884 | dt 335.40ms | 1563166.29 tokens/sec
Step 10585 | loss: 3.155215 | lr:2.9815e-04 | norm 0.2656 | dt 337.87ms | 1551723.39 tokens/sec
Step 10586 | loss: 3.169675 | lr:2.9810e-04 | norm 0.2649 | dt 338.55ms | 1548636.28 tokens/sec
Step 10587 | loss: 3.202913 | lr:2.9805e-04 | norm 0.2854 | dt 338.10ms | 1550689.34 tokens/sec
Step 10588 | loss: 3.158867 | lr:2.9801e-04 | norm 0.2777 | dt 338.58ms | 1548501.06 tokens/sec
Step 10589 | loss: 3.145606 | lr:2.9796e-04 | norm 0.2754 | dt 338.87ms | 1547158.83 tokens/sec
Step 10590 | loss: 3.173621 | lr:2.9792e-04 | norm 0.2653 | dt 338.72ms | 1547839.46 tokens/sec
Step 10591 | loss: 3.161822 | lr:2.9787e-04 | norm 0.2863 | dt 338.26ms | 1549945.03 tokens/sec
Step 10592 | loss: 3.185825 | lr:2.9783e-04 | norm 0.2680 | dt 338.37ms | 1549433.93 tokens/sec
Step 10593 | loss: 3.159209 | lr:2.9778e-04 | norm 0.3077 | dt 338.12ms | 1550592.03 tokens/sec
Step 10594 | loss: 3.167477 | lr:2.9773e-04 | norm 0.3009 | dt 338.07ms | 1550806.36 tokens/sec
Step 10595 | loss: 3.164141 | lr:2.9769e-04 | norm 0.2777 | dt 338.61ms | 1548361.50 tokens/sec
Step 10596 | loss: 3.187019 | lr:2.9764e-04 | norm 0.2829 | dt 339.00ms | 1546570.16 tokens/sec
Step 10597 | loss: 3.141863 | lr:2.9760e-04 | norm 0.2757 | dt 338.86ms | 1547219.79 tokens/sec
Step 10598 | loss: 3.213679 | lr:2.9755e-04 | norm 0.2882 | dt 338.14ms | 1550502.38 tokens/sec
Step 10599 | loss: 3.180498 | lr:2.9750e-04 | norm 0.2691 | dt 338.84ms | 1547289.46 tokens/sec
Step 10600 | loss: 3.190692 | lr:2.9746e-04 | norm 0.2909 | dt 338.68ms | 1548014.88 tokens/sec
Step 10601 | loss: 3.159540 | lr:2.9741e-04 | norm 0.2685 | dt 338.76ms | 1547674.96 tokens/sec
Step 10602 | loss: 3.124405 | lr:2.9737e-04 | norm 0.2655 | dt 338.62ms | 1548285.19 tokens/sec
Step 10603 | loss: 3.213608 | lr:2.9732e-04 | norm 0.2694 | dt 339.32ms | 1545127.04 tokens/sec
Step 10604 | loss: 3.160774 | lr:2.9728e-04 | norm 0.3055 | dt 338.04ms | 1550981.36 tokens/sec
Step 10605 | loss: 3.196312 | lr:2.9723e-04 | norm 0.2567 | dt 338.52ms | 1548766.08 tokens/sec
Step 10606 | loss: 3.140879 | lr:2.9718e-04 | norm 0.2835 | dt 342.49ms | 1530796.78 tokens/sec
Step 10607 | loss: 3.212620 | lr:2.9714e-04 | norm 0.2767 | dt 338.68ms | 1548036.68 tokens/sec
Step 10608 | loss: 3.255366 | lr:2.9709e-04 | norm 0.2944 | dt 338.38ms | 1549423.01 tokens/sec
Step 10609 | loss: 3.209310 | lr:2.9705e-04 | norm 0.2943 | dt 338.84ms | 1547304.70 tokens/sec
Step 10610 | loss: 3.228885 | lr:2.9700e-04 | norm 0.3049 | dt 338.54ms | 1548676.64 tokens/sec
Step 10611 | loss: 3.225440 | lr:2.9695e-04 | norm 0.3035 | dt 338.18ms | 1550337.32 tokens/sec
Step 10612 | loss: 3.238908 | lr:2.9691e-04 | norm 0.3005 | dt 338.67ms | 1548061.74 tokens/sec
Step 10613 | loss: 3.220320 | lr:2.9686e-04 | norm 0.2838 | dt 338.43ms | 1549178.50 tokens/sec
Step 10614 | loss: 3.232212 | lr:2.9682e-04 | norm 0.3104 | dt 338.37ms | 1549449.21 tokens/sec
Step 10615 | loss: 3.065152 | lr:2.9677e-04 | norm 0.3309 | dt 338.72ms | 1547873.23 tokens/sec
Step 10616 | loss: 3.280784 | lr:2.9672e-04 | norm 0.3030 | dt 338.96ms | 1546737.68 tokens/sec
Step 10617 | loss: 3.185551 | lr:2.9668e-04 | norm 0.2992 | dt 338.61ms | 1548349.50 tokens/sec
Step 10618 | loss: 3.192610 | lr:2.9663e-04 | norm 0.2853 | dt 338.47ms | 1549001.72 tokens/sec
Step 10619 | loss: 3.169615 | lr:2.9659e-04 | norm 0.2898 | dt 338.48ms | 1548930.80 tokens/sec
Step 10620 | loss: 3.214538 | lr:2.9654e-04 | norm 0.3076 | dt 339.35ms | 1544959.86 tokens/sec
Step 10621 | loss: 3.174891 | lr:2.9650e-04 | norm 0.2915 | dt 338.10ms | 1550693.72 tokens/sec
Step 10622 | loss: 3.183819 | lr:2.9645e-04 | norm 0.2822 | dt 337.84ms | 1551883.27 tokens/sec
Step 10623 | loss: 3.157748 | lr:2.9640e-04 | norm 0.2850 | dt 338.22ms | 1550146.06 tokens/sec
Step 10624 | loss: 3.149013 | lr:2.9636e-04 | norm 0.2743 | dt 337.63ms | 1552836.67 tokens/sec
Step 10625 | loss: 3.162042 | lr:2.9631e-04 | norm 0.2904 | dt 337.85ms | 1551855.90 tokens/sec
Step 10626 | loss: 3.150606 | lr:2.9627e-04 | norm 0.2853 | dt 338.65ms | 1548156.56 tokens/sec
Step 10627 | loss: 3.173821 | lr:2.9622e-04 | norm 0.2872 | dt 338.06ms | 1550894.95 tokens/sec
Step 10628 | loss: 3.183837 | lr:2.9617e-04 | norm 0.2649 | dt 337.98ms | 1551233.01 tokens/sec
Step 10629 | loss: 3.224105 | lr:2.9613e-04 | norm 0.2704 | dt 338.14ms | 1550515.50 tokens/sec
Step 10630 | loss: 3.153276 | lr:2.9608e-04 | norm 0.2875 | dt 337.96ms | 1551335.87 tokens/sec
Step 10631 | loss: 3.207147 | lr:2.9604e-04 | norm 0.2621 | dt 337.80ms | 1552048.67 tokens/sec
Step 10632 | loss: 3.180968 | lr:2.9599e-04 | norm 0.2911 | dt 337.46ms | 1553632.07 tokens/sec
Step 10633 | loss: 3.184641 | lr:2.9595e-04 | norm 0.2645 | dt 337.83ms | 1551932.56 tokens/sec
Step 10634 | loss: 3.167902 | lr:2.9590e-04 | norm 0.2935 | dt 338.46ms | 1549053.01 tokens/sec
Step 10635 | loss: 3.219311 | lr:2.9585e-04 | norm 0.3097 | dt 337.75ms | 1552288.60 tokens/sec
Step 10636 | loss: 3.254162 | lr:2.9581e-04 | norm 0.4366 | dt 338.21ms | 1550205.07 tokens/sec
Step 10637 | loss: 3.193288 | lr:2.9576e-04 | norm 0.3104 | dt 337.73ms | 1552372.98 tokens/sec
Step 10638 | loss: 3.138072 | lr:2.9572e-04 | norm 0.3312 | dt 338.11ms | 1550631.39 tokens/sec
Step 10639 | loss: 3.197604 | lr:2.9567e-04 | norm 0.3108 | dt 1044.11ms | 502139.55 tokens/sec
Step 10640 | loss: 3.160802 | lr:2.9562e-04 | norm 0.2941 | dt 335.69ms | 1561838.46 tokens/sec
Step 10641 | loss: 3.195167 | lr:2.9558e-04 | norm 0.3232 | dt 337.13ms | 1555158.21 tokens/sec
Step 10642 | loss: 3.199795 | lr:2.9553e-04 | norm 0.3189 | dt 339.19ms | 1545697.23 tokens/sec
Step 10643 | loss: 3.146879 | lr:2.9549e-04 | norm 0.3399 | dt 337.90ms | 1551616.09 tokens/sec
Step 10644 | loss: 3.232161 | lr:2.9544e-04 | norm 0.2697 | dt 337.31ms | 1554322.80 tokens/sec
Step 10645 | loss: 3.265526 | lr:2.9540e-04 | norm 0.3196 | dt 339.63ms | 1543692.03 tokens/sec
Step 10646 | loss: 3.213849 | lr:2.9535e-04 | norm 0.2805 | dt 338.26ms | 1549936.29 tokens/sec
Step 10647 | loss: 3.153968 | lr:2.9530e-04 | norm 0.2781 | dt 337.48ms | 1553515.72 tokens/sec
Step 10648 | loss: 3.223656 | lr:2.9526e-04 | norm 0.2841 | dt 337.09ms | 1555321.00 tokens/sec
Step 10649 | loss: 3.253635 | lr:2.9521e-04 | norm 0.2781 | dt 338.14ms | 1550498.01 tokens/sec
Step 10650 | loss: 3.240172 | lr:2.9517e-04 | norm 0.2683 | dt 338.21ms | 1550172.29 tokens/sec
Step 10651 | loss: 3.153360 | lr:2.9512e-04 | norm 0.2646 | dt 336.94ms | 1556015.44 tokens/sec
Step 10652 | loss: 3.265940 | lr:2.9507e-04 | norm 0.2784 | dt 337.80ms | 1552054.14 tokens/sec
Step 10653 | loss: 3.203484 | lr:2.9503e-04 | norm 0.2618 | dt 339.20ms | 1545641.82 tokens/sec
Step 10654 | loss: 3.115804 | lr:2.9498e-04 | norm 0.3017 | dt 337.32ms | 1554260.18 tokens/sec
Step 10655 | loss: 3.148119 | lr:2.9494e-04 | norm 0.2476 | dt 337.49ms | 1553494.87 tokens/sec
Step 10656 | loss: 3.141678 | lr:2.9489e-04 | norm 0.2943 | dt 337.65ms | 1552773.08 tokens/sec
Step 10657 | loss: 3.153978 | lr:2.9485e-04 | norm 0.2460 | dt 338.14ms | 1550507.84 tokens/sec
Step 10658 | loss: 3.219426 | lr:2.9480e-04 | norm 0.2685 | dt 338.60ms | 1548414.92 tokens/sec
Step 10659 | loss: 3.148577 | lr:2.9475e-04 | norm 0.2761 | dt 338.43ms | 1549189.42 tokens/sec
Step 10660 | loss: 3.219341 | lr:2.9471e-04 | norm 0.3126 | dt 338.99ms | 1546620.19 tokens/sec
Step 10661 | loss: 3.170399 | lr:2.9466e-04 | norm 0.3259 | dt 339.47ms | 1544419.50 tokens/sec
Step 10662 | loss: 3.145737 | lr:2.9462e-04 | norm 0.3109 | dt 338.85ms | 1547266.60 tokens/sec
Step 10663 | loss: 3.179400 | lr:2.9457e-04 | norm 0.2900 | dt 338.30ms | 1549786.64 tokens/sec
Step 10664 | loss: 3.176327 | lr:2.9453e-04 | norm 0.2733 | dt 338.69ms | 1547999.63 tokens/sec
Step 10665 | loss: 3.173271 | lr:2.9448e-04 | norm 0.2821 | dt 339.34ms | 1545044.53 tokens/sec
Step 10666 | loss: 3.203305 | lr:2.9443e-04 | norm 0.2769 | dt 338.25ms | 1550019.32 tokens/sec
Step 10667 | loss: 3.166935 | lr:2.9439e-04 | norm 0.2867 | dt 338.55ms | 1548607.93 tokens/sec
Step 10668 | loss: 3.136970 | lr:2.9434e-04 | norm 0.2984 | dt 338.77ms | 1547632.48 tokens/sec
Step 10669 | loss: 3.256464 | lr:2.9430e-04 | norm 0.3718 | dt 338.75ms | 1547707.64 tokens/sec
Step 10670 | loss: 3.181309 | lr:2.9425e-04 | norm 0.3150 | dt 338.96ms | 1546762.70 tokens/sec
Step 10671 | loss: 3.159565 | lr:2.9420e-04 | norm 0.3232 | dt 338.76ms | 1547667.34 tokens/sec
Step 10672 | loss: 3.208178 | lr:2.9416e-04 | norm 0.3364 | dt 339.15ms | 1545909.12 tokens/sec
Step 10673 | loss: 3.231541 | lr:2.9411e-04 | norm 0.3082 | dt 338.26ms | 1549961.41 tokens/sec
Step 10674 | loss: 3.155701 | lr:2.9407e-04 | norm 0.3085 | dt 340.46ms | 1539949.51 tokens/sec
Step 10675 | loss: 3.221206 | lr:2.9402e-04 | norm 0.2842 | dt 338.24ms | 1550047.72 tokens/sec
Step 10676 | loss: 3.190453 | lr:2.9398e-04 | norm 0.2871 | dt 338.27ms | 1549902.42 tokens/sec
Step 10677 | loss: 3.159660 | lr:2.9393e-04 | norm 0.2703 | dt 338.26ms | 1549942.84 tokens/sec
Step 10678 | loss: 3.185803 | lr:2.9388e-04 | norm 0.2809 | dt 337.61ms | 1552935.37 tokens/sec
Step 10679 | loss: 3.170328 | lr:2.9384e-04 | norm 0.2704 | dt 338.18ms | 1550302.34 tokens/sec
Step 10680 | loss: 3.206867 | lr:2.9379e-04 | norm 0.2722 | dt 338.20ms | 1550241.14 tokens/sec
Step 10681 | loss: 3.225800 | lr:2.9375e-04 | norm 0.2871 | dt 337.83ms | 1551908.46 tokens/sec
Step 10682 | loss: 3.240836 | lr:2.9370e-04 | norm 0.2833 | dt 338.17ms | 1550379.95 tokens/sec
Step 10683 | loss: 3.253130 | lr:2.9365e-04 | norm 0.2844 | dt 337.61ms | 1552945.24 tokens/sec
Step 10684 | loss: 3.165455 | lr:2.9361e-04 | norm 0.2853 | dt 338.39ms | 1549353.14 tokens/sec
Step 10685 | loss: 3.170784 | lr:2.9356e-04 | norm 0.2672 | dt 338.30ms | 1549758.24 tokens/sec
Step 10686 | loss: 3.197023 | lr:2.9352e-04 | norm 0.2758 | dt 339.40ms | 1544731.95 tokens/sec
Step 10687 | loss: 3.215844 | lr:2.9347e-04 | norm 0.2671 | dt 338.43ms | 1549181.78 tokens/sec
Step 10688 | loss: 3.191937 | lr:2.9343e-04 | norm 0.2828 | dt 337.62ms | 1552911.24 tokens/sec
Step 10689 | loss: 3.178647 | lr:2.9338e-04 | norm 0.2886 | dt 338.41ms | 1549283.28 tokens/sec
Step 10690 | loss: 3.181899 | lr:2.9333e-04 | norm 0.2921 | dt 338.22ms | 1550130.77 tokens/sec
Step 10691 | loss: 3.147080 | lr:2.9329e-04 | norm 0.2853 | dt 337.32ms | 1554259.08 tokens/sec
Step 10692 | loss: 3.181555 | lr:2.9324e-04 | norm 0.2847 | dt 337.85ms | 1551830.71 tokens/sec
Step 10693 | loss: 3.125116 | lr:2.9320e-04 | norm 0.2754 | dt 337.97ms | 1551297.57 tokens/sec
Step 10694 | loss: 3.180006 | lr:2.9315e-04 | norm 0.2730 | dt 337.38ms | 1554017.44 tokens/sec
Step 10695 | loss: 3.155179 | lr:2.9311e-04 | norm 0.2791 | dt 337.77ms | 1552187.80 tokens/sec
Step 10696 | loss: 3.155702 | lr:2.9306e-04 | norm 0.2800 | dt 337.81ms | 1552043.19 tokens/sec
Step 10697 | loss: 3.124786 | lr:2.9301e-04 | norm 0.2766 | dt 338.37ms | 1549443.75 tokens/sec
Step 10698 | loss: 3.132269 | lr:2.9297e-04 | norm 0.2668 | dt 337.80ms | 1552061.81 tokens/sec
Step 10699 | loss: 3.200778 | lr:2.9292e-04 | norm 0.2989 | dt 337.62ms | 1552899.18 tokens/sec
Step 10700 | loss: 3.122961 | lr:2.9288e-04 | norm 0.2928 | dt 337.62ms | 1552907.95 tokens/sec
Step 10701 | loss: 3.178434 | lr:2.9283e-04 | norm 0.3070 | dt 337.89ms | 1551665.36 tokens/sec
Step 10702 | loss: 3.161599 | lr:2.9279e-04 | norm 0.2871 | dt 337.65ms | 1552754.44 tokens/sec
Step 10703 | loss: 3.167488 | lr:2.9274e-04 | norm 0.2994 | dt 337.78ms | 1552147.26 tokens/sec
Step 10704 | loss: 3.171975 | lr:2.9269e-04 | norm 0.2770 | dt 338.36ms | 1549489.61 tokens/sec
Step 10705 | loss: 3.126156 | lr:2.9265e-04 | norm 0.2869 | dt 338.04ms | 1550956.21 tokens/sec
Step 10706 | loss: 3.150091 | lr:2.9260e-04 | norm 0.2914 | dt 336.96ms | 1555953.78 tokens/sec
Step 10707 | loss: 3.133676 | lr:2.9256e-04 | norm 0.2769 | dt 338.21ms | 1550184.31 tokens/sec
Step 10708 | loss: 3.196440 | lr:2.9251e-04 | norm 0.2954 | dt 338.07ms | 1550826.04 tokens/sec
Step 10709 | loss: 3.229633 | lr:2.9246e-04 | norm 0.3113 | dt 337.30ms | 1554351.36 tokens/sec
Step 10710 | loss: 3.153858 | lr:2.9242e-04 | norm 0.2959 | dt 337.86ms | 1551774.86 tokens/sec
Step 10711 | loss: 3.122842 | lr:2.9237e-04 | norm 0.3157 | dt 337.51ms | 1553404.88 tokens/sec
Step 10712 | loss: 3.181720 | lr:2.9233e-04 | norm 0.3141 | dt 337.39ms | 1553936.18 tokens/sec
Step 10713 | loss: 3.172630 | lr:2.9228e-04 | norm 0.2892 | dt 338.24ms | 1550042.26 tokens/sec
Step 10714 | loss: 3.123052 | lr:2.9224e-04 | norm 0.3046 | dt 337.90ms | 1551584.35 tokens/sec
Step 10715 | loss: 3.284349 | lr:2.9219e-04 | norm 0.3260 | dt 337.86ms | 1551800.04 tokens/sec
Step 10716 | loss: 3.193525 | lr:2.9214e-04 | norm 0.3443 | dt 338.59ms | 1548451.99 tokens/sec
Step 10717 | loss: 3.163246 | lr:2.9210e-04 | norm 0.2787 | dt 337.69ms | 1552591.09 tokens/sec
Step 10718 | loss: 3.150887 | lr:2.9205e-04 | norm 0.2936 | dt 337.72ms | 1552438.73 tokens/sec
Step 10719 | loss: 3.197834 | lr:2.9201e-04 | norm 0.3206 | dt 337.93ms | 1551447.51 tokens/sec
Step 10720 | loss: 3.197080 | lr:2.9196e-04 | norm 0.2951 | dt 338.08ms | 1550792.14 tokens/sec
Step 10721 | loss: 3.146376 | lr:2.9192e-04 | norm 0.3206 | dt 338.77ms | 1547619.41 tokens/sec
Step 10722 | loss: 3.144664 | lr:2.9187e-04 | norm 0.2973 | dt 338.05ms | 1550926.67 tokens/sec
Step 10723 | loss: 3.210814 | lr:2.9182e-04 | norm 0.2950 | dt 339.44ms | 1544547.51 tokens/sec
Step 10724 | loss: 3.267388 | lr:2.9178e-04 | norm 0.3137 | dt 338.97ms | 1546717.01 tokens/sec
Step 10725 | loss: 3.162798 | lr:2.9173e-04 | norm 0.2934 | dt 340.66ms | 1539034.48 tokens/sec
Step 10726 | loss: 3.159244 | lr:2.9169e-04 | norm 0.2732 | dt 338.45ms | 1549078.10 tokens/sec
Step 10727 | loss: 3.155393 | lr:2.9164e-04 | norm 0.2746 | dt 338.99ms | 1546602.79 tokens/sec
Step 10728 | loss: 3.189977 | lr:2.9160e-04 | norm 0.2588 | dt 338.66ms | 1548106.43 tokens/sec
Step 10729 | loss: 3.131027 | lr:2.9155e-04 | norm 0.2670 | dt 339.43ms | 1544591.99 tokens/sec
Step 10730 | loss: 3.138971 | lr:2.9150e-04 | norm 0.2699 | dt 338.27ms | 1549912.25 tokens/sec
Step 10731 | loss: 3.177601 | lr:2.9146e-04 | norm 0.2716 | dt 338.56ms | 1548563.21 tokens/sec
Step 10732 | loss: 3.175349 | lr:2.9141e-04 | norm 0.2607 | dt 339.85ms | 1542697.87 tokens/sec
Step 10733 | loss: 3.143790 | lr:2.9137e-04 | norm 0.2799 | dt 338.68ms | 1548038.86 tokens/sec
Step 10734 | loss: 3.118965 | lr:2.9132e-04 | norm 0.2664 | dt 339.06ms | 1546318.94 tokens/sec
Step 10735 | loss: 3.148711 | lr:2.9128e-04 | norm 0.2723 | dt 338.47ms | 1548979.90 tokens/sec
Step 10736 | loss: 3.142181 | lr:2.9123e-04 | norm 0.2608 | dt 339.02ms | 1546492.93 tokens/sec
Step 10737 | loss: 3.234747 | lr:2.9118e-04 | norm 0.2883 | dt 339.72ms | 1543305.26 tokens/sec
Step 10738 | loss: 3.131555 | lr:2.9114e-04 | norm 0.2751 | dt 338.42ms | 1549231.98 tokens/sec
Step 10739 | loss: 3.219363 | lr:2.9109e-04 | norm 0.2764 | dt 338.51ms | 1548808.62 tokens/sec
Step 10740 | loss: 3.155753 | lr:2.9105e-04 | norm 0.3122 | dt 337.53ms | 1553326.98 tokens/sec
Step 10741 | loss: 3.202400 | lr:2.9100e-04 | norm 0.3004 | dt 337.03ms | 1555614.77 tokens/sec
Step 10742 | loss: 3.229465 | lr:2.9096e-04 | norm 0.2767 | dt 338.42ms | 1549211.25 tokens/sec
Step 10743 | loss: 3.192770 | lr:2.9091e-04 | norm 0.2955 | dt 337.96ms | 1551324.93 tokens/sec
Step 10744 | loss: 3.151942 | lr:2.9086e-04 | norm 0.2528 | dt 338.09ms | 1550730.90 tokens/sec
Step 10745 | loss: 3.185788 | lr:2.9082e-04 | norm 0.2779 | dt 337.36ms | 1554097.61 tokens/sec
Step 10746 | loss: 3.155260 | lr:2.9077e-04 | norm 0.2633 | dt 337.51ms | 1553402.69 tokens/sec
Step 10747 | loss: 3.191444 | lr:2.9073e-04 | norm 0.2825 | dt 338.54ms | 1548676.64 tokens/sec
Step 10748 | loss: 3.128778 | lr:2.9068e-04 | norm 0.2877 | dt 337.18ms | 1554919.59 tokens/sec
Step 10749 | loss: 3.110858 | lr:2.9064e-04 | norm 0.2755 | dt 337.59ms | 1553040.65 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 10750: 3.1834
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2912/10042=0.2900


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not gonna talk to one who is going to put me by calling a programming language. Can you explain some


ddp_rank 5: ####### Printing generated samples ####### 

rank 3 sample 1 >Hello, I'm a language model, so what's the difference? You can use this example in a class.
I'm a big fan of being in
rank 3 sample 2 >Hello, I'm a language model, so let's create a text message. Just do this for some text, let's say the machine has a document with
rank 5 sample 0 >Hello, I'm a language model, but that is not something I'm a part of.
I need an implementation for this, and I'll add an
rank 3 sample 3 >Hello, I'm a language model, so if you've got a better idea, it may make a good book.
This story is brought up in a


rank 5 sample 1 >Hello, I'm a language model, one you may not remember from elementary school. "I am not. I don't have any friends, because they are
rank 5 sample 2 >Hello, I'm a language model, and I was a guy. How should I be able to program with a bunch of things?
Well, I'll
rank 5 sample 3 >Hello, I'm a language model, this is the right thing to do. I am really happy with this, since everyone knows how to use this, because




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, there should be just one for every 2 people in the group.
(2) Why is this important (and should
rank 2 sample 1 >Hello, I'm a language model, but I will have to learn a great game.” He also had a great time and a lot of fun!


ddp_rank 1: ####### Printing generated samples ####### 


rank 2 sample 2 >Hello, I'm a language model, and I want to do something in every environment.
It's called using object models. I just make my model to

ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 3 >Hello, I'm a language model, but how can I do it?
My solution is to get a simple version of my Language X, with the newrank 1 sample 0 >Hello, I'm a language model, but I don't like to try as many terms. A computer is a type of software program used to create and manipulate



rank 7 sample 0 >Hello, I'm a language model, so I guess I need some help. Otherwise, there's no way me to run an operating system that I can use
rank 1 sample 1 >Hello, I'm a language model, which is why I'm a good fit for my language class. I'm good enough at "print" and "print
rank 7 sample 1 >Hello, I'm a language model, now you need to learn some different features to make programming less complicated.
This module will show you how to create programs
rank 1 sample 2 >Hello, I'm a language model, but some other stuff.
I'm a bit confused on syntax. It's not a new thing but it can feel


ddp_rank 6: ####### Printing generated samples ####### 

rank 1 sample 3 >Hello, I'm a language model, so I'm working with what's really wrong.
On the last note – I see things in the literature of therank 7 sample 2 >Hello, I'm a language model, but I don't think I got around it. One of my favourite tasks is to find the answer to the second problem



rank 7 sample 3 >Hello, I'm a language model, and it's a little too scary stuff. Okay, I'm not good at that, but we should be, what


rank 6 sample 0 >Hello, I'm a language model, I know that some folks on this site don't even have the computer. The question is: how do you get around
rank 6 sample 1 >Hello, I'm a language model, a beginner who is in a lot of other jobs that I don't use.
You know, you've got a
rank 6 sample 2 >Hello, I'm a language model, but this isn't one that you want to teach yourself. You know what your language model is, but don't know
rank 6 sample 3 >Hello, I'm a language model, I am learning what the world language is and what it is not. I've learned the Spanish language just in grammar by




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I am going through grammar for myself with English.
So when I started my language model with a bunch of words


ddp_rank 4: ####### Printing generated samples ####### 

rank 0 sample 1 >Hello, I'm a language model, and there's a reason I can really understand what happened! I don't want to get that way - I'm trying
rank 0 sample 2 >Hello, I'm a language model, and I mean "using, by example, how to use and modify an existing or modified version of a language". However
rank 4 sample 0 >Hello, I'm a language model, and I am a linguist. I work with everyone, and I think in the language learning process, there's no
rank 0 sample 3 >Hello, I'm a language model, but can't think of any way to explain exactly how I found it. So then how would you start? Well of


rank 4 sample 1 >Hello, I'm a language model, someone can do. If you say, "Hello," use the name
- A is the first element of s,
rank 4 sample 2 >Hello, I'm a language model, I started work I had never heard of it. I thought this would be a work-in-progress, since it
rank 4 sample 3 >Hello, I'm a language model, so it had all the elements I didn't even try to work out together, but I know this was a project.


Step 10750 | loss: 3.170023 | lr:2.9059e-04 | norm 0.2831 | dt 12333.57ms | 42509.01 tokens/sec
Step 10751 | loss: 3.258942 | lr:2.9054e-04 | norm 0.2955 | dt 336.44ms | 1558331.03 tokens/sec
Step 10752 | loss: 3.169648 | lr:2.9050e-04 | norm 0.2758 | dt 337.44ms | 1553729.76 tokens/sec
Step 10753 | loss: 3.165376 | lr:2.9045e-04 | norm 0.2811 | dt 336.46ms | 1558242.69 tokens/sec
Step 10754 | loss: 3.156113 | lr:2.9041e-04 | norm 0.2772 | dt 336.24ms | 1559258.10 tokens/sec
Step 10755 | loss: 3.151790 | lr:2.9036e-04 | norm 0.2717 | dt 336.55ms | 1557830.94 tokens/sec
Step 10756 | loss: 3.155373 | lr:2.9032e-04 | norm 0.2806 | dt 337.23ms | 1554674.44 tokens/sec
Step 10757 | loss: 3.180843 | lr:2.9027e-04 | norm 0.2663 | dt 336.52ms | 1557951.25 tokens/sec
Step 10758 | loss: 3.175396 | lr:2.9022e-04 | norm 0.2650 | dt 336.96ms | 1555958.19 tokens/sec
Step 10759 | loss: 3.150520 | lr:2.9018e-04 | norm 0.2619 | dt 336.48ms | 1558153.26 tokens/sec
Step 10760 | loss: 3.228208 | lr:2.9013e-04 | norm 0.2838 | dt 337.05ms | 1555514.63 tokens/sec
Step 10761 | loss: 3.329373 | lr:2.9009e-04 | norm 0.3374 | dt 337.40ms | 1553888.96 tokens/sec
Step 10762 | loss: 3.143458 | lr:2.9004e-04 | norm 0.4132 | dt 337.48ms | 1553545.36 tokens/sec
Step 10763 | loss: 3.123577 | lr:2.9000e-04 | norm 0.3198 | dt 336.87ms | 1556371.15 tokens/sec
Step 10764 | loss: 3.155825 | lr:2.8995e-04 | norm 0.3087 | dt 336.54ms | 1557877.30 tokens/sec
Step 10765 | loss: 3.152890 | lr:2.8990e-04 | norm 0.3181 | dt 336.53ms | 1557928.07 tokens/sec
Step 10766 | loss: 3.160735 | lr:2.8986e-04 | norm 0.2901 | dt 337.94ms | 1551412.49 tokens/sec
Step 10767 | loss: 3.258788 | lr:2.8981e-04 | norm 0.3399 | dt 336.59ms | 1557655.49 tokens/sec
Step 10768 | loss: 3.153454 | lr:2.8977e-04 | norm 0.3430 | dt 336.16ms | 1559649.59 tokens/sec
Step 10769 | loss: 3.195611 | lr:2.8972e-04 | norm 0.2988 | dt 337.63ms | 1552860.80 tokens/sec
Step 10770 | loss: 3.176537 | lr:2.8968e-04 | norm 0.3501 | dt 337.05ms | 1555522.33 tokens/sec
Step 10771 | loss: 3.213201 | lr:2.8963e-04 | norm 0.3045 | dt 337.32ms | 1554277.76 tokens/sec
Step 10772 | loss: 3.153491 | lr:2.8958e-04 | norm 0.2999 | dt 906.18ms | 578570.35 tokens/sec
Step 10773 | loss: 3.160219 | lr:2.8954e-04 | norm 0.2944 | dt 334.82ms | 1565874.45 tokens/sec
Step 10774 | loss: 3.238787 | lr:2.8949e-04 | norm 0.3102 | dt 337.24ms | 1554662.35 tokens/sec
Step 10775 | loss: 3.224175 | lr:2.8945e-04 | norm 0.2922 | dt 336.92ms | 1556109.03 tokens/sec
Step 10776 | loss: 3.178950 | lr:2.8940e-04 | norm 0.3071 | dt 337.09ms | 1555346.30 tokens/sec
Step 10777 | loss: 3.205059 | lr:2.8936e-04 | norm 0.2986 | dt 337.17ms | 1554971.26 tokens/sec
Step 10778 | loss: 3.196617 | lr:2.8931e-04 | norm 0.3214 | dt 337.39ms | 1553961.43 tokens/sec
Step 10779 | loss: 3.200682 | lr:2.8926e-04 | norm 0.3235 | dt 338.00ms | 1551129.06 tokens/sec
Step 10780 | loss: 3.188031 | lr:2.8922e-04 | norm 0.3035 | dt 336.93ms | 1556055.07 tokens/sec
Step 10781 | loss: 3.174111 | lr:2.8917e-04 | norm 0.3077 | dt 337.29ms | 1554408.50 tokens/sec
Step 10782 | loss: 3.223960 | lr:2.8913e-04 | norm 0.3049 | dt 337.75ms | 1552300.65 tokens/sec
Step 10783 | loss: 3.228420 | lr:2.8908e-04 | norm 0.2827 | dt 337.55ms | 1553238.11 tokens/sec
Step 10784 | loss: 3.250372 | lr:2.8904e-04 | norm 0.3055 | dt 336.91ms | 1556188.32 tokens/sec
Step 10785 | loss: 3.198771 | lr:2.8899e-04 | norm 0.2830 | dt 338.26ms | 1549967.97 tokens/sec
Step 10786 | loss: 3.210583 | lr:2.8895e-04 | norm 0.2906 | dt 337.66ms | 1552687.56 tokens/sec
Step 10787 | loss: 3.228301 | lr:2.8890e-04 | norm 0.2707 | dt 337.43ms | 1553768.19 tokens/sec
Step 10788 | loss: 3.205575 | lr:2.8885e-04 | norm 0.3138 | dt 337.95ms | 1551399.35 tokens/sec
Step 10789 | loss: 3.186568 | lr:2.8881e-04 | norm 0.2699 | dt 338.75ms | 1547703.28 tokens/sec
Step 10790 | loss: 3.190064 | lr:2.8876e-04 | norm 0.2946 | dt 337.47ms | 1553564.01 tokens/sec
Step 10791 | loss: 3.257611 | lr:2.8872e-04 | norm 0.2842 | dt 338.29ms | 1549808.48 tokens/sec
Step 10792 | loss: 3.216091 | lr:2.8867e-04 | norm 0.3074 | dt 337.33ms | 1554211.85 tokens/sec
Step 10793 | loss: 3.167108 | lr:2.8863e-04 | norm 0.2827 | dt 338.04ms | 1550980.27 tokens/sec
Step 10794 | loss: 3.239056 | lr:2.8858e-04 | norm 0.3066 | dt 338.38ms | 1549384.80 tokens/sec
Step 10795 | loss: 3.176354 | lr:2.8853e-04 | norm 0.3038 | dt 338.08ms | 1550769.17 tokens/sec
Step 10796 | loss: 3.132582 | lr:2.8849e-04 | norm 0.2966 | dt 337.26ms | 1554564.54 tokens/sec
Step 10797 | loss: 3.155919 | lr:2.8844e-04 | norm 0.2836 | dt 337.34ms | 1554176.69 tokens/sec
Step 10798 | loss: 3.163036 | lr:2.8840e-04 | norm 0.2668 | dt 338.47ms | 1548986.45 tokens/sec
Step 10799 | loss: 3.169553 | lr:2.8835e-04 | norm 0.2947 | dt 337.81ms | 1552004.85 tokens/sec
Step 10800 | loss: 3.150649 | lr:2.8831e-04 | norm 0.2857 | dt 337.49ms | 1553503.65 tokens/sec
Step 10801 | loss: 3.188555 | lr:2.8826e-04 | norm 0.3178 | dt 339.63ms | 1543692.03 tokens/sec
Step 10802 | loss: 3.202425 | lr:2.8821e-04 | norm 0.2998 | dt 338.17ms | 1550390.88 tokens/sec
Step 10803 | loss: 3.112065 | lr:2.8817e-04 | norm 0.2983 | dt 338.88ms | 1547127.26 tokens/sec
Step 10804 | loss: 3.165229 | lr:2.8812e-04 | norm 0.2908 | dt 337.35ms | 1554123.97 tokens/sec
Step 10805 | loss: 3.171888 | lr:2.8808e-04 | norm 0.3000 | dt 337.27ms | 1554495.30 tokens/sec
Step 10806 | loss: 3.184369 | lr:2.8803e-04 | norm 0.3304 | dt 338.06ms | 1550887.29 tokens/sec
Step 10807 | loss: 3.105529 | lr:2.8799e-04 | norm 0.2920 | dt 337.78ms | 1552149.45 tokens/sec
Step 10808 | loss: 3.188344 | lr:2.8794e-04 | norm 0.2988 | dt 337.38ms | 1554003.16 tokens/sec
Step 10809 | loss: 3.174832 | lr:2.8790e-04 | norm 0.2953 | dt 338.15ms | 1550443.35 tokens/sec
Step 10810 | loss: 3.179281 | lr:2.8785e-04 | norm 0.3585 | dt 337.46ms | 1553622.19 tokens/sec
Step 10811 | loss: 3.200972 | lr:2.8780e-04 | norm 0.3371 | dt 337.52ms | 1553348.92 tokens/sec
Step 10812 | loss: 3.183774 | lr:2.8776e-04 | norm 0.3207 | dt 338.28ms | 1549849.99 tokens/sec
Step 10813 | loss: 3.173827 | lr:2.8771e-04 | norm 0.3401 | dt 337.99ms | 1551199.08 tokens/sec
Step 10814 | loss: 3.204564 | lr:2.8767e-04 | norm 0.3139 | dt 338.47ms | 1548992.99 tokens/sec
Step 10815 | loss: 3.192222 | lr:2.8762e-04 | norm 0.3081 | dt 337.73ms | 1552389.42 tokens/sec
Step 10816 | loss: 3.167306 | lr:2.8758e-04 | norm 0.3144 | dt 337.28ms | 1554438.16 tokens/sec
Step 10817 | loss: 3.164238 | lr:2.8753e-04 | norm 0.2696 | dt 338.15ms | 1550455.37 tokens/sec
Step 10818 | loss: 3.167525 | lr:2.8748e-04 | norm 0.3036 | dt 337.73ms | 1552404.76 tokens/sec
Step 10819 | loss: 3.221785 | lr:2.8744e-04 | norm 0.2886 | dt 337.45ms | 1553655.12 tokens/sec
Step 10820 | loss: 3.175411 | lr:2.8739e-04 | norm 0.2890 | dt 337.83ms | 1551948.99 tokens/sec
Step 10821 | loss: 3.171194 | lr:2.8735e-04 | norm 0.3407 | dt 338.15ms | 1550478.33 tokens/sec
Step 10822 | loss: 3.191117 | lr:2.8730e-04 | norm 0.2517 | dt 337.80ms | 1552083.72 tokens/sec
Step 10823 | loss: 3.205380 | lr:2.8726e-04 | norm 0.3045 | dt 338.02ms | 1551045.91 tokens/sec
Step 10824 | loss: 3.216779 | lr:2.8721e-04 | norm 0.2707 | dt 338.20ms | 1550247.70 tokens/sec
Step 10825 | loss: 3.205960 | lr:2.8716e-04 | norm 0.3192 | dt 337.99ms | 1551185.95 tokens/sec
Step 10826 | loss: 3.185444 | lr:2.8712e-04 | norm 0.2804 | dt 338.85ms | 1547257.89 tokens/sec
Step 10827 | loss: 3.181988 | lr:2.8707e-04 | norm 0.2950 | dt 338.75ms | 1547706.55 tokens/sec
Step 10828 | loss: 3.154521 | lr:2.8703e-04 | norm 0.2886 | dt 337.25ms | 1554575.53 tokens/sec
Step 10829 | loss: 3.200336 | lr:2.8698e-04 | norm 0.3201 | dt 1022.36ms | 512821.51 tokens/sec
Step 10830 | loss: 3.129799 | lr:2.8694e-04 | norm 0.2696 | dt 335.52ms | 1562620.89 tokens/sec
Step 10831 | loss: 3.234421 | lr:2.8689e-04 | norm 0.3038 | dt 337.43ms | 1553786.85 tokens/sec
Step 10832 | loss: 3.201679 | lr:2.8685e-04 | norm 0.2950 | dt 337.79ms | 1552092.48 tokens/sec
Step 10833 | loss: 3.203106 | lr:2.8680e-04 | norm 0.3058 | dt 337.26ms | 1554572.23 tokens/sec
Step 10834 | loss: 3.195385 | lr:2.8675e-04 | norm 0.2732 | dt 336.84ms | 1556491.23 tokens/sec
Step 10835 | loss: 3.081960 | lr:2.8671e-04 | norm 0.2814 | dt 337.69ms | 1552558.20 tokens/sec
Step 10836 | loss: 3.153407 | lr:2.8666e-04 | norm 0.2746 | dt 337.83ms | 1551932.56 tokens/sec
Step 10837 | loss: 3.194524 | lr:2.8662e-04 | norm 0.2891 | dt 338.71ms | 1547889.57 tokens/sec
Step 10838 | loss: 3.134802 | lr:2.8657e-04 | norm 0.3122 | dt 337.37ms | 1554054.78 tokens/sec
Step 10839 | loss: 3.107889 | lr:2.8653e-04 | norm 0.2735 | dt 337.17ms | 1554965.77 tokens/sec
Step 10840 | loss: 3.131328 | lr:2.8648e-04 | norm 0.2915 | dt 338.02ms | 1551065.60 tokens/sec
Step 10841 | loss: 3.156628 | lr:2.8644e-04 | norm 0.2627 | dt 338.03ms | 1550999.96 tokens/sec
Step 10842 | loss: 3.149909 | lr:2.8639e-04 | norm 0.2897 | dt 336.95ms | 1555983.51 tokens/sec
Step 10843 | loss: 3.114080 | lr:2.8634e-04 | norm 0.2721 | dt 338.16ms | 1550422.58 tokens/sec
Step 10844 | loss: 3.234746 | lr:2.8630e-04 | norm 0.2962 | dt 337.43ms | 1553755.01 tokens/sec
Step 10845 | loss: 3.170109 | lr:2.8625e-04 | norm 0.2840 | dt 338.64ms | 1548204.52 tokens/sec
Step 10846 | loss: 3.236215 | lr:2.8621e-04 | norm 0.2715 | dt 338.17ms | 1550371.20 tokens/sec
Step 10847 | loss: 3.135482 | lr:2.8616e-04 | norm 0.2556 | dt 339.01ms | 1546521.21 tokens/sec
Step 10848 | loss: 3.086373 | lr:2.8612e-04 | norm 0.2788 | dt 338.75ms | 1547722.89 tokens/sec
Step 10849 | loss: 3.198075 | lr:2.8607e-04 | norm 0.2715 | dt 338.09ms | 1550731.99 tokens/sec
Step 10850 | loss: 3.164449 | lr:2.8602e-04 | norm 0.2661 | dt 338.44ms | 1549132.67 tokens/sec
Step 10851 | loss: 3.179705 | lr:2.8598e-04 | norm 0.2732 | dt 339.27ms | 1545337.69 tokens/sec
Step 10852 | loss: 3.266450 | lr:2.8593e-04 | norm 0.2636 | dt 339.03ms | 1546442.91 tokens/sec
Step 10853 | loss: 3.167398 | lr:2.8589e-04 | norm 0.2895 | dt 339.01ms | 1546538.61 tokens/sec
Step 10854 | loss: 3.178105 | lr:2.8584e-04 | norm 0.2932 | dt 340.45ms | 1539994.80 tokens/sec
Step 10855 | loss: 3.183942 | lr:2.8580e-04 | norm 0.3163 | dt 337.39ms | 1553949.35 tokens/sec
Step 10856 | loss: 3.212698 | lr:2.8575e-04 | norm 0.3061 | dt 337.72ms | 1552448.60 tokens/sec
Step 10857 | loss: 3.190656 | lr:2.8571e-04 | norm 0.3055 | dt 338.37ms | 1549443.75 tokens/sec
Step 10858 | loss: 3.127497 | lr:2.8566e-04 | norm 0.2958 | dt 338.02ms | 1551039.34 tokens/sec
Step 10859 | loss: 3.150112 | lr:2.8561e-04 | norm 0.2789 | dt 338.12ms | 1550578.91 tokens/sec
Step 10860 | loss: 3.122671 | lr:2.8557e-04 | norm 0.2741 | dt 337.90ms | 1551602.96 tokens/sec
Step 10861 | loss: 3.134908 | lr:2.8552e-04 | norm 0.2767 | dt 338.13ms | 1550558.14 tokens/sec
Step 10862 | loss: 3.118271 | lr:2.8548e-04 | norm 0.2812 | dt 337.77ms | 1552222.86 tokens/sec
Step 10863 | loss: 3.192710 | lr:2.8543e-04 | norm 0.2782 | dt 337.87ms | 1551731.06 tokens/sec
Step 10864 | loss: 3.139300 | lr:2.8539e-04 | norm 0.2931 | dt 337.99ms | 1551183.77 tokens/sec
Step 10865 | loss: 3.163487 | lr:2.8534e-04 | norm 0.2937 | dt 339.44ms | 1544559.44 tokens/sec
Step 10866 | loss: 3.166979 | lr:2.8530e-04 | norm 0.2876 | dt 338.33ms | 1549627.19 tokens/sec
Step 10867 | loss: 3.186189 | lr:2.8525e-04 | norm 0.2764 | dt 338.16ms | 1550406.18 tokens/sec
Step 10868 | loss: 3.144735 | lr:2.8520e-04 | norm 0.2722 | dt 338.33ms | 1549647.94 tokens/sec
Step 10869 | loss: 3.096792 | lr:2.8516e-04 | norm 0.2776 | dt 338.14ms | 1550506.75 tokens/sec
Step 10870 | loss: 3.165270 | lr:2.8511e-04 | norm 0.3017 | dt 338.46ms | 1549036.64 tokens/sec
Step 10871 | loss: 3.148708 | lr:2.8507e-04 | norm 0.2640 | dt 338.87ms | 1547175.16 tokens/sec
Step 10872 | loss: 3.159729 | lr:2.8502e-04 | norm 0.2659 | dt 338.64ms | 1548199.07 tokens/sec
Step 10873 | loss: 3.125298 | lr:2.8498e-04 | norm 0.2517 | dt 338.62ms | 1548302.63 tokens/sec
Step 10874 | loss: 3.138400 | lr:2.8493e-04 | norm 0.2728 | dt 338.83ms | 1547359.14 tokens/sec
Step 10875 | loss: 3.144925 | lr:2.8489e-04 | norm 0.2661 | dt 337.98ms | 1551234.10 tokens/sec
Step 10876 | loss: 3.134203 | lr:2.8484e-04 | norm 0.2506 | dt 338.00ms | 1551150.94 tokens/sec
Step 10877 | loss: 3.090510 | lr:2.8479e-04 | norm 0.2680 | dt 339.33ms | 1545073.84 tokens/sec
Step 10878 | loss: 3.148918 | lr:2.8475e-04 | norm 0.2522 | dt 338.46ms | 1549044.28 tokens/sec
Step 10879 | loss: 3.191672 | lr:2.8470e-04 | norm 0.2788 | dt 338.38ms | 1549413.18 tokens/sec
Step 10880 | loss: 3.237440 | lr:2.8466e-04 | norm 0.2696 | dt 338.45ms | 1549067.19 tokens/sec
Step 10881 | loss: 3.143509 | lr:2.8461e-04 | norm 0.2619 | dt 338.57ms | 1548557.76 tokens/sec
Step 10882 | loss: 3.182424 | lr:2.8457e-04 | norm 0.2749 | dt 338.47ms | 1548983.17 tokens/sec
Step 10883 | loss: 3.247304 | lr:2.8452e-04 | norm 0.2853 | dt 337.55ms | 1553233.72 tokens/sec
Step 10884 | loss: 3.216276 | lr:2.8448e-04 | norm 0.2895 | dt 338.47ms | 1549001.72 tokens/sec
Step 10885 | loss: 3.182473 | lr:2.8443e-04 | norm 0.2773 | dt 337.89ms | 1551633.61 tokens/sec
Step 10886 | loss: 3.162536 | lr:2.8438e-04 | norm 0.2996 | dt 338.14ms | 1550490.35 tokens/sec
Step 10887 | loss: 3.176398 | lr:2.8434e-04 | norm 0.2802 | dt 338.34ms | 1549587.88 tokens/sec
Step 10888 | loss: 3.171091 | lr:2.8429e-04 | norm 0.2836 | dt 338.44ms | 1549141.40 tokens/sec
Step 10889 | loss: 3.186126 | lr:2.8425e-04 | norm 0.3011 | dt 337.90ms | 1551619.38 tokens/sec
Step 10890 | loss: 3.202432 | lr:2.8420e-04 | norm 0.2799 | dt 338.56ms | 1548592.66 tokens/sec
Step 10891 | loss: 3.192911 | lr:2.8416e-04 | norm 0.3115 | dt 338.09ms | 1550729.81 tokens/sec
Step 10892 | loss: 3.131002 | lr:2.8411e-04 | norm 0.2887 | dt 338.50ms | 1548848.98 tokens/sec
Step 10893 | loss: 3.131851 | lr:2.8407e-04 | norm 0.3045 | dt 337.65ms | 1552767.59 tokens/sec
Step 10894 | loss: 3.156476 | lr:2.8402e-04 | norm 0.2707 | dt 338.65ms | 1548187.08 tokens/sec
Step 10895 | loss: 3.187065 | lr:2.8397e-04 | norm 0.2795 | dt 338.61ms | 1548336.42 tokens/sec
Step 10896 | loss: 3.149380 | lr:2.8393e-04 | norm 0.2751 | dt 337.11ms | 1555245.10 tokens/sec
Step 10897 | loss: 3.187958 | lr:2.8388e-04 | norm 0.2604 | dt 338.34ms | 1549597.70 tokens/sec
Step 10898 | loss: 3.137465 | lr:2.8384e-04 | norm 0.2533 | dt 338.70ms | 1547945.14 tokens/sec
Step 10899 | loss: 3.220250 | lr:2.8379e-04 | norm 0.2541 | dt 338.37ms | 1549456.85 tokens/sec
Step 10900 | loss: 3.166805 | lr:2.8375e-04 | norm 0.2669 | dt 338.69ms | 1547968.03 tokens/sec
Step 10901 | loss: 3.216108 | lr:2.8370e-04 | norm 0.2746 | dt 338.84ms | 1547305.79 tokens/sec
Step 10902 | loss: 3.178113 | lr:2.8366e-04 | norm 0.2583 | dt 339.04ms | 1546402.67 tokens/sec
Step 10903 | loss: 3.177418 | lr:2.8361e-04 | norm 0.3190 | dt 339.38ms | 1544844.82 tokens/sec
Step 10904 | loss: 3.130371 | lr:2.8357e-04 | norm 0.2624 | dt 337.68ms | 1552626.17 tokens/sec
Step 10905 | loss: 3.159980 | lr:2.8352e-04 | norm 0.3105 | dt 338.53ms | 1548701.72 tokens/sec
Step 10906 | loss: 3.156250 | lr:2.8347e-04 | norm 0.2697 | dt 338.33ms | 1549650.12 tokens/sec
Step 10907 | loss: 3.160565 | lr:2.8343e-04 | norm 0.2677 | dt 340.54ms | 1539594.79 tokens/sec
Step 10908 | loss: 3.173409 | lr:2.8338e-04 | norm 0.2760 | dt 339.46ms | 1544478.08 tokens/sec
Step 10909 | loss: 3.191501 | lr:2.8334e-04 | norm 0.2766 | dt 338.74ms | 1547780.63 tokens/sec
Step 10910 | loss: 3.173680 | lr:2.8329e-04 | norm 0.2782 | dt 338.36ms | 1549491.79 tokens/sec
Step 10911 | loss: 3.152500 | lr:2.8325e-04 | norm 0.2863 | dt 338.94ms | 1546863.89 tokens/sec
Step 10912 | loss: 3.161384 | lr:2.8320e-04 | norm 0.2703 | dt 338.71ms | 1547874.32 tokens/sec
Step 10913 | loss: 3.156971 | lr:2.8316e-04 | norm 0.2722 | dt 338.42ms | 1549238.53 tokens/sec
Step 10914 | loss: 3.181105 | lr:2.8311e-04 | norm 0.2877 | dt 338.70ms | 1547949.50 tokens/sec
Step 10915 | loss: 3.181815 | lr:2.8306e-04 | norm 0.2833 | dt 338.59ms | 1548454.17 tokens/sec
Step 10916 | loss: 3.140007 | lr:2.8302e-04 | norm 0.2949 | dt 338.33ms | 1549620.64 tokens/sec
Step 10917 | loss: 3.151196 | lr:2.8297e-04 | norm 0.2829 | dt 338.23ms | 1550078.32 tokens/sec
Step 10918 | loss: 3.150222 | lr:2.8293e-04 | norm 0.2896 | dt 337.82ms | 1551989.51 tokens/sec
Step 10919 | loss: 3.133866 | lr:2.8288e-04 | norm 0.2893 | dt 338.49ms | 1548884.98 tokens/sec
Step 10920 | loss: 3.138043 | lr:2.8284e-04 | norm 0.2784 | dt 339.02ms | 1546467.92 tokens/sec
Step 10921 | loss: 3.174227 | lr:2.8279e-04 | norm 0.2902 | dt 338.71ms | 1547881.95 tokens/sec
Step 10922 | loss: 3.226632 | lr:2.8275e-04 | norm 0.3245 | dt 340.19ms | 1541159.35 tokens/sec
Step 10923 | loss: 3.145766 | lr:2.8270e-04 | norm 0.3559 | dt 339.14ms | 1545911.29 tokens/sec
Step 10924 | loss: 3.122835 | lr:2.8265e-04 | norm 0.3428 | dt 338.14ms | 1550523.15 tokens/sec
Step 10925 | loss: 3.084449 | lr:2.8261e-04 | norm 0.2867 | dt 338.66ms | 1548124.96 tokens/sec
Step 10926 | loss: 3.118319 | lr:2.8256e-04 | norm 0.2888 | dt 339.25ms | 1545444.12 tokens/sec
Step 10927 | loss: 3.205362 | lr:2.8252e-04 | norm 0.2855 | dt 339.34ms | 1545027.16 tokens/sec
Step 10928 | loss: 3.172527 | lr:2.8247e-04 | norm 0.2935 | dt 339.60ms | 1543841.59 tokens/sec
Step 10929 | loss: 3.143249 | lr:2.8243e-04 | norm 0.2999 | dt 337.79ms | 1552126.44 tokens/sec
Step 10930 | loss: 3.152493 | lr:2.8238e-04 | norm 0.3140 | dt 338.22ms | 1550127.49 tokens/sec
Step 10931 | loss: 3.151329 | lr:2.8234e-04 | norm 0.2789 | dt 338.14ms | 1550505.66 tokens/sec
Step 10932 | loss: 3.197893 | lr:2.8229e-04 | norm 0.3262 | dt 338.60ms | 1548406.20 tokens/sec
Step 10933 | loss: 3.145200 | lr:2.8225e-04 | norm 0.3172 | dt 338.82ms | 1547378.74 tokens/sec
Step 10934 | loss: 3.165368 | lr:2.8220e-04 | norm 0.2932 | dt 337.85ms | 1551844.94 tokens/sec
Step 10935 | loss: 3.211321 | lr:2.8215e-04 | norm 0.3009 | dt 338.47ms | 1548987.54 tokens/sec
Step 10936 | loss: 3.144528 | lr:2.8211e-04 | norm 0.2921 | dt 338.59ms | 1548438.91 tokens/sec
Step 10937 | loss: 3.174231 | lr:2.8206e-04 | norm 0.2810 | dt 337.61ms | 1552922.21 tokens/sec
Step 10938 | loss: 3.154045 | lr:2.8202e-04 | norm 0.2783 | dt 337.94ms | 1551415.77 tokens/sec
Step 10939 | loss: 3.208283 | lr:2.8197e-04 | norm 0.3099 | dt 337.89ms | 1551644.56 tokens/sec
Step 10940 | loss: 3.121295 | lr:2.8193e-04 | norm 0.3035 | dt 338.01ms | 1551112.65 tokens/sec
Step 10941 | loss: 3.169865 | lr:2.8188e-04 | norm 0.2777 | dt 338.02ms | 1551061.22 tokens/sec
Step 10942 | loss: 3.146731 | lr:2.8184e-04 | norm 0.2987 | dt 337.71ms | 1552467.23 tokens/sec
Step 10943 | loss: 3.148747 | lr:2.8179e-04 | norm 0.2708 | dt 337.73ms | 1552372.98 tokens/sec
Step 10944 | loss: 3.158128 | lr:2.8175e-04 | norm 0.2955 | dt 338.62ms | 1548298.27 tokens/sec
Step 10945 | loss: 3.130741 | lr:2.8170e-04 | norm 0.2687 | dt 337.85ms | 1551817.57 tokens/sec
Step 10946 | loss: 3.141879 | lr:2.8165e-04 | norm 0.2876 | dt 338.33ms | 1549650.12 tokens/sec
Step 10947 | loss: 3.145120 | lr:2.8161e-04 | norm 0.2819 | dt 338.50ms | 1548865.34 tokens/sec
Step 10948 | loss: 3.172636 | lr:2.8156e-04 | norm 0.2960 | dt 338.50ms | 1548839.16 tokens/sec
Step 10949 | loss: 3.137126 | lr:2.8152e-04 | norm 0.2678 | dt 337.96ms | 1551332.59 tokens/sec
Step 10950 | loss: 3.109512 | lr:2.8147e-04 | norm 0.2905 | dt 338.33ms | 1549645.75 tokens/sec
Step 10951 | loss: 3.151927 | lr:2.8143e-04 | norm 0.3093 | dt 338.64ms | 1548200.16 tokens/sec
Step 10952 | loss: 3.185232 | lr:2.8138e-04 | norm 0.3091 | dt 337.80ms | 1552069.48 tokens/sec
Step 10953 | loss: 3.159481 | lr:2.8134e-04 | norm 0.3520 | dt 338.59ms | 1548455.26 tokens/sec
Step 10954 | loss: 3.156233 | lr:2.8129e-04 | norm 0.2928 | dt 338.61ms | 1548372.40 tokens/sec
Step 10955 | loss: 3.218079 | lr:2.8125e-04 | norm 0.3370 | dt 338.49ms | 1548893.71 tokens/sec
Step 10956 | loss: 3.181582 | lr:2.8120e-04 | norm 0.2863 | dt 337.76ms | 1552245.87 tokens/sec
Step 10957 | loss: 3.159886 | lr:2.8115e-04 | norm 0.3117 | dt 338.60ms | 1548401.84 tokens/sec
Step 10958 | loss: 3.170241 | lr:2.8111e-04 | norm 0.2716 | dt 338.49ms | 1548920.98 tokens/sec
Step 10959 | loss: 3.170580 | lr:2.8106e-04 | norm 0.2950 | dt 338.57ms | 1548539.22 tokens/sec
Step 10960 | loss: 3.119992 | lr:2.8102e-04 | norm 0.2590 | dt 338.20ms | 1550229.12 tokens/sec
Step 10961 | loss: 3.139317 | lr:2.8097e-04 | norm 0.2760 | dt 913.31ms | 574052.74 tokens/sec
Step 10962 | loss: 3.147814 | lr:2.8093e-04 | norm 0.2842 | dt 335.16ms | 1564308.29 tokens/sec
Step 10963 | loss: 3.215549 | lr:2.8088e-04 | norm 0.2621 | dt 337.70ms | 1552541.76 tokens/sec
Step 10964 | loss: 3.147857 | lr:2.8084e-04 | norm 0.2657 | dt 338.15ms | 1550469.58 tokens/sec
Step 10965 | loss: 3.212959 | lr:2.8079e-04 | norm 0.2774 | dt 337.46ms | 1553611.21 tokens/sec
Step 10966 | loss: 3.181089 | lr:2.8075e-04 | norm 0.2679 | dt 339.00ms | 1546557.10 tokens/sec
Step 10967 | loss: 3.170366 | lr:2.8070e-04 | norm 0.2698 | dt 337.83ms | 1551918.32 tokens/sec
Step 10968 | loss: 3.134331 | lr:2.8065e-04 | norm 0.2782 | dt 337.94ms | 1551410.30 tokens/sec
Step 10969 | loss: 3.145371 | lr:2.8061e-04 | norm 0.2783 | dt 336.86ms | 1556376.66 tokens/sec
Step 10970 | loss: 3.184464 | lr:2.8056e-04 | norm 0.2680 | dt 337.67ms | 1552675.50 tokens/sec
Step 10971 | loss: 3.151146 | lr:2.8052e-04 | norm 0.2566 | dt 338.12ms | 1550581.10 tokens/sec
Step 10972 | loss: 3.178905 | lr:2.8047e-04 | norm 0.2694 | dt 337.52ms | 1553343.44 tokens/sec
Step 10973 | loss: 3.118898 | lr:2.8043e-04 | norm 0.2776 | dt 337.37ms | 1554038.31 tokens/sec
Step 10974 | loss: 3.111774 | lr:2.8038e-04 | norm 0.2686 | dt 338.39ms | 1549382.62 tokens/sec
Step 10975 | loss: 3.187973 | lr:2.8034e-04 | norm 0.3022 | dt 337.96ms | 1551349.01 tokens/sec
Step 10976 | loss: 3.159294 | lr:2.8029e-04 | norm 0.3253 | dt 337.99ms | 1551204.56 tokens/sec
Step 10977 | loss: 3.221700 | lr:2.8025e-04 | norm 0.3137 | dt 338.57ms | 1548526.14 tokens/sec
Step 10978 | loss: 3.153322 | lr:2.8020e-04 | norm 0.3073 | dt 338.21ms | 1550183.22 tokens/sec
Step 10979 | loss: 3.156226 | lr:2.8016e-04 | norm 0.3267 | dt 338.22ms | 1550135.14 tokens/sec
Step 10980 | loss: 3.163792 | lr:2.8011e-04 | norm 0.2884 | dt 337.63ms | 1552826.80 tokens/sec
Step 10981 | loss: 3.142123 | lr:2.8006e-04 | norm 0.3003 | dt 337.34ms | 1554163.51 tokens/sec
Step 10982 | loss: 3.148295 | lr:2.8002e-04 | norm 0.2859 | dt 338.42ms | 1549204.70 tokens/sec
Step 10983 | loss: 3.142124 | lr:2.7997e-04 | norm 0.2946 | dt 337.75ms | 1552285.31 tokens/sec
Step 10984 | loss: 3.115634 | lr:2.7993e-04 | norm 0.3026 | dt 337.91ms | 1551573.40 tokens/sec
Step 10985 | loss: 3.087754 | lr:2.7988e-04 | norm 0.2704 | dt 337.68ms | 1552625.07 tokens/sec
Step 10986 | loss: 3.121436 | lr:2.7984e-04 | norm 0.3325 | dt 338.82ms | 1547371.12 tokens/sec
Step 10987 | loss: 3.198804 | lr:2.7979e-04 | norm 0.2691 | dt 338.25ms | 1550008.39 tokens/sec
Step 10988 | loss: 3.147066 | lr:2.7975e-04 | norm 0.2767 | dt 338.25ms | 1550018.22 tokens/sec
Step 10989 | loss: 3.122291 | lr:2.7970e-04 | norm 0.2722 | dt 338.80ms | 1547498.52 tokens/sec
Step 10990 | loss: 3.180297 | lr:2.7966e-04 | norm 0.2818 | dt 339.06ms | 1546299.37 tokens/sec
Step 10991 | loss: 3.147089 | lr:2.7961e-04 | norm 0.2625 | dt 338.67ms | 1548085.72 tokens/sec
Step 10992 | loss: 3.197344 | lr:2.7956e-04 | norm 0.3418 | dt 338.61ms | 1548372.40 tokens/sec
Step 10993 | loss: 3.143451 | lr:2.7952e-04 | norm 0.2883 | dt 339.05ms | 1546341.77 tokens/sec
Step 10994 | loss: 3.109733 | lr:2.7947e-04 | norm 0.3246 | dt 339.36ms | 1544921.87 tokens/sec
Step 10995 | loss: 3.159764 | lr:2.7943e-04 | norm 0.3498 | dt 338.70ms | 1547939.70 tokens/sec
Step 10996 | loss: 3.139236 | lr:2.7938e-04 | norm 0.3366 | dt 339.08ms | 1546217.82 tokens/sec
Step 10997 | loss: 3.159765 | lr:2.7934e-04 | norm 0.2923 | dt 338.18ms | 1550307.81 tokens/sec
Step 10998 | loss: 3.305652 | lr:2.7929e-04 | norm 0.3399 | dt 338.28ms | 1549852.17 tokens/sec
Step 10999 | loss: 3.232736 | lr:2.7925e-04 | norm 0.3122 | dt 339.02ms | 1546494.02 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 11000: 3.1794
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2953/10042=0.2941



ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 1: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but my main goal is to get people to understand, to talk for themselves, to listen, and to be able to
rank 1 sample 0 >Hello, I'm a language model, as it is a cross-platform product that's been adopted since the 1990s. The first time I got to this
rank 5 sample 1 >Hello, I'm a language model, this's one for every other person in the United States and there are some major advantages. I mean, they are alwaysrank 1 sample 1 >Hello, I'm a language model, which means that as we learn a language, we use and use grammar. In other words, it is a language that

rank 5 sample 2 >Hello, I'm a language model, and it helps to remember the code when I want to use it. I am a high school student, and I can
rank 1 sample 2 >Hello, I'm a language model, but they have the same problem. They have to decide whether that language will be used in their home languages or their native
rank 5 sample 3 >Hello, I'm a language model, why don't you take another look at the "discovery" process? If you are interested in the process, this


rank 1 sample 3 >Hello, I'm a language model, so I'm just trying to describe and see what makes those differences true. Thanks for seeing first-hand what is the




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, is not an academic language, but rather an English language model.
To be useful, I need to get the meaning
rank 2 sample 1 >Hello, I'm a language model, but I wonder about how the teacher does or does it? What would students do if they didn't know what I'm
rank 2 sample 2 >Hello, I'm a language model, and I've used it for many occasions, but it's only recently that I learned something about English (which is really
rank 2 sample 3 >Hello, I'm a language model, but how can I get it to work like a book? So, I could do some typing or typing on a different




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I thought I was more of a compiler. I got a lot more mileage getting it in beta version 1.2
rank 7 sample 1 >Hello, I'm a language model, let me write a program to get information about the topic if you want to know information about that topic. If you can
rank 7 sample 2 >Hello, I'm a language model, but I don't think I made any differences. After all, why could I say, "I love that you have


rank 7 sample 3 >Hello, I'm a language model, and it's a great choice for newbies who want to learn a language. If you're really good at something youddp_rank 3: ####### Printing generated samples ####### 




rank 3 sample 0 >Hello, I'm a language model, so I'm not an English teacher for two years now. After I showed above, a question led me to an app
rank 3 sample 1 >Hello, I'm a language model, so let’s open our "Language" dialog box. I'm going to add a couple of more examples in
rank 3 sample 2 >Hello, I'm a language model, so this was a test of all the best ideas in this world. I'll see if that applies to you! :)
rank 3 sample 3 >Hello, I'm a language model, so don't go into a program that will run with any command lines or any syntax. For example, I would just




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know what you're going to expect that would be. There are so many other cases to which you can come
rank 0 sample 1 >Hello, I'm a language model, and can't remember how to say, "Hi: So I am having to learn to do the next word, but
rank 0 sample 2 >Hello, I'm a language model, and I teach English speaking with him on the third floor of an 8th grade classroom in New York City.
How
rank 0 sample 3 >Hello, I'm a language model, and can't do anything for you. So no, I'm not saying you could be anything other than a computer engineer




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I know that many of you are still confused.
But, for one thing, i get an understanding of how
rank 4 sample 1 >Hello, I'm a language model, now let's tell a little how-to lesson: let's pretend there were 4 numbers in the last 2 places,
rank 4 sample 2 >Hello, I'm a language model, I learn some important words but I am also a language model in a completely different way. How I communicate with an English
rank 4 sample 3 >Hello, I'm a language model, so it wasn't the perfect choice? But what better way to teach and understand the language? Here are some answers.




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, meaning to use it when designing programming to create a class, a new method, or a class that is not a single
rank 6 sample 1 >Hello, I'm a language model, not a linguist.
If you find this to be a bit technical, I'll be glad you, now you
rank 6 sample 2 >Hello, I'm a language model, but this one's an interesting topic. In Java, you're supposed to declare a variable called a named string called the
rank 6 sample 3 >Hello, I'm a language model, so I don't need to teach you what to do. I've got the basic syntax without the very long phrases here


Step 11000 | loss: 3.179812 | lr:2.7920e-04 | norm 0.3099 | dt 16655.07ms | 31479.18 tokens/sec
Step 11001 | loss: 3.161673 | lr:2.7916e-04 | norm 0.2924 | dt 333.30ms | 1573043.27 tokens/sec
Step 11002 | loss: 3.169816 | lr:2.7911e-04 | norm 0.3279 | dt 337.22ms | 1554732.70 tokens/sec
Step 11003 | loss: 3.181114 | lr:2.7907e-04 | norm 0.2984 | dt 337.64ms | 1552823.51 tokens/sec
Step 11004 | loss: 3.208673 | lr:2.7902e-04 | norm 0.3144 | dt 336.99ms | 1555786.46 tokens/sec
Step 11005 | loss: 3.175638 | lr:2.7898e-04 | norm 0.2817 | dt 336.84ms | 1556476.90 tokens/sec
Step 11006 | loss: 3.092896 | lr:2.7893e-04 | norm 0.3461 | dt 337.31ms | 1554318.40 tokens/sec
Step 11007 | loss: 3.198876 | lr:2.7888e-04 | norm 0.3630 | dt 336.33ms | 1558834.76 tokens/sec
Step 11008 | loss: 3.159952 | lr:2.7884e-04 | norm 0.3031 | dt 338.06ms | 1550853.39 tokens/sec
Step 11009 | loss: 3.141791 | lr:2.7879e-04 | norm 0.3141 | dt 336.95ms | 1555984.61 tokens/sec
Step 11010 | loss: 3.183030 | lr:2.7875e-04 | norm 0.3014 | dt 336.30ms | 1558981.74 tokens/sec
Step 11011 | loss: 3.147250 | lr:2.7870e-04 | norm 0.3181 | dt 337.05ms | 1555520.13 tokens/sec
Step 11012 | loss: 3.107456 | lr:2.7866e-04 | norm 0.2948 | dt 336.72ms | 1557048.89 tokens/sec
Step 11013 | loss: 3.148812 | lr:2.7861e-04 | norm 0.2808 | dt 336.38ms | 1558604.95 tokens/sec
Step 11014 | loss: 3.129344 | lr:2.7857e-04 | norm 0.2973 | dt 336.98ms | 1555832.69 tokens/sec
Step 11015 | loss: 3.163091 | lr:2.7852e-04 | norm 0.2763 | dt 337.98ms | 1551257.08 tokens/sec
Step 11016 | loss: 3.207127 | lr:2.7848e-04 | norm 0.3084 | dt 336.39ms | 1558574.02 tokens/sec
Step 11017 | loss: 3.194272 | lr:2.7843e-04 | norm 0.3017 | dt 336.87ms | 1556365.64 tokens/sec
Step 11018 | loss: 3.173790 | lr:2.7839e-04 | norm 0.2899 | dt 337.41ms | 1553861.51 tokens/sec
Step 11019 | loss: 3.128796 | lr:2.7834e-04 | norm 0.2897 | dt 1038.52ms | 504841.46 tokens/sec
Step 11020 | loss: 3.267817 | lr:2.7829e-04 | norm 0.2999 | dt 334.97ms | 1565181.21 tokens/sec
Step 11021 | loss: 3.125979 | lr:2.7825e-04 | norm 0.3130 | dt 336.10ms | 1559899.62 tokens/sec
Step 11022 | loss: 3.171209 | lr:2.7820e-04 | norm 0.2632 | dt 337.12ms | 1555175.81 tokens/sec
Step 11023 | loss: 3.109556 | lr:2.7816e-04 | norm 0.2912 | dt 335.58ms | 1562351.12 tokens/sec
Step 11024 | loss: 3.134620 | lr:2.7811e-04 | norm 0.2509 | dt 337.03ms | 1555632.37 tokens/sec
Step 11025 | loss: 3.176154 | lr:2.7807e-04 | norm 0.2921 | dt 336.29ms | 1559022.64 tokens/sec
Step 11026 | loss: 3.204469 | lr:2.7802e-04 | norm 0.2789 | dt 336.37ms | 1558653.56 tokens/sec
Step 11027 | loss: 3.181362 | lr:2.7798e-04 | norm 0.2982 | dt 337.55ms | 1553224.94 tokens/sec
Step 11028 | loss: 3.134219 | lr:2.7793e-04 | norm 0.2979 | dt 338.20ms | 1550222.56 tokens/sec
Step 11029 | loss: 3.169732 | lr:2.7789e-04 | norm 0.3039 | dt 337.63ms | 1552826.80 tokens/sec
Step 11030 | loss: 3.146578 | lr:2.7784e-04 | norm 0.3012 | dt 337.08ms | 1555371.60 tokens/sec
Step 11031 | loss: 3.130370 | lr:2.7780e-04 | norm 0.3080 | dt 337.46ms | 1553640.85 tokens/sec
Step 11032 | loss: 3.158855 | lr:2.7775e-04 | norm 0.2776 | dt 337.31ms | 1554333.79 tokens/sec
Step 11033 | loss: 3.187874 | lr:2.7771e-04 | norm 0.2997 | dt 337.52ms | 1553345.63 tokens/sec
Step 11034 | loss: 3.173332 | lr:2.7766e-04 | norm 0.3345 | dt 337.53ms | 1553306.13 tokens/sec
Step 11035 | loss: 3.168432 | lr:2.7761e-04 | norm 0.2794 | dt 337.73ms | 1552367.50 tokens/sec
Step 11036 | loss: 3.172186 | lr:2.7757e-04 | norm 0.3128 | dt 337.45ms | 1553654.02 tokens/sec
Step 11037 | loss: 3.172928 | lr:2.7752e-04 | norm 0.3064 | dt 337.50ms | 1553458.66 tokens/sec
Step 11038 | loss: 3.200196 | lr:2.7748e-04 | norm 0.2856 | dt 337.26ms | 1554551.35 tokens/sec
Step 11039 | loss: 3.186583 | lr:2.7743e-04 | norm 0.2961 | dt 337.73ms | 1552393.80 tokens/sec
Step 11040 | loss: 3.217009 | lr:2.7739e-04 | norm 0.2850 | dt 338.96ms | 1546773.58 tokens/sec
Step 11041 | loss: 3.222982 | lr:2.7734e-04 | norm 0.2813 | dt 338.16ms | 1550405.09 tokens/sec
Step 11042 | loss: 3.186537 | lr:2.7730e-04 | norm 0.2954 | dt 338.66ms | 1548116.24 tokens/sec
Step 11043 | loss: 3.191147 | lr:2.7725e-04 | norm 0.2928 | dt 338.49ms | 1548912.25 tokens/sec
Step 11044 | loss: 3.166278 | lr:2.7721e-04 | norm 0.2795 | dt 338.88ms | 1547138.15 tokens/sec
Step 11045 | loss: 3.127062 | lr:2.7716e-04 | norm 0.2925 | dt 337.72ms | 1552432.16 tokens/sec
Step 11046 | loss: 3.147114 | lr:2.7712e-04 | norm 0.3181 | dt 337.63ms | 1552870.67 tokens/sec
Step 11047 | loss: 3.174983 | lr:2.7707e-04 | norm 0.2858 | dt 337.15ms | 1555068.03 tokens/sec
Step 11048 | loss: 3.139465 | lr:2.7703e-04 | norm 0.3021 | dt 338.97ms | 1546705.04 tokens/sec
Step 11049 | loss: 3.160231 | lr:2.7698e-04 | norm 0.2893 | dt 338.52ms | 1548763.89 tokens/sec
Step 11050 | loss: 3.141033 | lr:2.7693e-04 | norm 0.2948 | dt 338.39ms | 1549372.79 tokens/sec
Step 11051 | loss: 3.158240 | lr:2.7689e-04 | norm 0.2826 | dt 338.36ms | 1549491.79 tokens/sec
Step 11052 | loss: 3.238287 | lr:2.7684e-04 | norm 0.3087 | dt 339.03ms | 1546416.81 tokens/sec
Step 11053 | loss: 3.245923 | lr:2.7680e-04 | norm 0.3086 | dt 337.98ms | 1551261.46 tokens/sec
Step 11054 | loss: 3.170845 | lr:2.7675e-04 | norm 0.2957 | dt 337.23ms | 1554688.73 tokens/sec
Step 11055 | loss: 3.144892 | lr:2.7671e-04 | norm 0.3069 | dt 338.41ms | 1549289.83 tokens/sec
Step 11056 | loss: 3.188888 | lr:2.7666e-04 | norm 0.2986 | dt 337.74ms | 1552320.38 tokens/sec
Step 11057 | loss: 3.185133 | lr:2.7662e-04 | norm 0.2819 | dt 339.53ms | 1544174.41 tokens/sec
Step 11058 | loss: 3.119919 | lr:2.7657e-04 | norm 0.2810 | dt 337.90ms | 1551619.38 tokens/sec
Step 11059 | loss: 3.213710 | lr:2.7653e-04 | norm 0.2813 | dt 337.72ms | 1552452.98 tokens/sec
Step 11060 | loss: 3.170985 | lr:2.7648e-04 | norm 0.2895 | dt 338.55ms | 1548612.29 tokens/sec
Step 11061 | loss: 3.168213 | lr:2.7644e-04 | norm 0.3028 | dt 338.16ms | 1550413.83 tokens/sec
Step 11062 | loss: 3.105329 | lr:2.7639e-04 | norm 0.3013 | dt 338.23ms | 1550078.32 tokens/sec
Step 11063 | loss: 3.162474 | lr:2.7635e-04 | norm 0.3344 | dt 338.66ms | 1548115.15 tokens/sec
Step 11064 | loss: 3.187502 | lr:2.7630e-04 | norm 0.2789 | dt 337.12ms | 1555196.70 tokens/sec
Step 11065 | loss: 3.134572 | lr:2.7626e-04 | norm 0.2780 | dt 337.82ms | 1551953.37 tokens/sec
Step 11066 | loss: 3.129606 | lr:2.7621e-04 | norm 0.2779 | dt 338.41ms | 1549273.46 tokens/sec
Step 11067 | loss: 3.169878 | lr:2.7616e-04 | norm 0.2724 | dt 338.80ms | 1547499.61 tokens/sec
Step 11068 | loss: 3.150008 | lr:2.7612e-04 | norm 0.2829 | dt 338.89ms | 1547095.69 tokens/sec
Step 11069 | loss: 3.218064 | lr:2.7607e-04 | norm 0.2581 | dt 339.06ms | 1546315.68 tokens/sec
Step 11070 | loss: 3.151467 | lr:2.7603e-04 | norm 0.2697 | dt 338.49ms | 1548880.62 tokens/sec
Step 11071 | loss: 3.150249 | lr:2.7598e-04 | norm 0.2729 | dt 340.14ms | 1541382.97 tokens/sec
Step 11072 | loss: 3.197654 | lr:2.7594e-04 | norm 0.2772 | dt 338.87ms | 1547166.45 tokens/sec
Step 11073 | loss: 3.187203 | lr:2.7589e-04 | norm 0.2643 | dt 338.94ms | 1546827.98 tokens/sec
Step 11074 | loss: 3.193335 | lr:2.7585e-04 | norm 0.2779 | dt 338.10ms | 1550677.32 tokens/sec
Step 11075 | loss: 3.182225 | lr:2.7580e-04 | norm 0.2847 | dt 338.94ms | 1546823.63 tokens/sec
Step 11076 | loss: 3.151387 | lr:2.7576e-04 | norm 0.2831 | dt 337.93ms | 1551474.88 tokens/sec
Step 11077 | loss: 3.123202 | lr:2.7571e-04 | norm 0.2625 | dt 337.94ms | 1551445.32 tokens/sec
Step 11078 | loss: 3.219181 | lr:2.7567e-04 | norm 0.3258 | dt 337.27ms | 1554487.61 tokens/sec
Step 11079 | loss: 3.228484 | lr:2.7562e-04 | norm 0.3003 | dt 338.61ms | 1548339.69 tokens/sec
Step 11080 | loss: 3.199761 | lr:2.7558e-04 | norm 0.3208 | dt 337.89ms | 1551644.56 tokens/sec
Step 11081 | loss: 3.155087 | lr:2.7553e-04 | norm 0.3138 | dt 337.82ms | 1551962.13 tokens/sec
Step 11082 | loss: 3.148658 | lr:2.7549e-04 | norm 0.2945 | dt 338.04ms | 1550966.05 tokens/sec
Step 11083 | loss: 3.134139 | lr:2.7544e-04 | norm 0.2948 | dt 337.06ms | 1555477.22 tokens/sec
Step 11084 | loss: 3.139615 | lr:2.7540e-04 | norm 0.2836 | dt 337.92ms | 1551513.19 tokens/sec
Step 11085 | loss: 3.090194 | lr:2.7535e-04 | norm 0.2928 | dt 338.23ms | 1550083.78 tokens/sec
Step 11086 | loss: 3.140123 | lr:2.7530e-04 | norm 0.2819 | dt 337.39ms | 1553938.37 tokens/sec
Step 11087 | loss: 3.101825 | lr:2.7526e-04 | norm 0.2983 | dt 337.71ms | 1552494.63 tokens/sec
Step 11088 | loss: 3.172241 | lr:2.7521e-04 | norm 0.2754 | dt 338.03ms | 1551005.43 tokens/sec
Step 11089 | loss: 3.174666 | lr:2.7517e-04 | norm 0.3055 | dt 337.97ms | 1551273.49 tokens/sec
Step 11090 | loss: 3.112812 | lr:2.7512e-04 | norm 0.2834 | dt 337.36ms | 1554078.94 tokens/sec
Step 11091 | loss: 3.216145 | lr:2.7508e-04 | norm 0.3099 | dt 337.88ms | 1551677.41 tokens/sec
Step 11092 | loss: 3.164999 | lr:2.7503e-04 | norm 0.2914 | dt 338.29ms | 1549796.47 tokens/sec
Step 11093 | loss: 3.190409 | lr:2.7499e-04 | norm 0.3307 | dt 337.95ms | 1551398.26 tokens/sec
Step 11094 | loss: 3.127198 | lr:2.7494e-04 | norm 0.2801 | dt 337.88ms | 1551683.98 tokens/sec
Step 11095 | loss: 3.159168 | lr:2.7490e-04 | norm 0.2788 | dt 337.57ms | 1553116.34 tokens/sec
Step 11096 | loss: 3.116559 | lr:2.7485e-04 | norm 0.2642 | dt 337.90ms | 1551599.67 tokens/sec
Step 11097 | loss: 3.170268 | lr:2.7481e-04 | norm 0.2927 | dt 343.45ms | 1526528.10 tokens/sec
Step 11098 | loss: 3.203949 | lr:2.7476e-04 | norm 0.2874 | dt 337.40ms | 1553893.35 tokens/sec
Step 11099 | loss: 3.160326 | lr:2.7472e-04 | norm 0.2894 | dt 337.73ms | 1552392.70 tokens/sec
Step 11100 | loss: 3.206104 | lr:2.7467e-04 | norm 0.2751 | dt 337.64ms | 1552818.03 tokens/sec
Step 11101 | loss: 3.116252 | lr:2.7463e-04 | norm 0.2996 | dt 338.23ms | 1550112.19 tokens/sec
Step 11102 | loss: 3.159458 | lr:2.7458e-04 | norm 0.2864 | dt 338.21ms | 1550170.10 tokens/sec
Step 11103 | loss: 3.195235 | lr:2.7454e-04 | norm 0.2738 | dt 337.55ms | 1553207.39 tokens/sec
Step 11104 | loss: 3.162525 | lr:2.7449e-04 | norm 0.2891 | dt 338.51ms | 1548812.98 tokens/sec
Step 11105 | loss: 3.176838 | lr:2.7445e-04 | norm 0.3033 | dt 338.25ms | 1549993.10 tokens/sec
Step 11106 | loss: 3.251361 | lr:2.7440e-04 | norm 0.3062 | dt 338.03ms | 1550991.21 tokens/sec
Step 11107 | loss: 3.169246 | lr:2.7436e-04 | norm 0.2886 | dt 339.82ms | 1542819.10 tokens/sec
Step 11108 | loss: 3.188659 | lr:2.7431e-04 | norm 0.2936 | dt 338.04ms | 1550955.11 tokens/sec
Step 11109 | loss: 3.177038 | lr:2.7426e-04 | norm 0.2824 | dt 338.44ms | 1549135.94 tokens/sec
Step 11110 | loss: 3.196885 | lr:2.7422e-04 | norm 0.3173 | dt 338.62ms | 1548285.19 tokens/sec
Step 11111 | loss: 3.206560 | lr:2.7417e-04 | norm 0.3112 | dt 338.53ms | 1548739.90 tokens/sec
Step 11112 | loss: 3.205585 | lr:2.7413e-04 | norm 0.2834 | dt 338.71ms | 1547895.02 tokens/sec
Step 11113 | loss: 3.213928 | lr:2.7408e-04 | norm 0.3012 | dt 337.80ms | 1552078.24 tokens/sec
Step 11114 | loss: 3.112373 | lr:2.7404e-04 | norm 0.3307 | dt 337.44ms | 1553728.67 tokens/sec
Step 11115 | loss: 3.184879 | lr:2.7399e-04 | norm 0.3224 | dt 338.50ms | 1548851.16 tokens/sec
Step 11116 | loss: 3.139919 | lr:2.7395e-04 | norm 0.3105 | dt 338.14ms | 1550510.03 tokens/sec
Step 11117 | loss: 3.213222 | lr:2.7390e-04 | norm 0.2941 | dt 337.63ms | 1552853.12 tokens/sec
Step 11118 | loss: 3.150203 | lr:2.7386e-04 | norm 0.3254 | dt 337.70ms | 1552507.78 tokens/sec
Step 11119 | loss: 3.138476 | lr:2.7381e-04 | norm 0.2765 | dt 340.41ms | 1540158.75 tokens/sec
Step 11120 | loss: 3.160642 | lr:2.7377e-04 | norm 0.3058 | dt 339.48ms | 1544398.90 tokens/sec
Step 11121 | loss: 3.124134 | lr:2.7372e-04 | norm 0.2867 | dt 338.85ms | 1547268.78 tokens/sec
Step 11122 | loss: 3.156042 | lr:2.7368e-04 | norm 0.2796 | dt 338.84ms | 1547288.37 tokens/sec
Step 11123 | loss: 3.194252 | lr:2.7363e-04 | norm 0.3210 | dt 339.37ms | 1544867.61 tokens/sec
Step 11124 | loss: 3.129097 | lr:2.7359e-04 | norm 0.2683 | dt 339.33ms | 1545057.56 tokens/sec
Step 11125 | loss: 3.124667 | lr:2.7354e-04 | norm 0.3317 | dt 338.34ms | 1549595.52 tokens/sec
Step 11126 | loss: 3.143855 | lr:2.7350e-04 | norm 0.2753 | dt 339.52ms | 1544216.70 tokens/sec
Step 11127 | loss: 3.156276 | lr:2.7345e-04 | norm 0.3084 | dt 338.83ms | 1547342.81 tokens/sec
Step 11128 | loss: 3.210545 | lr:2.7341e-04 | norm 0.2881 | dt 338.53ms | 1548716.99 tokens/sec
Step 11129 | loss: 3.170758 | lr:2.7336e-04 | norm 0.2864 | dt 338.77ms | 1547601.99 tokens/sec
Step 11130 | loss: 3.168593 | lr:2.7332e-04 | norm 0.2687 | dt 339.22ms | 1545549.48 tokens/sec
Step 11131 | loss: 3.188764 | lr:2.7327e-04 | norm 0.2697 | dt 339.16ms | 1545855.87 tokens/sec
Step 11132 | loss: 3.164502 | lr:2.7323e-04 | norm 0.2693 | dt 338.72ms | 1547847.08 tokens/sec
Step 11133 | loss: 3.178809 | lr:2.7318e-04 | norm 0.2940 | dt 339.20ms | 1545674.41 tokens/sec
Step 11134 | loss: 3.136674 | lr:2.7313e-04 | norm 0.2885 | dt 338.36ms | 1549485.24 tokens/sec
Step 11135 | loss: 3.165897 | lr:2.7309e-04 | norm 0.2773 | dt 339.00ms | 1546565.81 tokens/sec
Step 11136 | loss: 3.177941 | lr:2.7304e-04 | norm 0.2855 | dt 339.14ms | 1545942.81 tokens/sec
Step 11137 | loss: 3.165237 | lr:2.7300e-04 | norm 0.2889 | dt 340.37ms | 1540351.86 tokens/sec
Step 11138 | loss: 3.180773 | lr:2.7295e-04 | norm 0.2703 | dt 338.75ms | 1547715.27 tokens/sec
Step 11139 | loss: 3.202014 | lr:2.7291e-04 | norm 0.2803 | dt 338.18ms | 1550322.01 tokens/sec
Step 11140 | loss: 3.194465 | lr:2.7286e-04 | norm 0.2767 | dt 338.02ms | 1551076.54 tokens/sec
Step 11141 | loss: 3.194858 | lr:2.7282e-04 | norm 0.2781 | dt 339.02ms | 1546501.63 tokens/sec
Step 11142 | loss: 3.167135 | lr:2.7277e-04 | norm 0.2898 | dt 338.06ms | 1550865.42 tokens/sec
Step 11143 | loss: 3.201115 | lr:2.7273e-04 | norm 0.2985 | dt 338.47ms | 1548995.18 tokens/sec
Step 11144 | loss: 3.247190 | lr:2.7268e-04 | norm 0.3068 | dt 338.44ms | 1549142.49 tokens/sec
Step 11145 | loss: 3.184360 | lr:2.7264e-04 | norm 0.2899 | dt 338.75ms | 1547691.30 tokens/sec
Step 11146 | loss: 3.160268 | lr:2.7259e-04 | norm 0.2788 | dt 338.04ms | 1550964.96 tokens/sec
Step 11147 | loss: 3.162547 | lr:2.7255e-04 | norm 0.2799 | dt 337.85ms | 1551843.85 tokens/sec
Step 11148 | loss: 3.175484 | lr:2.7250e-04 | norm 0.2776 | dt 338.10ms | 1550676.22 tokens/sec
Step 11149 | loss: 3.135861 | lr:2.7246e-04 | norm 0.2679 | dt 338.18ms | 1550320.92 tokens/sec
Step 11150 | loss: 3.159299 | lr:2.7241e-04 | norm 0.2698 | dt 1020.54ms | 513734.67 tokens/sec
Step 11151 | loss: 3.108725 | lr:2.7237e-04 | norm 0.2794 | dt 337.03ms | 1555610.36 tokens/sec
Step 11152 | loss: 3.173909 | lr:2.7232e-04 | norm 0.2584 | dt 339.43ms | 1544613.69 tokens/sec
Step 11153 | loss: 3.151524 | lr:2.7228e-04 | norm 0.2808 | dt 339.23ms | 1545521.24 tokens/sec
Step 11154 | loss: 3.171119 | lr:2.7223e-04 | norm 0.3034 | dt 338.74ms | 1547779.54 tokens/sec
Step 11155 | loss: 3.196347 | lr:2.7219e-04 | norm 0.3152 | dt 338.76ms | 1547656.44 tokens/sec
Step 11156 | loss: 3.158436 | lr:2.7214e-04 | norm 0.2890 | dt 339.20ms | 1545680.93 tokens/sec
Step 11157 | loss: 3.142191 | lr:2.7210e-04 | norm 0.2882 | dt 339.75ms | 1543157.97 tokens/sec
Step 11158 | loss: 3.180501 | lr:2.7205e-04 | norm 0.3063 | dt 338.59ms | 1548446.54 tokens/sec
Step 11159 | loss: 3.159702 | lr:2.7201e-04 | norm 0.2763 | dt 340.84ms | 1538206.62 tokens/sec
Step 11160 | loss: 3.151083 | lr:2.7196e-04 | norm 0.2996 | dt 339.72ms | 1543309.59 tokens/sec
Step 11161 | loss: 3.118061 | lr:2.7192e-04 | norm 0.2740 | dt 339.45ms | 1544499.77 tokens/sec
Step 11162 | loss: 3.196543 | lr:2.7187e-04 | norm 0.3172 | dt 338.75ms | 1547705.46 tokens/sec
Step 11163 | loss: 3.131960 | lr:2.7183e-04 | norm 0.2768 | dt 338.98ms | 1546655.00 tokens/sec
Step 11164 | loss: 3.197781 | lr:2.7178e-04 | norm 0.3009 | dt 339.33ms | 1545079.27 tokens/sec
Step 11165 | loss: 3.160697 | lr:2.7174e-04 | norm 0.2837 | dt 338.16ms | 1550407.27 tokens/sec
Step 11166 | loss: 3.186332 | lr:2.7169e-04 | norm 0.3041 | dt 340.08ms | 1541643.39 tokens/sec
Step 11167 | loss: 3.175928 | lr:2.7165e-04 | norm 0.2752 | dt 338.46ms | 1549043.19 tokens/sec
Step 11168 | loss: 3.146650 | lr:2.7160e-04 | norm 0.2990 | dt 338.09ms | 1550721.06 tokens/sec
Step 11169 | loss: 3.143129 | lr:2.7156e-04 | norm 0.2810 | dt 337.97ms | 1551264.74 tokens/sec
Step 11170 | loss: 3.186251 | lr:2.7151e-04 | norm 0.3097 | dt 338.57ms | 1548522.87 tokens/sec
Step 11171 | loss: 3.165104 | lr:2.7146e-04 | norm 0.2999 | dt 338.61ms | 1548364.77 tokens/sec
Step 11172 | loss: 3.170980 | lr:2.7142e-04 | norm 0.2988 | dt 337.68ms | 1552638.23 tokens/sec
Step 11173 | loss: 3.273165 | lr:2.7137e-04 | norm 0.3191 | dt 337.86ms | 1551770.48 tokens/sec
Step 11174 | loss: 3.208795 | lr:2.7133e-04 | norm 0.2861 | dt 338.84ms | 1547299.26 tokens/sec
Step 11175 | loss: 3.198975 | lr:2.7128e-04 | norm 0.3071 | dt 338.76ms | 1547677.14 tokens/sec
Step 11176 | loss: 3.158782 | lr:2.7124e-04 | norm 0.2876 | dt 337.32ms | 1554271.17 tokens/sec
Step 11177 | loss: 3.183779 | lr:2.7119e-04 | norm 0.2833 | dt 338.33ms | 1549628.28 tokens/sec
Step 11178 | loss: 3.152743 | lr:2.7115e-04 | norm 0.2720 | dt 338.24ms | 1550057.56 tokens/sec
Step 11179 | loss: 3.174354 | lr:2.7110e-04 | norm 0.2868 | dt 337.83ms | 1551916.13 tokens/sec
Step 11180 | loss: 3.170638 | lr:2.7106e-04 | norm 0.2695 | dt 339.17ms | 1545795.01 tokens/sec
Step 11181 | loss: 3.209892 | lr:2.7101e-04 | norm 0.2834 | dt 337.48ms | 1553557.43 tokens/sec
Step 11182 | loss: 3.168309 | lr:2.7097e-04 | norm 0.2808 | dt 338.88ms | 1547107.67 tokens/sec
Step 11183 | loss: 3.148345 | lr:2.7092e-04 | norm 0.2944 | dt 339.03ms | 1546443.99 tokens/sec
Step 11184 | loss: 3.172054 | lr:2.7088e-04 | norm 0.2957 | dt 339.28ms | 1545293.16 tokens/sec
Step 11185 | loss: 3.105892 | lr:2.7083e-04 | norm 0.2803 | dt 337.88ms | 1551678.50 tokens/sec
Step 11186 | loss: 3.159306 | lr:2.7079e-04 | norm 0.2667 | dt 339.15ms | 1545894.99 tokens/sec
Step 11187 | loss: 3.138362 | lr:2.7074e-04 | norm 0.2962 | dt 338.00ms | 1551162.98 tokens/sec
Step 11188 | loss: 3.141570 | lr:2.7070e-04 | norm 0.2747 | dt 338.78ms | 1547588.92 tokens/sec
Step 11189 | loss: 3.154850 | lr:2.7065e-04 | norm 0.2808 | dt 339.33ms | 1545080.36 tokens/sec
Step 11190 | loss: 3.143296 | lr:2.7061e-04 | norm 0.2660 | dt 338.48ms | 1548929.71 tokens/sec
Step 11191 | loss: 3.157774 | lr:2.7056e-04 | norm 0.2791 | dt 337.92ms | 1551525.23 tokens/sec
Step 11192 | loss: 3.152829 | lr:2.7052e-04 | norm 0.2894 | dt 338.49ms | 1548919.89 tokens/sec
Step 11193 | loss: 3.160626 | lr:2.7047e-04 | norm 0.3491 | dt 337.87ms | 1551727.77 tokens/sec
Step 11194 | loss: 3.110872 | lr:2.7043e-04 | norm 0.2881 | dt 337.68ms | 1552610.82 tokens/sec
Step 11195 | loss: 3.223556 | lr:2.7038e-04 | norm 0.3225 | dt 337.96ms | 1551345.72 tokens/sec
Step 11196 | loss: 3.135826 | lr:2.7034e-04 | norm 0.2882 | dt 338.73ms | 1547795.88 tokens/sec
Step 11197 | loss: 3.153263 | lr:2.7029e-04 | norm 0.2848 | dt 337.69ms | 1552583.41 tokens/sec
Step 11198 | loss: 3.172428 | lr:2.7025e-04 | norm 0.2746 | dt 338.82ms | 1547397.25 tokens/sec
Step 11199 | loss: 3.177372 | lr:2.7020e-04 | norm 0.2742 | dt 337.45ms | 1553667.19 tokens/sec
Step 11200 | loss: 3.242586 | lr:2.7016e-04 | norm 0.3441 | dt 337.65ms | 1552740.18 tokens/sec
Step 11201 | loss: 3.241930 | lr:2.7011e-04 | norm 0.3077 | dt 338.99ms | 1546632.16 tokens/sec
Step 11202 | loss: 3.129610 | lr:2.7007e-04 | norm 0.3227 | dt 342.49ms | 1530799.98 tokens/sec
Step 11203 | loss: 3.213016 | lr:2.7002e-04 | norm 0.2779 | dt 337.16ms | 1555002.05 tokens/sec
Step 11204 | loss: 3.212839 | lr:2.6998e-04 | norm 0.3249 | dt 338.24ms | 1550029.15 tokens/sec
Step 11205 | loss: 3.194909 | lr:2.6993e-04 | norm 0.2846 | dt 338.27ms | 1549899.15 tokens/sec
Step 11206 | loss: 3.140407 | lr:2.6989e-04 | norm 0.3017 | dt 338.22ms | 1550135.14 tokens/sec
Step 11207 | loss: 3.219924 | lr:2.6984e-04 | norm 0.2928 | dt 338.79ms | 1547532.28 tokens/sec
Step 11208 | loss: 3.179111 | lr:2.6980e-04 | norm 0.2832 | dt 338.30ms | 1549789.91 tokens/sec
Step 11209 | loss: 3.201869 | lr:2.6975e-04 | norm 0.2969 | dt 1042.46ms | 502933.69 tokens/sec
Step 11210 | loss: 3.193027 | lr:2.6971e-04 | norm 0.2854 | dt 338.78ms | 1547570.40 tokens/sec
Step 11211 | loss: 3.187020 | lr:2.6966e-04 | norm 0.2731 | dt 339.98ms | 1542125.57 tokens/sec
Step 11212 | loss: 3.174383 | lr:2.6962e-04 | norm 0.2852 | dt 336.78ms | 1556753.47 tokens/sec
Step 11213 | loss: 3.131021 | lr:2.6957e-04 | norm 0.2654 | dt 337.95ms | 1551386.22 tokens/sec
Step 11214 | loss: 3.172116 | lr:2.6953e-04 | norm 0.2946 | dt 337.52ms | 1553375.26 tokens/sec
Step 11215 | loss: 3.249724 | lr:2.6948e-04 | norm 0.2983 | dt 338.08ms | 1550788.86 tokens/sec
Step 11216 | loss: 3.135333 | lr:2.6944e-04 | norm 0.3068 | dt 338.52ms | 1548767.17 tokens/sec
Step 11217 | loss: 3.164831 | lr:2.6939e-04 | norm 0.2711 | dt 337.77ms | 1552191.08 tokens/sec
Step 11218 | loss: 3.150121 | lr:2.6935e-04 | norm 0.2810 | dt 338.75ms | 1547714.18 tokens/sec
Step 11219 | loss: 3.176038 | lr:2.6930e-04 | norm 0.2797 | dt 338.34ms | 1549606.44 tokens/sec
Step 11220 | loss: 3.096334 | lr:2.6926e-04 | norm 0.2643 | dt 337.68ms | 1552632.74 tokens/sec
Step 11221 | loss: 3.130106 | lr:2.6921e-04 | norm 0.2901 | dt 338.54ms | 1548684.27 tokens/sec
Step 11222 | loss: 3.219584 | lr:2.6917e-04 | norm 0.2711 | dt 338.72ms | 1547835.10 tokens/sec
Step 11223 | loss: 3.113996 | lr:2.6912e-04 | norm 0.2831 | dt 338.43ms | 1549154.49 tokens/sec
Step 11224 | loss: 3.145228 | lr:2.6908e-04 | norm 0.2928 | dt 338.43ms | 1549173.05 tokens/sec
Step 11225 | loss: 3.198602 | lr:2.6903e-04 | norm 0.2891 | dt 339.03ms | 1546458.13 tokens/sec
Step 11226 | loss: 3.152748 | lr:2.6899e-04 | norm 0.2694 | dt 339.08ms | 1546208.04 tokens/sec
Step 11227 | loss: 3.200126 | lr:2.6894e-04 | norm 0.3301 | dt 338.17ms | 1550364.64 tokens/sec
Step 11228 | loss: 3.155056 | lr:2.6890e-04 | norm 0.2884 | dt 339.10ms | 1546112.37 tokens/sec
Step 11229 | loss: 3.150710 | lr:2.6885e-04 | norm 0.2869 | dt 338.45ms | 1549094.47 tokens/sec
Step 11230 | loss: 3.235913 | lr:2.6881e-04 | norm 0.3032 | dt 339.47ms | 1544429.27 tokens/sec
Step 11231 | loss: 3.154897 | lr:2.6876e-04 | norm 0.2901 | dt 339.38ms | 1544829.62 tokens/sec
Step 11232 | loss: 3.155187 | lr:2.6872e-04 | norm 0.3103 | dt 338.80ms | 1547496.34 tokens/sec
Step 11233 | loss: 3.169838 | lr:2.6867e-04 | norm 0.2770 | dt 338.64ms | 1548221.96 tokens/sec
Step 11234 | loss: 3.104778 | lr:2.6863e-04 | norm 0.3216 | dt 338.22ms | 1550161.36 tokens/sec
Step 11235 | loss: 3.186435 | lr:2.6858e-04 | norm 0.2860 | dt 339.66ms | 1543580.42 tokens/sec
Step 11236 | loss: 3.167510 | lr:2.6854e-04 | norm 0.2997 | dt 339.42ms | 1544644.07 tokens/sec
Step 11237 | loss: 3.217051 | lr:2.6849e-04 | norm 0.2858 | dt 338.40ms | 1549298.56 tokens/sec
Step 11238 | loss: 3.146754 | lr:2.6845e-04 | norm 0.2810 | dt 338.21ms | 1550190.87 tokens/sec
Step 11239 | loss: 3.184676 | lr:2.6840e-04 | norm 0.2771 | dt 338.41ms | 1549277.83 tokens/sec
Step 11240 | loss: 3.146500 | lr:2.6836e-04 | norm 0.2775 | dt 338.40ms | 1549305.11 tokens/sec
Step 11241 | loss: 3.258095 | lr:2.6831e-04 | norm 0.3052 | dt 338.28ms | 1549881.67 tokens/sec
Step 11242 | loss: 3.214153 | lr:2.6827e-04 | norm 0.3302 | dt 337.58ms | 1553076.85 tokens/sec
Step 11243 | loss: 3.178545 | lr:2.6822e-04 | norm 0.3014 | dt 338.48ms | 1548931.89 tokens/sec
Step 11244 | loss: 3.200137 | lr:2.6818e-04 | norm 0.3023 | dt 337.70ms | 1552519.84 tokens/sec
Step 11245 | loss: 3.196697 | lr:2.6813e-04 | norm 0.3147 | dt 338.60ms | 1548417.10 tokens/sec
Step 11246 | loss: 3.198460 | lr:2.6809e-04 | norm 0.2798 | dt 337.31ms | 1554339.28 tokens/sec
Step 11247 | loss: 3.221802 | lr:2.6804e-04 | norm 0.3214 | dt 338.66ms | 1548132.58 tokens/sec
Step 11248 | loss: 3.201323 | lr:2.6800e-04 | norm 0.3123 | dt 339.51ms | 1544248.14 tokens/sec
Step 11249 | loss: 3.152662 | lr:2.6795e-04 | norm 0.2893 | dt 339.11ms | 1546085.19 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 11250: 3.1737
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2950/10042=0.2938


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what I'm missing is the way it works. I'm really focused on using the language. I can't make
rank 5 sample 1 >Hello, I'm a language model, now this is what I'll talk about. Well, if I put in my textbook that's really helpful you, I


rank 5 sample 2 >Hello, I'm a language model, and you know that with the development I'm sure you've all the answers. So my question is what do you use
ddp_rank 1: ####### Printing generated samples ####### 

rank 5 sample 3 >Hello, I'm a language model, meaning I'm using a compiler to generate the class code, and then I get what it's to produce a compiler.


rank 1 sample 0 >Hello, I'm a language model, so I did a test to see whether the two-letter spelling of the word is similar, which I think is a
rank 1 sample 1 >Hello, I'm a language model, a computer programmer and a computer engineer. I'm trying in my math class to discover I'm reading this book. I
rank 1 sample 2 >Hello, I'm a language model, but first time I'm going to get into it. It works on Windows, but I've gotten it to run in
rank 1 sample 3 >Hello, I'm a language model, so I'm working with other people for over a year figuring out how to tell the standard "what's the "s




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I wanted to use some of the files I just downloaded. I didn't download any of them.
I started
rank 7 sample 1 >Hello, I'm a language model, using OLE, a universal object paradigm, and for lots of programming, I learned how to use Object Modeling with
rank 7 sample 2 >Hello, I'm a language model, so I'll make a little of it to show you how to create dynamic graphics. I'll show you how to do
rank 7 sample 3 >Hello, I'm a language model, and you need to start off by showing up one of the various languages with the "S" letter.
Then follow




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, my friends, and a scientist, so this is really neat to try and see what's being said. I'm excited
rank 6 sample 1 >Hello, I'm a language model, so I want to show how to use machine learning, and how to implement it. So I'll start my next course
rank 6 sample 2 >Hello, I'm a language model, but a student doesn't have any of the basics. I have one question - how do I get enough knowledge with the


ddp_rank 2: ####### Printing generated samples ####### 

rank 6 sample 3 >Hello, I'm a language model, so I want to teach my third grade class. I'm going to take it home, cut it, then re-


rank 2 sample 0 >Hello, I'm a language model, learning and writing to the point where I'm a computer user and I want my machine to be a computer."
-
rank 2 sample 1 >Hello, I'm a language model, but I will not teach it to another two-party school in college. I will teach the basics to the students as
rank 2 sample 2 >Hello, I'm a language model, and I want to do a post called "Why I'm here," just like in the example you'll see when you
rank 2 sample 3 >Hello, I'm a language model, but if I'm going to use the L, instead of "I'm so much smarter of all" as I can




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I hope you've read it. (Thanks!)
This is your final chance to join the movement and explore your
rank 4 sample 1 >Hello, I'm a language model, since the first piece of this document was released, the main idea is to have more students use it as a replacement to
rank 4 sample 2 >Hello, I'm a language model, I started making animations using ICT (Integrated Development Environment). So I'm interested in trying to model the visual effects
rank 4 sample 3 >Hello, I'm a language model, so it always says something different to me." And since the introduction to me to the "Easter" site, it




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not quite so good with languages, and I will try them every week with this set. I will teach
rank 3 sample 1 >Hello, I'm a language model, so my students are going to love the language and get engaged in the process.
I'm looking for a topic and
rank 3 sample 2 >Hello, I'm a language model, so this week I'm going to talk about learning the new, and the most fun, and often, the more complex
rank 3 sample 3 >Hello, I'm a language model, so don't get me wrong here. I don't use it as much as I should.
Also, the idea




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I'd like you to read that on their blog.
There are several ways in which people use languages. They
rank 0 sample 1 >Hello, I'm a language model, and that's why I'm doing it. It comes next to how we learn languages and how to build our own language
rank 0 sample 2 >Hello, I'm a language model, and I had the help of a person who would listen to music in an old-fashioned way. I'm not happy
rank 0 sample 3 >Hello, I'm a language model, and want to write a program to show you these things.
Here's what they mean:
What they do isn


Step 11250 | loss: 3.222622 | lr:2.6791e-04 | norm 0.3076 | dt 18655.26ms | 28104.03 tokens/sec
Step 11251 | loss: 3.177500 | lr:2.6786e-04 | norm 0.2899 | dt 334.81ms | 1565906.79 tokens/sec
Step 11252 | loss: 3.168684 | lr:2.6782e-04 | norm 0.2732 | dt 336.22ms | 1559370.88 tokens/sec
Step 11253 | loss: 3.196544 | lr:2.6777e-04 | norm 0.2730 | dt 337.31ms | 1554303.02 tokens/sec
Step 11254 | loss: 3.109550 | lr:2.6773e-04 | norm 0.2692 | dt 336.66ms | 1557314.63 tokens/sec
Step 11255 | loss: 3.156103 | lr:2.6768e-04 | norm 0.2603 | dt 336.29ms | 1559032.58 tokens/sec
Step 11256 | loss: 3.137917 | lr:2.6764e-04 | norm 0.2842 | dt 337.70ms | 1552536.28 tokens/sec
Step 11257 | loss: 3.128046 | lr:2.6759e-04 | norm 0.2590 | dt 337.17ms | 1554962.47 tokens/sec
Step 11258 | loss: 3.152683 | lr:2.6755e-04 | norm 0.2671 | dt 337.41ms | 1553851.63 tokens/sec
Step 11259 | loss: 3.225559 | lr:2.6750e-04 | norm 0.2875 | dt 337.71ms | 1552492.44 tokens/sec
Step 11260 | loss: 3.171023 | lr:2.6746e-04 | norm 0.2837 | dt 336.59ms | 1557647.77 tokens/sec
Step 11261 | loss: 3.119071 | lr:2.6741e-04 | norm 0.2664 | dt 337.42ms | 1553818.69 tokens/sec
Step 11262 | loss: 3.179132 | lr:2.6737e-04 | norm 0.3402 | dt 337.66ms | 1552688.65 tokens/sec
Step 11263 | loss: 3.141112 | lr:2.6732e-04 | norm 0.3131 | dt 338.07ms | 1550828.23 tokens/sec
Step 11264 | loss: 3.248479 | lr:2.6728e-04 | norm 0.3184 | dt 337.70ms | 1552518.74 tokens/sec
Step 11265 | loss: 3.163620 | lr:2.6723e-04 | norm 0.2852 | dt 338.44ms | 1549145.76 tokens/sec
Step 11266 | loss: 3.233164 | lr:2.6719e-04 | norm 0.3209 | dt 337.75ms | 1552311.61 tokens/sec
Step 11267 | loss: 3.172148 | lr:2.6714e-04 | norm 0.3022 | dt 337.70ms | 1552538.47 tokens/sec
Step 11268 | loss: 3.253933 | lr:2.6710e-04 | norm 0.3370 | dt 338.13ms | 1550573.44 tokens/sec
Step 11269 | loss: 3.196671 | lr:2.6705e-04 | norm 0.2968 | dt 337.90ms | 1551612.81 tokens/sec
Step 11270 | loss: 3.121905 | lr:2.6701e-04 | norm 0.3133 | dt 337.47ms | 1553605.72 tokens/sec
Step 11271 | loss: 3.182350 | lr:2.6696e-04 | norm 0.2825 | dt 339.97ms | 1542166.67 tokens/sec
Step 11272 | loss: 3.143573 | lr:2.6692e-04 | norm 0.2758 | dt 338.62ms | 1548297.18 tokens/sec
Step 11273 | loss: 3.204592 | lr:2.6687e-04 | norm 0.2847 | dt 338.23ms | 1550114.38 tokens/sec
Step 11274 | loss: 3.184496 | lr:2.6683e-04 | norm 0.2791 | dt 338.59ms | 1548463.98 tokens/sec
Step 11275 | loss: 3.129804 | lr:2.6678e-04 | norm 0.2867 | dt 337.78ms | 1552161.50 tokens/sec
Step 11276 | loss: 3.171000 | lr:2.6674e-04 | norm 0.2925 | dt 340.86ms | 1538116.24 tokens/sec
Step 11277 | loss: 3.184001 | lr:2.6669e-04 | norm 0.2838 | dt 338.16ms | 1550413.83 tokens/sec
Step 11278 | loss: 3.190486 | lr:2.6665e-04 | norm 0.2903 | dt 338.04ms | 1550978.08 tokens/sec
Step 11279 | loss: 3.134386 | lr:2.6660e-04 | norm 0.2877 | dt 338.29ms | 1549831.42 tokens/sec
Step 11280 | loss: 3.171125 | lr:2.6656e-04 | norm 0.2883 | dt 339.71ms | 1543346.42 tokens/sec
Step 11281 | loss: 3.163855 | lr:2.6651e-04 | norm 0.2995 | dt 338.34ms | 1549592.24 tokens/sec
Step 11282 | loss: 3.188364 | lr:2.6647e-04 | norm 0.2876 | dt 338.36ms | 1549520.18 tokens/sec
Step 11283 | loss: 3.102961 | lr:2.6642e-04 | norm 0.2730 | dt 337.55ms | 1553217.26 tokens/sec
Step 11284 | loss: 3.136210 | lr:2.6638e-04 | norm 0.2590 | dt 337.91ms | 1551579.97 tokens/sec
Step 11285 | loss: 3.160275 | lr:2.6633e-04 | norm 0.2798 | dt 339.31ms | 1545156.35 tokens/sec
Step 11286 | loss: 3.181915 | lr:2.6629e-04 | norm 0.2703 | dt 338.77ms | 1547641.20 tokens/sec
Step 11287 | loss: 3.158418 | lr:2.6624e-04 | norm 0.2979 | dt 337.77ms | 1552221.76 tokens/sec
Step 11288 | loss: 3.146538 | lr:2.6620e-04 | norm 0.2843 | dt 339.02ms | 1546502.72 tokens/sec
Step 11289 | loss: 3.128863 | lr:2.6615e-04 | norm 0.2689 | dt 338.16ms | 1550411.64 tokens/sec
Step 11290 | loss: 3.137619 | lr:2.6611e-04 | norm 0.2914 | dt 338.42ms | 1549201.42 tokens/sec
Step 11291 | loss: 3.159401 | lr:2.6606e-04 | norm 0.2762 | dt 339.57ms | 1543993.34 tokens/sec
Step 11292 | loss: 3.098965 | lr:2.6602e-04 | norm 0.2886 | dt 338.24ms | 1550030.24 tokens/sec
Step 11293 | loss: 3.372874 | lr:2.6598e-04 | norm 0.4065 | dt 338.74ms | 1547765.38 tokens/sec
Step 11294 | loss: 3.105918 | lr:2.6593e-04 | norm 0.4323 | dt 338.73ms | 1547786.07 tokens/sec
Step 11295 | loss: 3.124696 | lr:2.6589e-04 | norm 0.2840 | dt 338.97ms | 1546688.73 tokens/sec
Step 11296 | loss: 3.105831 | lr:2.6584e-04 | norm 0.3182 | dt 339.19ms | 1545709.18 tokens/sec
Step 11297 | loss: 3.167110 | lr:2.6580e-04 | norm 0.2883 | dt 338.23ms | 1550114.38 tokens/sec
Step 11298 | loss: 3.133017 | lr:2.6575e-04 | norm 0.3039 | dt 339.20ms | 1545658.11 tokens/sec
Step 11299 | loss: 3.138489 | lr:2.6571e-04 | norm 0.2985 | dt 338.45ms | 1549098.84 tokens/sec
Step 11300 | loss: 3.139520 | lr:2.6566e-04 | norm 0.2880 | dt 339.35ms | 1544965.29 tokens/sec
Step 11301 | loss: 3.162116 | lr:2.6562e-04 | norm 0.2917 | dt 338.97ms | 1546710.48 tokens/sec
Step 11302 | loss: 3.254649 | lr:2.6557e-04 | norm 0.3075 | dt 338.90ms | 1547029.30 tokens/sec
Step 11303 | loss: 3.203857 | lr:2.6553e-04 | norm 0.2947 | dt 338.78ms | 1547563.87 tokens/sec
Step 11304 | loss: 3.156085 | lr:2.6548e-04 | norm 0.3376 | dt 338.20ms | 1550250.97 tokens/sec
Step 11305 | loss: 3.222043 | lr:2.6544e-04 | norm 0.3141 | dt 341.11ms | 1537023.96 tokens/sec
Step 11306 | loss: 3.182307 | lr:2.6539e-04 | norm 0.2968 | dt 337.98ms | 1551255.99 tokens/sec
Step 11307 | loss: 3.188289 | lr:2.6535e-04 | norm 0.3006 | dt 339.82ms | 1542840.74 tokens/sec
Step 11308 | loss: 3.202201 | lr:2.6530e-04 | norm 0.2834 | dt 340.02ms | 1541952.56 tokens/sec
Step 11309 | loss: 3.214678 | lr:2.6526e-04 | norm 0.3712 | dt 338.87ms | 1547176.24 tokens/sec
Step 11310 | loss: 3.154941 | lr:2.6521e-04 | norm 0.3361 | dt 339.06ms | 1546278.71 tokens/sec
Step 11311 | loss: 3.211099 | lr:2.6517e-04 | norm 0.3393 | dt 339.76ms | 1543128.73 tokens/sec
Step 11312 | loss: 3.139035 | lr:2.6512e-04 | norm 0.3162 | dt 338.90ms | 1547044.54 tokens/sec
Step 11313 | loss: 3.116572 | lr:2.6508e-04 | norm 0.3025 | dt 340.68ms | 1538936.47 tokens/sec
Step 11314 | loss: 3.115565 | lr:2.6503e-04 | norm 0.3005 | dt 338.55ms | 1548633.01 tokens/sec
Step 11315 | loss: 3.160423 | lr:2.6499e-04 | norm 0.2917 | dt 338.59ms | 1548460.71 tokens/sec
Step 11316 | loss: 3.177248 | lr:2.6494e-04 | norm 0.3022 | dt 338.27ms | 1549890.41 tokens/sec
Step 11317 | loss: 3.131652 | lr:2.6490e-04 | norm 0.2880 | dt 338.03ms | 1550995.59 tokens/sec
Step 11318 | loss: 3.128847 | lr:2.6485e-04 | norm 0.2941 | dt 337.47ms | 1553589.26 tokens/sec
Step 11319 | loss: 3.131695 | lr:2.6481e-04 | norm 0.2813 | dt 337.81ms | 1552020.18 tokens/sec
Step 11320 | loss: 3.131656 | lr:2.6476e-04 | norm 0.2982 | dt 338.51ms | 1548803.16 tokens/sec
Step 11321 | loss: 3.116225 | lr:2.6472e-04 | norm 0.2801 | dt 338.26ms | 1549954.86 tokens/sec
Step 11322 | loss: 3.144076 | lr:2.6467e-04 | norm 0.3017 | dt 338.51ms | 1548833.71 tokens/sec
Step 11323 | loss: 3.216376 | lr:2.6463e-04 | norm 0.2860 | dt 337.42ms | 1553820.89 tokens/sec
Step 11324 | loss: 3.224316 | lr:2.6458e-04 | norm 0.3035 | dt 338.76ms | 1547669.52 tokens/sec
Step 11325 | loss: 3.136177 | lr:2.6454e-04 | norm 0.2788 | dt 339.03ms | 1546458.13 tokens/sec
Step 11326 | loss: 3.164296 | lr:2.6449e-04 | norm 0.3100 | dt 338.38ms | 1549420.83 tokens/sec
Step 11327 | loss: 3.080565 | lr:2.6445e-04 | norm 0.2888 | dt 338.36ms | 1549487.42 tokens/sec
Step 11328 | loss: 3.142572 | lr:2.6441e-04 | norm 0.2913 | dt 338.31ms | 1549706.91 tokens/sec
Step 11329 | loss: 3.145035 | lr:2.6436e-04 | norm 0.2875 | dt 337.92ms | 1551533.99 tokens/sec
Step 11330 | loss: 3.162915 | lr:2.6432e-04 | norm 0.2729 | dt 338.81ms | 1547424.47 tokens/sec
Step 11331 | loss: 3.173188 | lr:2.6427e-04 | norm 0.2646 | dt 337.64ms | 1552792.81 tokens/sec
Step 11332 | loss: 3.155017 | lr:2.6423e-04 | norm 0.2702 | dt 338.23ms | 1550089.24 tokens/sec
Step 11333 | loss: 3.112115 | lr:2.6418e-04 | norm 0.2746 | dt 339.35ms | 1544998.94 tokens/sec
Step 11334 | loss: 3.090748 | lr:2.6414e-04 | norm 0.2644 | dt 338.59ms | 1548455.26 tokens/sec
Step 11335 | loss: 3.111952 | lr:2.6409e-04 | norm 0.2690 | dt 338.50ms | 1548863.16 tokens/sec
Step 11336 | loss: 3.113698 | lr:2.6405e-04 | norm 0.2789 | dt 338.59ms | 1548458.53 tokens/sec
Step 11337 | loss: 3.123438 | lr:2.6400e-04 | norm 0.2665 | dt 337.77ms | 1552182.32 tokens/sec
Step 11338 | loss: 3.168169 | lr:2.6396e-04 | norm 0.2817 | dt 339.81ms | 1542884.04 tokens/sec
Step 11339 | loss: 3.161861 | lr:2.6391e-04 | norm 0.2776 | dt 1026.00ms | 511003.96 tokens/sec
Step 11340 | loss: 3.170026 | lr:2.6387e-04 | norm 0.2592 | dt 337.13ms | 1555146.11 tokens/sec
Step 11341 | loss: 3.115576 | lr:2.6382e-04 | norm 0.2671 | dt 338.81ms | 1547426.65 tokens/sec
Step 11342 | loss: 3.208160 | lr:2.6378e-04 | norm 0.2774 | dt 338.65ms | 1548178.36 tokens/sec
Step 11343 | loss: 3.137514 | lr:2.6373e-04 | norm 0.2920 | dt 338.27ms | 1549926.46 tokens/sec
Step 11344 | loss: 3.123246 | lr:2.6369e-04 | norm 0.2778 | dt 338.41ms | 1549248.36 tokens/sec
Step 11345 | loss: 3.115339 | lr:2.6364e-04 | norm 0.2936 | dt 337.44ms | 1553735.25 tokens/sec
Step 11346 | loss: 3.140843 | lr:2.6360e-04 | norm 0.2942 | dt 339.01ms | 1546547.32 tokens/sec
Step 11347 | loss: 3.158982 | lr:2.6355e-04 | norm 0.2964 | dt 338.72ms | 1547850.35 tokens/sec
Step 11348 | loss: 3.182120 | lr:2.6351e-04 | norm 0.3159 | dt 337.95ms | 1551374.18 tokens/sec
Step 11349 | loss: 3.146955 | lr:2.6346e-04 | norm 0.3019 | dt 338.16ms | 1550418.20 tokens/sec
Step 11350 | loss: 3.139821 | lr:2.6342e-04 | norm 0.2830 | dt 338.81ms | 1547458.23 tokens/sec
Step 11351 | loss: 3.176776 | lr:2.6337e-04 | norm 0.2788 | dt 338.09ms | 1550748.40 tokens/sec
Step 11352 | loss: 3.274255 | lr:2.6333e-04 | norm 0.3430 | dt 338.42ms | 1549215.61 tokens/sec
Step 11353 | loss: 3.167906 | lr:2.6329e-04 | norm 0.3250 | dt 338.11ms | 1550634.67 tokens/sec
Step 11354 | loss: 3.142183 | lr:2.6324e-04 | norm 0.2855 | dt 338.95ms | 1546812.75 tokens/sec
Step 11355 | loss: 3.230038 | lr:2.6320e-04 | norm 0.3218 | dt 338.84ms | 1547282.93 tokens/sec
Step 11356 | loss: 3.174158 | lr:2.6315e-04 | norm 0.3338 | dt 338.84ms | 1547318.86 tokens/sec
Step 11357 | loss: 3.171704 | lr:2.6311e-04 | norm 0.3100 | dt 338.02ms | 1551072.17 tokens/sec
Step 11358 | loss: 3.210263 | lr:2.6306e-04 | norm 0.3023 | dt 339.35ms | 1544980.49 tokens/sec
Step 11359 | loss: 3.168363 | lr:2.6302e-04 | norm 0.3031 | dt 338.53ms | 1548700.63 tokens/sec
Step 11360 | loss: 3.214404 | lr:2.6297e-04 | norm 0.3202 | dt 339.44ms | 1544548.59 tokens/sec
Step 11361 | loss: 3.144111 | lr:2.6293e-04 | norm 0.2894 | dt 338.45ms | 1549073.74 tokens/sec
Step 11362 | loss: 3.141894 | lr:2.6288e-04 | norm 0.3213 | dt 338.57ms | 1548537.04 tokens/sec
Step 11363 | loss: 3.137358 | lr:2.6284e-04 | norm 0.2829 | dt 337.96ms | 1551313.99 tokens/sec
Step 11364 | loss: 3.148361 | lr:2.6279e-04 | norm 0.3076 | dt 337.85ms | 1551819.76 tokens/sec
Step 11365 | loss: 3.107939 | lr:2.6275e-04 | norm 0.2670 | dt 338.34ms | 1549571.50 tokens/sec
Step 11366 | loss: 3.111112 | lr:2.6270e-04 | norm 0.2882 | dt 338.66ms | 1548139.12 tokens/sec
Step 11367 | loss: 3.171518 | lr:2.6266e-04 | norm 0.2653 | dt 337.81ms | 1552022.38 tokens/sec
Step 11368 | loss: 3.130533 | lr:2.6261e-04 | norm 0.2772 | dt 338.63ms | 1548259.02 tokens/sec
Step 11369 | loss: 3.146607 | lr:2.6257e-04 | norm 0.2704 | dt 338.50ms | 1548869.71 tokens/sec
Step 11370 | loss: 3.126529 | lr:2.6252e-04 | norm 0.2652 | dt 338.10ms | 1550679.50 tokens/sec
Step 11371 | loss: 3.141213 | lr:2.6248e-04 | norm 0.2880 | dt 339.45ms | 1544534.49 tokens/sec
Step 11372 | loss: 3.207960 | lr:2.6243e-04 | norm 0.2833 | dt 338.60ms | 1548402.93 tokens/sec
Step 11373 | loss: 3.190070 | lr:2.6239e-04 | norm 0.3020 | dt 339.27ms | 1545327.91 tokens/sec
Step 11374 | loss: 3.165912 | lr:2.6235e-04 | norm 0.2662 | dt 338.95ms | 1546820.37 tokens/sec
Step 11375 | loss: 3.178325 | lr:2.6230e-04 | norm 0.3009 | dt 339.81ms | 1542890.54 tokens/sec
Step 11376 | loss: 3.164717 | lr:2.6226e-04 | norm 0.2663 | dt 339.38ms | 1544838.30 tokens/sec
Step 11377 | loss: 3.126919 | lr:2.6221e-04 | norm 0.2919 | dt 338.09ms | 1550718.87 tokens/sec
Step 11378 | loss: 3.110532 | lr:2.6217e-04 | norm 0.2618 | dt 339.31ms | 1545160.69 tokens/sec
Step 11379 | loss: 3.156053 | lr:2.6212e-04 | norm 0.2873 | dt 339.59ms | 1543901.20 tokens/sec
Step 11380 | loss: 3.155988 | lr:2.6208e-04 | norm 0.2917 | dt 339.27ms | 1545359.41 tokens/sec
Step 11381 | loss: 3.113638 | lr:2.6203e-04 | norm 0.2985 | dt 338.29ms | 1549803.02 tokens/sec
Step 11382 | loss: 3.139664 | lr:2.6199e-04 | norm 0.2839 | dt 338.34ms | 1549593.34 tokens/sec
Step 11383 | loss: 3.130654 | lr:2.6194e-04 | norm 0.2673 | dt 338.01ms | 1551122.49 tokens/sec
Step 11384 | loss: 3.104742 | lr:2.6190e-04 | norm 0.2692 | dt 338.50ms | 1548846.80 tokens/sec
Step 11385 | loss: 3.137850 | lr:2.6185e-04 | norm 0.2777 | dt 338.06ms | 1550879.64 tokens/sec
Step 11386 | loss: 3.156109 | lr:2.6181e-04 | norm 0.2666 | dt 338.23ms | 1550076.13 tokens/sec
Step 11387 | loss: 3.185462 | lr:2.6176e-04 | norm 0.2729 | dt 338.28ms | 1549855.45 tokens/sec
Step 11388 | loss: 3.175452 | lr:2.6172e-04 | norm 0.2609 | dt 338.65ms | 1548157.65 tokens/sec
Step 11389 | loss: 3.148510 | lr:2.6167e-04 | norm 0.2671 | dt 338.09ms | 1550716.68 tokens/sec
Step 11390 | loss: 3.162772 | lr:2.6163e-04 | norm 0.2845 | dt 338.41ms | 1549265.82 tokens/sec
Step 11391 | loss: 3.155042 | lr:2.6159e-04 | norm 0.2669 | dt 337.97ms | 1551304.14 tokens/sec
Step 11392 | loss: 3.272093 | lr:2.6154e-04 | norm 0.3053 | dt 339.33ms | 1545077.10 tokens/sec
Step 11393 | loss: 3.174066 | lr:2.6150e-04 | norm 0.2948 | dt 338.24ms | 1550036.80 tokens/sec
Step 11394 | loss: 3.138099 | lr:2.6145e-04 | norm 0.3149 | dt 338.19ms | 1550277.20 tokens/sec
Step 11395 | loss: 3.135348 | lr:2.6141e-04 | norm 0.3379 | dt 338.47ms | 1549014.82 tokens/sec
Step 11396 | loss: 3.129075 | lr:2.6136e-04 | norm 0.2944 | dt 338.74ms | 1547740.32 tokens/sec
Step 11397 | loss: 3.077367 | lr:2.6132e-04 | norm 0.3005 | dt 337.97ms | 1551276.78 tokens/sec
Step 11398 | loss: 3.113150 | lr:2.6127e-04 | norm 0.2919 | dt 338.00ms | 1551165.16 tokens/sec
Step 11399 | loss: 3.133154 | lr:2.6123e-04 | norm 0.2876 | dt 1042.73ms | 502805.13 tokens/sec
Step 11400 | loss: 3.162526 | lr:2.6118e-04 | norm 0.2995 | dt 336.45ms | 1558272.51 tokens/sec
Step 11401 | loss: 3.134868 | lr:2.6114e-04 | norm 0.2671 | dt 338.53ms | 1548715.90 tokens/sec
Step 11402 | loss: 3.179494 | lr:2.6109e-04 | norm 0.2749 | dt 336.79ms | 1556738.05 tokens/sec
Step 11403 | loss: 3.079098 | lr:2.6105e-04 | norm 0.2904 | dt 337.81ms | 1551999.37 tokens/sec
Step 11404 | loss: 3.117699 | lr:2.6100e-04 | norm 0.2705 | dt 337.68ms | 1552607.53 tokens/sec
Step 11405 | loss: 3.125410 | lr:2.6096e-04 | norm 0.3257 | dt 337.88ms | 1551692.74 tokens/sec
Step 11406 | loss: 3.162430 | lr:2.6092e-04 | norm 0.3196 | dt 338.38ms | 1549408.82 tokens/sec
Step 11407 | loss: 3.140214 | lr:2.6087e-04 | norm 0.2907 | dt 336.34ms | 1558793.88 tokens/sec
Step 11408 | loss: 3.135623 | lr:2.6083e-04 | norm 0.3225 | dt 337.44ms | 1553720.98 tokens/sec
Step 11409 | loss: 3.127784 | lr:2.6078e-04 | norm 0.2839 | dt 338.86ms | 1547213.26 tokens/sec
Step 11410 | loss: 3.136415 | lr:2.6074e-04 | norm 0.3259 | dt 337.49ms | 1553511.33 tokens/sec
Step 11411 | loss: 3.149929 | lr:2.6069e-04 | norm 0.2631 | dt 338.00ms | 1551167.35 tokens/sec
Step 11412 | loss: 3.160277 | lr:2.6065e-04 | norm 0.3417 | dt 338.30ms | 1549776.81 tokens/sec
Step 11413 | loss: 3.181704 | lr:2.6060e-04 | norm 0.2909 | dt 338.16ms | 1550418.20 tokens/sec
Step 11414 | loss: 3.151133 | lr:2.6056e-04 | norm 0.3169 | dt 338.64ms | 1548237.22 tokens/sec
Step 11415 | loss: 3.156422 | lr:2.6051e-04 | norm 0.3022 | dt 337.88ms | 1551705.87 tokens/sec
Step 11416 | loss: 3.259154 | lr:2.6047e-04 | norm 0.2773 | dt 337.73ms | 1552397.09 tokens/sec
Step 11417 | loss: 3.148654 | lr:2.6042e-04 | norm 0.2965 | dt 344.93ms | 1519995.61 tokens/sec
Step 11418 | loss: 3.156918 | lr:2.6038e-04 | norm 0.2986 | dt 337.93ms | 1551447.51 tokens/sec
Step 11419 | loss: 3.212994 | lr:2.6033e-04 | norm 0.2828 | dt 338.09ms | 1550737.46 tokens/sec
Step 11420 | loss: 3.151115 | lr:2.6029e-04 | norm 0.2779 | dt 338.63ms | 1548259.02 tokens/sec
Step 11421 | loss: 3.062862 | lr:2.6025e-04 | norm 0.2854 | dt 338.40ms | 1549295.29 tokens/sec
Step 11422 | loss: 3.165774 | lr:2.6020e-04 | norm 0.2873 | dt 338.55ms | 1548605.75 tokens/sec
Step 11423 | loss: 3.160513 | lr:2.6016e-04 | norm 0.3042 | dt 339.10ms | 1546094.98 tokens/sec
Step 11424 | loss: 3.166598 | lr:2.6011e-04 | norm 0.2804 | dt 338.32ms | 1549695.99 tokens/sec
Step 11425 | loss: 3.141713 | lr:2.6007e-04 | norm 0.2781 | dt 338.91ms | 1546991.21 tokens/sec
Step 11426 | loss: 3.139051 | lr:2.6002e-04 | norm 0.2594 | dt 338.55ms | 1548609.02 tokens/sec
Step 11427 | loss: 3.161620 | lr:2.5998e-04 | norm 0.3203 | dt 339.09ms | 1546172.16 tokens/sec
Step 11428 | loss: 3.153888 | lr:2.5993e-04 | norm 0.2639 | dt 337.95ms | 1551364.33 tokens/sec
Step 11429 | loss: 3.150014 | lr:2.5989e-04 | norm 0.3036 | dt 339.16ms | 1545841.74 tokens/sec
Step 11430 | loss: 3.146777 | lr:2.5984e-04 | norm 0.2658 | dt 338.73ms | 1547796.97 tokens/sec
Step 11431 | loss: 3.162418 | lr:2.5980e-04 | norm 0.2973 | dt 337.96ms | 1551316.17 tokens/sec
Step 11432 | loss: 3.131455 | lr:2.5975e-04 | norm 0.2692 | dt 338.17ms | 1550363.55 tokens/sec
Step 11433 | loss: 3.131059 | lr:2.5971e-04 | norm 0.2932 | dt 338.77ms | 1547618.32 tokens/sec
Step 11434 | loss: 3.141933 | lr:2.5967e-04 | norm 0.2741 | dt 344.60ms | 1521433.20 tokens/sec
Step 11435 | loss: 3.179834 | lr:2.5962e-04 | norm 0.2907 | dt 337.85ms | 1551817.57 tokens/sec
Step 11436 | loss: 3.170709 | lr:2.5958e-04 | norm 0.2687 | dt 337.75ms | 1552285.31 tokens/sec
Step 11437 | loss: 3.131756 | lr:2.5953e-04 | norm 0.2932 | dt 337.61ms | 1552922.21 tokens/sec
Step 11438 | loss: 3.131821 | lr:2.5949e-04 | norm 0.2859 | dt 339.06ms | 1546299.37 tokens/sec
Step 11439 | loss: 3.130947 | lr:2.5944e-04 | norm 0.2709 | dt 337.58ms | 1553083.43 tokens/sec
Step 11440 | loss: 3.102274 | lr:2.5940e-04 | norm 0.2814 | dt 337.98ms | 1551231.91 tokens/sec
Step 11441 | loss: 3.113547 | lr:2.5935e-04 | norm 0.2714 | dt 337.83ms | 1551920.51 tokens/sec
Step 11442 | loss: 3.140805 | lr:2.5931e-04 | norm 0.2730 | dt 337.93ms | 1551456.27 tokens/sec
Step 11443 | loss: 3.173258 | lr:2.5926e-04 | norm 0.2750 | dt 337.93ms | 1551446.42 tokens/sec
Step 11444 | loss: 3.153960 | lr:2.5922e-04 | norm 0.2935 | dt 337.38ms | 1554016.34 tokens/sec
Step 11445 | loss: 3.134968 | lr:2.5917e-04 | norm 0.2848 | dt 337.96ms | 1551339.16 tokens/sec
Step 11446 | loss: 3.120866 | lr:2.5913e-04 | norm 0.2841 | dt 337.71ms | 1552488.05 tokens/sec
Step 11447 | loss: 3.107260 | lr:2.5909e-04 | norm 0.2757 | dt 338.48ms | 1548954.81 tokens/sec
Step 11448 | loss: 3.087712 | lr:2.5904e-04 | norm 0.2825 | dt 337.66ms | 1552723.74 tokens/sec
Step 11449 | loss: 3.167985 | lr:2.5900e-04 | norm 0.2976 | dt 338.21ms | 1550170.10 tokens/sec
Step 11450 | loss: 3.182698 | lr:2.5895e-04 | norm 0.3005 | dt 338.04ms | 1550980.27 tokens/sec
Step 11451 | loss: 3.163111 | lr:2.5891e-04 | norm 0.3049 | dt 338.78ms | 1547574.76 tokens/sec
Step 11452 | loss: 3.195494 | lr:2.5886e-04 | norm 0.3236 | dt 338.46ms | 1549029.00 tokens/sec
Step 11453 | loss: 3.101772 | lr:2.5882e-04 | norm 0.3673 | dt 337.79ms | 1552115.49 tokens/sec
Step 11454 | loss: 3.188202 | lr:2.5877e-04 | norm 0.2989 | dt 338.22ms | 1550119.84 tokens/sec
Step 11455 | loss: 3.226635 | lr:2.5873e-04 | norm 0.3282 | dt 338.48ms | 1548965.72 tokens/sec
Step 11456 | loss: 3.156156 | lr:2.5868e-04 | norm 0.3050 | dt 338.16ms | 1550401.81 tokens/sec
Step 11457 | loss: 3.177319 | lr:2.5864e-04 | norm 0.3031 | dt 338.14ms | 1550490.35 tokens/sec
Step 11458 | loss: 3.159953 | lr:2.5860e-04 | norm 0.3042 | dt 338.47ms | 1549013.72 tokens/sec
Step 11459 | loss: 3.108774 | lr:2.5855e-04 | norm 0.3243 | dt 338.32ms | 1549662.13 tokens/sec
Step 11460 | loss: 3.198327 | lr:2.5851e-04 | norm 0.3409 | dt 338.04ms | 1550986.83 tokens/sec
Step 11461 | loss: 3.171673 | lr:2.5846e-04 | norm 0.3154 | dt 338.04ms | 1550949.64 tokens/sec
Step 11462 | loss: 3.157540 | lr:2.5842e-04 | norm 0.2974 | dt 338.47ms | 1548974.44 tokens/sec
Step 11463 | loss: 3.171326 | lr:2.5837e-04 | norm 0.2940 | dt 337.85ms | 1551814.28 tokens/sec
Step 11464 | loss: 3.127136 | lr:2.5833e-04 | norm 0.2917 | dt 338.85ms | 1547251.36 tokens/sec
Step 11465 | loss: 3.090562 | lr:2.5828e-04 | norm 0.2947 | dt 337.61ms | 1552958.40 tokens/sec
Step 11466 | loss: 3.119508 | lr:2.5824e-04 | norm 0.2608 | dt 337.88ms | 1551702.59 tokens/sec
Step 11467 | loss: 3.147605 | lr:2.5819e-04 | norm 0.3049 | dt 338.12ms | 1550590.94 tokens/sec
Step 11468 | loss: 3.162654 | lr:2.5815e-04 | norm 0.2750 | dt 338.65ms | 1548189.26 tokens/sec
Step 11469 | loss: 3.126290 | lr:2.5811e-04 | norm 0.2863 | dt 337.64ms | 1552799.39 tokens/sec
Step 11470 | loss: 3.135488 | lr:2.5806e-04 | norm 0.2762 | dt 338.06ms | 1550856.67 tokens/sec
Step 11471 | loss: 3.175608 | lr:2.5802e-04 | norm 0.2915 | dt 338.52ms | 1548764.98 tokens/sec
Step 11472 | loss: 3.187711 | lr:2.5797e-04 | norm 0.2992 | dt 338.26ms | 1549958.14 tokens/sec
Step 11473 | loss: 3.058340 | lr:2.5793e-04 | norm 0.2855 | dt 337.91ms | 1551571.21 tokens/sec
Step 11474 | loss: 3.110863 | lr:2.5788e-04 | norm 0.2531 | dt 338.28ms | 1549869.65 tokens/sec
Step 11475 | loss: 3.177747 | lr:2.5784e-04 | norm 0.3161 | dt 339.06ms | 1546288.49 tokens/sec
Step 11476 | loss: 3.166281 | lr:2.5779e-04 | norm 0.3000 | dt 338.28ms | 1549853.27 tokens/sec
Step 11477 | loss: 3.148879 | lr:2.5775e-04 | norm 0.2878 | dt 337.90ms | 1551606.24 tokens/sec
Step 11478 | loss: 3.085122 | lr:2.5770e-04 | norm 0.2820 | dt 337.81ms | 1552007.04 tokens/sec
Step 11479 | loss: 3.205716 | lr:2.5766e-04 | norm 0.2721 | dt 338.28ms | 1549876.21 tokens/sec
Step 11480 | loss: 3.145676 | lr:2.5762e-04 | norm 0.2698 | dt 337.94ms | 1551407.01 tokens/sec
Step 11481 | loss: 3.168357 | lr:2.5757e-04 | norm 0.2743 | dt 338.07ms | 1550807.45 tokens/sec
Step 11482 | loss: 3.119946 | lr:2.5753e-04 | norm 0.2755 | dt 338.68ms | 1548014.88 tokens/sec
Step 11483 | loss: 3.240554 | lr:2.5748e-04 | norm 0.2967 | dt 337.78ms | 1552148.35 tokens/sec
Step 11484 | loss: 3.176600 | lr:2.5744e-04 | norm 0.2950 | dt 337.93ms | 1551462.84 tokens/sec
Step 11485 | loss: 3.179620 | lr:2.5739e-04 | norm 0.2959 | dt 338.03ms | 1551009.81 tokens/sec
Step 11486 | loss: 3.158386 | lr:2.5735e-04 | norm 0.2761 | dt 337.27ms | 1554523.88 tokens/sec
Step 11487 | loss: 3.185285 | lr:2.5730e-04 | norm 0.2751 | dt 337.68ms | 1552610.82 tokens/sec
Step 11488 | loss: 3.166470 | lr:2.5726e-04 | norm 0.2783 | dt 337.40ms | 1553927.39 tokens/sec
Step 11489 | loss: 3.189856 | lr:2.5721e-04 | norm 0.2872 | dt 337.95ms | 1551381.84 tokens/sec
Step 11490 | loss: 3.125932 | lr:2.5717e-04 | norm 0.2828 | dt 338.17ms | 1550376.67 tokens/sec
Step 11491 | loss: 3.152721 | lr:2.5713e-04 | norm 0.2720 | dt 337.65ms | 1552776.37 tokens/sec
Step 11492 | loss: 3.150448 | lr:2.5708e-04 | norm 0.2689 | dt 338.02ms | 1551077.64 tokens/sec
Step 11493 | loss: 3.146138 | lr:2.5704e-04 | norm 0.2836 | dt 337.77ms | 1552215.19 tokens/sec
Step 11494 | loss: 3.191329 | lr:2.5699e-04 | norm 0.2973 | dt 337.58ms | 1553075.75 tokens/sec
Step 11495 | loss: 3.128518 | lr:2.5695e-04 | norm 0.2890 | dt 337.53ms | 1553324.78 tokens/sec
Step 11496 | loss: 3.185093 | lr:2.5690e-04 | norm 0.2996 | dt 338.36ms | 1549479.78 tokens/sec
Step 11497 | loss: 3.192848 | lr:2.5686e-04 | norm 0.2767 | dt 337.73ms | 1552375.17 tokens/sec
Step 11498 | loss: 3.122669 | lr:2.5681e-04 | norm 0.2989 | dt 337.97ms | 1551291.00 tokens/sec
Step 11499 | loss: 3.264403 | lr:2.5677e-04 | norm 0.3404 | dt 337.84ms | 1551895.32 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 11500: 3.1673
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2991/10042=0.2978




ddp_rank 3: ####### Printing generated samples ####### 

ddp_rank 5: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not that much interested in computers, and I use a high precision, simple word processing application that has justrank 5 sample 0 >Hello, I'm a language model, I can do a lot of things in which you can use English for your purposes and have a little fun learning a second

rank 5 sample 1 >Hello, I'm a language model, as this is how one gets it. It goes through all of the steps and how I'm going to take the content
rank 5 sample 2 >Hello, I'm a language model, and I was a school teacher in school. I'm a teacher of two students and this is how I teach. You
rank 3 sample 1 >Hello, I'm a language model, so what I'm doing is a word processing and math based language, so I'm going to make it better in that
rank 5 sample 3 >Hello, I'm a language model, that I would use to understand how to use this to get the best out of your software, including how to write applications


rank 3 sample 2 >Hello, I'm a language model, so it seems like to me that I love a lot of books.
What I love about programming languages is learning the
rank 3 sample 3 >Hello, I'm a language model, so don't know how to read a text and how much I want to read. And in the worst cases, it




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to show you how our language models work.
To translate "Hello, that is the first letter
rank 7 sample 1 >Hello, I'm a language model, well I think I should go into C and I got lost. You can't remember where I'm now. So now
rank 7 sample 2 >Hello, I'm a language model, so I've already added a module that's actually installed on the board now. I've also added a module where the
rank 7 sample 3 >Hello, I'm a language model, and you'll see the way it would do with people. I've read a lot of books and some of them give




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, there were no native speakers.
If I were to use the language model in your company, you would write a code
rank 2 sample 1 >Hello, I'm a language model, I'm no linguist. I used ASL to read a dictionary. I'm not fluent. That's why I
rank 2 sample 2 >Hello, I'm a language model, and I want to be a programmer on a Linux computer. What is one thing I would want to go over for someone
rank 2 sample 3 >Hello, I'm a language model, I don't know if it's what you're working with (I'm always on top...) then you might have to




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I'd like to help you out if I could.
I know my code. In each module in this tutorial
rank 6 sample 0 >Hello, I'm a language model, but that's not where I want to go. If I wanted to write the machine, and I wanted a machine without
rank 4 sample 1 >Hello, I'm a language model, language, and user experience designer; we work in some combination of Python and Python3. I'm going to follow along
rank 6 sample 1 >Hello, I'm a language model, how do you write programs that will be automatically implemented on a computer? You can write your program without writing your program.
rank 4 sample 2 >Hello, I'm a language model, but they're like me. I love learning. I'm an app developer. I'm a language analyst.<|endoftext|>G
rank 6 sample 2 >Hello, I'm a language model, I need some work on the grammar but I want to know the most basic language structures.
I got in some work
rank 4 sample 3 >Hello, I'm a language model, so you ought at least read this by now and feel secure so you begin.
I've also written a whole bunch


rank 6 sample 3 >Hello, I'm a language model, so I can't use the other parts. But I'm going to use a different kind of language to explain each part




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so when you're talking about things within your context, you think you're gonna be using some other language that's not
rank 1 sample 1 >Hello, I'm a language model, a programmer who likes to write, debug, and troubleshoot . I know enough I've built up a lot of
rank 1 sample 2 >Hello, I'm a language model, I find the answer to my question is, "Which one thing are we talking about?" So the question, is it
rank 1 sample 3 >Hello, I'm a language model, so I'm doing some code in Ruby about it. Please know why that you're missing (it's a pretty simple




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't have all these tools around yet. I've written everything for the computer with this book, but if
rank 0 sample 1 >Hello, I'm a language model, and when I'm doing something in Windows, you write "hello,
- "I'm going to write a program
rank 0 sample 2 >Hello, I'm a language model, and I understand it is an engineering style. When I'm done learning how to build my language model, I can explain
rank 0 sample 3 >Hello, I'm a language model, and i'm doing a lot of work in general. I'm writing a lot in English, but am also working with


Step 11500 | loss: 3.149542 | lr:2.5673e-04 | norm 0.2800 | dt 12384.76ms | 42333.32 tokens/sec
Step 11501 | loss: 3.141605 | lr:2.5668e-04 | norm 0.3032 | dt 334.59ms | 1566967.94 tokens/sec
Step 11502 | loss: 3.122680 | lr:2.5664e-04 | norm 0.2732 | dt 336.55ms | 1557819.91 tokens/sec
Step 11503 | loss: 3.149732 | lr:2.5659e-04 | norm 0.2885 | dt 336.58ms | 1557689.70 tokens/sec
Step 11504 | loss: 3.196655 | lr:2.5655e-04 | norm 0.2826 | dt 337.19ms | 1554892.10 tokens/sec
Step 11505 | loss: 3.206023 | lr:2.5650e-04 | norm 0.2998 | dt 337.08ms | 1555374.90 tokens/sec
Step 11506 | loss: 3.158114 | lr:2.5646e-04 | norm 0.3105 | dt 338.10ms | 1550702.47 tokens/sec
Step 11507 | loss: 3.106487 | lr:2.5641e-04 | norm 0.2625 | dt 336.88ms | 1556297.35 tokens/sec
Step 11508 | loss: 3.163672 | lr:2.5637e-04 | norm 0.3080 | dt 337.26ms | 1554562.34 tokens/sec
Step 11509 | loss: 3.135603 | lr:2.5633e-04 | norm 0.2758 | dt 336.22ms | 1559366.46 tokens/sec
Step 11510 | loss: 3.065179 | lr:2.5628e-04 | norm 0.3741 | dt 337.54ms | 1553245.79 tokens/sec
Step 11511 | loss: 3.126388 | lr:2.5624e-04 | norm 0.2881 | dt 336.41ms | 1558463.56 tokens/sec
Step 11512 | loss: 3.205229 | lr:2.5619e-04 | norm 0.3522 | dt 337.34ms | 1554200.86 tokens/sec
Step 11513 | loss: 3.135417 | lr:2.5615e-04 | norm 0.2910 | dt 337.30ms | 1554353.56 tokens/sec
Step 11514 | loss: 3.193496 | lr:2.5610e-04 | norm 0.3027 | dt 337.30ms | 1554368.94 tokens/sec
Step 11515 | loss: 3.165447 | lr:2.5606e-04 | norm 0.2892 | dt 337.90ms | 1551594.20 tokens/sec
Step 11516 | loss: 3.143531 | lr:2.5601e-04 | norm 0.2994 | dt 337.25ms | 1554607.40 tokens/sec
Step 11517 | loss: 3.203954 | lr:2.5597e-04 | norm 0.3184 | dt 337.48ms | 1553551.94 tokens/sec
Step 11518 | loss: 3.144966 | lr:2.5593e-04 | norm 0.2784 | dt 337.10ms | 1555296.80 tokens/sec
Step 11519 | loss: 3.194703 | lr:2.5588e-04 | norm 0.3079 | dt 338.49ms | 1548893.71 tokens/sec
Step 11520 | loss: 3.172901 | lr:2.5584e-04 | norm 0.2921 | dt 338.04ms | 1550958.39 tokens/sec
Step 11521 | loss: 3.170336 | lr:2.5579e-04 | norm 0.2913 | dt 337.84ms | 1551866.85 tokens/sec
Step 11522 | loss: 3.138350 | lr:2.5575e-04 | norm 0.3382 | dt 336.57ms | 1557721.70 tokens/sec
Step 11523 | loss: 3.170252 | lr:2.5570e-04 | norm 0.3007 | dt 338.06ms | 1550867.61 tokens/sec
Step 11524 | loss: 3.189222 | lr:2.5566e-04 | norm 0.2938 | dt 337.20ms | 1554841.53 tokens/sec
Step 11525 | loss: 3.191705 | lr:2.5561e-04 | norm 0.3163 | dt 336.93ms | 1556052.87 tokens/sec
Step 11526 | loss: 3.179087 | lr:2.5557e-04 | norm 0.2961 | dt 337.47ms | 1553603.53 tokens/sec
Step 11527 | loss: 3.171750 | lr:2.5553e-04 | norm 0.3145 | dt 339.53ms | 1544171.15 tokens/sec
Step 11528 | loss: 3.105492 | lr:2.5548e-04 | norm 0.2847 | dt 898.05ms | 583804.61 tokens/sec
Step 11529 | loss: 3.172961 | lr:2.5544e-04 | norm 0.2828 | dt 335.71ms | 1561736.41 tokens/sec
Step 11530 | loss: 3.165990 | lr:2.5539e-04 | norm 0.2950 | dt 337.05ms | 1555523.43 tokens/sec
Step 11531 | loss: 3.144432 | lr:2.5535e-04 | norm 0.2742 | dt 339.42ms | 1544678.79 tokens/sec
Step 11532 | loss: 3.163012 | lr:2.5530e-04 | norm 0.2762 | dt 337.89ms | 1551675.22 tokens/sec
Step 11533 | loss: 3.148203 | lr:2.5526e-04 | norm 0.2655 | dt 337.07ms | 1555407.91 tokens/sec
Step 11534 | loss: 3.195581 | lr:2.5521e-04 | norm 0.2811 | dt 338.49ms | 1548891.53 tokens/sec
Step 11535 | loss: 3.215032 | lr:2.5517e-04 | norm 0.2942 | dt 337.34ms | 1554199.76 tokens/sec
Step 11536 | loss: 3.223986 | lr:2.5513e-04 | norm 0.3616 | dt 337.25ms | 1554604.10 tokens/sec
Step 11537 | loss: 3.157784 | lr:2.5508e-04 | norm 0.2941 | dt 337.87ms | 1551746.39 tokens/sec
Step 11538 | loss: 3.160356 | lr:2.5504e-04 | norm 0.3116 | dt 337.23ms | 1554707.42 tokens/sec
Step 11539 | loss: 3.147404 | lr:2.5499e-04 | norm 0.2562 | dt 337.40ms | 1553910.92 tokens/sec
Step 11540 | loss: 3.140113 | lr:2.5495e-04 | norm 0.2866 | dt 338.19ms | 1550287.04 tokens/sec
Step 11541 | loss: 3.131957 | lr:2.5490e-04 | norm 0.2853 | dt 337.67ms | 1552668.92 tokens/sec
Step 11542 | loss: 3.130697 | lr:2.5486e-04 | norm 0.2932 | dt 337.64ms | 1552814.74 tokens/sec
Step 11543 | loss: 3.114117 | lr:2.5482e-04 | norm 0.3344 | dt 337.69ms | 1552591.09 tokens/sec
Step 11544 | loss: 3.146597 | lr:2.5477e-04 | norm 0.2632 | dt 337.34ms | 1554185.48 tokens/sec
Step 11545 | loss: 3.182229 | lr:2.5473e-04 | norm 0.3168 | dt 338.00ms | 1551169.54 tokens/sec
Step 11546 | loss: 3.176053 | lr:2.5468e-04 | norm 0.2616 | dt 338.76ms | 1547689.12 tokens/sec
Step 11547 | loss: 3.155313 | lr:2.5464e-04 | norm 0.3310 | dt 339.07ms | 1546255.88 tokens/sec
Step 11548 | loss: 3.224408 | lr:2.5459e-04 | norm 0.2809 | dt 337.81ms | 1552034.42 tokens/sec
Step 11549 | loss: 3.116514 | lr:2.5455e-04 | norm 0.3324 | dt 339.49ms | 1544327.31 tokens/sec
Step 11550 | loss: 3.067472 | lr:2.5450e-04 | norm 0.2704 | dt 338.17ms | 1550383.22 tokens/sec
Step 11551 | loss: 3.205147 | lr:2.5446e-04 | norm 0.2996 | dt 337.87ms | 1551739.82 tokens/sec
Step 11552 | loss: 3.202691 | lr:2.5442e-04 | norm 0.2994 | dt 338.83ms | 1547370.03 tokens/sec
Step 11553 | loss: 3.193531 | lr:2.5437e-04 | norm 0.2826 | dt 338.17ms | 1550360.27 tokens/sec
Step 11554 | loss: 3.218220 | lr:2.5433e-04 | norm 0.3019 | dt 338.68ms | 1548030.14 tokens/sec
Step 11555 | loss: 3.209115 | lr:2.5428e-04 | norm 0.2793 | dt 338.38ms | 1549397.90 tokens/sec
Step 11556 | loss: 3.261639 | lr:2.5424e-04 | norm 0.3938 | dt 338.05ms | 1550908.08 tokens/sec
Step 11557 | loss: 3.107232 | lr:2.5419e-04 | norm 0.2971 | dt 338.43ms | 1549158.86 tokens/sec
Step 11558 | loss: 3.105261 | lr:2.5415e-04 | norm 0.2896 | dt 338.03ms | 1550994.49 tokens/sec
Step 11559 | loss: 3.234360 | lr:2.5411e-04 | norm 0.3435 | dt 337.99ms | 1551185.95 tokens/sec
Step 11560 | loss: 3.150635 | lr:2.5406e-04 | norm 0.3582 | dt 338.49ms | 1548903.53 tokens/sec
Step 11561 | loss: 3.193765 | lr:2.5402e-04 | norm 0.2816 | dt 338.90ms | 1547032.57 tokens/sec
Step 11562 | loss: 3.157945 | lr:2.5397e-04 | norm 0.3124 | dt 338.63ms | 1548264.47 tokens/sec
Step 11563 | loss: 3.142720 | lr:2.5393e-04 | norm 0.3117 | dt 338.56ms | 1548581.75 tokens/sec
Step 11564 | loss: 3.138136 | lr:2.5388e-04 | norm 0.3295 | dt 338.62ms | 1548323.34 tokens/sec
Step 11565 | loss: 3.255570 | lr:2.5384e-04 | norm 0.3119 | dt 338.49ms | 1548899.16 tokens/sec
Step 11566 | loss: 3.145806 | lr:2.5380e-04 | norm 0.3252 | dt 338.17ms | 1550381.04 tokens/sec
Step 11567 | loss: 3.193580 | lr:2.5375e-04 | norm 0.2972 | dt 338.23ms | 1550110.00 tokens/sec
Step 11568 | loss: 3.167292 | lr:2.5371e-04 | norm 0.3105 | dt 338.37ms | 1549429.56 tokens/sec
Step 11569 | loss: 3.157542 | lr:2.5366e-04 | norm 0.2799 | dt 338.92ms | 1546948.77 tokens/sec
Step 11570 | loss: 3.119678 | lr:2.5362e-04 | norm 0.2840 | dt 338.08ms | 1550762.61 tokens/sec
Step 11571 | loss: 3.111102 | lr:2.5357e-04 | norm 0.2802 | dt 337.95ms | 1551390.60 tokens/sec
Step 11572 | loss: 3.102913 | lr:2.5353e-04 | norm 0.2734 | dt 338.12ms | 1550574.54 tokens/sec
Step 11573 | loss: 3.119141 | lr:2.5348e-04 | norm 0.2737 | dt 338.24ms | 1550066.30 tokens/sec
Step 11574 | loss: 3.173354 | lr:2.5344e-04 | norm 0.2674 | dt 337.98ms | 1551228.63 tokens/sec
Step 11575 | loss: 3.108480 | lr:2.5340e-04 | norm 0.2731 | dt 337.77ms | 1552181.22 tokens/sec
Step 11576 | loss: 3.098244 | lr:2.5335e-04 | norm 0.2554 | dt 338.72ms | 1547847.08 tokens/sec
Step 11577 | loss: 3.155931 | lr:2.5331e-04 | norm 0.2699 | dt 337.60ms | 1553006.65 tokens/sec
Step 11578 | loss: 3.157344 | lr:2.5326e-04 | norm 0.2614 | dt 337.96ms | 1551319.46 tokens/sec
Step 11579 | loss: 3.116754 | lr:2.5322e-04 | norm 0.2865 | dt 337.75ms | 1552309.42 tokens/sec
Step 11580 | loss: 3.127139 | lr:2.5317e-04 | norm 0.2701 | dt 337.83ms | 1551933.65 tokens/sec
Step 11581 | loss: 3.081470 | lr:2.5313e-04 | norm 0.2868 | dt 338.90ms | 1547033.66 tokens/sec
Step 11582 | loss: 3.159870 | lr:2.5309e-04 | norm 0.3093 | dt 338.12ms | 1550614.99 tokens/sec
Step 11583 | loss: 3.158716 | lr:2.5304e-04 | norm 0.2930 | dt 338.85ms | 1547274.22 tokens/sec
Step 11584 | loss: 3.097417 | lr:2.5300e-04 | norm 0.3037 | dt 340.26ms | 1540833.23 tokens/sec
Step 11585 | loss: 3.172855 | lr:2.5295e-04 | norm 0.3143 | dt 338.21ms | 1550165.73 tokens/sec
Step 11586 | loss: 3.144173 | lr:2.5291e-04 | norm 0.3016 | dt 337.66ms | 1552707.29 tokens/sec
Step 11587 | loss: 3.148766 | lr:2.5286e-04 | norm 0.3039 | dt 338.00ms | 1551132.34 tokens/sec
Step 11588 | loss: 3.131192 | lr:2.5282e-04 | norm 0.2768 | dt 338.11ms | 1550653.26 tokens/sec
Step 11589 | loss: 3.093286 | lr:2.5278e-04 | norm 0.3155 | dt 1027.36ms | 510326.58 tokens/sec
Step 11590 | loss: 3.120300 | lr:2.5273e-04 | norm 0.2970 | dt 335.96ms | 1560584.87 tokens/sec
Step 11591 | loss: 3.134775 | lr:2.5269e-04 | norm 0.2989 | dt 336.80ms | 1556670.82 tokens/sec
Step 11592 | loss: 3.120250 | lr:2.5264e-04 | norm 0.2942 | dt 340.30ms | 1540682.09 tokens/sec
Step 11593 | loss: 3.163654 | lr:2.5260e-04 | norm 0.2866 | dt 336.91ms | 1556148.67 tokens/sec
Step 11594 | loss: 3.162117 | lr:2.5255e-04 | norm 0.3086 | dt 337.42ms | 1553812.10 tokens/sec
Step 11595 | loss: 3.156363 | lr:2.5251e-04 | norm 0.3056 | dt 337.70ms | 1552547.24 tokens/sec
Step 11596 | loss: 3.133807 | lr:2.5247e-04 | norm 0.2977 | dt 337.48ms | 1553546.45 tokens/sec
Step 11597 | loss: 3.180344 | lr:2.5242e-04 | norm 0.3085 | dt 337.48ms | 1553555.23 tokens/sec
Step 11598 | loss: 3.158908 | lr:2.5238e-04 | norm 0.3164 | dt 337.29ms | 1554411.79 tokens/sec
Step 11599 | loss: 3.147362 | lr:2.5233e-04 | norm 0.3190 | dt 338.00ms | 1551157.51 tokens/sec
Step 11600 | loss: 3.162060 | lr:2.5229e-04 | norm 0.2917 | dt 338.02ms | 1551045.91 tokens/sec
Step 11601 | loss: 3.180973 | lr:2.5225e-04 | norm 0.3033 | dt 337.67ms | 1552682.08 tokens/sec
Step 11602 | loss: 3.149996 | lr:2.5220e-04 | norm 0.3319 | dt 338.26ms | 1549966.88 tokens/sec
Step 11603 | loss: 3.153149 | lr:2.5216e-04 | norm 0.2655 | dt 338.00ms | 1551165.16 tokens/sec
Step 11604 | loss: 3.079096 | lr:2.5211e-04 | norm 0.3343 | dt 337.74ms | 1552322.57 tokens/sec
Step 11605 | loss: 3.130256 | lr:2.5207e-04 | norm 0.2803 | dt 339.26ms | 1545401.76 tokens/sec
Step 11606 | loss: 3.177365 | lr:2.5202e-04 | norm 0.3356 | dt 339.46ms | 1544497.61 tokens/sec
Step 11607 | loss: 3.109179 | lr:2.5198e-04 | norm 0.2836 | dt 339.61ms | 1543788.48 tokens/sec
Step 11608 | loss: 3.141933 | lr:2.5194e-04 | norm 0.3137 | dt 339.20ms | 1545643.99 tokens/sec
Step 11609 | loss: 3.152455 | lr:2.5189e-04 | norm 0.2802 | dt 339.62ms | 1543760.30 tokens/sec
Step 11610 | loss: 3.132304 | lr:2.5185e-04 | norm 0.2936 | dt 338.71ms | 1547912.46 tokens/sec
Step 11611 | loss: 3.123275 | lr:2.5180e-04 | norm 0.3289 | dt 338.87ms | 1547149.03 tokens/sec
Step 11612 | loss: 3.211107 | lr:2.5176e-04 | norm 0.3081 | dt 339.20ms | 1545639.65 tokens/sec
Step 11613 | loss: 3.086863 | lr:2.5171e-04 | norm 0.2976 | dt 338.36ms | 1549520.18 tokens/sec
Step 11614 | loss: 3.141917 | lr:2.5167e-04 | norm 0.2872 | dt 338.55ms | 1548623.19 tokens/sec
Step 11615 | loss: 3.128526 | lr:2.5163e-04 | norm 0.2830 | dt 337.71ms | 1552476.00 tokens/sec
Step 11616 | loss: 3.182360 | lr:2.5158e-04 | norm 0.3077 | dt 339.01ms | 1546517.95 tokens/sec
Step 11617 | loss: 3.186295 | lr:2.5154e-04 | norm 0.3180 | dt 338.08ms | 1550801.98 tokens/sec
Step 11618 | loss: 3.149687 | lr:2.5149e-04 | norm 0.2652 | dt 337.37ms | 1554037.21 tokens/sec
Step 11619 | loss: 3.141466 | lr:2.5145e-04 | norm 0.5291 | dt 337.73ms | 1552395.99 tokens/sec
Step 11620 | loss: 3.161962 | lr:2.5140e-04 | norm 0.3184 | dt 339.03ms | 1546450.52 tokens/sec
Step 11621 | loss: 3.188201 | lr:2.5136e-04 | norm 0.2952 | dt 337.32ms | 1554294.24 tokens/sec
Step 11622 | loss: 3.238709 | lr:2.5132e-04 | norm 0.3605 | dt 338.14ms | 1550517.68 tokens/sec
Step 11623 | loss: 3.130201 | lr:2.5127e-04 | norm 0.2923 | dt 337.69ms | 1552564.78 tokens/sec
Step 11624 | loss: 3.148867 | lr:2.5123e-04 | norm 0.3170 | dt 338.09ms | 1550738.55 tokens/sec
Step 11625 | loss: 3.167969 | lr:2.5118e-04 | norm 0.3210 | dt 340.32ms | 1540561.21 tokens/sec
Step 11626 | loss: 3.219470 | lr:2.5114e-04 | norm 0.3227 | dt 338.37ms | 1549451.40 tokens/sec
Step 11627 | loss: 3.210041 | lr:2.5110e-04 | norm 0.3217 | dt 338.43ms | 1549169.77 tokens/sec
Step 11628 | loss: 3.099281 | lr:2.5105e-04 | norm 0.2796 | dt 338.24ms | 1550028.06 tokens/sec
Step 11629 | loss: 3.185347 | lr:2.5101e-04 | norm 0.3089 | dt 337.96ms | 1551327.12 tokens/sec
Step 11630 | loss: 3.203357 | lr:2.5096e-04 | norm 0.2791 | dt 338.77ms | 1547609.61 tokens/sec
Step 11631 | loss: 3.181668 | lr:2.5092e-04 | norm 0.2917 | dt 338.86ms | 1547221.96 tokens/sec
Step 11632 | loss: 3.171962 | lr:2.5087e-04 | norm 0.2867 | dt 337.78ms | 1552165.88 tokens/sec
Step 11633 | loss: 3.180732 | lr:2.5083e-04 | norm 0.2977 | dt 338.33ms | 1549626.10 tokens/sec
Step 11634 | loss: 3.147257 | lr:2.5079e-04 | norm 0.2805 | dt 338.93ms | 1546880.21 tokens/sec
Step 11635 | loss: 3.155298 | lr:2.5074e-04 | norm 0.2732 | dt 340.90ms | 1537940.90 tokens/sec
Step 11636 | loss: 3.263420 | lr:2.5070e-04 | norm 0.3934 | dt 337.80ms | 1552062.91 tokens/sec
Step 11637 | loss: 3.139386 | lr:2.5065e-04 | norm 0.4124 | dt 338.56ms | 1548566.49 tokens/sec
Step 11638 | loss: 3.140563 | lr:2.5061e-04 | norm 0.2912 | dt 337.90ms | 1551621.57 tokens/sec
Step 11639 | loss: 3.232316 | lr:2.5057e-04 | norm 0.3756 | dt 337.85ms | 1551816.47 tokens/sec
Step 11640 | loss: 3.174016 | lr:2.5052e-04 | norm 0.3315 | dt 337.74ms | 1552320.38 tokens/sec
Step 11641 | loss: 3.116741 | lr:2.5048e-04 | norm 0.3536 | dt 337.94ms | 1551437.66 tokens/sec
Step 11642 | loss: 3.159420 | lr:2.5043e-04 | norm 0.3222 | dt 337.61ms | 1552923.30 tokens/sec
Step 11643 | loss: 3.149972 | lr:2.5039e-04 | norm 0.3191 | dt 337.46ms | 1553643.04 tokens/sec
Step 11644 | loss: 3.163829 | lr:2.5034e-04 | norm 0.3201 | dt 338.09ms | 1550756.05 tokens/sec
Step 11645 | loss: 3.127538 | lr:2.5030e-04 | norm 0.2880 | dt 337.60ms | 1552975.94 tokens/sec
Step 11646 | loss: 3.124716 | lr:2.5026e-04 | norm 0.2989 | dt 337.96ms | 1551328.21 tokens/sec
Step 11647 | loss: 3.227268 | lr:2.5021e-04 | norm 0.3234 | dt 338.27ms | 1549917.72 tokens/sec
Step 11648 | loss: 3.132718 | lr:2.5017e-04 | norm 0.2888 | dt 338.01ms | 1551090.76 tokens/sec
Step 11649 | loss: 3.130102 | lr:2.5012e-04 | norm 0.2908 | dt 337.88ms | 1551719.01 tokens/sec
Step 11650 | loss: 3.138713 | lr:2.5008e-04 | norm 0.3080 | dt 338.47ms | 1549003.90 tokens/sec
Step 11651 | loss: 3.089591 | lr:2.5004e-04 | norm 0.2658 | dt 338.52ms | 1548750.80 tokens/sec
Step 11652 | loss: 3.222152 | lr:2.4999e-04 | norm 0.3025 | dt 338.76ms | 1547662.98 tokens/sec
Step 11653 | loss: 3.165078 | lr:2.4995e-04 | norm 0.2849 | dt 338.88ms | 1547113.11 tokens/sec
Step 11654 | loss: 3.114883 | lr:2.4990e-04 | norm 0.2905 | dt 339.32ms | 1545096.64 tokens/sec
Step 11655 | loss: 3.193821 | lr:2.4986e-04 | norm 0.2996 | dt 338.32ms | 1549682.88 tokens/sec
Step 11656 | loss: 3.105092 | lr:2.4982e-04 | norm 0.2722 | dt 339.04ms | 1546371.13 tokens/sec
Step 11657 | loss: 3.259120 | lr:2.4977e-04 | norm 0.2997 | dt 338.27ms | 1549896.96 tokens/sec
Step 11658 | loss: 3.182115 | lr:2.4973e-04 | norm 0.3024 | dt 337.53ms | 1553285.28 tokens/sec
Step 11659 | loss: 3.117224 | lr:2.4968e-04 | norm 0.2738 | dt 337.56ms | 1553163.51 tokens/sec
Step 11660 | loss: 3.166709 | lr:2.4964e-04 | norm 0.3177 | dt 338.52ms | 1548745.35 tokens/sec
Step 11661 | loss: 3.109673 | lr:2.4959e-04 | norm 0.3068 | dt 337.42ms | 1553806.61 tokens/sec
Step 11662 | loss: 3.228253 | lr:2.4955e-04 | norm 0.2819 | dt 337.94ms | 1551434.38 tokens/sec
Step 11663 | loss: 3.130976 | lr:2.4951e-04 | norm 0.2787 | dt 337.60ms | 1552970.46 tokens/sec
Step 11664 | loss: 3.161849 | lr:2.4946e-04 | norm 0.2959 | dt 338.24ms | 1550040.08 tokens/sec
Step 11665 | loss: 3.169143 | lr:2.4942e-04 | norm 0.2740 | dt 337.99ms | 1551208.93 tokens/sec
Step 11666 | loss: 3.129525 | lr:2.4937e-04 | norm 0.3372 | dt 338.15ms | 1550449.90 tokens/sec
Step 11667 | loss: 3.163822 | lr:2.4933e-04 | norm 0.3259 | dt 339.03ms | 1546415.72 tokens/sec
Step 11668 | loss: 3.148195 | lr:2.4929e-04 | norm 0.2840 | dt 338.39ms | 1549340.04 tokens/sec
Step 11669 | loss: 3.221525 | lr:2.4924e-04 | norm 0.3020 | dt 339.00ms | 1546550.58 tokens/sec
Step 11670 | loss: 3.099202 | lr:2.4920e-04 | norm 0.2977 | dt 338.34ms | 1549611.90 tokens/sec
Step 11671 | loss: 3.181830 | lr:2.4915e-04 | norm 0.3210 | dt 338.43ms | 1549161.04 tokens/sec
Step 11672 | loss: 3.125941 | lr:2.4911e-04 | norm 0.2981 | dt 338.87ms | 1547182.77 tokens/sec
Step 11673 | loss: 3.175535 | lr:2.4907e-04 | norm 0.3184 | dt 337.68ms | 1552606.43 tokens/sec
Step 11674 | loss: 3.098509 | lr:2.4902e-04 | norm 0.2818 | dt 338.77ms | 1547632.48 tokens/sec
Step 11675 | loss: 3.142000 | lr:2.4898e-04 | norm 0.2794 | dt 339.01ms | 1546510.34 tokens/sec
Step 11676 | loss: 3.135773 | lr:2.4893e-04 | norm 0.3102 | dt 338.46ms | 1549017.00 tokens/sec
Step 11677 | loss: 3.132545 | lr:2.4889e-04 | norm 0.2834 | dt 337.80ms | 1552069.48 tokens/sec
Step 11678 | loss: 3.092043 | lr:2.4884e-04 | norm 0.2888 | dt 338.60ms | 1548419.28 tokens/sec
Step 11679 | loss: 3.083260 | lr:2.4880e-04 | norm 0.2785 | dt 339.04ms | 1546386.36 tokens/sec
Step 11680 | loss: 3.175440 | lr:2.4876e-04 | norm 0.2781 | dt 338.06ms | 1550861.04 tokens/sec
Step 11681 | loss: 3.156714 | lr:2.4871e-04 | norm 0.2825 | dt 338.99ms | 1546595.17 tokens/sec
Step 11682 | loss: 3.163049 | lr:2.4867e-04 | norm 0.2818 | dt 338.46ms | 1549054.10 tokens/sec
Step 11683 | loss: 3.142492 | lr:2.4862e-04 | norm 0.2796 | dt 337.40ms | 1553914.22 tokens/sec
Step 11684 | loss: 3.102432 | lr:2.4858e-04 | norm 0.2743 | dt 340.76ms | 1538569.30 tokens/sec
Step 11685 | loss: 3.158282 | lr:2.4854e-04 | norm 0.2928 | dt 337.69ms | 1552587.80 tokens/sec
Step 11686 | loss: 3.080930 | lr:2.4849e-04 | norm 0.3440 | dt 337.43ms | 1553782.46 tokens/sec
Step 11687 | loss: 3.143273 | lr:2.4845e-04 | norm 0.3196 | dt 338.13ms | 1550559.23 tokens/sec
Step 11688 | loss: 3.165333 | lr:2.4840e-04 | norm 0.3008 | dt 340.63ms | 1539187.45 tokens/sec
Step 11689 | loss: 3.212069 | lr:2.4836e-04 | norm 0.3118 | dt 337.11ms | 1555246.20 tokens/sec
Step 11690 | loss: 3.128631 | lr:2.4832e-04 | norm 0.3022 | dt 338.57ms | 1548533.77 tokens/sec
Step 11691 | loss: 3.194040 | lr:2.4827e-04 | norm 0.3440 | dt 338.95ms | 1546820.37 tokens/sec
Step 11692 | loss: 3.145715 | lr:2.4823e-04 | norm 0.3231 | dt 344.28ms | 1522861.91 tokens/sec
Step 11693 | loss: 3.145615 | lr:2.4818e-04 | norm 0.3354 | dt 337.54ms | 1553244.69 tokens/sec
Step 11694 | loss: 3.174934 | lr:2.4814e-04 | norm 0.2899 | dt 337.92ms | 1551532.89 tokens/sec
Step 11695 | loss: 3.170963 | lr:2.4810e-04 | norm 0.3166 | dt 337.83ms | 1551936.94 tokens/sec
Step 11696 | loss: 3.191510 | lr:2.4805e-04 | norm 0.3064 | dt 338.21ms | 1550199.61 tokens/sec
Step 11697 | loss: 3.143086 | lr:2.4801e-04 | norm 0.2793 | dt 337.66ms | 1552712.77 tokens/sec
Step 11698 | loss: 3.143202 | lr:2.4796e-04 | norm 0.3079 | dt 338.60ms | 1548418.19 tokens/sec
Step 11699 | loss: 3.139809 | lr:2.4792e-04 | norm 0.2895 | dt 338.45ms | 1549091.20 tokens/sec
Step 11700 | loss: 3.201331 | lr:2.4788e-04 | norm 0.2962 | dt 338.58ms | 1548505.42 tokens/sec
Step 11701 | loss: 3.121902 | lr:2.4783e-04 | norm 0.2763 | dt 338.03ms | 1551026.22 tokens/sec
Step 11702 | loss: 3.159921 | lr:2.4779e-04 | norm 0.2974 | dt 337.99ms | 1551201.27 tokens/sec
Step 11703 | loss: 3.275249 | lr:2.4774e-04 | norm 0.3205 | dt 338.14ms | 1550503.47 tokens/sec
Step 11704 | loss: 3.178455 | lr:2.4770e-04 | norm 0.2756 | dt 337.66ms | 1552718.26 tokens/sec
Step 11705 | loss: 3.226392 | lr:2.4766e-04 | norm 0.2880 | dt 338.31ms | 1549741.86 tokens/sec
Step 11706 | loss: 3.178308 | lr:2.4761e-04 | norm 0.2730 | dt 337.67ms | 1552652.48 tokens/sec
Step 11707 | loss: 3.118969 | lr:2.4757e-04 | norm 0.2785 | dt 338.01ms | 1551119.21 tokens/sec
Step 11708 | loss: 3.232795 | lr:2.4752e-04 | norm 0.3022 | dt 338.36ms | 1549520.18 tokens/sec
Step 11709 | loss: 3.178623 | lr:2.4748e-04 | norm 0.2744 | dt 338.74ms | 1547755.57 tokens/sec
Step 11710 | loss: 3.123791 | lr:2.4744e-04 | norm 0.3004 | dt 337.53ms | 1553321.49 tokens/sec
Step 11711 | loss: 3.162748 | lr:2.4739e-04 | norm 0.2807 | dt 337.57ms | 1553115.24 tokens/sec
Step 11712 | loss: 3.107927 | lr:2.4735e-04 | norm 0.2889 | dt 337.91ms | 1551564.64 tokens/sec
Step 11713 | loss: 3.087993 | lr:2.4730e-04 | norm 0.2985 | dt 338.12ms | 1550581.10 tokens/sec
Step 11714 | loss: 3.138052 | lr:2.4726e-04 | norm 0.2763 | dt 337.46ms | 1553623.28 tokens/sec
Step 11715 | loss: 3.116966 | lr:2.4722e-04 | norm 0.2931 | dt 338.26ms | 1549969.06 tokens/sec
Step 11716 | loss: 3.158629 | lr:2.4717e-04 | norm 0.2900 | dt 337.42ms | 1553808.81 tokens/sec
Step 11717 | loss: 3.112251 | lr:2.4713e-04 | norm 0.2927 | dt 1021.28ms | 513363.72 tokens/sec
Step 11718 | loss: 3.116178 | lr:2.4708e-04 | norm 0.2735 | dt 336.97ms | 1555870.11 tokens/sec
Step 11719 | loss: 3.149112 | lr:2.4704e-04 | norm 0.2781 | dt 338.07ms | 1550809.64 tokens/sec
Step 11720 | loss: 3.080833 | lr:2.4700e-04 | norm 0.2948 | dt 339.92ms | 1542371.10 tokens/sec
Step 11721 | loss: 3.164483 | lr:2.4695e-04 | norm 0.3035 | dt 338.59ms | 1548440.00 tokens/sec
Step 11722 | loss: 3.201033 | lr:2.4691e-04 | norm 0.2901 | dt 337.88ms | 1551691.64 tokens/sec
Step 11723 | loss: 3.168192 | lr:2.4686e-04 | norm 0.2953 | dt 337.67ms | 1552649.19 tokens/sec
Step 11724 | loss: 3.132203 | lr:2.4682e-04 | norm 0.2937 | dt 338.07ms | 1550806.36 tokens/sec
Step 11725 | loss: 3.212392 | lr:2.4678e-04 | norm 0.2982 | dt 338.48ms | 1548946.08 tokens/sec
Step 11726 | loss: 3.203269 | lr:2.4673e-04 | norm 0.2906 | dt 337.42ms | 1553829.67 tokens/sec
Step 11727 | loss: 3.149041 | lr:2.4669e-04 | norm 0.2931 | dt 338.13ms | 1550563.60 tokens/sec
Step 11728 | loss: 3.144892 | lr:2.4664e-04 | norm 0.3043 | dt 337.92ms | 1551496.77 tokens/sec
Step 11729 | loss: 3.222406 | lr:2.4660e-04 | norm 0.3045 | dt 338.04ms | 1550971.52 tokens/sec
Step 11730 | loss: 3.169428 | lr:2.4656e-04 | norm 0.3055 | dt 337.63ms | 1552868.47 tokens/sec
Step 11731 | loss: 3.221281 | lr:2.4651e-04 | norm 0.3011 | dt 337.70ms | 1552540.67 tokens/sec
Step 11732 | loss: 3.125819 | lr:2.4647e-04 | norm 0.2719 | dt 337.96ms | 1551328.21 tokens/sec
Step 11733 | loss: 3.158721 | lr:2.4642e-04 | norm 0.3123 | dt 337.79ms | 1552104.53 tokens/sec
Step 11734 | loss: 3.168045 | lr:2.4638e-04 | norm 0.2765 | dt 338.66ms | 1548136.94 tokens/sec
Step 11735 | loss: 3.122181 | lr:2.4634e-04 | norm 0.2962 | dt 337.97ms | 1551272.40 tokens/sec
Step 11736 | loss: 3.210523 | lr:2.4629e-04 | norm 0.3075 | dt 337.23ms | 1554692.03 tokens/sec
Step 11737 | loss: 3.143567 | lr:2.4625e-04 | norm 0.2930 | dt 338.04ms | 1550963.86 tokens/sec
Step 11738 | loss: 3.185643 | lr:2.4621e-04 | norm 0.3201 | dt 340.07ms | 1541709.32 tokens/sec
Step 11739 | loss: 3.102827 | lr:2.4616e-04 | norm 0.2840 | dt 336.97ms | 1555911.95 tokens/sec
Step 11740 | loss: 3.158006 | lr:2.4612e-04 | norm 0.2873 | dt 338.19ms | 1550280.48 tokens/sec
Step 11741 | loss: 3.160239 | lr:2.4607e-04 | norm 0.3254 | dt 338.34ms | 1549605.35 tokens/sec
Step 11742 | loss: 3.158293 | lr:2.4603e-04 | norm 0.2828 | dt 338.79ms | 1547536.64 tokens/sec
Step 11743 | loss: 3.166198 | lr:2.4599e-04 | norm 0.3138 | dt 338.90ms | 1547042.36 tokens/sec
Step 11744 | loss: 3.112851 | lr:2.4594e-04 | norm 0.2679 | dt 339.13ms | 1545997.15 tokens/sec
Step 11745 | loss: 3.194648 | lr:2.4590e-04 | norm 0.2973 | dt 338.52ms | 1548771.53 tokens/sec
Step 11746 | loss: 3.156244 | lr:2.4585e-04 | norm 0.2666 | dt 339.08ms | 1546218.91 tokens/sec
Step 11747 | loss: 3.171181 | lr:2.4581e-04 | norm 0.2798 | dt 338.81ms | 1547421.21 tokens/sec
Step 11748 | loss: 3.156115 | lr:2.4577e-04 | norm 0.2889 | dt 338.19ms | 1550291.41 tokens/sec
Step 11749 | loss: 3.164887 | lr:2.4572e-04 | norm 0.2931 | dt 338.81ms | 1547431.01 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 11750: 3.1645
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2977/10042=0.2965


ddp_rank 3: ####### Printing generated samples ####### 



ddp_rank 5: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not saying, 'Well, you should try these, how and how. How do I do that if
rank 3 sample 1 >Hello, I'm a language model, so what's the difference between language model and language model?
The language model is a descriptive one, but its definition
rank 5 sample 0 >Hello, I'm a language model, but there is a big difference between how people think about it and how you think about it. For example, what I
rank 3 sample 2 >Hello, I'm a language model, so you have to follow the correct rules. We'll have to learn the syntax, but if you don't write them
rank 3 sample 3 >Hello, I'm a language model, so let's see what you did. I have two types of people. One is a speaker and your son, who


rank 5 sample 1 >Hello, I'm a language model, because so many things were learned during childhood. However, the process of learning was the same for people that weren't born
rank 5 sample 2 >Hello, I'm a language model, and you see how fun it is.
It's a pretty low-key version you can use for a lot of
rank 5 sample 3 >Hello, I'm a language model, who wants to write a programming language.
First we're going to write a machine that is pretty good at doing arithmetic




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, since my mother works in the computer industry, so I have been studying C. This is a very interesting question, since
rank 2 sample 1 >Hello, I'm a language model, but I got the idea from a project involving a model of how people talk. I'm going to make it a little


ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 2 >Hello, I'm a language model, and I love writing, and this helps me in my writing. It does my best to create your own words when you
rank 2 sample 3 >Hello, I'm a language model, but not if I know what I'm talking about right now?
I haven't had great teachers, I've spent


rank 7 sample 0 >Hello, I'm a language model, so I like to see people who are better at what they do. Some of those people are I don't know how
rank 7 sample 1 >Hello, I'm a language model, now you need to be a part of a language model what is the difference between using the native language and learning the target
rank 7 sample 2 >Hello, I'm a language model, so I've never lived in France, I've met English, and then I've been in France for the very long
rank 7 sample 3 >Hello, I'm a language model, and you should be. There's very little talk about it, but people are interested in it all.
When are




ddp_rank 6: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, just because I'm new, and like all of it I can figure it out. I do it for the first time
rank 6 sample 1 >Hello, I'm a language model, so I want to help. It's in my language, so I use it to make a few changes and adjust the
rank 1 sample 0 >Hello, I'm a language model, so let me try to write a project in Haskell that works perfectly with Haskell.
- Haskell for beginners.
-
rank 6 sample 2 >Hello, I'm a language model, but this is a great book, for my sake.
- Language:
- Linguistic Structures – the
rank 1 sample 1 >Hello, I'm a language model, a person who studies languages. I think languages are so amazing. For me, I always understand how languages work. I
rank 6 sample 3 >Hello, I'm a language model, so why don't you just make it available?
I'm not a huge language expert/model language person. Some


rank 1 sample 2 >Hello, I'm a language model, but they have different ways of doing it. I'm trying to understand what happens when you try to understand something new,
rank 1 sample 3 >Hello, I'm a language model, so I'm doing some homework. Here's a problem! As for it;
[w] = [i]




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I need to keep an eye on me for getting the right answers when I'm working on coding. That's a
rank 4 sample 1 >Hello, I'm a language model, let's say .NET. Let's say you can install a simple and powerful .NET Framework. We're very excited
rank 4 sample 2 >Hello, I'm a language model, I didn't feel I wanted to speak in a language. What if I want to do something in school? Well I
rank 4 sample 3 >Hello, I'm a language model, so it came together as a team under Dr. L'Edmond from UC Berkeley.
As you can see in




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I'd like you to have a post you can include here with grammar lessons. If you are a language model who
rank 0 sample 1 >Hello, I'm a language model, and when I'm writing a post I'm not fluent (although I should know what to do with these, etc.)
rank 0 sample 2 >Hello, I'm a language model, and I wanted to look forward to hearing from teachers. I can do some research on the language model, and it sounds
rank 0 sample 3 >Hello, I'm a language model, so don't feel bad about that.
2. It's pretty clear what a language model is.
Language as


Step 11750 | loss: 3.266821 | lr:2.4568e-04 | norm 0.2952 | dt 12332.75ms | 42511.84 tokens/sec
Step 11751 | loss: 3.188998 | lr:2.4563e-04 | norm 0.2629 | dt 335.08ms | 1564642.20 tokens/sec
Step 11752 | loss: 3.095882 | lr:2.4559e-04 | norm 0.3006 | dt 338.42ms | 1549206.88 tokens/sec
Step 11753 | loss: 3.131226 | lr:2.4555e-04 | norm 0.3044 | dt 338.81ms | 1547422.30 tokens/sec
Step 11754 | loss: 3.125686 | lr:2.4550e-04 | norm 0.2521 | dt 337.08ms | 1555369.40 tokens/sec
Step 11755 | loss: 3.121937 | lr:2.4546e-04 | norm 0.3024 | dt 337.20ms | 1554817.34 tokens/sec
Step 11756 | loss: 3.104920 | lr:2.4541e-04 | norm 0.2719 | dt 337.19ms | 1554895.40 tokens/sec
Step 11757 | loss: 3.108583 | lr:2.4537e-04 | norm 0.2841 | dt 337.66ms | 1552712.77 tokens/sec
Step 11758 | loss: 3.154673 | lr:2.4533e-04 | norm 0.3082 | dt 336.47ms | 1558189.70 tokens/sec
Step 11759 | loss: 3.143344 | lr:2.4528e-04 | norm 0.2910 | dt 338.91ms | 1546977.06 tokens/sec
Step 11760 | loss: 3.131595 | lr:2.4524e-04 | norm 0.3189 | dt 336.34ms | 1558799.40 tokens/sec
Step 11761 | loss: 3.210392 | lr:2.4520e-04 | norm 0.3319 | dt 337.17ms | 1554983.36 tokens/sec
Step 11762 | loss: 3.169212 | lr:2.4515e-04 | norm 0.3230 | dt 336.62ms | 1557493.32 tokens/sec
Step 11763 | loss: 3.103791 | lr:2.4511e-04 | norm 0.3093 | dt 337.21ms | 1554790.96 tokens/sec
Step 11764 | loss: 3.176628 | lr:2.4506e-04 | norm 0.3066 | dt 337.25ms | 1554610.69 tokens/sec
Step 11765 | loss: 3.184882 | lr:2.4502e-04 | norm 0.2912 | dt 337.43ms | 1553790.14 tokens/sec
Step 11766 | loss: 3.141410 | lr:2.4498e-04 | norm 0.2848 | dt 338.45ms | 1549087.93 tokens/sec
Step 11767 | loss: 3.117048 | lr:2.4493e-04 | norm 0.2896 | dt 337.97ms | 1551268.02 tokens/sec
Step 11768 | loss: 3.156626 | lr:2.4489e-04 | norm 0.3206 | dt 338.61ms | 1548338.60 tokens/sec
Step 11769 | loss: 3.249308 | lr:2.4484e-04 | norm 0.2982 | dt 337.72ms | 1552439.83 tokens/sec
Step 11770 | loss: 3.062174 | lr:2.4480e-04 | norm 0.3002 | dt 338.26ms | 1549963.60 tokens/sec
Step 11771 | loss: 3.156728 | lr:2.4476e-04 | norm 0.2893 | dt 336.90ms | 1556214.75 tokens/sec
Step 11772 | loss: 3.138488 | lr:2.4471e-04 | norm 0.2730 | dt 337.27ms | 1554492.01 tokens/sec
Step 11773 | loss: 3.138405 | lr:2.4467e-04 | norm 0.2680 | dt 336.46ms | 1558249.32 tokens/sec
Step 11774 | loss: 3.243616 | lr:2.4463e-04 | norm 0.2725 | dt 337.02ms | 1555643.38 tokens/sec
Step 11775 | loss: 3.132141 | lr:2.4458e-04 | norm 0.2850 | dt 337.65ms | 1552762.11 tokens/sec
Step 11776 | loss: 3.150455 | lr:2.4454e-04 | norm 0.2686 | dt 337.13ms | 1555145.01 tokens/sec
Step 11777 | loss: 3.180137 | lr:2.4449e-04 | norm 0.2862 | dt 337.26ms | 1554535.96 tokens/sec
Step 11778 | loss: 3.111515 | lr:2.4445e-04 | norm 0.2944 | dt 337.98ms | 1551247.23 tokens/sec
Step 11779 | loss: 3.159678 | lr:2.4441e-04 | norm 0.2747 | dt 1024.50ms | 511750.65 tokens/sec
Step 11780 | loss: 3.113640 | lr:2.4436e-04 | norm 0.2751 | dt 336.13ms | 1559786.76 tokens/sec
Step 11781 | loss: 3.129718 | lr:2.4432e-04 | norm 0.2706 | dt 337.79ms | 1552092.48 tokens/sec
Step 11782 | loss: 3.136433 | lr:2.4427e-04 | norm 0.2783 | dt 337.47ms | 1553594.75 tokens/sec
Step 11783 | loss: 3.133138 | lr:2.4423e-04 | norm 0.2801 | dt 336.82ms | 1556574.96 tokens/sec
Step 11784 | loss: 3.132977 | lr:2.4419e-04 | norm 0.2617 | dt 338.26ms | 1549959.23 tokens/sec
Step 11785 | loss: 3.116950 | lr:2.4414e-04 | norm 0.2934 | dt 337.62ms | 1552878.34 tokens/sec
Step 11786 | loss: 3.129053 | lr:2.4410e-04 | norm 0.2620 | dt 338.20ms | 1550249.88 tokens/sec
Step 11787 | loss: 3.174488 | lr:2.4406e-04 | norm 0.2867 | dt 338.71ms | 1547898.29 tokens/sec
Step 11788 | loss: 3.148170 | lr:2.4401e-04 | norm 0.2746 | dt 338.38ms | 1549427.38 tokens/sec
Step 11789 | loss: 3.119795 | lr:2.4397e-04 | norm 0.2778 | dt 338.89ms | 1547053.25 tokens/sec
Step 11790 | loss: 3.084752 | lr:2.4392e-04 | norm 0.3055 | dt 339.41ms | 1544695.06 tokens/sec
Step 11791 | loss: 3.108417 | lr:2.4388e-04 | norm 0.2743 | dt 338.07ms | 1550839.17 tokens/sec
Step 11792 | loss: 3.182109 | lr:2.4384e-04 | norm 0.3036 | dt 338.33ms | 1549640.29 tokens/sec
Step 11793 | loss: 3.181258 | lr:2.4379e-04 | norm 0.3347 | dt 339.14ms | 1545950.42 tokens/sec
Step 11794 | loss: 3.120265 | lr:2.4375e-04 | norm 0.2828 | dt 338.30ms | 1549791.01 tokens/sec
Step 11795 | loss: 3.129680 | lr:2.4371e-04 | norm 0.2918 | dt 337.76ms | 1552240.39 tokens/sec
Step 11796 | loss: 3.126252 | lr:2.4366e-04 | norm 0.2803 | dt 338.16ms | 1550422.58 tokens/sec
Step 11797 | loss: 3.199098 | lr:2.4362e-04 | norm 0.2810 | dt 338.05ms | 1550927.76 tokens/sec
Step 11798 | loss: 3.165341 | lr:2.4357e-04 | norm 0.3065 | dt 337.60ms | 1552995.69 tokens/sec
Step 11799 | loss: 3.179443 | lr:2.4353e-04 | norm 0.2836 | dt 337.53ms | 1553295.16 tokens/sec
Step 11800 | loss: 3.136252 | lr:2.4349e-04 | norm 0.3222 | dt 337.68ms | 1552597.67 tokens/sec
Step 11801 | loss: 3.125116 | lr:2.4344e-04 | norm 0.2809 | dt 338.29ms | 1549818.31 tokens/sec
Step 11802 | loss: 3.147932 | lr:2.4340e-04 | norm 0.3058 | dt 338.09ms | 1550737.46 tokens/sec
Step 11803 | loss: 3.165379 | lr:2.4336e-04 | norm 0.3045 | dt 337.99ms | 1551181.58 tokens/sec
Step 11804 | loss: 3.144037 | lr:2.4331e-04 | norm 0.2994 | dt 338.61ms | 1548350.59 tokens/sec
Step 11805 | loss: 3.176249 | lr:2.4327e-04 | norm 0.2874 | dt 339.16ms | 1545859.13 tokens/sec
Step 11806 | loss: 3.217363 | lr:2.4322e-04 | norm 0.2982 | dt 338.33ms | 1549640.29 tokens/sec
Step 11807 | loss: 3.192796 | lr:2.4318e-04 | norm 0.2854 | dt 337.88ms | 1551680.69 tokens/sec
Step 11808 | loss: 3.197781 | lr:2.4314e-04 | norm 0.3128 | dt 338.86ms | 1547230.67 tokens/sec
Step 11809 | loss: 3.164688 | lr:2.4309e-04 | norm 0.2872 | dt 338.86ms | 1547190.39 tokens/sec
Step 11810 | loss: 3.121729 | lr:2.4305e-04 | norm 0.2950 | dt 339.14ms | 1545949.33 tokens/sec
Step 11811 | loss: 3.183739 | lr:2.4301e-04 | norm 0.2801 | dt 337.91ms | 1551554.79 tokens/sec
Step 11812 | loss: 3.149983 | lr:2.4296e-04 | norm 0.2935 | dt 338.71ms | 1547896.11 tokens/sec
Step 11813 | loss: 3.161961 | lr:2.4292e-04 | norm 0.2939 | dt 338.18ms | 1550320.92 tokens/sec
Step 11814 | loss: 3.149041 | lr:2.4287e-04 | norm 0.3179 | dt 338.10ms | 1550682.78 tokens/sec
Step 11815 | loss: 3.290990 | lr:2.4283e-04 | norm 0.2959 | dt 338.31ms | 1549720.01 tokens/sec
Step 11816 | loss: 3.163137 | lr:2.4279e-04 | norm 0.3398 | dt 337.83ms | 1551919.42 tokens/sec
Step 11817 | loss: 3.082644 | lr:2.4274e-04 | norm 0.2726 | dt 338.12ms | 1550578.91 tokens/sec
Step 11818 | loss: 3.115589 | lr:2.4270e-04 | norm 0.3114 | dt 338.07ms | 1550845.73 tokens/sec
Step 11819 | loss: 3.155012 | lr:2.4266e-04 | norm 0.2784 | dt 338.69ms | 1547983.28 tokens/sec
Step 11820 | loss: 3.163598 | lr:2.4261e-04 | norm 0.3368 | dt 338.37ms | 1549464.50 tokens/sec
Step 11821 | loss: 3.151237 | lr:2.4257e-04 | norm 0.2708 | dt 339.88ms | 1542587.49 tokens/sec
Step 11822 | loss: 3.086812 | lr:2.4252e-04 | norm 0.3102 | dt 338.49ms | 1548920.98 tokens/sec
Step 11823 | loss: 3.094290 | lr:2.4248e-04 | norm 0.2817 | dt 338.19ms | 1550291.41 tokens/sec
Step 11824 | loss: 3.108078 | lr:2.4244e-04 | norm 0.2890 | dt 338.41ms | 1549284.37 tokens/sec
Step 11825 | loss: 3.164902 | lr:2.4239e-04 | norm 0.2806 | dt 337.95ms | 1551398.26 tokens/sec
Step 11826 | loss: 3.181845 | lr:2.4235e-04 | norm 0.3145 | dt 337.89ms | 1551665.36 tokens/sec
Step 11827 | loss: 3.132082 | lr:2.4231e-04 | norm 0.3139 | dt 338.57ms | 1548531.59 tokens/sec
Step 11828 | loss: 3.123316 | lr:2.4226e-04 | norm 0.2840 | dt 338.46ms | 1549060.64 tokens/sec
Step 11829 | loss: 3.145105 | lr:2.4222e-04 | norm 0.2930 | dt 336.74ms | 1556949.67 tokens/sec
Step 11830 | loss: 3.151451 | lr:2.4217e-04 | norm 0.2826 | dt 337.83ms | 1551917.23 tokens/sec
Step 11831 | loss: 3.139491 | lr:2.4213e-04 | norm 0.2911 | dt 338.37ms | 1549431.74 tokens/sec
Step 11832 | loss: 3.160662 | lr:2.4209e-04 | norm 0.3076 | dt 338.85ms | 1547277.49 tokens/sec
Step 11833 | loss: 3.168422 | lr:2.4204e-04 | norm 0.2840 | dt 338.75ms | 1547713.09 tokens/sec
Step 11834 | loss: 3.155363 | lr:2.4200e-04 | norm 0.3170 | dt 339.12ms | 1546037.37 tokens/sec
Step 11835 | loss: 3.113662 | lr:2.4196e-04 | norm 0.2921 | dt 337.74ms | 1552328.05 tokens/sec
Step 11836 | loss: 3.136608 | lr:2.4191e-04 | norm 0.2949 | dt 338.24ms | 1550042.26 tokens/sec
Step 11837 | loss: 3.126136 | lr:2.4187e-04 | norm 0.2870 | dt 337.60ms | 1552982.53 tokens/sec
Step 11838 | loss: 3.103129 | lr:2.4183e-04 | norm 0.2654 | dt 337.41ms | 1553882.37 tokens/sec
Step 11839 | loss: 3.148086 | lr:2.4178e-04 | norm 0.2755 | dt 337.97ms | 1551295.38 tokens/sec
Step 11840 | loss: 3.167705 | lr:2.4174e-04 | norm 0.2749 | dt 338.35ms | 1549538.74 tokens/sec
Step 11841 | loss: 3.151885 | lr:2.4169e-04 | norm 0.2521 | dt 336.94ms | 1556024.24 tokens/sec
Step 11842 | loss: 3.134509 | lr:2.4165e-04 | norm 0.2824 | dt 337.93ms | 1551484.73 tokens/sec
Step 11843 | loss: 3.195374 | lr:2.4161e-04 | norm 0.2654 | dt 337.89ms | 1551653.32 tokens/sec
Step 11844 | loss: 3.154143 | lr:2.4156e-04 | norm 0.2794 | dt 338.70ms | 1547921.17 tokens/sec
Step 11845 | loss: 3.132650 | lr:2.4152e-04 | norm 0.2880 | dt 338.69ms | 1547994.18 tokens/sec
Step 11846 | loss: 3.229627 | lr:2.4148e-04 | norm 0.2987 | dt 339.55ms | 1544089.83 tokens/sec
Step 11847 | loss: 3.191940 | lr:2.4143e-04 | norm 0.2595 | dt 338.43ms | 1549185.05 tokens/sec
Step 11848 | loss: 3.136423 | lr:2.4139e-04 | norm 0.3232 | dt 338.09ms | 1550745.12 tokens/sec
Step 11849 | loss: 3.093764 | lr:2.4135e-04 | norm 0.2755 | dt 338.62ms | 1548286.28 tokens/sec
Step 11850 | loss: 3.168629 | lr:2.4130e-04 | norm 0.3044 | dt 338.18ms | 1550319.83 tokens/sec
Step 11851 | loss: 3.200730 | lr:2.4126e-04 | norm 0.3265 | dt 338.16ms | 1550398.53 tokens/sec
Step 11852 | loss: 3.104395 | lr:2.4121e-04 | norm 0.2750 | dt 337.82ms | 1551989.51 tokens/sec
Step 11853 | loss: 3.130751 | lr:2.4117e-04 | norm 0.3064 | dt 338.44ms | 1549130.49 tokens/sec
Step 11854 | loss: 3.150953 | lr:2.4113e-04 | norm 0.2959 | dt 339.03ms | 1546442.91 tokens/sec
Step 11855 | loss: 3.126078 | lr:2.4108e-04 | norm 0.2660 | dt 337.49ms | 1553502.55 tokens/sec
Step 11856 | loss: 3.051537 | lr:2.4104e-04 | norm 0.2740 | dt 338.29ms | 1549806.30 tokens/sec
Step 11857 | loss: 3.113897 | lr:2.4100e-04 | norm 0.2737 | dt 338.30ms | 1549769.16 tokens/sec
Step 11858 | loss: 3.152016 | lr:2.4095e-04 | norm 0.3044 | dt 337.72ms | 1552448.60 tokens/sec
Step 11859 | loss: 3.093256 | lr:2.4091e-04 | norm 0.3044 | dt 339.54ms | 1544118.02 tokens/sec
Step 11860 | loss: 3.144636 | lr:2.4087e-04 | norm 0.2907 | dt 337.93ms | 1551473.78 tokens/sec
Step 11861 | loss: 3.135343 | lr:2.4082e-04 | norm 0.2764 | dt 338.65ms | 1548150.02 tokens/sec
Step 11862 | loss: 3.106291 | lr:2.4078e-04 | norm 0.3047 | dt 339.11ms | 1546051.50 tokens/sec
Step 11863 | loss: 3.156559 | lr:2.4073e-04 | norm 0.2645 | dt 338.15ms | 1550442.25 tokens/sec
Step 11864 | loss: 3.148903 | lr:2.4069e-04 | norm 0.3117 | dt 338.20ms | 1550230.21 tokens/sec
Step 11865 | loss: 3.160637 | lr:2.4065e-04 | norm 0.2752 | dt 338.05ms | 1550912.45 tokens/sec
Step 11866 | loss: 3.156807 | lr:2.4060e-04 | norm 0.3055 | dt 338.85ms | 1547250.27 tokens/sec
Step 11867 | loss: 3.088992 | lr:2.4056e-04 | norm 0.2775 | dt 338.13ms | 1550562.51 tokens/sec
Step 11868 | loss: 3.201084 | lr:2.4052e-04 | norm 0.3018 | dt 338.27ms | 1549888.22 tokens/sec
Step 11869 | loss: 3.149683 | lr:2.4047e-04 | norm 0.3321 | dt 338.15ms | 1550465.21 tokens/sec
Step 11870 | loss: 3.148791 | lr:2.4043e-04 | norm 0.2721 | dt 338.37ms | 1549466.68 tokens/sec
Step 11871 | loss: 3.220106 | lr:2.4039e-04 | norm 0.3231 | dt 339.10ms | 1546106.94 tokens/sec
Step 11872 | loss: 3.162581 | lr:2.4034e-04 | norm 0.2975 | dt 337.92ms | 1551514.28 tokens/sec
Step 11873 | loss: 3.145271 | lr:2.4030e-04 | norm 0.2928 | dt 338.69ms | 1547981.10 tokens/sec
Step 11874 | loss: 3.147132 | lr:2.4025e-04 | norm 0.3192 | dt 338.16ms | 1550431.32 tokens/sec
Step 11875 | loss: 3.188174 | lr:2.4021e-04 | norm 0.2804 | dt 338.23ms | 1550089.24 tokens/sec
Step 11876 | loss: 3.166716 | lr:2.4017e-04 | norm 0.2991 | dt 337.73ms | 1552374.07 tokens/sec
Step 11877 | loss: 3.229298 | lr:2.4012e-04 | norm 0.2798 | dt 338.33ms | 1549646.84 tokens/sec
Step 11878 | loss: 3.159357 | lr:2.4008e-04 | norm 0.2800 | dt 338.38ms | 1549409.91 tokens/sec
Step 11879 | loss: 3.143661 | lr:2.4004e-04 | norm 0.3019 | dt 337.83ms | 1551927.08 tokens/sec
Step 11880 | loss: 3.142777 | lr:2.3999e-04 | norm 0.2702 | dt 338.02ms | 1551056.85 tokens/sec
Step 11881 | loss: 3.121069 | lr:2.3995e-04 | norm 0.3056 | dt 339.69ms | 1543452.58 tokens/sec
Step 11882 | loss: 3.171645 | lr:2.3991e-04 | norm 0.2704 | dt 339.34ms | 1545028.25 tokens/sec
Step 11883 | loss: 3.125469 | lr:2.3986e-04 | norm 0.3063 | dt 339.49ms | 1544360.93 tokens/sec
Step 11884 | loss: 3.084733 | lr:2.3982e-04 | norm 0.2796 | dt 337.83ms | 1551907.37 tokens/sec
Step 11885 | loss: 3.125337 | lr:2.3978e-04 | norm 0.3326 | dt 338.18ms | 1550309.99 tokens/sec
Step 11886 | loss: 3.086571 | lr:2.3973e-04 | norm 0.3032 | dt 338.09ms | 1550731.99 tokens/sec
Step 11887 | loss: 3.081326 | lr:2.3969e-04 | norm 0.2869 | dt 338.43ms | 1549166.50 tokens/sec
Step 11888 | loss: 3.175645 | lr:2.3964e-04 | norm 0.3151 | dt 337.67ms | 1552686.46 tokens/sec
Step 11889 | loss: 3.091834 | lr:2.3960e-04 | norm 0.2819 | dt 337.70ms | 1552529.70 tokens/sec
Step 11890 | loss: 3.155533 | lr:2.3956e-04 | norm 0.3084 | dt 337.02ms | 1555677.49 tokens/sec
Step 11891 | loss: 3.120845 | lr:2.3951e-04 | norm 0.2871 | dt 338.19ms | 1550297.97 tokens/sec
Step 11892 | loss: 3.134526 | lr:2.3947e-04 | norm 0.2671 | dt 338.07ms | 1550834.79 tokens/sec
Step 11893 | loss: 3.106079 | lr:2.3943e-04 | norm 0.2754 | dt 337.69ms | 1552566.97 tokens/sec
Step 11894 | loss: 3.112702 | lr:2.3938e-04 | norm 0.2728 | dt 337.90ms | 1551600.77 tokens/sec
Step 11895 | loss: 3.167920 | lr:2.3934e-04 | norm 0.3071 | dt 339.17ms | 1545811.31 tokens/sec
Step 11896 | loss: 3.148913 | lr:2.3930e-04 | norm 0.2638 | dt 337.69ms | 1552563.68 tokens/sec
Step 11897 | loss: 3.101553 | lr:2.3925e-04 | norm 0.3014 | dt 338.16ms | 1550425.86 tokens/sec
Step 11898 | loss: 3.177310 | lr:2.3921e-04 | norm 0.2910 | dt 338.57ms | 1548551.22 tokens/sec
Step 11899 | loss: 3.145857 | lr:2.3917e-04 | norm 0.2936 | dt 338.36ms | 1549513.63 tokens/sec
Step 11900 | loss: 3.152730 | lr:2.3912e-04 | norm 0.2839 | dt 337.29ms | 1554396.41 tokens/sec
Step 11901 | loss: 3.106741 | lr:2.3908e-04 | norm 0.2994 | dt 337.99ms | 1551206.74 tokens/sec
Step 11902 | loss: 3.192948 | lr:2.3904e-04 | norm 0.2973 | dt 338.38ms | 1549420.83 tokens/sec
Step 11903 | loss: 3.152071 | lr:2.3899e-04 | norm 0.3096 | dt 337.83ms | 1551927.08 tokens/sec
Step 11904 | loss: 3.164224 | lr:2.3895e-04 | norm 0.2943 | dt 337.74ms | 1552351.06 tokens/sec
Step 11905 | loss: 3.169042 | lr:2.3891e-04 | norm 0.2970 | dt 338.12ms | 1550592.03 tokens/sec
Step 11906 | loss: 3.121760 | lr:2.3886e-04 | norm 0.2893 | dt 1022.85ms | 512577.30 tokens/sec
Step 11907 | loss: 3.201553 | lr:2.3882e-04 | norm 0.2999 | dt 336.27ms | 1559107.75 tokens/sec
Step 11908 | loss: 3.130105 | lr:2.3877e-04 | norm 0.2966 | dt 337.57ms | 1553115.24 tokens/sec
Step 11909 | loss: 3.181102 | lr:2.3873e-04 | norm 0.2747 | dt 338.20ms | 1550210.54 tokens/sec
Step 11910 | loss: 3.113061 | lr:2.3869e-04 | norm 0.2900 | dt 339.03ms | 1546449.43 tokens/sec
Step 11911 | loss: 3.194174 | lr:2.3864e-04 | norm 0.2952 | dt 337.99ms | 1551213.31 tokens/sec
Step 11912 | loss: 3.126351 | lr:2.3860e-04 | norm 0.2979 | dt 336.87ms | 1556355.73 tokens/sec
Step 11913 | loss: 3.137354 | lr:2.3856e-04 | norm 0.2923 | dt 338.33ms | 1549638.11 tokens/sec
Step 11914 | loss: 3.157959 | lr:2.3851e-04 | norm 0.3021 | dt 338.06ms | 1550890.58 tokens/sec
Step 11915 | loss: 3.100910 | lr:2.3847e-04 | norm 0.2949 | dt 338.08ms | 1550775.74 tokens/sec
Step 11916 | loss: 3.042181 | lr:2.3843e-04 | norm 0.3270 | dt 337.56ms | 1553149.25 tokens/sec
Step 11917 | loss: 3.087715 | lr:2.3838e-04 | norm 0.3147 | dt 338.95ms | 1546822.54 tokens/sec
Step 11918 | loss: 3.110813 | lr:2.3834e-04 | norm 0.2902 | dt 338.20ms | 1550246.60 tokens/sec
Step 11919 | loss: 3.135163 | lr:2.3830e-04 | norm 0.2834 | dt 338.18ms | 1550303.43 tokens/sec
Step 11920 | loss: 3.171880 | lr:2.3825e-04 | norm 0.3115 | dt 342.25ms | 1531872.76 tokens/sec
Step 11921 | loss: 3.121195 | lr:2.3821e-04 | norm 0.2583 | dt 337.90ms | 1551590.91 tokens/sec
Step 11922 | loss: 3.121737 | lr:2.3817e-04 | norm 0.3325 | dt 338.21ms | 1550188.68 tokens/sec
Step 11923 | loss: 3.089874 | lr:2.3812e-04 | norm 0.2681 | dt 339.35ms | 1544958.78 tokens/sec
Step 11924 | loss: 3.159615 | lr:2.3808e-04 | norm 0.3057 | dt 338.02ms | 1551078.73 tokens/sec
Step 11925 | loss: 3.096936 | lr:2.3804e-04 | norm 0.2695 | dt 338.20ms | 1550234.58 tokens/sec
Step 11926 | loss: 3.085304 | lr:2.3799e-04 | norm 0.2852 | dt 338.42ms | 1549240.71 tokens/sec
Step 11927 | loss: 3.087198 | lr:2.3795e-04 | norm 0.2813 | dt 338.17ms | 1550370.11 tokens/sec
Step 11928 | loss: 3.142378 | lr:2.3791e-04 | norm 0.2843 | dt 337.75ms | 1552305.04 tokens/sec
Step 11929 | loss: 3.151886 | lr:2.3786e-04 | norm 0.2746 | dt 339.30ms | 1545191.10 tokens/sec
Step 11930 | loss: 3.169937 | lr:2.3782e-04 | norm 0.3177 | dt 337.62ms | 1552907.95 tokens/sec
Step 11931 | loss: 3.130772 | lr:2.3778e-04 | norm 0.2922 | dt 338.01ms | 1551110.46 tokens/sec
Step 11932 | loss: 3.180560 | lr:2.3773e-04 | norm 0.3362 | dt 340.19ms | 1541166.91 tokens/sec
Step 11933 | loss: 3.150644 | lr:2.3769e-04 | norm 0.2828 | dt 338.17ms | 1550353.71 tokens/sec
Step 11934 | loss: 3.156395 | lr:2.3764e-04 | norm 0.3142 | dt 339.18ms | 1545759.16 tokens/sec
Step 11935 | loss: 3.104964 | lr:2.3760e-04 | norm 0.2777 | dt 338.58ms | 1548490.15 tokens/sec
Step 11936 | loss: 3.158676 | lr:2.3756e-04 | norm 0.3111 | dt 337.92ms | 1551498.96 tokens/sec
Step 11937 | loss: 3.143116 | lr:2.3751e-04 | norm 0.2645 | dt 339.14ms | 1545942.81 tokens/sec
Step 11938 | loss: 3.184317 | lr:2.3747e-04 | norm 0.3002 | dt 338.92ms | 1546943.33 tokens/sec
Step 11939 | loss: 3.258534 | lr:2.3743e-04 | norm 0.2892 | dt 338.63ms | 1548244.85 tokens/sec
Step 11940 | loss: 3.202878 | lr:2.3738e-04 | norm 0.2891 | dt 338.79ms | 1547508.32 tokens/sec
Step 11941 | loss: 3.155316 | lr:2.3734e-04 | norm 0.2978 | dt 338.82ms | 1547413.58 tokens/sec
Step 11942 | loss: 3.206021 | lr:2.3730e-04 | norm 0.3305 | dt 338.26ms | 1549936.29 tokens/sec
Step 11943 | loss: 3.152619 | lr:2.3725e-04 | norm 0.3091 | dt 339.04ms | 1546404.84 tokens/sec
Step 11944 | loss: 3.111356 | lr:2.3721e-04 | norm 0.2716 | dt 337.96ms | 1551326.02 tokens/sec
Step 11945 | loss: 3.126650 | lr:2.3717e-04 | norm 0.2959 | dt 339.35ms | 1544966.38 tokens/sec
Step 11946 | loss: 3.123594 | lr:2.3712e-04 | norm 0.2805 | dt 338.80ms | 1547499.61 tokens/sec
Step 11947 | loss: 3.126811 | lr:2.3708e-04 | norm 0.3303 | dt 338.73ms | 1547812.22 tokens/sec
Step 11948 | loss: 3.154350 | lr:2.3704e-04 | norm 0.3195 | dt 338.14ms | 1550505.66 tokens/sec
Step 11949 | loss: 3.115075 | lr:2.3699e-04 | norm 0.2835 | dt 339.28ms | 1545298.59 tokens/sec
Step 11950 | loss: 3.119969 | lr:2.3695e-04 | norm 0.3125 | dt 339.00ms | 1546559.28 tokens/sec
Step 11951 | loss: 3.173491 | lr:2.3691e-04 | norm 0.2984 | dt 338.55ms | 1548633.01 tokens/sec
Step 11952 | loss: 3.104246 | lr:2.3686e-04 | norm 0.2983 | dt 338.30ms | 1549749.50 tokens/sec
Step 11953 | loss: 3.123747 | lr:2.3682e-04 | norm 0.2985 | dt 338.76ms | 1547681.50 tokens/sec
Step 11954 | loss: 3.124479 | lr:2.3678e-04 | norm 0.3169 | dt 338.68ms | 1548043.22 tokens/sec
Step 11955 | loss: 3.112090 | lr:2.3673e-04 | norm 0.3103 | dt 338.99ms | 1546631.07 tokens/sec
Step 11956 | loss: 3.104818 | lr:2.3669e-04 | norm 0.3102 | dt 338.48ms | 1548966.81 tokens/sec
Step 11957 | loss: 3.109672 | lr:2.3665e-04 | norm 0.3119 | dt 338.84ms | 1547300.35 tokens/sec
Step 11958 | loss: 3.080166 | lr:2.3660e-04 | norm 0.2798 | dt 339.11ms | 1546077.59 tokens/sec
Step 11959 | loss: 3.131179 | lr:2.3656e-04 | norm 0.3128 | dt 339.00ms | 1546563.63 tokens/sec
Step 11960 | loss: 3.091260 | lr:2.3652e-04 | norm 0.2928 | dt 338.84ms | 1547305.79 tokens/sec
Step 11961 | loss: 3.110101 | lr:2.3647e-04 | norm 0.2709 | dt 338.31ms | 1549739.67 tokens/sec
Step 11962 | loss: 3.133622 | lr:2.3643e-04 | norm 0.2724 | dt 338.77ms | 1547617.23 tokens/sec
Step 11963 | loss: 3.139411 | lr:2.3639e-04 | norm 0.2751 | dt 339.10ms | 1546097.15 tokens/sec
Step 11964 | loss: 3.122860 | lr:2.3634e-04 | norm 0.3066 | dt 338.21ms | 1550207.26 tokens/sec
Step 11965 | loss: 3.148870 | lr:2.3630e-04 | norm 0.2765 | dt 338.85ms | 1547238.29 tokens/sec
Step 11966 | loss: 3.138338 | lr:2.3626e-04 | norm 0.3120 | dt 339.17ms | 1545805.88 tokens/sec
Step 11967 | loss: 3.139334 | lr:2.3621e-04 | norm 0.2921 | dt 338.60ms | 1548409.47 tokens/sec
Step 11968 | loss: 3.216081 | lr:2.3617e-04 | norm 0.2809 | dt 338.33ms | 1549631.56 tokens/sec
Step 11969 | loss: 3.166616 | lr:2.3613e-04 | norm 0.2900 | dt 1000.88ms | 523827.40 tokens/sec
Step 11970 | loss: 3.151585 | lr:2.3608e-04 | norm 0.2829 | dt 336.39ms | 1558595.01 tokens/sec
Step 11971 | loss: 3.119230 | lr:2.3604e-04 | norm 0.3001 | dt 337.50ms | 1553453.17 tokens/sec
Step 11972 | loss: 3.165131 | lr:2.3600e-04 | norm 0.2737 | dt 338.55ms | 1548648.28 tokens/sec
Step 11973 | loss: 3.161228 | lr:2.3595e-04 | norm 0.2957 | dt 337.77ms | 1552202.04 tokens/sec
Step 11974 | loss: 3.067324 | lr:2.3591e-04 | norm 0.3276 | dt 338.12ms | 1550614.99 tokens/sec
Step 11975 | loss: 3.126266 | lr:2.3587e-04 | norm 0.3437 | dt 338.40ms | 1549336.77 tokens/sec
Step 11976 | loss: 3.182746 | lr:2.3582e-04 | norm 0.3509 | dt 337.28ms | 1554450.25 tokens/sec
Step 11977 | loss: 3.142070 | lr:2.3578e-04 | norm 0.3626 | dt 338.63ms | 1548261.20 tokens/sec
Step 11978 | loss: 3.156631 | lr:2.3574e-04 | norm 0.3073 | dt 337.55ms | 1553222.75 tokens/sec
Step 11979 | loss: 3.111413 | lr:2.3569e-04 | norm 0.3454 | dt 337.69ms | 1552580.13 tokens/sec
Step 11980 | loss: 3.135185 | lr:2.3565e-04 | norm 0.3252 | dt 338.07ms | 1550812.92 tokens/sec
Step 11981 | loss: 3.059127 | lr:2.3561e-04 | norm 0.2918 | dt 338.95ms | 1546806.22 tokens/sec
Step 11982 | loss: 3.100034 | lr:2.3556e-04 | norm 0.2916 | dt 339.40ms | 1544738.47 tokens/sec
Step 11983 | loss: 3.164901 | lr:2.3552e-04 | norm 0.2821 | dt 338.83ms | 1547368.94 tokens/sec
Step 11984 | loss: 3.105806 | lr:2.3548e-04 | norm 0.2812 | dt 338.82ms | 1547403.78 tokens/sec
Step 11985 | loss: 3.145563 | lr:2.3543e-04 | norm 0.2812 | dt 338.95ms | 1546780.11 tokens/sec
Step 11986 | loss: 3.149073 | lr:2.3539e-04 | norm 0.2663 | dt 338.38ms | 1549413.18 tokens/sec
Step 11987 | loss: 3.149269 | lr:2.3535e-04 | norm 0.2766 | dt 339.03ms | 1546418.98 tokens/sec
Step 11988 | loss: 3.115196 | lr:2.3530e-04 | norm 0.2858 | dt 338.82ms | 1547404.87 tokens/sec
Step 11989 | loss: 3.124341 | lr:2.3526e-04 | norm 0.2872 | dt 341.15ms | 1536820.94 tokens/sec
Step 11990 | loss: 3.134665 | lr:2.3522e-04 | norm 0.3035 | dt 339.11ms | 1546091.72 tokens/sec
Step 11991 | loss: 3.124732 | lr:2.3517e-04 | norm 0.2636 | dt 339.17ms | 1545781.98 tokens/sec
Step 11992 | loss: 3.102781 | lr:2.3513e-04 | norm 0.3170 | dt 338.15ms | 1550470.68 tokens/sec
Step 11993 | loss: 3.132671 | lr:2.3509e-04 | norm 0.2641 | dt 337.69ms | 1552582.32 tokens/sec
Step 11994 | loss: 3.048997 | lr:2.3504e-04 | norm 0.2752 | dt 337.83ms | 1551940.23 tokens/sec
Step 11995 | loss: 3.067082 | lr:2.3500e-04 | norm 0.2859 | dt 337.77ms | 1552205.33 tokens/sec
Step 11996 | loss: 3.136455 | lr:2.3496e-04 | norm 0.2667 | dt 338.46ms | 1549044.28 tokens/sec
Step 11997 | loss: 3.143494 | lr:2.3492e-04 | norm 0.2922 | dt 337.61ms | 1552928.79 tokens/sec
Step 11998 | loss: 3.182464 | lr:2.3487e-04 | norm 0.2847 | dt 337.84ms | 1551871.23 tokens/sec
Step 11999 | loss: 3.142885 | lr:2.3483e-04 | norm 0.2681 | dt 337.83ms | 1551913.94 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 12000: 3.1592
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2974/10042=0.2962



ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, I am an expert at writing articles. One thing I'm happy to see is that when I talk to a client I
rank 3 sample 0 >Hello, I'm a language model, so I'm gonna teach the basics like this:
- There's tons of words I have to make up in every
rank 5 sample 1 >Hello, I'm a language model, that uses Haskell as a compiler in a lot of ways. If you want to be an Haskell fan, let me know
rank 3 sample 1 >Hello, I'm a language model, so here I'm going to show you a couple of ideas. First, I'm going to explain what a lexic
rank 3 sample 2 >Hello, I'm a language model, so this one is perfect. What's an alternative to that?
- If you're trying to learn a simple gamerank 5 sample 2 >Hello, I'm a language model, and you should know these things because when you look at the vocabulary around us (we need to know how to use our

rank 5 sample 3 >Hello, I'm a language model, if you can follow the example above, I would need you to get the mainframe and then this would be a sub
rank 3 sample 3 >Hello, I'm a language model, so what do I do?
When you think of your own grammar, you are usually more likely to be thinking about






ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, using all of it's common sense grammar, but I can't think of any use for it in the rest of this
rank 2 sample 1 >Hello, I'm a language model, I'm only learning this one. A way to apply this knowledge I just need to know what's in it. This
rank 2 sample 2 >Hello, I'm a language model, and I'll give you some background (we're not talking about grammar anymore):
- "An ordinary word is used
rank 2 sample 3 >Hello, I'm a language model, I just decided to do my own work from scratch as well to make sure I knew all language expressions and that's my




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so why do I need a language or programming in order to implement this?
I've talked over the basics of programming
rank 1 sample 1 >Hello, I'm a language model, a computer language which is used to understand text and code through the power of words and computer languages. I'm a computer
rank 1 sample 2 >Hello, I'm a language model, I started to get frustrated with the way I was in class: how I became a teacher and then, after working with
rank 1 sample 3 >Hello, I'm a language model, so I'm really interested in how that machine works. Well, now I love that question you guys have about that.




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I'm going to make a set -- what is a language model here? One thing I mean to do is say
rank 7 sample 1 >Hello, I'm a language model, learning basic building blocks for the process is incredibly simple. Everything is just a little bit different than the usual stuff we learned
rank 7 sample 2 >Hello, I'm a language model, so I'll keep your attention down so much. For instance, it uses the word "class" for a whole range
rank 7 sample 3 >Hello, I'm a language model, and you need to learn languages. Most of my work is in languages with a native speaker, i.e. many




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know that you want to teach to everybody. You don't write in a way that will work. You do
rank 0 sample 1 >Hello, I'm a language model, and like all the models I'll keep in the app have a couple of quirks that are necessary for some language to work
rank 0 sample 2 >Hello, I'm a language model, and I will probably mention in a while, when I'm working, "I have to learn how to write code without
rank 0 sample 3 >Hello, I'm a language model, and thanks for the help I got. I did a lot of good work with ERCL. There are many programs




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I think I'll probably be the fastest. But I think I need to improve a lot more. For instance,
rank 4 sample 1 >Hello, I'm a language model, by default, what does the function 'I'm asking in the title function?' (i'm not the answer -- the
rank 4 sample 2 >Hello, I'm a language model, but some of the more difficult things you do with a language include vocabulary, grammar, spelling, punctuation, pronunciation and
rank 4 sample 3 >Hello, I'm a language model, so you only write it with a font you use as your name, my font is a kind that is a lot like




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, just not a language scientist either, so this is something to keep thinking about when I'm on the same level as myself
rank 6 sample 1 >Hello, I'm a language model, which is very important and very powerful. Because I've been working on different languages, it has become a huge world to
rank 6 sample 2 >Hello, I'm a language model, I'm going to show you some pictures of this.
The pictures are going to be the same colors for everyone,
rank 6 sample 3 >Hello, I'm a language model, so what are the 3 main ways of saying it?
- Learn the 2nd person English
- Know the 3


Step 12000 | loss: 3.153836 | lr:2.3479e-04 | norm 0.2863 | dt 18787.93ms | 27905.57 tokens/sec
Step 12001 | loss: 3.134828 | lr:2.3474e-04 | norm 0.2801 | dt 333.46ms | 1572278.47 tokens/sec
Step 12002 | loss: 3.143417 | lr:2.3470e-04 | norm 0.2878 | dt 334.79ms | 1566041.72 tokens/sec
Step 12003 | loss: 3.133573 | lr:2.3466e-04 | norm 0.2789 | dt 336.37ms | 1558654.66 tokens/sec
Step 12004 | loss: 3.159767 | lr:2.3461e-04 | norm 0.2944 | dt 335.67ms | 1561927.21 tokens/sec
Step 12005 | loss: 3.086352 | lr:2.3457e-04 | norm 0.2765 | dt 335.91ms | 1560785.35 tokens/sec
Step 12006 | loss: 3.161318 | lr:2.3453e-04 | norm 0.2903 | dt 337.75ms | 1552286.41 tokens/sec
Step 12007 | loss: 3.107712 | lr:2.3448e-04 | norm 0.2868 | dt 335.86ms | 1561040.18 tokens/sec
Step 12008 | loss: 3.170110 | lr:2.3444e-04 | norm 0.2990 | dt 335.71ms | 1561724.21 tokens/sec
Step 12009 | loss: 3.111715 | lr:2.3440e-04 | norm 0.2797 | dt 336.71ms | 1557099.60 tokens/sec
Step 12010 | loss: 3.107381 | lr:2.3435e-04 | norm 0.3033 | dt 337.01ms | 1555712.71 tokens/sec
Step 12011 | loss: 3.167068 | lr:2.3431e-04 | norm 0.2870 | dt 336.57ms | 1557734.94 tokens/sec
Step 12012 | loss: 3.098773 | lr:2.3427e-04 | norm 0.3121 | dt 336.08ms | 1560024.67 tokens/sec
Step 12013 | loss: 3.109671 | lr:2.3422e-04 | norm 0.2757 | dt 336.13ms | 1559797.83 tokens/sec
Step 12014 | loss: 3.126131 | lr:2.3418e-04 | norm 0.3112 | dt 335.58ms | 1562345.56 tokens/sec
Step 12015 | loss: 3.089498 | lr:2.3414e-04 | norm 0.2676 | dt 336.74ms | 1556964.00 tokens/sec
Step 12016 | loss: 3.091730 | lr:2.3409e-04 | norm 0.2959 | dt 336.64ms | 1557402.87 tokens/sec
Step 12017 | loss: 3.129136 | lr:2.3405e-04 | norm 0.2750 | dt 335.88ms | 1560952.64 tokens/sec
Step 12018 | loss: 3.141304 | lr:2.3401e-04 | norm 0.3010 | dt 336.57ms | 1557755.90 tokens/sec
Step 12019 | loss: 3.127448 | lr:2.3396e-04 | norm 0.2812 | dt 336.75ms | 1556915.50 tokens/sec
Step 12020 | loss: 3.147789 | lr:2.3392e-04 | norm 0.2871 | dt 336.42ms | 1558418.28 tokens/sec
Step 12021 | loss: 3.117396 | lr:2.3388e-04 | norm 0.2880 | dt 336.30ms | 1558980.64 tokens/sec
Step 12022 | loss: 3.125243 | lr:2.3383e-04 | norm 0.2717 | dt 336.14ms | 1559721.49 tokens/sec
Step 12023 | loss: 3.142396 | lr:2.3379e-04 | norm 0.2811 | dt 336.70ms | 1557118.35 tokens/sec
Step 12024 | loss: 3.139468 | lr:2.3375e-04 | norm 0.2915 | dt 336.84ms | 1556500.04 tokens/sec
Step 12025 | loss: 3.122918 | lr:2.3371e-04 | norm 0.2673 | dt 336.50ms | 1558078.19 tokens/sec
Step 12026 | loss: 3.103150 | lr:2.3366e-04 | norm 0.3086 | dt 336.91ms | 1556189.42 tokens/sec
Step 12027 | loss: 3.068408 | lr:2.3362e-04 | norm 0.3087 | dt 336.58ms | 1557697.42 tokens/sec
Step 12028 | loss: 3.100547 | lr:2.3358e-04 | norm 0.2872 | dt 336.98ms | 1555840.39 tokens/sec
Step 12029 | loss: 3.108416 | lr:2.3353e-04 | norm 0.2795 | dt 337.29ms | 1554430.47 tokens/sec
Step 12030 | loss: 3.088480 | lr:2.3349e-04 | norm 0.2705 | dt 337.52ms | 1553362.09 tokens/sec
Step 12031 | loss: 3.123292 | lr:2.3345e-04 | norm 0.2778 | dt 337.77ms | 1552197.66 tokens/sec
Step 12032 | loss: 3.133576 | lr:2.3340e-04 | norm 0.3008 | dt 338.04ms | 1550983.55 tokens/sec
Step 12033 | loss: 3.144398 | lr:2.3336e-04 | norm 0.3022 | dt 337.64ms | 1552795.01 tokens/sec
Step 12034 | loss: 3.181226 | lr:2.3332e-04 | norm 0.2792 | dt 337.15ms | 1555076.83 tokens/sec
Step 12035 | loss: 3.266882 | lr:2.3327e-04 | norm 0.3870 | dt 336.69ms | 1557197.74 tokens/sec
Step 12036 | loss: 3.166634 | lr:2.3323e-04 | norm 0.3086 | dt 337.88ms | 1551717.92 tokens/sec
Step 12037 | loss: 3.180873 | lr:2.3319e-04 | norm 0.3342 | dt 336.81ms | 1556625.64 tokens/sec
Step 12038 | loss: 3.252505 | lr:2.3314e-04 | norm 0.3638 | dt 338.98ms | 1546641.95 tokens/sec
Step 12039 | loss: 3.150548 | lr:2.3310e-04 | norm 0.2907 | dt 336.95ms | 1555973.60 tokens/sec
Step 12040 | loss: 3.164635 | lr:2.3306e-04 | norm 0.3337 | dt 336.41ms | 1558502.22 tokens/sec
Step 12041 | loss: 3.189420 | lr:2.3301e-04 | norm 0.3297 | dt 338.34ms | 1549591.15 tokens/sec
Step 12042 | loss: 3.185192 | lr:2.3297e-04 | norm 0.3743 | dt 339.39ms | 1544811.17 tokens/sec
Step 12043 | loss: 3.113786 | lr:2.3293e-04 | norm 0.3187 | dt 338.56ms | 1548583.93 tokens/sec
Step 12044 | loss: 3.153010 | lr:2.3289e-04 | norm 0.3434 | dt 338.51ms | 1548827.16 tokens/sec
Step 12045 | loss: 3.147748 | lr:2.3284e-04 | norm 0.3567 | dt 338.19ms | 1550289.23 tokens/sec
Step 12046 | loss: 3.150700 | lr:2.3280e-04 | norm 0.3172 | dt 338.91ms | 1546987.95 tokens/sec
Step 12047 | loss: 3.136127 | lr:2.3276e-04 | norm 0.3258 | dt 338.20ms | 1550216.00 tokens/sec
Step 12048 | loss: 3.116437 | lr:2.3271e-04 | norm 0.3091 | dt 336.86ms | 1556396.49 tokens/sec
Step 12049 | loss: 3.144893 | lr:2.3267e-04 | norm 0.3087 | dt 337.48ms | 1553534.38 tokens/sec
Step 12050 | loss: 3.186629 | lr:2.3263e-04 | norm 0.2877 | dt 337.90ms | 1551597.48 tokens/sec
Step 12051 | loss: 3.106018 | lr:2.3258e-04 | norm 0.2955 | dt 337.50ms | 1553437.81 tokens/sec
Step 12052 | loss: 3.107177 | lr:2.3254e-04 | norm 0.2730 | dt 339.26ms | 1545397.42 tokens/sec
Step 12053 | loss: 3.226387 | lr:2.3250e-04 | norm 0.3619 | dt 338.05ms | 1550927.76 tokens/sec
Step 12054 | loss: 3.139307 | lr:2.3245e-04 | norm 0.3180 | dt 337.65ms | 1552768.69 tokens/sec
Step 12055 | loss: 3.129719 | lr:2.3241e-04 | norm 0.2980 | dt 337.87ms | 1551737.63 tokens/sec
Step 12056 | loss: 3.178024 | lr:2.3237e-04 | norm 0.3055 | dt 337.78ms | 1552136.30 tokens/sec
Step 12057 | loss: 3.093544 | lr:2.3233e-04 | norm 0.2906 | dt 337.75ms | 1552309.42 tokens/sec
Step 12058 | loss: 3.095273 | lr:2.3228e-04 | norm 0.3227 | dt 338.02ms | 1551053.57 tokens/sec
Step 12059 | loss: 3.094356 | lr:2.3224e-04 | norm 0.2757 | dt 337.95ms | 1551379.65 tokens/sec
Step 12060 | loss: 3.091180 | lr:2.3220e-04 | norm 0.3035 | dt 338.06ms | 1550885.11 tokens/sec
Step 12061 | loss: 3.108928 | lr:2.3215e-04 | norm 0.2770 | dt 337.59ms | 1553042.85 tokens/sec
Step 12062 | loss: 3.084806 | lr:2.3211e-04 | norm 0.3065 | dt 339.51ms | 1544237.30 tokens/sec
Step 12063 | loss: 3.065747 | lr:2.3207e-04 | norm 0.2797 | dt 338.59ms | 1548432.36 tokens/sec
Step 12064 | loss: 3.088638 | lr:2.3202e-04 | norm 0.2889 | dt 338.96ms | 1546754.00 tokens/sec
Step 12065 | loss: 3.107811 | lr:2.3198e-04 | norm 0.2890 | dt 339.10ms | 1546105.85 tokens/sec
Step 12066 | loss: 3.186589 | lr:2.3194e-04 | norm 0.3117 | dt 337.76ms | 1552267.78 tokens/sec
Step 12067 | loss: 3.136842 | lr:2.3189e-04 | norm 0.2929 | dt 339.28ms | 1545307.28 tokens/sec
Step 12068 | loss: 3.131240 | lr:2.3185e-04 | norm 0.3228 | dt 338.39ms | 1549344.41 tokens/sec
Step 12069 | loss: 3.122946 | lr:2.3181e-04 | norm 0.2863 | dt 339.36ms | 1544918.62 tokens/sec
Step 12070 | loss: 3.136122 | lr:2.3177e-04 | norm 0.2759 | dt 339.61ms | 1543799.32 tokens/sec
Step 12071 | loss: 3.123480 | lr:2.3172e-04 | norm 0.2847 | dt 337.71ms | 1552470.52 tokens/sec
Step 12072 | loss: 3.155617 | lr:2.3168e-04 | norm 0.2983 | dt 338.43ms | 1549171.96 tokens/sec
Step 12073 | loss: 3.205145 | lr:2.3164e-04 | norm 0.2849 | dt 340.07ms | 1541723.37 tokens/sec
Step 12074 | loss: 3.213885 | lr:2.3159e-04 | norm 0.2922 | dt 338.02ms | 1551049.19 tokens/sec
Step 12075 | loss: 3.160347 | lr:2.3155e-04 | norm 0.2928 | dt 339.06ms | 1546292.84 tokens/sec
Step 12076 | loss: 3.121540 | lr:2.3151e-04 | norm 0.3143 | dt 338.77ms | 1547642.28 tokens/sec
Step 12077 | loss: 3.132975 | lr:2.3146e-04 | norm 0.3008 | dt 338.56ms | 1548564.30 tokens/sec
Step 12078 | loss: 3.117586 | lr:2.3142e-04 | norm 0.3003 | dt 340.06ms | 1541744.99 tokens/sec
Step 12079 | loss: 3.157031 | lr:2.3138e-04 | norm 0.2917 | dt 338.31ms | 1549737.49 tokens/sec
Step 12080 | loss: 3.166065 | lr:2.3134e-04 | norm 0.2851 | dt 338.07ms | 1550849.01 tokens/sec
Step 12081 | loss: 3.117072 | lr:2.3129e-04 | norm 0.2918 | dt 338.54ms | 1548659.18 tokens/sec
Step 12082 | loss: 3.118223 | lr:2.3125e-04 | norm 0.2860 | dt 338.41ms | 1549272.37 tokens/sec
Step 12083 | loss: 3.109249 | lr:2.3121e-04 | norm 0.2915 | dt 337.13ms | 1555171.41 tokens/sec
Step 12084 | loss: 3.085632 | lr:2.3116e-04 | norm 0.3572 | dt 338.08ms | 1550793.23 tokens/sec
Step 12085 | loss: 3.161231 | lr:2.3112e-04 | norm 0.2891 | dt 338.30ms | 1549770.25 tokens/sec
Step 12086 | loss: 3.145972 | lr:2.3108e-04 | norm 0.3024 | dt 337.77ms | 1552217.38 tokens/sec
Step 12087 | loss: 3.116793 | lr:2.3103e-04 | norm 0.2811 | dt 337.99ms | 1551188.14 tokens/sec
Step 12088 | loss: 3.151002 | lr:2.3099e-04 | norm 0.3223 | dt 338.97ms | 1546710.48 tokens/sec
Step 12089 | loss: 3.147631 | lr:2.3095e-04 | norm 0.2656 | dt 338.33ms | 1549641.38 tokens/sec
Step 12090 | loss: 3.157151 | lr:2.3091e-04 | norm 0.3041 | dt 338.36ms | 1549477.60 tokens/sec
Step 12091 | loss: 3.118368 | lr:2.3086e-04 | norm 0.2812 | dt 338.19ms | 1550275.02 tokens/sec
Step 12092 | loss: 3.174086 | lr:2.3082e-04 | norm 0.2599 | dt 338.46ms | 1549033.37 tokens/sec
Step 12093 | loss: 3.227885 | lr:2.3078e-04 | norm 0.2946 | dt 338.09ms | 1550729.81 tokens/sec
Step 12094 | loss: 3.134671 | lr:2.3073e-04 | norm 0.2584 | dt 337.69ms | 1552561.49 tokens/sec
Step 12095 | loss: 3.125367 | lr:2.3069e-04 | norm 0.2759 | dt 903.64ms | 580197.16 tokens/sec
Step 12096 | loss: 3.082405 | lr:2.3065e-04 | norm 0.2859 | dt 334.62ms | 1566797.12 tokens/sec
Step 12097 | loss: 3.046721 | lr:2.3060e-04 | norm 0.2892 | dt 337.28ms | 1554454.65 tokens/sec
Step 12098 | loss: 3.093203 | lr:2.3056e-04 | norm 0.2515 | dt 338.50ms | 1548864.25 tokens/sec
Step 12099 | loss: 3.126253 | lr:2.3052e-04 | norm 0.3092 | dt 337.65ms | 1552755.53 tokens/sec
Step 12100 | loss: 3.137604 | lr:2.3048e-04 | norm 0.2880 | dt 337.47ms | 1553595.84 tokens/sec
Step 12101 | loss: 3.120767 | lr:2.3043e-04 | norm 0.2908 | dt 337.88ms | 1551714.63 tokens/sec
Step 12102 | loss: 3.119642 | lr:2.3039e-04 | norm 0.2984 | dt 338.30ms | 1549760.42 tokens/sec
Step 12103 | loss: 3.145227 | lr:2.3035e-04 | norm 0.2969 | dt 338.30ms | 1549783.36 tokens/sec
Step 12104 | loss: 3.232421 | lr:2.3030e-04 | norm 0.3099 | dt 338.22ms | 1550148.25 tokens/sec
Step 12105 | loss: 3.133217 | lr:2.3026e-04 | norm 0.2991 | dt 337.51ms | 1553408.18 tokens/sec
Step 12106 | loss: 3.164742 | lr:2.3022e-04 | norm 0.3092 | dt 338.90ms | 1547020.60 tokens/sec
Step 12107 | loss: 3.113109 | lr:2.3018e-04 | norm 0.3028 | dt 337.58ms | 1553070.27 tokens/sec
Step 12108 | loss: 3.199286 | lr:2.3013e-04 | norm 0.3065 | dt 338.04ms | 1550975.90 tokens/sec
Step 12109 | loss: 3.116441 | lr:2.3009e-04 | norm 0.3073 | dt 338.20ms | 1550211.63 tokens/sec
Step 12110 | loss: 3.190050 | lr:2.3005e-04 | norm 0.3039 | dt 337.64ms | 1552780.75 tokens/sec
Step 12111 | loss: 3.152215 | lr:2.3000e-04 | norm 0.3133 | dt 337.65ms | 1552743.47 tokens/sec
Step 12112 | loss: 3.141798 | lr:2.2996e-04 | norm 0.3165 | dt 339.19ms | 1545686.36 tokens/sec
Step 12113 | loss: 3.146055 | lr:2.2992e-04 | norm 0.3124 | dt 337.49ms | 1553469.63 tokens/sec
Step 12114 | loss: 3.205873 | lr:2.2987e-04 | norm 0.3123 | dt 337.97ms | 1551299.76 tokens/sec
Step 12115 | loss: 3.135603 | lr:2.2983e-04 | norm 0.3378 | dt 338.58ms | 1548499.97 tokens/sec
Step 12116 | loss: 3.126009 | lr:2.2979e-04 | norm 0.3285 | dt 338.06ms | 1550880.73 tokens/sec
Step 12117 | loss: 3.133070 | lr:2.2975e-04 | norm 0.3140 | dt 338.72ms | 1547849.26 tokens/sec
Step 12118 | loss: 3.130462 | lr:2.2970e-04 | norm 0.2894 | dt 339.59ms | 1543902.29 tokens/sec
Step 12119 | loss: 3.139828 | lr:2.2966e-04 | norm 0.3110 | dt 338.22ms | 1550127.49 tokens/sec
Step 12120 | loss: 3.138548 | lr:2.2962e-04 | norm 0.2629 | dt 338.34ms | 1549596.61 tokens/sec
Step 12121 | loss: 3.140654 | lr:2.2957e-04 | norm 0.3105 | dt 339.18ms | 1545768.94 tokens/sec
Step 12122 | loss: 3.126568 | lr:2.2953e-04 | norm 0.2605 | dt 339.57ms | 1543967.33 tokens/sec
Step 12123 | loss: 3.162025 | lr:2.2949e-04 | norm 0.2876 | dt 338.39ms | 1549380.43 tokens/sec
Step 12124 | loss: 3.098562 | lr:2.2945e-04 | norm 0.2657 | dt 338.49ms | 1548896.98 tokens/sec
Step 12125 | loss: 3.094740 | lr:2.2940e-04 | norm 0.3428 | dt 338.03ms | 1550999.96 tokens/sec
Step 12126 | loss: 3.131058 | lr:2.2936e-04 | norm 0.2733 | dt 338.71ms | 1547878.68 tokens/sec
Step 12127 | loss: 3.116930 | lr:2.2932e-04 | norm 0.2932 | dt 339.66ms | 1543558.75 tokens/sec
Step 12128 | loss: 3.121691 | lr:2.2927e-04 | norm 0.2785 | dt 339.23ms | 1545508.20 tokens/sec
Step 12129 | loss: 3.156980 | lr:2.2923e-04 | norm 0.2716 | dt 339.94ms | 1542275.91 tokens/sec
Step 12130 | loss: 3.095098 | lr:2.2919e-04 | norm 0.2832 | dt 339.52ms | 1544183.08 tokens/sec
Step 12131 | loss: 3.123397 | lr:2.2915e-04 | norm 0.2656 | dt 338.64ms | 1548196.89 tokens/sec
Step 12132 | loss: 3.118703 | lr:2.2910e-04 | norm 0.2776 | dt 338.80ms | 1547464.76 tokens/sec
Step 12133 | loss: 3.127830 | lr:2.2906e-04 | norm 0.2563 | dt 338.88ms | 1547128.35 tokens/sec
Step 12134 | loss: 3.123868 | lr:2.2902e-04 | norm 0.2768 | dt 338.12ms | 1550586.56 tokens/sec
Step 12135 | loss: 3.157514 | lr:2.2897e-04 | norm 0.2632 | dt 338.84ms | 1547322.12 tokens/sec
Step 12136 | loss: 3.178728 | lr:2.2893e-04 | norm 0.2752 | dt 339.34ms | 1545036.93 tokens/sec
Step 12137 | loss: 3.107807 | lr:2.2889e-04 | norm 0.2671 | dt 338.74ms | 1547780.63 tokens/sec
Step 12138 | loss: 3.152309 | lr:2.2885e-04 | norm 0.2902 | dt 338.82ms | 1547374.39 tokens/sec
Step 12139 | loss: 3.178806 | lr:2.2880e-04 | norm 0.2608 | dt 338.77ms | 1547615.06 tokens/sec
Step 12140 | loss: 3.128204 | lr:2.2876e-04 | norm 0.2846 | dt 338.23ms | 1550083.78 tokens/sec
Step 12141 | loss: 3.162827 | lr:2.2872e-04 | norm 0.2862 | dt 338.92ms | 1546919.39 tokens/sec
Step 12142 | loss: 3.116698 | lr:2.2867e-04 | norm 0.2945 | dt 338.97ms | 1546686.55 tokens/sec
Step 12143 | loss: 3.170399 | lr:2.2863e-04 | norm 0.3016 | dt 337.74ms | 1552342.29 tokens/sec
Step 12144 | loss: 3.097806 | lr:2.2859e-04 | norm 0.2949 | dt 337.75ms | 1552317.09 tokens/sec
Step 12145 | loss: 3.131192 | lr:2.2855e-04 | norm 0.2991 | dt 338.14ms | 1550525.34 tokens/sec
Step 12146 | loss: 3.146827 | lr:2.2850e-04 | norm 0.2937 | dt 337.88ms | 1551692.74 tokens/sec
Step 12147 | loss: 3.106276 | lr:2.2846e-04 | norm 0.2828 | dt 338.84ms | 1547316.68 tokens/sec
Step 12148 | loss: 3.149714 | lr:2.2842e-04 | norm 0.2984 | dt 338.21ms | 1550182.13 tokens/sec
Step 12149 | loss: 3.135098 | lr:2.2837e-04 | norm 0.2856 | dt 337.75ms | 1552305.04 tokens/sec
Step 12150 | loss: 3.132332 | lr:2.2833e-04 | norm 0.2862 | dt 338.41ms | 1549259.27 tokens/sec
Step 12151 | loss: 3.163661 | lr:2.2829e-04 | norm 0.2864 | dt 338.39ms | 1549381.53 tokens/sec
Step 12152 | loss: 3.161324 | lr:2.2825e-04 | norm 0.2666 | dt 339.40ms | 1544746.06 tokens/sec
Step 12153 | loss: 3.168005 | lr:2.2820e-04 | norm 0.2907 | dt 337.50ms | 1553433.42 tokens/sec
Step 12154 | loss: 3.209375 | lr:2.2816e-04 | norm 0.2977 | dt 337.52ms | 1553345.63 tokens/sec
Step 12155 | loss: 3.141257 | lr:2.2812e-04 | norm 0.3165 | dt 338.40ms | 1549295.29 tokens/sec
Step 12156 | loss: 3.125778 | lr:2.2808e-04 | norm 0.2826 | dt 338.88ms | 1547119.64 tokens/sec
Step 12157 | loss: 3.151664 | lr:2.2803e-04 | norm 0.3139 | dt 337.31ms | 1554339.28 tokens/sec
Step 12158 | loss: 3.111701 | lr:2.2799e-04 | norm 0.3091 | dt 337.79ms | 1552126.44 tokens/sec
Step 12159 | loss: 3.137685 | lr:2.2795e-04 | norm 0.3645 | dt 990.54ms | 529293.72 tokens/sec
Step 12160 | loss: 3.114570 | lr:2.2790e-04 | norm 0.2970 | dt 335.40ms | 1563186.29 tokens/sec
Step 12161 | loss: 3.140939 | lr:2.2786e-04 | norm 0.3251 | dt 337.68ms | 1552611.92 tokens/sec
Step 12162 | loss: 3.058369 | lr:2.2782e-04 | norm 0.2897 | dt 338.12ms | 1550610.62 tokens/sec
Step 12163 | loss: 3.177104 | lr:2.2778e-04 | norm 0.3318 | dt 337.97ms | 1551296.48 tokens/sec
Step 12164 | loss: 3.137301 | lr:2.2773e-04 | norm 0.3092 | dt 336.97ms | 1555880.02 tokens/sec
Step 12165 | loss: 3.191394 | lr:2.2769e-04 | norm 0.2861 | dt 337.73ms | 1552380.65 tokens/sec
Step 12166 | loss: 3.091783 | lr:2.2765e-04 | norm 0.2951 | dt 337.26ms | 1554539.26 tokens/sec
Step 12167 | loss: 3.156374 | lr:2.2760e-04 | norm 0.2769 | dt 337.66ms | 1552732.51 tokens/sec
Step 12168 | loss: 3.105772 | lr:2.2756e-04 | norm 0.2926 | dt 337.83ms | 1551911.75 tokens/sec
Step 12169 | loss: 3.130941 | lr:2.2752e-04 | norm 0.2912 | dt 338.42ms | 1549217.79 tokens/sec
Step 12170 | loss: 3.145592 | lr:2.2748e-04 | norm 0.3091 | dt 337.74ms | 1552347.77 tokens/sec
Step 12171 | loss: 3.156936 | lr:2.2743e-04 | norm 0.3190 | dt 337.74ms | 1552335.72 tokens/sec
Step 12172 | loss: 3.144879 | lr:2.2739e-04 | norm 0.3347 | dt 338.97ms | 1546713.75 tokens/sec
Step 12173 | loss: 3.127246 | lr:2.2735e-04 | norm 0.3337 | dt 337.56ms | 1553154.73 tokens/sec
Step 12174 | loss: 3.137079 | lr:2.2731e-04 | norm 0.2995 | dt 337.53ms | 1553321.49 tokens/sec
Step 12175 | loss: 3.158787 | lr:2.2726e-04 | norm 0.3388 | dt 338.19ms | 1550267.37 tokens/sec
Step 12176 | loss: 3.173353 | lr:2.2722e-04 | norm 0.2962 | dt 337.92ms | 1551498.96 tokens/sec
Step 12177 | loss: 3.112431 | lr:2.2718e-04 | norm 0.3223 | dt 337.77ms | 1552209.71 tokens/sec
Step 12178 | loss: 3.136458 | lr:2.2713e-04 | norm 0.2874 | dt 338.34ms | 1549575.87 tokens/sec
Step 12179 | loss: 3.129129 | lr:2.2709e-04 | norm 0.2932 | dt 339.11ms | 1546069.98 tokens/sec
Step 12180 | loss: 3.142921 | lr:2.2705e-04 | norm 0.2936 | dt 337.77ms | 1552219.57 tokens/sec
Step 12181 | loss: 3.143843 | lr:2.2701e-04 | norm 0.3067 | dt 338.53ms | 1548721.35 tokens/sec
Step 12182 | loss: 3.138768 | lr:2.2696e-04 | norm 0.2934 | dt 338.43ms | 1549181.78 tokens/sec
Step 12183 | loss: 3.175804 | lr:2.2692e-04 | norm 0.3121 | dt 337.79ms | 1552116.58 tokens/sec
Step 12184 | loss: 3.167670 | lr:2.2688e-04 | norm 0.2864 | dt 338.02ms | 1551042.63 tokens/sec
Step 12185 | loss: 3.139604 | lr:2.2684e-04 | norm 0.2980 | dt 338.07ms | 1550834.79 tokens/sec
Step 12186 | loss: 3.129405 | lr:2.2679e-04 | norm 0.2834 | dt 337.84ms | 1551878.89 tokens/sec
Step 12187 | loss: 3.098583 | lr:2.2675e-04 | norm 0.2825 | dt 337.92ms | 1551528.51 tokens/sec
Step 12188 | loss: 3.095424 | lr:2.2671e-04 | norm 0.2842 | dt 338.77ms | 1547618.32 tokens/sec
Step 12189 | loss: 3.121073 | lr:2.2666e-04 | norm 0.2860 | dt 338.99ms | 1546603.88 tokens/sec
Step 12190 | loss: 3.138771 | lr:2.2662e-04 | norm 0.2675 | dt 338.44ms | 1549125.03 tokens/sec
Step 12191 | loss: 3.092454 | lr:2.2658e-04 | norm 0.3049 | dt 338.47ms | 1548999.54 tokens/sec
Step 12192 | loss: 3.121453 | lr:2.2654e-04 | norm 0.2879 | dt 338.07ms | 1550811.83 tokens/sec
Step 12193 | loss: 3.095932 | lr:2.2649e-04 | norm 0.3275 | dt 339.46ms | 1544484.59 tokens/sec
Step 12194 | loss: 3.125979 | lr:2.2645e-04 | norm 0.2854 | dt 339.08ms | 1546213.47 tokens/sec
Step 12195 | loss: 3.148418 | lr:2.2641e-04 | norm 0.3140 | dt 337.93ms | 1551484.73 tokens/sec
Step 12196 | loss: 3.162692 | lr:2.2637e-04 | norm 0.3014 | dt 338.61ms | 1548341.87 tokens/sec
Step 12197 | loss: 3.171658 | lr:2.2632e-04 | norm 0.2955 | dt 339.05ms | 1546357.00 tokens/sec
Step 12198 | loss: 3.118250 | lr:2.2628e-04 | norm 0.2978 | dt 338.28ms | 1549886.04 tokens/sec
Step 12199 | loss: 3.151798 | lr:2.2624e-04 | norm 0.2963 | dt 338.38ms | 1549405.54 tokens/sec
Step 12200 | loss: 3.127120 | lr:2.2620e-04 | norm 0.2996 | dt 338.30ms | 1549784.45 tokens/sec
Step 12201 | loss: 3.165992 | lr:2.2615e-04 | norm 0.2918 | dt 337.69ms | 1552575.74 tokens/sec
Step 12202 | loss: 3.136416 | lr:2.2611e-04 | norm 0.2857 | dt 337.67ms | 1552651.38 tokens/sec
Step 12203 | loss: 3.109601 | lr:2.2607e-04 | norm 0.2711 | dt 337.70ms | 1552547.24 tokens/sec
Step 12204 | loss: 3.098823 | lr:2.2602e-04 | norm 0.2937 | dt 337.50ms | 1553426.83 tokens/sec
Step 12205 | loss: 3.169898 | lr:2.2598e-04 | norm 0.2773 | dt 338.25ms | 1549982.17 tokens/sec
Step 12206 | loss: 3.116297 | lr:2.2594e-04 | norm 0.2830 | dt 337.65ms | 1552757.73 tokens/sec
Step 12207 | loss: 3.183440 | lr:2.2590e-04 | norm 0.3218 | dt 337.30ms | 1554359.05 tokens/sec
Step 12208 | loss: 3.116682 | lr:2.2585e-04 | norm 0.2875 | dt 338.70ms | 1547962.58 tokens/sec
Step 12209 | loss: 3.165642 | lr:2.2581e-04 | norm 0.2847 | dt 338.21ms | 1550171.20 tokens/sec
Step 12210 | loss: 3.106846 | lr:2.2577e-04 | norm 0.2989 | dt 338.48ms | 1548936.26 tokens/sec
Step 12211 | loss: 3.163312 | lr:2.2573e-04 | norm 0.2820 | dt 338.05ms | 1550937.61 tokens/sec
Step 12212 | loss: 3.225233 | lr:2.2568e-04 | norm 0.3166 | dt 337.90ms | 1551599.67 tokens/sec
Step 12213 | loss: 3.156926 | lr:2.2564e-04 | norm 0.3029 | dt 336.84ms | 1556498.94 tokens/sec
Step 12214 | loss: 3.143493 | lr:2.2560e-04 | norm 0.3099 | dt 337.51ms | 1553419.15 tokens/sec
Step 12215 | loss: 3.106957 | lr:2.2556e-04 | norm 0.3096 | dt 338.05ms | 1550925.58 tokens/sec
Step 12216 | loss: 3.088347 | lr:2.2551e-04 | norm 0.2926 | dt 338.08ms | 1550795.42 tokens/sec
Step 12217 | loss: 3.133501 | lr:2.2547e-04 | norm 0.3029 | dt 337.09ms | 1555321.00 tokens/sec
Step 12218 | loss: 3.082747 | lr:2.2543e-04 | norm 0.2737 | dt 337.72ms | 1552429.97 tokens/sec
Step 12219 | loss: 3.173414 | lr:2.2539e-04 | norm 0.2855 | dt 338.13ms | 1550541.74 tokens/sec
Step 12220 | loss: 3.122747 | lr:2.2534e-04 | norm 0.2833 | dt 338.38ms | 1549417.55 tokens/sec
Step 12221 | loss: 3.146328 | lr:2.2530e-04 | norm 0.2703 | dt 337.54ms | 1553240.30 tokens/sec
Step 12222 | loss: 3.135253 | lr:2.2526e-04 | norm 0.2860 | dt 337.26ms | 1554574.43 tokens/sec
Step 12223 | loss: 3.165279 | lr:2.2522e-04 | norm 0.3003 | dt 338.31ms | 1549744.04 tokens/sec
Step 12224 | loss: 3.179461 | lr:2.2517e-04 | norm 0.2905 | dt 337.39ms | 1553935.08 tokens/sec
Step 12225 | loss: 3.165393 | lr:2.2513e-04 | norm 0.2958 | dt 338.33ms | 1549622.82 tokens/sec
Step 12226 | loss: 3.123444 | lr:2.2509e-04 | norm 0.2875 | dt 338.23ms | 1550115.47 tokens/sec
Step 12227 | loss: 3.090232 | lr:2.2504e-04 | norm 0.2672 | dt 337.38ms | 1554000.97 tokens/sec
Step 12228 | loss: 3.169523 | lr:2.2500e-04 | norm 0.4060 | dt 337.06ms | 1555491.52 tokens/sec
Step 12229 | loss: 3.213862 | lr:2.2496e-04 | norm 0.2929 | dt 338.33ms | 1549655.58 tokens/sec
Step 12230 | loss: 3.086989 | lr:2.2492e-04 | norm 0.2843 | dt 337.16ms | 1555026.24 tokens/sec
Step 12231 | loss: 3.113525 | lr:2.2487e-04 | norm 0.3024 | dt 338.39ms | 1549342.23 tokens/sec
Step 12232 | loss: 3.146412 | lr:2.2483e-04 | norm 0.2756 | dt 337.81ms | 1552016.90 tokens/sec
Step 12233 | loss: 3.144651 | lr:2.2479e-04 | norm 0.3289 | dt 337.35ms | 1554136.05 tokens/sec
Step 12234 | loss: 3.114154 | lr:2.2475e-04 | norm 0.3317 | dt 337.99ms | 1551185.95 tokens/sec
Step 12235 | loss: 3.123968 | lr:2.2470e-04 | norm 0.3181 | dt 338.10ms | 1550684.97 tokens/sec
Step 12236 | loss: 3.136515 | lr:2.2466e-04 | norm 0.3264 | dt 337.15ms | 1555063.63 tokens/sec
Step 12237 | loss: 3.142881 | lr:2.2462e-04 | norm 0.3409 | dt 337.43ms | 1553786.85 tokens/sec
Step 12238 | loss: 3.114206 | lr:2.2458e-04 | norm 0.3163 | dt 337.27ms | 1554516.18 tokens/sec
Step 12239 | loss: 3.173567 | lr:2.2453e-04 | norm 0.2988 | dt 338.12ms | 1550586.56 tokens/sec
Step 12240 | loss: 3.172822 | lr:2.2449e-04 | norm 0.3228 | dt 338.18ms | 1550307.81 tokens/sec
Step 12241 | loss: 3.116171 | lr:2.2445e-04 | norm 0.2967 | dt 337.50ms | 1553440.00 tokens/sec
Step 12242 | loss: 3.120652 | lr:2.2441e-04 | norm 0.3055 | dt 338.47ms | 1548980.99 tokens/sec
Step 12243 | loss: 3.201288 | lr:2.2436e-04 | norm 0.3103 | dt 338.88ms | 1547101.14 tokens/sec
Step 12244 | loss: 3.166305 | lr:2.2432e-04 | norm 0.3025 | dt 337.96ms | 1551342.44 tokens/sec
Step 12245 | loss: 3.151863 | lr:2.2428e-04 | norm 0.3357 | dt 337.48ms | 1553520.11 tokens/sec
Step 12246 | loss: 3.161165 | lr:2.2424e-04 | norm 0.2986 | dt 337.64ms | 1552789.52 tokens/sec
Step 12247 | loss: 3.147061 | lr:2.2419e-04 | norm 0.3157 | dt 337.67ms | 1552656.86 tokens/sec
Step 12248 | loss: 3.118921 | lr:2.2415e-04 | norm 0.3346 | dt 338.74ms | 1547759.93 tokens/sec
Step 12249 | loss: 3.142800 | lr:2.2411e-04 | norm 0.3037 | dt 337.70ms | 1552543.95 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 12250: 3.1554
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2944/10042=0.2932


ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so you're ready to dive in and learn how to program yourself. I've also looked at some of other languages you
rank 2 sample 1 >Hello, I'm a language model, but I wouldn't give you a great command line replacement. You could take a look at a text program, and a
rank 2 sample 2 >Hello, I'm a language model, and I've seen many languages in high school. I've seen some grammar classes where kids read about something that looks great
rank 2 sample 3 >Hello, I'm a language model, but that really doesn't make me feel I'm here for more than a brief moment." You'd think this is how




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this one is too complicated to make any generalizations, so if I wanted to tell you, I would add this
rank 5 sample 1 >Hello, I'm a language model, one to get your job done right. I didn't even realize that I could just do that. You want me to
rank 5 sample 2 >Hello, I'm a language model, and I am going to be creating web pages that are much more flexible.
So to make the page more flexible

rank 5 sample 3 >Hello, I'm a language model, don't you mean the kind of thing you see the people do in your everyday life?" (What's your favorite job




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so I knew I wanted to take to work! But if you're not like me, it can be frustrating. I
rank 1 sample 1 >Hello, I'm a language model, a programmer who makes things. I like machine learning, learning how to look at the features of hardware, how to use
rank 1 sample 2 >Hello, I'm a language model, but someone's done it. I'm not sure if you think that I'm going to do anything, or even to
rank 1 sample 3 >Hello, I'm a language model, so I'm using the right type of environment. When speaking with your class please explain - let's say you can't




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I am going to give you a summary of what I'm doing now. Have you checked those definitions before? I
rank 7 sample 1 >Hello, I'm a language model, let's see what's inside! My book is really excellent!
I'm still confused by the spelling and grammar problems
rank 7 sample 2 >Hello, I'm a language model, so I'll put a bunch of quotes and all those things in quotes right here. I'm going to use them for
rank 7 sample 3 >Hello, I'm a language model, and you need to use them in various ways.
To model the process of language models, think about the various





ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I'd like you to have a full blown idea of how all this stuff works!
And if anyone has more
rank 0 sample 1 >Hello, I'm a language model, and in the meantime, I have used the "ROBE" acronym, and the "COW" acronym in
rank 0 sample 2 >Hello, I'm a language model, and I believe in how one can do it."
The other key is that "the word "really" means something
rank 0 sample 3 >Hello, I'm a language model, and want to write a program to run in Spanish. I'm really looking forward to a day of fun and a fantastic




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not an author, I'm an interpreter. So, yes, I'd like to explain to my boss
rank 3 sample 1 >Hello, I'm a language model, so here is my first task!
- It's your first time understanding the language.
- If you ask the
rank 3 sample 2 >Hello, I'm a language model, so this one is interesting. Here's where I need to find the answer in a language. Any help? You've
rank 3 sample 3 >Hello, I'm a language model, so don't let me guess I'm a bad one (or maybe you're wrong!). What I used was a list




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I think I was pretty good at speaking. But I also was an editor, so one of the issues was how
rank 6 sample 0 >Hello, I'm a language model, I need to find and do some code to do things. So, what if I'm going to do some code from
rank 4 sample 1 >Hello, I'm a language model, anyway?
Not at all right, so that I mean, when I was asked for an answer. I replied:
rank 6 sample 1 >Hello, I'm a language model, so I know what that is. I really want people to understand what we're talking about, so I know what to
rank 4 sample 2 >Hello, I'm a language model, I write from source and put it all together. I'm an English language model, I'm the designer, because it
rank 4 sample 3 >Hello, I'm a language model, so what follows:
- A) If your system has input, click the mouse button. Your system will work finerank 6 sample 2 >Hello, I'm a language model, but a bit like any programming tool. I don't have the knowledge or a background in programming, but I still want



rank 6 sample 3 >Hello, I'm a language model, so I know how to use an object of the type "Hello, I'm going to speak English. It's about


Step 12250 | loss: 3.213367 | lr:2.2407e-04 | norm 0.3554 | dt 18762.24ms | 27943.78 tokens/sec
Step 12251 | loss: 3.221439 | lr:2.2402e-04 | norm 0.2903 | dt 332.78ms | 1575495.65 tokens/sec
Step 12252 | loss: 3.141012 | lr:2.2398e-04 | norm 0.3332 | dt 334.84ms | 1565791.94 tokens/sec
Step 12253 | loss: 3.129830 | lr:2.2394e-04 | norm 0.3365 | dt 338.85ms | 1547241.56 tokens/sec
Step 12254 | loss: 3.163348 | lr:2.2390e-04 | norm 0.3502 | dt 336.47ms | 1558189.70 tokens/sec
Step 12255 | loss: 3.166707 | lr:2.2385e-04 | norm 0.3077 | dt 336.49ms | 1558125.66 tokens/sec
Step 12256 | loss: 3.115294 | lr:2.2381e-04 | norm 0.3318 | dt 336.74ms | 1556936.44 tokens/sec
Step 12257 | loss: 3.158660 | lr:2.2377e-04 | norm 0.3076 | dt 337.76ms | 1552261.21 tokens/sec
Step 12258 | loss: 3.117122 | lr:2.2373e-04 | norm 0.3402 | dt 337.07ms | 1555439.81 tokens/sec
Step 12259 | loss: 3.157104 | lr:2.2368e-04 | norm 0.3363 | dt 337.30ms | 1554385.42 tokens/sec
Step 12260 | loss: 3.113327 | lr:2.2364e-04 | norm 0.2862 | dt 337.73ms | 1552365.31 tokens/sec
Step 12261 | loss: 3.129863 | lr:2.2360e-04 | norm 0.3332 | dt 338.24ms | 1550056.47 tokens/sec
Step 12262 | loss: 3.128493 | lr:2.2356e-04 | norm 0.2760 | dt 336.78ms | 1556773.31 tokens/sec
Step 12263 | loss: 3.134105 | lr:2.2351e-04 | norm 0.3037 | dt 339.19ms | 1545697.23 tokens/sec
Step 12264 | loss: 3.123613 | lr:2.2347e-04 | norm 0.3096 | dt 337.91ms | 1551540.56 tokens/sec
Step 12265 | loss: 3.143264 | lr:2.2343e-04 | norm 0.3104 | dt 337.08ms | 1555387.00 tokens/sec
Step 12266 | loss: 3.131195 | lr:2.2339e-04 | norm 0.2900 | dt 338.42ms | 1549204.70 tokens/sec
Step 12267 | loss: 3.126330 | lr:2.2334e-04 | norm 0.2783 | dt 337.58ms | 1553065.88 tokens/sec
Step 12268 | loss: 3.110843 | lr:2.2330e-04 | norm 0.3049 | dt 336.93ms | 1556083.70 tokens/sec
Step 12269 | loss: 3.137644 | lr:2.2326e-04 | norm 0.2872 | dt 337.15ms | 1555058.13 tokens/sec
Step 12270 | loss: 3.098235 | lr:2.2322e-04 | norm 0.2990 | dt 336.61ms | 1557571.64 tokens/sec
Step 12271 | loss: 3.167808 | lr:2.2317e-04 | norm 0.2857 | dt 336.30ms | 1558986.16 tokens/sec
Step 12272 | loss: 3.161897 | lr:2.2313e-04 | norm 0.2966 | dt 337.21ms | 1554776.67 tokens/sec
Step 12273 | loss: 3.144308 | lr:2.2309e-04 | norm 0.3149 | dt 336.80ms | 1556656.50 tokens/sec
Step 12274 | loss: 3.138343 | lr:2.2305e-04 | norm 0.2910 | dt 338.36ms | 1549497.25 tokens/sec
Step 12275 | loss: 3.136361 | lr:2.2301e-04 | norm 0.2968 | dt 337.02ms | 1555638.98 tokens/sec
Step 12276 | loss: 3.197661 | lr:2.2296e-04 | norm 0.2999 | dt 336.92ms | 1556112.33 tokens/sec
Step 12277 | loss: 3.093897 | lr:2.2292e-04 | norm 0.3216 | dt 337.14ms | 1555116.42 tokens/sec
Step 12278 | loss: 3.153939 | lr:2.2288e-04 | norm 0.3013 | dt 336.93ms | 1556052.87 tokens/sec
Step 12279 | loss: 3.168335 | lr:2.2284e-04 | norm 0.3781 | dt 337.64ms | 1552820.22 tokens/sec
Step 12280 | loss: 3.138669 | lr:2.2279e-04 | norm 0.2973 | dt 337.29ms | 1554428.28 tokens/sec
Step 12281 | loss: 3.094662 | lr:2.2275e-04 | norm 0.3114 | dt 338.08ms | 1550765.89 tokens/sec
Step 12282 | loss: 3.093904 | lr:2.2271e-04 | norm 0.2799 | dt 337.44ms | 1553719.88 tokens/sec
Step 12283 | loss: 3.189252 | lr:2.2267e-04 | norm 0.3003 | dt 338.81ms | 1547433.18 tokens/sec
Step 12284 | loss: 3.099500 | lr:2.2262e-04 | norm 0.2789 | dt 915.38ms | 572756.87 tokens/sec
Step 12285 | loss: 3.194790 | lr:2.2258e-04 | norm 0.3145 | dt 336.01ms | 1560322.43 tokens/sec
Step 12286 | loss: 3.167904 | lr:2.2254e-04 | norm 0.2900 | dt 338.18ms | 1550302.34 tokens/sec
Step 12287 | loss: 3.137386 | lr:2.2250e-04 | norm 0.3052 | dt 338.99ms | 1546613.67 tokens/sec
Step 12288 | loss: 3.088936 | lr:2.2245e-04 | norm 0.2759 | dt 337.04ms | 1555547.64 tokens/sec
Step 12289 | loss: 3.160380 | lr:2.2241e-04 | norm 0.2715 | dt 338.76ms | 1547662.98 tokens/sec
Step 12290 | loss: 3.156696 | lr:2.2237e-04 | norm 0.2919 | dt 338.29ms | 1549821.59 tokens/sec
Step 12291 | loss: 3.156228 | lr:2.2233e-04 | norm 0.2632 | dt 337.86ms | 1551804.42 tokens/sec
Step 12292 | loss: 3.094703 | lr:2.2228e-04 | norm 0.2970 | dt 338.35ms | 1549561.67 tokens/sec
Step 12293 | loss: 3.147793 | lr:2.2224e-04 | norm 0.2763 | dt 338.85ms | 1547240.47 tokens/sec
Step 12294 | loss: 3.308053 | lr:2.2220e-04 | norm 0.3515 | dt 337.53ms | 1553292.96 tokens/sec
Step 12295 | loss: 3.093287 | lr:2.2216e-04 | norm 0.3809 | dt 337.82ms | 1551959.94 tokens/sec
Step 12296 | loss: 3.125916 | lr:2.2211e-04 | norm 0.3139 | dt 339.12ms | 1546033.02 tokens/sec
Step 12297 | loss: 3.087032 | lr:2.2207e-04 | norm 0.3313 | dt 337.51ms | 1553421.34 tokens/sec
Step 12298 | loss: 3.125936 | lr:2.2203e-04 | norm 0.3046 | dt 337.58ms | 1553094.40 tokens/sec
Step 12299 | loss: 3.053333 | lr:2.2199e-04 | norm 0.3033 | dt 337.20ms | 1554845.92 tokens/sec
Step 12300 | loss: 3.160819 | lr:2.2195e-04 | norm 0.3082 | dt 339.10ms | 1546109.11 tokens/sec
Step 12301 | loss: 3.093523 | lr:2.2190e-04 | norm 0.2799 | dt 337.82ms | 1551968.70 tokens/sec
Step 12302 | loss: 3.118573 | lr:2.2186e-04 | norm 0.3261 | dt 337.19ms | 1554878.91 tokens/sec
Step 12303 | loss: 3.116517 | lr:2.2182e-04 | norm 0.2890 | dt 337.43ms | 1553765.99 tokens/sec
Step 12304 | loss: 3.118077 | lr:2.2178e-04 | norm 0.3184 | dt 338.08ms | 1550769.17 tokens/sec
Step 12305 | loss: 3.180380 | lr:2.2173e-04 | norm 0.2833 | dt 338.04ms | 1550959.49 tokens/sec
Step 12306 | loss: 3.148735 | lr:2.2169e-04 | norm 0.3149 | dt 337.45ms | 1553696.83 tokens/sec
Step 12307 | loss: 3.075686 | lr:2.2165e-04 | norm 0.2839 | dt 337.95ms | 1551388.41 tokens/sec
Step 12308 | loss: 3.198848 | lr:2.2161e-04 | norm 0.2918 | dt 337.87ms | 1551740.91 tokens/sec
Step 12309 | loss: 3.207408 | lr:2.2156e-04 | norm 0.2981 | dt 339.15ms | 1545893.90 tokens/sec
Step 12310 | loss: 3.116804 | lr:2.2152e-04 | norm 0.3186 | dt 337.72ms | 1552435.45 tokens/sec
Step 12311 | loss: 3.105024 | lr:2.2148e-04 | norm 0.2936 | dt 338.12ms | 1550590.94 tokens/sec
Step 12312 | loss: 3.187533 | lr:2.2144e-04 | norm 0.2733 | dt 340.74ms | 1538681.27 tokens/sec
Step 12313 | loss: 3.151978 | lr:2.2140e-04 | norm 0.3056 | dt 338.87ms | 1547150.12 tokens/sec
Step 12314 | loss: 3.123957 | lr:2.2135e-04 | norm 0.2702 | dt 338.45ms | 1549084.65 tokens/sec
Step 12315 | loss: 3.134883 | lr:2.2131e-04 | norm 0.2840 | dt 338.65ms | 1548169.64 tokens/sec
Step 12316 | loss: 3.163861 | lr:2.2127e-04 | norm 0.2783 | dt 337.95ms | 1551359.95 tokens/sec
Step 12317 | loss: 3.092192 | lr:2.2123e-04 | norm 0.2660 | dt 338.63ms | 1548243.76 tokens/sec
Step 12318 | loss: 3.120104 | lr:2.2118e-04 | norm 0.2778 | dt 338.26ms | 1549949.40 tokens/sec
Step 12319 | loss: 3.121392 | lr:2.2114e-04 | norm 0.2676 | dt 337.80ms | 1552089.20 tokens/sec
Step 12320 | loss: 3.171606 | lr:2.2110e-04 | norm 0.3000 | dt 337.81ms | 1552016.90 tokens/sec
Step 12321 | loss: 3.170657 | lr:2.2106e-04 | norm 0.2714 | dt 338.41ms | 1549260.36 tokens/sec
Step 12322 | loss: 3.112200 | lr:2.2101e-04 | norm 0.2742 | dt 338.36ms | 1549484.15 tokens/sec
Step 12323 | loss: 3.155510 | lr:2.2097e-04 | norm 0.2869 | dt 338.19ms | 1550282.67 tokens/sec
Step 12324 | loss: 3.108956 | lr:2.2093e-04 | norm 0.2845 | dt 337.61ms | 1552938.66 tokens/sec
Step 12325 | loss: 3.130289 | lr:2.2089e-04 | norm 0.2738 | dt 338.77ms | 1547617.23 tokens/sec
Step 12326 | loss: 3.091801 | lr:2.2085e-04 | norm 0.2842 | dt 337.75ms | 1552273.26 tokens/sec
Step 12327 | loss: 3.143341 | lr:2.2080e-04 | norm 0.2809 | dt 337.85ms | 1551838.37 tokens/sec
Step 12328 | loss: 3.108575 | lr:2.2076e-04 | norm 0.2795 | dt 338.33ms | 1549623.91 tokens/sec
Step 12329 | loss: 3.155059 | lr:2.2072e-04 | norm 0.2877 | dt 338.78ms | 1547579.11 tokens/sec
Step 12330 | loss: 3.144460 | lr:2.2068e-04 | norm 0.2761 | dt 338.18ms | 1550314.36 tokens/sec
Step 12331 | loss: 3.115452 | lr:2.2063e-04 | norm 0.2483 | dt 338.55ms | 1548605.75 tokens/sec
Step 12332 | loss: 3.122138 | lr:2.2059e-04 | norm 0.2759 | dt 337.83ms | 1551947.89 tokens/sec
Step 12333 | loss: 3.176651 | lr:2.2055e-04 | norm 0.2988 | dt 339.52ms | 1544195.01 tokens/sec
Step 12334 | loss: 3.099209 | lr:2.2051e-04 | norm 0.2735 | dt 337.19ms | 1554867.91 tokens/sec
Step 12335 | loss: 3.170943 | lr:2.2047e-04 | norm 0.2824 | dt 338.33ms | 1549654.49 tokens/sec
Step 12336 | loss: 3.115329 | lr:2.2042e-04 | norm 0.2649 | dt 338.24ms | 1550030.24 tokens/sec
Step 12337 | loss: 3.072173 | lr:2.2038e-04 | norm 0.2887 | dt 338.36ms | 1549509.26 tokens/sec
Step 12338 | loss: 3.124260 | lr:2.2034e-04 | norm 0.2700 | dt 338.34ms | 1549576.96 tokens/sec
Step 12339 | loss: 3.185214 | lr:2.2030e-04 | norm 0.2875 | dt 338.80ms | 1547465.85 tokens/sec
Step 12340 | loss: 3.180110 | lr:2.2025e-04 | norm 0.2744 | dt 338.28ms | 1549880.57 tokens/sec
Step 12341 | loss: 3.138573 | lr:2.2021e-04 | norm 0.2796 | dt 338.46ms | 1549059.55 tokens/sec
Step 12342 | loss: 3.138215 | lr:2.2017e-04 | norm 0.3027 | dt 339.40ms | 1544729.78 tokens/sec
Step 12343 | loss: 3.137403 | lr:2.2013e-04 | norm 0.2618 | dt 338.69ms | 1547973.48 tokens/sec
Step 12344 | loss: 3.123634 | lr:2.2009e-04 | norm 0.2960 | dt 340.05ms | 1541797.96 tokens/sec
Step 12345 | loss: 3.132208 | lr:2.2004e-04 | norm 0.2755 | dt 338.26ms | 1549943.93 tokens/sec
Step 12346 | loss: 3.120587 | lr:2.2000e-04 | norm 0.3030 | dt 338.48ms | 1548966.81 tokens/sec
Step 12347 | loss: 3.222021 | lr:2.1996e-04 | norm 0.3043 | dt 338.56ms | 1548602.47 tokens/sec
Step 12348 | loss: 3.187022 | lr:2.1992e-04 | norm 0.3190 | dt 338.54ms | 1548690.81 tokens/sec
Step 12349 | loss: 3.169088 | lr:2.1987e-04 | norm 0.2991 | dt 998.21ms | 525229.94 tokens/sec
Step 12350 | loss: 3.097171 | lr:2.1983e-04 | norm 0.2990 | dt 336.73ms | 1556982.74 tokens/sec
Step 12351 | loss: 3.178751 | lr:2.1979e-04 | norm 0.3020 | dt 339.36ms | 1544921.87 tokens/sec
Step 12352 | loss: 3.165487 | lr:2.1975e-04 | norm 0.2859 | dt 338.38ms | 1549388.08 tokens/sec
Step 12353 | loss: 3.167817 | lr:2.1971e-04 | norm 0.3109 | dt 337.56ms | 1553147.05 tokens/sec
Step 12354 | loss: 3.140485 | lr:2.1966e-04 | norm 0.2951 | dt 338.15ms | 1550455.37 tokens/sec
Step 12355 | loss: 3.127225 | lr:2.1962e-04 | norm 0.3089 | dt 339.00ms | 1546558.19 tokens/sec
Step 12356 | loss: 3.208732 | lr:2.1958e-04 | norm 0.3092 | dt 338.14ms | 1550527.52 tokens/sec
Step 12357 | loss: 3.147599 | lr:2.1954e-04 | norm 0.2889 | dt 337.86ms | 1551784.71 tokens/sec
Step 12358 | loss: 3.106161 | lr:2.1949e-04 | norm 0.3049 | dt 339.00ms | 1546557.10 tokens/sec
Step 12359 | loss: 3.117449 | lr:2.1945e-04 | norm 0.2863 | dt 338.30ms | 1549761.52 tokens/sec
Step 12360 | loss: 3.162339 | lr:2.1941e-04 | norm 0.2770 | dt 338.09ms | 1550746.21 tokens/sec
Step 12361 | loss: 3.190354 | lr:2.1937e-04 | norm 0.3123 | dt 338.66ms | 1548131.49 tokens/sec
Step 12362 | loss: 3.098012 | lr:2.1933e-04 | norm 0.2846 | dt 338.54ms | 1548681.00 tokens/sec
Step 12363 | loss: 3.131972 | lr:2.1928e-04 | norm 0.3035 | dt 338.14ms | 1550524.24 tokens/sec
Step 12364 | loss: 3.108241 | lr:2.1924e-04 | norm 0.2705 | dt 338.41ms | 1549251.63 tokens/sec
Step 12365 | loss: 3.145267 | lr:2.1920e-04 | norm 0.2743 | dt 339.34ms | 1545000.02 tokens/sec
Step 12366 | loss: 3.100419 | lr:2.1916e-04 | norm 0.2873 | dt 338.86ms | 1547217.61 tokens/sec
Step 12367 | loss: 3.133324 | lr:2.1912e-04 | norm 0.2664 | dt 339.26ms | 1545391.99 tokens/sec
Step 12368 | loss: 3.107950 | lr:2.1907e-04 | norm 0.2651 | dt 340.23ms | 1540971.44 tokens/sec
Step 12369 | loss: 3.122004 | lr:2.1903e-04 | norm 0.3015 | dt 338.13ms | 1550555.95 tokens/sec
Step 12370 | loss: 3.132640 | lr:2.1899e-04 | norm 0.2628 | dt 339.05ms | 1546349.39 tokens/sec
Step 12371 | loss: 3.106133 | lr:2.1895e-04 | norm 0.2871 | dt 338.01ms | 1551120.30 tokens/sec
Step 12372 | loss: 3.145711 | lr:2.1891e-04 | norm 0.2917 | dt 337.68ms | 1552614.11 tokens/sec
Step 12373 | loss: 3.102771 | lr:2.1886e-04 | norm 0.3136 | dt 337.93ms | 1551485.82 tokens/sec
Step 12374 | loss: 3.080034 | lr:2.1882e-04 | norm 0.2901 | dt 337.67ms | 1552678.79 tokens/sec
Step 12375 | loss: 3.131950 | lr:2.1878e-04 | norm 0.3001 | dt 337.05ms | 1555531.14 tokens/sec
Step 12376 | loss: 3.080152 | lr:2.1874e-04 | norm 0.2991 | dt 339.34ms | 1545000.02 tokens/sec
Step 12377 | loss: 3.196801 | lr:2.1869e-04 | norm 0.3523 | dt 337.52ms | 1553371.97 tokens/sec
Step 12378 | loss: 3.185072 | lr:2.1865e-04 | norm 0.3418 | dt 337.91ms | 1551574.49 tokens/sec
Step 12379 | loss: 3.090512 | lr:2.1861e-04 | norm 0.3352 | dt 338.41ms | 1549268.00 tokens/sec
Step 12380 | loss: 3.162202 | lr:2.1857e-04 | norm 0.3315 | dt 339.02ms | 1546479.88 tokens/sec
Step 12381 | loss: 3.132091 | lr:2.1853e-04 | norm 0.3362 | dt 338.54ms | 1548695.18 tokens/sec
Step 12382 | loss: 3.144166 | lr:2.1848e-04 | norm 0.3040 | dt 338.69ms | 1548001.81 tokens/sec
Step 12383 | loss: 3.180698 | lr:2.1844e-04 | norm 0.3359 | dt 338.12ms | 1550612.80 tokens/sec
Step 12384 | loss: 3.160294 | lr:2.1840e-04 | norm 0.3113 | dt 337.90ms | 1551597.48 tokens/sec
Step 12385 | loss: 3.158182 | lr:2.1836e-04 | norm 0.3214 | dt 338.55ms | 1548614.47 tokens/sec
Step 12386 | loss: 3.199780 | lr:2.1832e-04 | norm 0.3126 | dt 337.79ms | 1552096.87 tokens/sec
Step 12387 | loss: 3.132648 | lr:2.1827e-04 | norm 0.3880 | dt 338.06ms | 1550889.48 tokens/sec
Step 12388 | loss: 3.112969 | lr:2.1823e-04 | norm 0.3724 | dt 337.92ms | 1551497.86 tokens/sec
Step 12389 | loss: 3.128624 | lr:2.1819e-04 | norm 0.3050 | dt 337.91ms | 1551570.11 tokens/sec
Step 12390 | loss: 3.161994 | lr:2.1815e-04 | norm 0.2909 | dt 338.07ms | 1550807.45 tokens/sec
Step 12391 | loss: 3.135418 | lr:2.1811e-04 | norm 0.3103 | dt 337.92ms | 1551514.28 tokens/sec
Step 12392 | loss: 3.104361 | lr:2.1806e-04 | norm 0.2856 | dt 337.59ms | 1553017.62 tokens/sec
Step 12393 | loss: 3.173418 | lr:2.1802e-04 | norm 0.3008 | dt 338.98ms | 1546662.62 tokens/sec
Step 12394 | loss: 3.121882 | lr:2.1798e-04 | norm 0.2714 | dt 337.73ms | 1552385.03 tokens/sec
Step 12395 | loss: 3.108157 | lr:2.1794e-04 | norm 0.2825 | dt 338.23ms | 1550101.26 tokens/sec
Step 12396 | loss: 3.123668 | lr:2.1790e-04 | norm 0.2733 | dt 338.33ms | 1549629.37 tokens/sec
Step 12397 | loss: 3.205854 | lr:2.1785e-04 | norm 0.3184 | dt 337.78ms | 1552156.02 tokens/sec
Step 12398 | loss: 3.140378 | lr:2.1781e-04 | norm 0.2867 | dt 338.72ms | 1547849.26 tokens/sec
Step 12399 | loss: 3.126168 | lr:2.1777e-04 | norm 0.2906 | dt 338.16ms | 1550418.20 tokens/sec
Step 12400 | loss: 3.110538 | lr:2.1773e-04 | norm 0.2968 | dt 338.01ms | 1551087.48 tokens/sec
Step 12401 | loss: 3.115511 | lr:2.1769e-04 | norm 0.2891 | dt 338.43ms | 1549193.78 tokens/sec
Step 12402 | loss: 3.099283 | lr:2.1764e-04 | norm 0.2980 | dt 337.41ms | 1553861.51 tokens/sec
Step 12403 | loss: 3.052346 | lr:2.1760e-04 | norm 0.2646 | dt 337.92ms | 1551498.96 tokens/sec
Step 12404 | loss: 3.252150 | lr:2.1756e-04 | norm 0.3370 | dt 337.82ms | 1551961.04 tokens/sec
Step 12405 | loss: 3.132061 | lr:2.1752e-04 | norm 0.2700 | dt 337.69ms | 1552570.26 tokens/sec
Step 12406 | loss: 3.093681 | lr:2.1748e-04 | norm 0.3155 | dt 337.41ms | 1553847.24 tokens/sec
Step 12407 | loss: 3.147225 | lr:2.1743e-04 | norm 0.3353 | dt 338.91ms | 1546993.39 tokens/sec
Step 12408 | loss: 3.105595 | lr:2.1739e-04 | norm 0.3059 | dt 338.27ms | 1549930.83 tokens/sec
Step 12409 | loss: 3.115474 | lr:2.1735e-04 | norm 0.2895 | dt 337.39ms | 1553966.92 tokens/sec
Step 12410 | loss: 3.084084 | lr:2.1731e-04 | norm 0.3178 | dt 337.92ms | 1551535.08 tokens/sec
Step 12411 | loss: 3.125051 | lr:2.1727e-04 | norm 0.3109 | dt 337.97ms | 1551275.68 tokens/sec
Step 12412 | loss: 3.162546 | lr:2.1722e-04 | norm 0.3376 | dt 337.44ms | 1553703.42 tokens/sec
Step 12413 | loss: 3.163825 | lr:2.1718e-04 | norm 0.2766 | dt 338.04ms | 1550960.58 tokens/sec
Step 12414 | loss: 3.112274 | lr:2.1714e-04 | norm 0.3181 | dt 337.79ms | 1552131.92 tokens/sec
Step 12415 | loss: 3.124177 | lr:2.1710e-04 | norm 0.2696 | dt 337.41ms | 1553839.55 tokens/sec
Step 12416 | loss: 3.224919 | lr:2.1706e-04 | norm 0.3185 | dt 337.93ms | 1551466.12 tokens/sec
Step 12417 | loss: 3.203390 | lr:2.1701e-04 | norm 0.2873 | dt 338.37ms | 1549460.13 tokens/sec
Step 12418 | loss: 3.106560 | lr:2.1697e-04 | norm 0.3143 | dt 338.01ms | 1551096.23 tokens/sec
Step 12419 | loss: 3.174443 | lr:2.1693e-04 | norm 0.3093 | dt 337.20ms | 1554821.74 tokens/sec
Step 12420 | loss: 3.102958 | lr:2.1689e-04 | norm 0.2818 | dt 338.05ms | 1550912.45 tokens/sec
Step 12421 | loss: 3.157569 | lr:2.1685e-04 | norm 0.3007 | dt 338.08ms | 1550762.61 tokens/sec
Step 12422 | loss: 3.088610 | lr:2.1680e-04 | norm 0.3035 | dt 336.92ms | 1556124.45 tokens/sec
Step 12423 | loss: 3.146434 | lr:2.1676e-04 | norm 0.2835 | dt 337.68ms | 1552639.32 tokens/sec
Step 12424 | loss: 3.108979 | lr:2.1672e-04 | norm 0.2796 | dt 338.19ms | 1550268.46 tokens/sec
Step 12425 | loss: 3.097229 | lr:2.1668e-04 | norm 0.3178 | dt 337.69ms | 1552594.38 tokens/sec
Step 12426 | loss: 3.120091 | lr:2.1664e-04 | norm 0.2974 | dt 337.05ms | 1555536.64 tokens/sec
Step 12427 | loss: 3.101879 | lr:2.1659e-04 | norm 0.2989 | dt 338.90ms | 1547026.04 tokens/sec
Step 12428 | loss: 3.137412 | lr:2.1655e-04 | norm 0.2846 | dt 336.74ms | 1556950.77 tokens/sec
Step 12429 | loss: 3.112648 | lr:2.1651e-04 | norm 0.2668 | dt 337.61ms | 1552928.79 tokens/sec
Step 12430 | loss: 3.169772 | lr:2.1647e-04 | norm 0.2984 | dt 337.45ms | 1553663.90 tokens/sec
Step 12431 | loss: 3.086945 | lr:2.1643e-04 | norm 0.5524 | dt 337.68ms | 1552640.42 tokens/sec
Step 12432 | loss: 3.140761 | lr:2.1638e-04 | norm 0.2961 | dt 338.48ms | 1548926.44 tokens/sec
Step 12433 | loss: 3.148031 | lr:2.1634e-04 | norm 0.3069 | dt 337.67ms | 1552675.50 tokens/sec
Step 12434 | loss: 3.135714 | lr:2.1630e-04 | norm 0.2849 | dt 338.03ms | 1551020.75 tokens/sec
Step 12435 | loss: 3.141140 | lr:2.1626e-04 | norm 0.3064 | dt 338.11ms | 1550637.95 tokens/sec
Step 12436 | loss: 3.223876 | lr:2.1622e-04 | norm 0.2913 | dt 338.93ms | 1546895.45 tokens/sec
Step 12437 | loss: 3.120048 | lr:2.1617e-04 | norm 0.2905 | dt 338.22ms | 1550120.93 tokens/sec
Step 12438 | loss: 3.121268 | lr:2.1613e-04 | norm 0.3079 | dt 337.97ms | 1551271.31 tokens/sec
Step 12439 | loss: 3.086332 | lr:2.1609e-04 | norm 0.2737 | dt 338.04ms | 1550985.74 tokens/sec
Step 12440 | loss: 3.191421 | lr:2.1605e-04 | norm 0.2721 | dt 338.91ms | 1546999.92 tokens/sec
Step 12441 | loss: 3.106337 | lr:2.1601e-04 | norm 0.3065 | dt 338.04ms | 1550986.83 tokens/sec
Step 12442 | loss: 3.139140 | lr:2.1597e-04 | norm 0.2713 | dt 338.41ms | 1549248.36 tokens/sec
Step 12443 | loss: 3.142396 | lr:2.1592e-04 | norm 0.2604 | dt 338.48ms | 1548963.53 tokens/sec
Step 12444 | loss: 3.135698 | lr:2.1588e-04 | norm 0.3013 | dt 338.70ms | 1547939.70 tokens/sec
Step 12445 | loss: 3.179331 | lr:2.1584e-04 | norm 0.2854 | dt 337.45ms | 1553684.76 tokens/sec
Step 12446 | loss: 3.133115 | lr:2.1580e-04 | norm 0.2778 | dt 338.55ms | 1548621.01 tokens/sec
Step 12447 | loss: 3.173818 | lr:2.1576e-04 | norm 0.3007 | dt 338.31ms | 1549712.37 tokens/sec
Step 12448 | loss: 3.176592 | lr:2.1571e-04 | norm 0.3081 | dt 338.12ms | 1550576.72 tokens/sec
Step 12449 | loss: 3.177586 | lr:2.1567e-04 | norm 0.3210 | dt 338.31ms | 1549729.84 tokens/sec
Step 12450 | loss: 3.133798 | lr:2.1563e-04 | norm 0.2830 | dt 338.07ms | 1550819.48 tokens/sec
Step 12451 | loss: 3.161525 | lr:2.1559e-04 | norm 0.2856 | dt 337.99ms | 1551181.58 tokens/sec
Step 12452 | loss: 3.300259 | lr:2.1555e-04 | norm 0.3075 | dt 338.92ms | 1546934.62 tokens/sec
Step 12453 | loss: 3.142732 | lr:2.1550e-04 | norm 0.3284 | dt 339.61ms | 1543803.65 tokens/sec
Step 12454 | loss: 3.150090 | lr:2.1546e-04 | norm 0.2947 | dt 339.02ms | 1546503.81 tokens/sec
Step 12455 | loss: 3.137121 | lr:2.1542e-04 | norm 0.2932 | dt 340.06ms | 1541768.77 tokens/sec
Step 12456 | loss: 3.116083 | lr:2.1538e-04 | norm 0.2938 | dt 338.40ms | 1549308.39 tokens/sec
Step 12457 | loss: 3.070323 | lr:2.1534e-04 | norm 0.2848 | dt 338.97ms | 1546691.99 tokens/sec
Step 12458 | loss: 3.104356 | lr:2.1530e-04 | norm 0.3137 | dt 339.04ms | 1546375.48 tokens/sec
Step 12459 | loss: 3.150474 | lr:2.1525e-04 | norm 0.2931 | dt 338.24ms | 1550043.35 tokens/sec
Step 12460 | loss: 3.145569 | lr:2.1521e-04 | norm 0.3059 | dt 339.11ms | 1546050.41 tokens/sec
Step 12461 | loss: 3.081287 | lr:2.1517e-04 | norm 0.2780 | dt 338.24ms | 1550040.08 tokens/sec
Step 12462 | loss: 3.171180 | lr:2.1513e-04 | norm 0.3066 | dt 339.04ms | 1546374.40 tokens/sec
Step 12463 | loss: 3.124684 | lr:2.1509e-04 | norm 0.2849 | dt 338.16ms | 1550413.83 tokens/sec
Step 12464 | loss: 3.104082 | lr:2.1504e-04 | norm 0.2709 | dt 336.75ms | 1556883.53 tokens/sec
Step 12465 | loss: 3.114191 | lr:2.1500e-04 | norm 0.2891 | dt 337.74ms | 1552358.73 tokens/sec
Step 12466 | loss: 3.082808 | lr:2.1496e-04 | norm 0.2701 | dt 338.03ms | 1550990.12 tokens/sec
Step 12467 | loss: 3.114313 | lr:2.1492e-04 | norm 0.2716 | dt 337.44ms | 1553713.30 tokens/sec
Step 12468 | loss: 3.165680 | lr:2.1488e-04 | norm 0.2856 | dt 337.94ms | 1551416.87 tokens/sec
Step 12469 | loss: 3.136558 | lr:2.1484e-04 | norm 0.2845 | dt 338.21ms | 1550193.05 tokens/sec
Step 12470 | loss: 3.132622 | lr:2.1479e-04 | norm 0.2846 | dt 337.79ms | 1552131.92 tokens/sec
Step 12471 | loss: 3.075883 | lr:2.1475e-04 | norm 0.2887 | dt 337.91ms | 1551553.69 tokens/sec
Step 12472 | loss: 3.153229 | lr:2.1471e-04 | norm 0.3133 | dt 337.65ms | 1552742.38 tokens/sec
Step 12473 | loss: 3.122869 | lr:2.1467e-04 | norm 0.3195 | dt 895.17ms | 585688.07 tokens/sec
Step 12474 | loss: 3.112902 | lr:2.1463e-04 | norm 0.2911 | dt 337.17ms | 1554988.86 tokens/sec
Step 12475 | loss: 3.110848 | lr:2.1458e-04 | norm 0.2834 | dt 338.30ms | 1549789.91 tokens/sec
Step 12476 | loss: 3.142854 | lr:2.1454e-04 | norm 0.2874 | dt 338.08ms | 1550788.86 tokens/sec
Step 12477 | loss: 3.102646 | lr:2.1450e-04 | norm 0.2688 | dt 337.66ms | 1552730.32 tokens/sec
Step 12478 | loss: 3.095181 | lr:2.1446e-04 | norm 0.2988 | dt 337.85ms | 1551859.18 tokens/sec
Step 12479 | loss: 3.161103 | lr:2.1442e-04 | norm 0.2948 | dt 338.00ms | 1551125.77 tokens/sec
Step 12480 | loss: 3.095616 | lr:2.1438e-04 | norm 0.2827 | dt 340.76ms | 1538585.45 tokens/sec
Step 12481 | loss: 3.139275 | lr:2.1433e-04 | norm 0.2845 | dt 336.81ms | 1556645.48 tokens/sec
Step 12482 | loss: 3.155788 | lr:2.1429e-04 | norm 0.2898 | dt 338.47ms | 1548979.90 tokens/sec
Step 12483 | loss: 3.143111 | lr:2.1425e-04 | norm 0.2978 | dt 338.26ms | 1549943.93 tokens/sec
Step 12484 | loss: 3.117467 | lr:2.1421e-04 | norm 0.2723 | dt 337.72ms | 1552433.25 tokens/sec
Step 12485 | loss: 3.120754 | lr:2.1417e-04 | norm 0.2915 | dt 337.95ms | 1551397.16 tokens/sec
Step 12486 | loss: 3.174980 | lr:2.1413e-04 | norm 0.2921 | dt 337.82ms | 1551980.75 tokens/sec
Step 12487 | loss: 3.123523 | lr:2.1408e-04 | norm 0.2736 | dt 337.54ms | 1553275.41 tokens/sec
Step 12488 | loss: 3.143086 | lr:2.1404e-04 | norm 0.2910 | dt 338.51ms | 1548820.62 tokens/sec
Step 12489 | loss: 3.113245 | lr:2.1400e-04 | norm 0.2826 | dt 337.87ms | 1551739.82 tokens/sec
Step 12490 | loss: 3.150800 | lr:2.1396e-04 | norm 0.2907 | dt 338.29ms | 1549796.47 tokens/sec
Step 12491 | loss: 3.164477 | lr:2.1392e-04 | norm 0.2913 | dt 339.58ms | 1543942.39 tokens/sec
Step 12492 | loss: 3.152278 | lr:2.1388e-04 | norm 0.2959 | dt 338.59ms | 1548446.54 tokens/sec
Step 12493 | loss: 3.169792 | lr:2.1383e-04 | norm 0.2965 | dt 340.12ms | 1541478.05 tokens/sec
Step 12494 | loss: 3.067434 | lr:2.1379e-04 | norm 0.2723 | dt 338.94ms | 1546830.16 tokens/sec
Step 12495 | loss: 3.153623 | lr:2.1375e-04 | norm 0.2909 | dt 338.75ms | 1547722.89 tokens/sec
Step 12496 | loss: 3.175107 | lr:2.1371e-04 | norm 0.2980 | dt 339.59ms | 1543903.37 tokens/sec
Step 12497 | loss: 3.138079 | lr:2.1367e-04 | norm 0.2863 | dt 339.43ms | 1544609.35 tokens/sec
Step 12498 | loss: 3.126077 | lr:2.1363e-04 | norm 0.2764 | dt 338.63ms | 1548244.85 tokens/sec
Step 12499 | loss: 3.159507 | lr:2.1358e-04 | norm 0.2935 | dt 339.51ms | 1544245.98 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 12500: 3.1507
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2959/10042=0.2947


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but that is not a good thing. Because it's a small piece of the code, and there's not much that
rank 5 sample 1 >Hello, I'm a language model, one of the few, very few, and you're going to spend some time thinking about it. How I define it
rank 5 sample 2 >Hello, I'm a language model, and you have a "real-life" world where you can ask "Do you hear me?"
I'm very
rank 5 sample 3 >Hello, I'm a language model, an adult who likes to see things done. I don't know what to do to do that's what I do.




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I just want to tell you a real life example of how to read a standard file. Today I'll show you
rank 7 sample 1 >Hello, I'm a language model, someone writing an article in the news of my own creation, and I'm not as a language model as I was previously
rank 7 sample 2 >Hello, I'm a language model, so I'll share some of the awesome thing about computer science and some tricks I can do to make it easier in a


ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 3 >Hello, I'm a language model, and you need to have knowledge of one! It's a step by step process of building a programming language. What you


rank 2 sample 0 >Hello, I'm a language model, so what's an example of this? I've used this for a long time. I'm not sure it's correct
rank 2 sample 1 >Hello, I'm a language model, but I work hard to teach them language with the ones that they learn more than I do, so why should I go
rank 2 sample 2 >Hello, I'm a language model, and I'll give you a brief definition of this term. You know, if a system has three distinct classes, they
rank 2 sample 3 >Hello, I'm a language model, but this problem is actually quite simple. All you need is that you have a basic program built around a Java program.




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not saying "butterfly", "makarukaik" instead, but not the first line
rank 3 sample 1 >Hello, I'm a language model, so it was a good reason we tried it with that model. I think it's a good thing. I just thought
rank 3 sample 2 >Hello, I'm a language model, so this isn't on my page. (Yeah, I do have some kind of a problem as well, in which
rank 3 sample 3 >Hello, I'm a language model, so my daughter will be learning this language in my first lesson, and it's fine with us. Thank you so much




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, that you can't tell? Let's see. In my book, you are not going to be able to tell about
rank 6 sample 1 >Hello, I'm a language model, so I need to find out why this program is a little bit difficult (and I don't really understand that...)

rank 6 sample 2 >Hello, I'm a language model, but how does that differ with your model?<|endoftext|>The first time people thought about the possibility of a waterborne pandemic
rank 6 sample 3 >Hello, I'm a language model, so let me simplify my language learning process...
- I'm trying to identify a topic-based game with 10+




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so my parents are just going to drop you the question! Is it the second language of our culture? If so,
rank 1 sample 1 >Hello, I'm a language model, a computer program. I'm just wondering why I'm asking, which is correct. As far as the computer is concerned
rank 1 sample 2 >Hello, I'm a language model, but someone's a language model. I'm a language model at a language school. I'm a language model that understands
rank 1 sample 3 >Hello, I'm a language model, so I'm here for programming. In contrast to C+, Python only needs scripting, yet for all the other languages,




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I don't have a lot of other stuff, but I'm doing a lot to promote student interaction."
"

ddp_rank 0: ####### Printing generated samples ####### 


rank 0 sample 0 >Hello, I'm a language model, so I think I'll take some practice this night. I'm an english learner so I need some practice. Andrank 4 sample 1 >Hello, I'm a language model, do you know why this is it? You see. "I'm here, because I want to work in college,"

rank 4 sample 2 >Hello, I'm a language model, I see someone writing the language code that is "Hello, My name is...and it's my only language in this
rank 0 sample 1 >Hello, I'm a language model, and like people, I'm not stupid. But by building my models, I can get my students in to the same
rank 4 sample 3 >Hello, I'm a language model, so here: The question is: the difference between native English and English sounds is still there, there are some questions to


rank 0 sample 2 >Hello, I'm a language model, and I teach computer science on a university level with a computer, I don't even have to teach it to a 4
rank 0 sample 3 >Hello, I'm a language model, so far I've been learning to use a regular expression to change the input and expression expression in the real world. How


Step 12500 | loss: 3.076108 | lr:2.1354e-04 | norm 0.2718 | dt 18903.07ms | 27735.60 tokens/sec
Step 12501 | loss: 3.171413 | lr:2.1350e-04 | norm 0.2783 | dt 335.44ms | 1562988.52 tokens/sec
Step 12502 | loss: 3.097601 | lr:2.1346e-04 | norm 0.2826 | dt 335.60ms | 1562233.46 tokens/sec
Step 12503 | loss: 3.102194 | lr:2.1342e-04 | norm 0.2448 | dt 336.13ms | 1559770.17 tokens/sec
Step 12504 | loss: 3.097177 | lr:2.1337e-04 | norm 0.2865 | dt 336.34ms | 1558804.93 tokens/sec
Step 12505 | loss: 3.151318 | lr:2.1333e-04 | norm 0.2789 | dt 335.32ms | 1563543.07 tokens/sec
Step 12506 | loss: 3.142335 | lr:2.1329e-04 | norm 0.2681 | dt 335.88ms | 1560960.40 tokens/sec
Step 12507 | loss: 3.102669 | lr:2.1325e-04 | norm 0.2813 | dt 336.90ms | 1556229.07 tokens/sec
Step 12508 | loss: 3.084310 | lr:2.1321e-04 | norm 0.2616 | dt 336.42ms | 1558444.79 tokens/sec
Step 12509 | loss: 3.101609 | lr:2.1317e-04 | norm 0.2689 | dt 336.06ms | 1560101.04 tokens/sec
Step 12510 | loss: 3.161313 | lr:2.1313e-04 | norm 0.2519 | dt 336.14ms | 1559728.13 tokens/sec
Step 12511 | loss: 3.139964 | lr:2.1308e-04 | norm 0.2756 | dt 336.98ms | 1555842.59 tokens/sec
Step 12512 | loss: 3.185217 | lr:2.1304e-04 | norm 0.2734 | dt 336.42ms | 1558412.76 tokens/sec
Step 12513 | loss: 3.157268 | lr:2.1300e-04 | norm 0.2643 | dt 336.61ms | 1557550.68 tokens/sec
Step 12514 | loss: 3.136497 | lr:2.1296e-04 | norm 0.2580 | dt 336.69ms | 1557160.25 tokens/sec
Step 12515 | loss: 3.119314 | lr:2.1292e-04 | norm 0.2518 | dt 341.18ms | 1536703.88 tokens/sec
Step 12516 | loss: 3.084341 | lr:2.1288e-04 | norm 0.2747 | dt 336.98ms | 1555862.41 tokens/sec
Step 12517 | loss: 3.114358 | lr:2.1283e-04 | norm 0.2608 | dt 335.96ms | 1560570.47 tokens/sec
Step 12518 | loss: 3.147733 | lr:2.1279e-04 | norm 0.2964 | dt 342.18ms | 1532218.58 tokens/sec
Step 12519 | loss: 3.122689 | lr:2.1275e-04 | norm 0.2686 | dt 337.16ms | 1555004.25 tokens/sec
Step 12520 | loss: 3.149631 | lr:2.1271e-04 | norm 0.3082 | dt 338.02ms | 1551034.97 tokens/sec
Step 12521 | loss: 3.148805 | lr:2.1267e-04 | norm 0.3103 | dt 341.07ms | 1537163.63 tokens/sec
Step 12522 | loss: 3.107383 | lr:2.1263e-04 | norm 0.3283 | dt 339.04ms | 1546382.01 tokens/sec
Step 12523 | loss: 3.190733 | lr:2.1258e-04 | norm 0.2987 | dt 337.22ms | 1554739.29 tokens/sec
Step 12524 | loss: 3.086292 | lr:2.1254e-04 | norm 0.2852 | dt 337.16ms | 1555007.55 tokens/sec
Step 12525 | loss: 3.192717 | lr:2.1250e-04 | norm 0.2779 | dt 338.32ms | 1549667.59 tokens/sec
Step 12526 | loss: 3.140192 | lr:2.1246e-04 | norm 0.2862 | dt 336.78ms | 1556778.82 tokens/sec
Step 12527 | loss: 3.112983 | lr:2.1242e-04 | norm 0.2886 | dt 337.46ms | 1553649.63 tokens/sec
Step 12528 | loss: 3.138617 | lr:2.1238e-04 | norm 0.2992 | dt 337.95ms | 1551382.94 tokens/sec
Step 12529 | loss: 3.150492 | lr:2.1233e-04 | norm 0.2670 | dt 335.92ms | 1560767.63 tokens/sec
Step 12530 | loss: 3.127873 | lr:2.1229e-04 | norm 0.2765 | dt 338.90ms | 1547039.10 tokens/sec
Step 12531 | loss: 3.129568 | lr:2.1225e-04 | norm 0.2746 | dt 337.90ms | 1551599.67 tokens/sec
Step 12532 | loss: 3.179688 | lr:2.1221e-04 | norm 0.2926 | dt 336.96ms | 1555939.47 tokens/sec
Step 12533 | loss: 3.135793 | lr:2.1217e-04 | norm 0.2722 | dt 337.47ms | 1553576.09 tokens/sec
Step 12534 | loss: 3.128436 | lr:2.1213e-04 | norm 0.2751 | dt 338.10ms | 1550705.75 tokens/sec
Step 12535 | loss: 3.139849 | lr:2.1208e-04 | norm 0.2812 | dt 337.12ms | 1555186.80 tokens/sec
Step 12536 | loss: 3.112390 | lr:2.1204e-04 | norm 0.2731 | dt 337.82ms | 1551993.90 tokens/sec
Step 12537 | loss: 3.160417 | lr:2.1200e-04 | norm 0.2767 | dt 337.83ms | 1551909.56 tokens/sec
Step 12538 | loss: 3.174222 | lr:2.1196e-04 | norm 0.2864 | dt 339.09ms | 1546153.68 tokens/sec
Step 12539 | loss: 3.154905 | lr:2.1192e-04 | norm 0.2703 | dt 1002.13ms | 523174.49 tokens/sec
Step 12540 | loss: 3.131518 | lr:2.1188e-04 | norm 0.2859 | dt 336.65ms | 1557388.53 tokens/sec
Step 12541 | loss: 3.128661 | lr:2.1184e-04 | norm 0.2756 | dt 338.93ms | 1546876.95 tokens/sec
Step 12542 | loss: 3.116311 | lr:2.1179e-04 | norm 0.2744 | dt 337.29ms | 1554421.68 tokens/sec
Step 12543 | loss: 3.159382 | lr:2.1175e-04 | norm 0.2648 | dt 338.09ms | 1550718.87 tokens/sec
Step 12544 | loss: 3.078877 | lr:2.1171e-04 | norm 0.2859 | dt 338.38ms | 1549385.89 tokens/sec
Step 12545 | loss: 3.095312 | lr:2.1167e-04 | norm 0.2622 | dt 338.89ms | 1547052.16 tokens/sec
Step 12546 | loss: 3.125526 | lr:2.1163e-04 | norm 0.2756 | dt 337.35ms | 1554121.78 tokens/sec
Step 12547 | loss: 3.080546 | lr:2.1159e-04 | norm 0.2618 | dt 337.42ms | 1553807.71 tokens/sec
Step 12548 | loss: 3.071066 | lr:2.1154e-04 | norm 0.2561 | dt 337.42ms | 1553827.47 tokens/sec
Step 12549 | loss: 3.164666 | lr:2.1150e-04 | norm 0.2777 | dt 337.56ms | 1553192.03 tokens/sec
Step 12550 | loss: 3.123138 | lr:2.1146e-04 | norm 0.2793 | dt 338.63ms | 1548263.38 tokens/sec
Step 12551 | loss: 3.131925 | lr:2.1142e-04 | norm 0.2703 | dt 339.90ms | 1542455.48 tokens/sec
Step 12552 | loss: 3.205708 | lr:2.1138e-04 | norm 0.3237 | dt 338.60ms | 1548410.56 tokens/sec
Step 12553 | loss: 3.081382 | lr:2.1134e-04 | norm 0.3061 | dt 339.54ms | 1544124.53 tokens/sec
Step 12554 | loss: 3.104042 | lr:2.1130e-04 | norm 0.3001 | dt 338.74ms | 1547781.72 tokens/sec
Step 12555 | loss: 3.104826 | lr:2.1125e-04 | norm 0.3013 | dt 338.14ms | 1550510.03 tokens/sec
Step 12556 | loss: 3.102807 | lr:2.1121e-04 | norm 0.2978 | dt 338.89ms | 1547089.16 tokens/sec
Step 12557 | loss: 3.130955 | lr:2.1117e-04 | norm 0.3114 | dt 338.31ms | 1549747.32 tokens/sec
Step 12558 | loss: 3.196213 | lr:2.1113e-04 | norm 0.3083 | dt 338.93ms | 1546872.60 tokens/sec
Step 12559 | loss: 3.039825 | lr:2.1109e-04 | norm 0.3014 | dt 338.27ms | 1549923.18 tokens/sec
Step 12560 | loss: 3.098276 | lr:2.1105e-04 | norm 0.2880 | dt 338.82ms | 1547397.25 tokens/sec
Step 12561 | loss: 3.088286 | lr:2.1101e-04 | norm 0.2852 | dt 338.47ms | 1549002.81 tokens/sec
Step 12562 | loss: 3.113556 | lr:2.1096e-04 | norm 0.2980 | dt 338.45ms | 1549085.74 tokens/sec
Step 12563 | loss: 3.153087 | lr:2.1092e-04 | norm 0.2840 | dt 339.12ms | 1546005.85 tokens/sec
Step 12564 | loss: 3.097619 | lr:2.1088e-04 | norm 0.3189 | dt 338.12ms | 1550574.54 tokens/sec
Step 12565 | loss: 3.122399 | lr:2.1084e-04 | norm 0.2786 | dt 338.84ms | 1547290.55 tokens/sec
Step 12566 | loss: 3.094215 | lr:2.1080e-04 | norm 0.2837 | dt 338.10ms | 1550686.06 tokens/sec
Step 12567 | loss: 3.110539 | lr:2.1076e-04 | norm 0.2843 | dt 337.78ms | 1552156.02 tokens/sec
Step 12568 | loss: 3.143394 | lr:2.1072e-04 | norm 0.2706 | dt 339.86ms | 1542678.39 tokens/sec
Step 12569 | loss: 3.127620 | lr:2.1067e-04 | norm 0.2751 | dt 337.77ms | 1552204.23 tokens/sec
Step 12570 | loss: 3.153180 | lr:2.1063e-04 | norm 0.2789 | dt 338.08ms | 1550799.80 tokens/sec
Step 12571 | loss: 3.093423 | lr:2.1059e-04 | norm 0.2971 | dt 338.97ms | 1546698.52 tokens/sec
Step 12572 | loss: 3.112706 | lr:2.1055e-04 | norm 0.2909 | dt 338.45ms | 1549089.02 tokens/sec
Step 12573 | loss: 3.110288 | lr:2.1051e-04 | norm 0.2894 | dt 338.13ms | 1550571.26 tokens/sec
Step 12574 | loss: 3.160471 | lr:2.1047e-04 | norm 0.2868 | dt 338.03ms | 1551026.22 tokens/sec
Step 12575 | loss: 3.175931 | lr:2.1042e-04 | norm 0.2781 | dt 337.94ms | 1551402.64 tokens/sec
Step 12576 | loss: 3.106043 | lr:2.1038e-04 | norm 0.2953 | dt 337.61ms | 1552924.40 tokens/sec
Step 12577 | loss: 3.100879 | lr:2.1034e-04 | norm 0.2589 | dt 338.07ms | 1550808.55 tokens/sec
Step 12578 | loss: 3.077017 | lr:2.1030e-04 | norm 0.3106 | dt 338.55ms | 1548627.56 tokens/sec
Step 12579 | loss: 3.118526 | lr:2.1026e-04 | norm 0.2598 | dt 338.09ms | 1550721.06 tokens/sec
Step 12580 | loss: 3.117752 | lr:2.1022e-04 | norm 0.2922 | dt 338.04ms | 1550981.36 tokens/sec
Step 12581 | loss: 3.136374 | lr:2.1018e-04 | norm 0.2740 | dt 338.07ms | 1550804.17 tokens/sec
Step 12582 | loss: 3.111002 | lr:2.1014e-04 | norm 0.2814 | dt 338.09ms | 1550719.96 tokens/sec
Step 12583 | loss: 3.128988 | lr:2.1009e-04 | norm 0.2759 | dt 337.82ms | 1551989.51 tokens/sec
Step 12584 | loss: 3.165674 | lr:2.1005e-04 | norm 0.2917 | dt 337.90ms | 1551609.53 tokens/sec
Step 12585 | loss: 3.137766 | lr:2.1001e-04 | norm 0.2772 | dt 338.94ms | 1546860.63 tokens/sec
Step 12586 | loss: 3.140684 | lr:2.0997e-04 | norm 0.3042 | dt 338.05ms | 1550900.42 tokens/sec
Step 12587 | loss: 3.162377 | lr:2.0993e-04 | norm 0.2798 | dt 338.49ms | 1548891.53 tokens/sec
Step 12588 | loss: 3.112050 | lr:2.0989e-04 | norm 0.3202 | dt 338.47ms | 1548992.99 tokens/sec
Step 12589 | loss: 3.104212 | lr:2.0985e-04 | norm 0.2921 | dt 338.04ms | 1550970.43 tokens/sec
Step 12590 | loss: 3.114516 | lr:2.0980e-04 | norm 0.3085 | dt 338.06ms | 1550884.01 tokens/sec
Step 12591 | loss: 3.115144 | lr:2.0976e-04 | norm 0.2948 | dt 338.02ms | 1551061.22 tokens/sec
Step 12592 | loss: 3.084065 | lr:2.0972e-04 | norm 0.2841 | dt 337.67ms | 1552661.25 tokens/sec
Step 12593 | loss: 3.118984 | lr:2.0968e-04 | norm 0.3015 | dt 339.00ms | 1546590.82 tokens/sec
Step 12594 | loss: 3.171290 | lr:2.0964e-04 | norm 0.2983 | dt 338.42ms | 1549236.35 tokens/sec
Step 12595 | loss: 3.181584 | lr:2.0960e-04 | norm 0.2934 | dt 338.39ms | 1549371.70 tokens/sec
Step 12596 | loss: 3.170290 | lr:2.0956e-04 | norm 0.3114 | dt 337.88ms | 1551701.49 tokens/sec
Step 12597 | loss: 3.136235 | lr:2.0951e-04 | norm 0.3072 | dt 337.64ms | 1552810.36 tokens/sec
Step 12598 | loss: 3.185468 | lr:2.0947e-04 | norm 0.3086 | dt 339.47ms | 1544421.67 tokens/sec
Step 12599 | loss: 3.056221 | lr:2.0943e-04 | norm 0.3372 | dt 338.68ms | 1548031.23 tokens/sec
Step 12600 | loss: 3.140504 | lr:2.0939e-04 | norm 0.2924 | dt 338.16ms | 1550396.34 tokens/sec
Step 12601 | loss: 3.096239 | lr:2.0935e-04 | norm 0.3259 | dt 339.02ms | 1546478.80 tokens/sec
Step 12602 | loss: 3.107111 | lr:2.0931e-04 | norm 0.2896 | dt 338.76ms | 1547662.98 tokens/sec
Step 12603 | loss: 3.112377 | lr:2.0927e-04 | norm 0.3098 | dt 338.56ms | 1548565.40 tokens/sec
Step 12604 | loss: 3.149455 | lr:2.0923e-04 | norm 0.2914 | dt 337.83ms | 1551913.94 tokens/sec
Step 12605 | loss: 3.143419 | lr:2.0918e-04 | norm 0.2931 | dt 338.50ms | 1548854.43 tokens/sec
Step 12606 | loss: 3.171790 | lr:2.0914e-04 | norm 0.3066 | dt 338.31ms | 1549734.21 tokens/sec
Step 12607 | loss: 3.101432 | lr:2.0910e-04 | norm 0.2808 | dt 338.70ms | 1547963.67 tokens/sec
Step 12608 | loss: 3.178661 | lr:2.0906e-04 | norm 0.3163 | dt 338.85ms | 1547279.66 tokens/sec
Step 12609 | loss: 3.152334 | lr:2.0902e-04 | norm 0.2609 | dt 338.74ms | 1547746.86 tokens/sec
Step 12610 | loss: 3.134782 | lr:2.0898e-04 | norm 0.2920 | dt 339.24ms | 1545475.62 tokens/sec
Step 12611 | loss: 3.134116 | lr:2.0894e-04 | norm 0.2673 | dt 337.82ms | 1551958.85 tokens/sec
Step 12612 | loss: 3.120523 | lr:2.0889e-04 | norm 0.2897 | dt 338.84ms | 1547325.39 tokens/sec
Step 12613 | loss: 3.066644 | lr:2.0885e-04 | norm 0.2605 | dt 339.73ms | 1543252.19 tokens/sec
Step 12614 | loss: 3.145685 | lr:2.0881e-04 | norm 0.2856 | dt 338.75ms | 1547731.61 tokens/sec
Step 12615 | loss: 3.131717 | lr:2.0877e-04 | norm 0.3015 | dt 338.85ms | 1547273.13 tokens/sec
Step 12616 | loss: 3.084405 | lr:2.0873e-04 | norm 0.2792 | dt 338.97ms | 1546697.43 tokens/sec
Step 12617 | loss: 3.165533 | lr:2.0869e-04 | norm 0.2936 | dt 339.64ms | 1543673.61 tokens/sec
Step 12618 | loss: 3.125010 | lr:2.0865e-04 | norm 0.2773 | dt 338.69ms | 1547969.12 tokens/sec
Step 12619 | loss: 3.121387 | lr:2.0861e-04 | norm 0.3132 | dt 338.56ms | 1548564.30 tokens/sec
Step 12620 | loss: 3.105127 | lr:2.0856e-04 | norm 0.2759 | dt 338.24ms | 1550045.54 tokens/sec
Step 12621 | loss: 3.105653 | lr:2.0852e-04 | norm 0.2959 | dt 337.86ms | 1551785.81 tokens/sec
Step 12622 | loss: 3.098878 | lr:2.0848e-04 | norm 0.2781 | dt 337.80ms | 1552068.38 tokens/sec
Step 12623 | loss: 3.171094 | lr:2.0844e-04 | norm 0.2807 | dt 338.05ms | 1550898.23 tokens/sec
Step 12624 | loss: 3.135640 | lr:2.0840e-04 | norm 0.2810 | dt 337.46ms | 1553614.50 tokens/sec
Step 12625 | loss: 3.110290 | lr:2.0836e-04 | norm 0.2809 | dt 338.21ms | 1550193.05 tokens/sec
Step 12626 | loss: 3.103379 | lr:2.0832e-04 | norm 0.2710 | dt 338.00ms | 1551149.85 tokens/sec
Step 12627 | loss: 3.121712 | lr:2.0828e-04 | norm 0.2789 | dt 338.25ms | 1550009.48 tokens/sec
Step 12628 | loss: 3.136107 | lr:2.0823e-04 | norm 0.2833 | dt 338.85ms | 1547269.86 tokens/sec
Step 12629 | loss: 3.151131 | lr:2.0819e-04 | norm 0.3334 | dt 338.59ms | 1548459.62 tokens/sec
Step 12630 | loss: 3.148612 | lr:2.0815e-04 | norm 0.2956 | dt 338.32ms | 1549691.62 tokens/sec
Step 12631 | loss: 3.149890 | lr:2.0811e-04 | norm 0.3152 | dt 337.39ms | 1553953.75 tokens/sec
Step 12632 | loss: 3.167500 | lr:2.0807e-04 | norm 0.2845 | dt 338.47ms | 1548998.45 tokens/sec
Step 12633 | loss: 3.113210 | lr:2.0803e-04 | norm 0.3360 | dt 338.07ms | 1550844.64 tokens/sec
Step 12634 | loss: 3.160055 | lr:2.0799e-04 | norm 0.2953 | dt 338.27ms | 1549917.72 tokens/sec
Step 12635 | loss: 3.081019 | lr:2.0795e-04 | norm 0.3045 | dt 337.87ms | 1551747.48 tokens/sec
Step 12636 | loss: 3.140707 | lr:2.0790e-04 | norm 0.2944 | dt 337.54ms | 1553276.51 tokens/sec
Step 12637 | loss: 3.107122 | lr:2.0786e-04 | norm 0.3039 | dt 338.19ms | 1550299.06 tokens/sec
Step 12638 | loss: 3.252088 | lr:2.0782e-04 | norm 0.3107 | dt 337.75ms | 1552312.71 tokens/sec
Step 12639 | loss: 3.116329 | lr:2.0778e-04 | norm 0.3027 | dt 337.98ms | 1551239.57 tokens/sec
Step 12640 | loss: 3.181996 | lr:2.0774e-04 | norm 0.3026 | dt 338.58ms | 1548474.89 tokens/sec
Step 12641 | loss: 3.119734 | lr:2.0770e-04 | norm 0.2963 | dt 338.87ms | 1547155.56 tokens/sec
Step 12642 | loss: 3.063193 | lr:2.0766e-04 | norm 0.2894 | dt 337.52ms | 1553353.31 tokens/sec
Step 12643 | loss: 3.129179 | lr:2.0762e-04 | norm 0.2989 | dt 338.76ms | 1547672.78 tokens/sec
Step 12644 | loss: 3.139811 | lr:2.0757e-04 | norm 0.2823 | dt 338.11ms | 1550663.10 tokens/sec
Step 12645 | loss: 3.108732 | lr:2.0753e-04 | norm 0.2785 | dt 338.14ms | 1550493.63 tokens/sec
Step 12646 | loss: 3.157150 | lr:2.0749e-04 | norm 0.2911 | dt 337.90ms | 1551601.86 tokens/sec
Step 12647 | loss: 3.095635 | lr:2.0745e-04 | norm 0.2662 | dt 338.55ms | 1548643.92 tokens/sec
Step 12648 | loss: 3.110544 | lr:2.0741e-04 | norm 0.2863 | dt 337.69ms | 1552571.36 tokens/sec
Step 12649 | loss: 3.132483 | lr:2.0737e-04 | norm 0.2854 | dt 339.52ms | 1544201.51 tokens/sec
Step 12650 | loss: 3.123296 | lr:2.0733e-04 | norm 0.2872 | dt 338.20ms | 1550252.07 tokens/sec
Step 12651 | loss: 3.170101 | lr:2.0729e-04 | norm 0.2904 | dt 337.72ms | 1552415.72 tokens/sec
Step 12652 | loss: 3.119881 | lr:2.0725e-04 | norm 0.2732 | dt 339.39ms | 1544789.47 tokens/sec
Step 12653 | loss: 3.124695 | lr:2.0720e-04 | norm 0.2906 | dt 338.64ms | 1548220.87 tokens/sec
Step 12654 | loss: 3.159826 | lr:2.0716e-04 | norm 0.2843 | dt 338.50ms | 1548834.80 tokens/sec
Step 12655 | loss: 3.152444 | lr:2.0712e-04 | norm 0.2921 | dt 338.69ms | 1547978.92 tokens/sec
Step 12656 | loss: 3.154067 | lr:2.0708e-04 | norm 0.2799 | dt 339.44ms | 1544581.14 tokens/sec
Step 12657 | loss: 3.188859 | lr:2.0704e-04 | norm 0.2962 | dt 338.81ms | 1547439.72 tokens/sec
Step 12658 | loss: 3.121041 | lr:2.0700e-04 | norm 0.2824 | dt 339.12ms | 1546041.71 tokens/sec
Step 12659 | loss: 3.151288 | lr:2.0696e-04 | norm 0.2944 | dt 338.66ms | 1548122.78 tokens/sec
Step 12660 | loss: 3.110310 | lr:2.0692e-04 | norm 0.2797 | dt 338.51ms | 1548790.07 tokens/sec
Step 12661 | loss: 3.142136 | lr:2.0688e-04 | norm 0.2783 | dt 339.47ms | 1544448.79 tokens/sec
Step 12662 | loss: 3.113018 | lr:2.0683e-04 | norm 0.2905 | dt 1031.51ms | 508270.87 tokens/sec
Step 12663 | loss: 3.165867 | lr:2.0679e-04 | norm 0.3086 | dt 335.77ms | 1561429.24 tokens/sec
Step 12664 | loss: 3.095265 | lr:2.0675e-04 | norm 0.2882 | dt 337.10ms | 1555282.50 tokens/sec
Step 12665 | loss: 3.176432 | lr:2.0671e-04 | norm 0.3113 | dt 340.10ms | 1541588.27 tokens/sec
Step 12666 | loss: 3.112248 | lr:2.0667e-04 | norm 0.2865 | dt 337.47ms | 1553570.60 tokens/sec
Step 12667 | loss: 3.345522 | lr:2.0663e-04 | norm 0.4155 | dt 338.17ms | 1550365.74 tokens/sec
Step 12668 | loss: 3.038076 | lr:2.0659e-04 | norm 0.3259 | dt 339.75ms | 1543139.56 tokens/sec
Step 12669 | loss: 3.130751 | lr:2.0655e-04 | norm 0.3123 | dt 338.71ms | 1547897.20 tokens/sec
Step 12670 | loss: 3.180584 | lr:2.0651e-04 | norm 0.3366 | dt 338.41ms | 1549282.19 tokens/sec
Step 12671 | loss: 3.063434 | lr:2.0646e-04 | norm 0.3214 | dt 339.49ms | 1544329.48 tokens/sec
Step 12672 | loss: 3.132306 | lr:2.0642e-04 | norm 0.3128 | dt 339.01ms | 1546529.91 tokens/sec
Step 12673 | loss: 3.075853 | lr:2.0638e-04 | norm 0.3152 | dt 338.99ms | 1546600.61 tokens/sec
Step 12674 | loss: 3.115479 | lr:2.0634e-04 | norm 0.3109 | dt 338.52ms | 1548750.80 tokens/sec
Step 12675 | loss: 3.139720 | lr:2.0630e-04 | norm 0.3173 | dt 338.80ms | 1547462.59 tokens/sec
Step 12676 | loss: 3.144621 | lr:2.0626e-04 | norm 0.3121 | dt 339.31ms | 1545167.21 tokens/sec
Step 12677 | loss: 3.125971 | lr:2.0622e-04 | norm 0.2855 | dt 338.91ms | 1546967.27 tokens/sec
Step 12678 | loss: 3.123828 | lr:2.0618e-04 | norm 0.3020 | dt 338.74ms | 1547745.77 tokens/sec
Step 12679 | loss: 3.120482 | lr:2.0614e-04 | norm 0.2976 | dt 339.56ms | 1544017.19 tokens/sec
Step 12680 | loss: 3.227324 | lr:2.0609e-04 | norm 0.3314 | dt 339.02ms | 1546462.48 tokens/sec
Step 12681 | loss: 3.122851 | lr:2.0605e-04 | norm 0.3362 | dt 339.68ms | 1543456.91 tokens/sec
Step 12682 | loss: 3.112952 | lr:2.0601e-04 | norm 0.2782 | dt 339.63ms | 1543718.04 tokens/sec
Step 12683 | loss: 3.114754 | lr:2.0597e-04 | norm 0.3179 | dt 339.07ms | 1546266.75 tokens/sec
Step 12684 | loss: 3.109333 | lr:2.0593e-04 | norm 0.2754 | dt 338.36ms | 1549489.61 tokens/sec
Step 12685 | loss: 3.138879 | lr:2.0589e-04 | norm 0.2785 | dt 338.79ms | 1547537.73 tokens/sec
Step 12686 | loss: 3.136268 | lr:2.0585e-04 | norm 0.2838 | dt 339.35ms | 1544997.85 tokens/sec
Step 12687 | loss: 3.140535 | lr:2.0581e-04 | norm 0.2666 | dt 339.19ms | 1545703.75 tokens/sec
Step 12688 | loss: 3.145665 | lr:2.0577e-04 | norm 0.2909 | dt 339.99ms | 1542057.44 tokens/sec
Step 12689 | loss: 3.165719 | lr:2.0573e-04 | norm 0.2708 | dt 339.37ms | 1544900.17 tokens/sec
Step 12690 | loss: 3.135119 | lr:2.0568e-04 | norm 0.2767 | dt 338.42ms | 1549226.53 tokens/sec
Step 12691 | loss: 3.134947 | lr:2.0564e-04 | norm 0.2898 | dt 338.10ms | 1550706.84 tokens/sec
Step 12692 | loss: 3.119719 | lr:2.0560e-04 | norm 0.2578 | dt 338.60ms | 1548398.57 tokens/sec
Step 12693 | loss: 3.081250 | lr:2.0556e-04 | norm 0.2690 | dt 338.68ms | 1548039.95 tokens/sec
Step 12694 | loss: 3.088035 | lr:2.0552e-04 | norm 0.2802 | dt 338.68ms | 1548056.30 tokens/sec
Step 12695 | loss: 3.129365 | lr:2.0548e-04 | norm 0.2963 | dt 338.49ms | 1548922.07 tokens/sec
Step 12696 | loss: 3.252363 | lr:2.0544e-04 | norm 0.2863 | dt 338.27ms | 1549913.35 tokens/sec
Step 12697 | loss: 3.123971 | lr:2.0540e-04 | norm 0.2754 | dt 338.64ms | 1548228.50 tokens/sec
Step 12698 | loss: 3.166410 | lr:2.0536e-04 | norm 0.2886 | dt 337.96ms | 1551350.10 tokens/sec
Step 12699 | loss: 3.112299 | lr:2.0532e-04 | norm 0.2738 | dt 338.80ms | 1547481.10 tokens/sec
Step 12700 | loss: 3.140258 | lr:2.0527e-04 | norm 0.2723 | dt 337.91ms | 1551552.60 tokens/sec
Step 12701 | loss: 3.107106 | lr:2.0523e-04 | norm 0.2931 | dt 337.77ms | 1552214.09 tokens/sec
Step 12702 | loss: 3.188950 | lr:2.0519e-04 | norm 0.3342 | dt 338.22ms | 1550124.21 tokens/sec
Step 12703 | loss: 3.187248 | lr:2.0515e-04 | norm 0.3008 | dt 338.07ms | 1550815.11 tokens/sec
Step 12704 | loss: 3.180816 | lr:2.0511e-04 | norm 0.3163 | dt 337.89ms | 1551660.98 tokens/sec
Step 12705 | loss: 3.108573 | lr:2.0507e-04 | norm 0.2896 | dt 338.10ms | 1550697.00 tokens/sec
Step 12706 | loss: 3.148566 | lr:2.0503e-04 | norm 0.3028 | dt 338.59ms | 1548447.63 tokens/sec
Step 12707 | loss: 3.143585 | lr:2.0499e-04 | norm 0.2894 | dt 338.22ms | 1550125.30 tokens/sec
Step 12708 | loss: 3.104294 | lr:2.0495e-04 | norm 0.2944 | dt 338.29ms | 1549824.87 tokens/sec
Step 12709 | loss: 3.105679 | lr:2.0491e-04 | norm 0.2851 | dt 338.30ms | 1549783.36 tokens/sec
Step 12710 | loss: 3.146357 | lr:2.0486e-04 | norm 0.2916 | dt 337.99ms | 1551178.29 tokens/sec
Step 12711 | loss: 3.157345 | lr:2.0482e-04 | norm 0.2796 | dt 338.12ms | 1550580.00 tokens/sec
Step 12712 | loss: 3.111681 | lr:2.0478e-04 | norm 0.2998 | dt 338.84ms | 1547302.53 tokens/sec
Step 12713 | loss: 3.113909 | lr:2.0474e-04 | norm 0.2743 | dt 338.70ms | 1547952.77 tokens/sec
Step 12714 | loss: 3.158537 | lr:2.0470e-04 | norm 0.2932 | dt 337.53ms | 1553296.26 tokens/sec
Step 12715 | loss: 3.110724 | lr:2.0466e-04 | norm 0.2841 | dt 338.17ms | 1550383.22 tokens/sec
Step 12716 | loss: 3.143706 | lr:2.0462e-04 | norm 0.2834 | dt 338.58ms | 1548502.15 tokens/sec
Step 12717 | loss: 3.137131 | lr:2.0458e-04 | norm 0.2775 | dt 337.64ms | 1552781.85 tokens/sec
Step 12718 | loss: 3.190658 | lr:2.0454e-04 | norm 0.2645 | dt 337.70ms | 1552532.99 tokens/sec
Step 12719 | loss: 3.137394 | lr:2.0450e-04 | norm 0.3443 | dt 338.85ms | 1547237.21 tokens/sec
Step 12720 | loss: 3.126016 | lr:2.0446e-04 | norm 0.2824 | dt 337.86ms | 1551770.48 tokens/sec
Step 12721 | loss: 3.144888 | lr:2.0441e-04 | norm 0.2855 | dt 338.84ms | 1547325.39 tokens/sec
Step 12722 | loss: 3.133755 | lr:2.0437e-04 | norm 0.2749 | dt 337.82ms | 1551954.46 tokens/sec
Step 12723 | loss: 3.171194 | lr:2.0433e-04 | norm 0.2755 | dt 337.73ms | 1552392.70 tokens/sec
Step 12724 | loss: 3.097415 | lr:2.0429e-04 | norm 0.2807 | dt 339.23ms | 1545513.63 tokens/sec
Step 12725 | loss: 3.118036 | lr:2.0425e-04 | norm 0.2827 | dt 338.83ms | 1547367.85 tokens/sec
Step 12726 | loss: 3.119312 | lr:2.0421e-04 | norm 0.2714 | dt 339.17ms | 1545809.14 tokens/sec
Step 12727 | loss: 3.162798 | lr:2.0417e-04 | norm 0.2802 | dt 337.53ms | 1553291.87 tokens/sec
Step 12728 | loss: 3.094777 | lr:2.0413e-04 | norm 0.2624 | dt 338.61ms | 1548352.78 tokens/sec
Step 12729 | loss: 3.105699 | lr:2.0409e-04 | norm 0.2841 | dt 1045.72ms | 501366.89 tokens/sec
Step 12730 | loss: 3.207566 | lr:2.0405e-04 | norm 0.2939 | dt 337.57ms | 1553113.05 tokens/sec
Step 12731 | loss: 3.095372 | lr:2.0401e-04 | norm 0.2966 | dt 337.72ms | 1552447.50 tokens/sec
Step 12732 | loss: 3.140279 | lr:2.0396e-04 | norm 0.2945 | dt 339.25ms | 1545434.34 tokens/sec
Step 12733 | loss: 3.126097 | lr:2.0392e-04 | norm 0.2761 | dt 338.74ms | 1547752.30 tokens/sec
Step 12734 | loss: 3.023368 | lr:2.0388e-04 | norm 0.3000 | dt 337.37ms | 1554021.83 tokens/sec
Step 12735 | loss: 3.182371 | lr:2.0384e-04 | norm 0.3209 | dt 337.74ms | 1552343.39 tokens/sec
Step 12736 | loss: 3.190408 | lr:2.0380e-04 | norm 0.2865 | dt 338.00ms | 1551157.51 tokens/sec
Step 12737 | loss: 3.161128 | lr:2.0376e-04 | norm 0.3281 | dt 337.93ms | 1551455.17 tokens/sec
Step 12738 | loss: 3.143452 | lr:2.0372e-04 | norm 0.2808 | dt 337.92ms | 1551537.27 tokens/sec
Step 12739 | loss: 3.109744 | lr:2.0368e-04 | norm 0.2980 | dt 337.44ms | 1553710.00 tokens/sec
Step 12740 | loss: 3.156624 | lr:2.0364e-04 | norm 0.2997 | dt 337.08ms | 1555398.01 tokens/sec
Step 12741 | loss: 3.090141 | lr:2.0360e-04 | norm 0.3453 | dt 338.33ms | 1549616.27 tokens/sec
Step 12742 | loss: 3.161679 | lr:2.0356e-04 | norm 0.3017 | dt 337.53ms | 1553323.69 tokens/sec
Step 12743 | loss: 3.163069 | lr:2.0352e-04 | norm 0.2933 | dt 337.65ms | 1552746.76 tokens/sec
Step 12744 | loss: 3.138396 | lr:2.0347e-04 | norm 0.3066 | dt 338.06ms | 1550875.26 tokens/sec
Step 12745 | loss: 3.127844 | lr:2.0343e-04 | norm 0.2837 | dt 337.71ms | 1552478.19 tokens/sec
Step 12746 | loss: 3.113645 | lr:2.0339e-04 | norm 0.2688 | dt 338.58ms | 1548503.24 tokens/sec
Step 12747 | loss: 3.125606 | lr:2.0335e-04 | norm 0.2838 | dt 339.31ms | 1545168.29 tokens/sec
Step 12748 | loss: 3.157452 | lr:2.0331e-04 | norm 0.2591 | dt 337.87ms | 1551766.10 tokens/sec
Step 12749 | loss: 3.156259 | lr:2.0327e-04 | norm 0.2709 | dt 338.16ms | 1550434.60 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 12750: 3.1458
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2990/10042=0.2977


ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, for instance. We're writing our language, we're doing the grammar.
In the previous lesson, if you had
rank 2 sample 1 >Hello, I'm a language model, but I remember when I was a college undergrad, language models weren't all that different. The difference is in the way
rank 2 sample 2 >Hello, I'm a language model, and I've got a bunch of posts just like that. You're all in love with me as I'm doing some
rank 2 sample 3 >Hello, I'm a language model, but how can I do that?
How can I write it?
We need to put words into a word to




ddp_rank 3: ####### Printing generated samples ####### 



ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I thought I'm gonna go back where I was and find out whether I were actually talking about my language or whatever
rank 3 sample 0 >Hello, I'm a language model, so I'm gonna take some notes. How can we talk to people now? Well in Japanese, which I'll just
rank 3 sample 1 >Hello, I'm a language model, so what is it?
If you want to go deep into a language, you can also take a look of howrank 7 sample 1 >Hello, I'm a language model, we can learn a language from it with our help. Learn a language from scratch here.
Learning a language can teach

rank 7 sample 2 >Hello, I'm a language model, so I'll use a lot of text, but with a little bit of code, and I'm a little old fashionedrank 3 sample 2 >Hello, I'm a language model, so this should be done in order to know some basics. For example:
if (this should be done using a

rank 3 sample 3 >Hello, I'm a language model, so i'm trying to answer questions about the syntax. What do you want to understand. It's this big question,


rank 7 sample 3 >Hello, I'm a language model, and you'll be the best-equipped to start speaking English. I started speaking English at the age of four, the




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't have to learn that as in most languages, yet (it's like just being in the same world
rank 0 sample 1 >Hello, I'm a language model, and there's a lot of people reading about languages now as I know we don't really know how or where to go
rank 0 sample 2 >Hello, I'm a language model, and I understand that one too. The other people, I guess are not so great. I'm not a linguistics
rank 0 sample 3 >Hello, I'm a language model, and there's a lot of things to consider depending on your target language. So we're going to define a syntax (





ddp_rank 4: ####### Printing generated samples ####### 


ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I like to talk about how I teach myself, and how I do it. If not, I can't do
rank 6 sample 0 >Hello, I'm a language model, I need to learn something with a programming background. What is the difference? Let me know in the comments below.

rank 6 sample 1 >Hello, I'm a language model, so I can't just write it out but I need to know how to do that. I think it should be.
rank 4 sample 1 >Hello, I'm a language model, right?
Sorry, I had a very interesting discussion post on this before and a new one on what we need to
rank 6 sample 2 >Hello, I'm a language model, but a native language modeler who doesn't need to know the code, it's easier to learn
There’
rank 4 sample 2 >Hello, I'm a language model, I used to look into software, I don't know how much software I know of, but I use it myself,
rank 6 sample 3 >Hello, I'm a language model, so I can just use a dictionary. I've got a dictionary that's on my phonebook, I don't have


rank 4 sample 3 >Hello, I'm a language model, so it always gives me more than being perfect to express my concepts and help me to understand what he is saying. So




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this one is pretty cool. It's not quite the same old one as I'm used to. It's the
rank 5 sample 1 >Hello, I'm a language model, please write down some comments in a paragraph, etc. and let me know how things work out. Please add your comments
rank 5 sample 2 >Hello, I'm a language model, and I wanna learn (or go!) to say something like "what is it that means "That" or "the
rank 5 sample 3 >Hello, I'm a language model, that's it. I love it.
Thank you so much.
Ciao is the English version of this story




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so I'll be in a second studio and write this with that.
First, you can tell me if I'm
rank 1 sample 1 >Hello, I'm a language model, a computer science person, and I am just a computer designer. I'm interested in technology and learning more about computers.
rank 1 sample 2 >Hello, I'm a language model, but just as an example, I'm a language modeler -- a computer system that can understand languages -- but what do
rank 1 sample 3 >Hello, I'm a language model, so I'm doing this now and I use the first six values that I will use first. The first six values are


Step 12750 | loss: 3.143670 | lr:2.0323e-04 | norm 0.2769 | dt 18919.03ms | 27712.20 tokens/sec
Step 12751 | loss: 3.115594 | lr:2.0319e-04 | norm 0.2890 | dt 334.71ms | 1566381.95 tokens/sec
Step 12752 | loss: 3.258804 | lr:2.0315e-04 | norm 0.2971 | dt 335.41ms | 1563119.62 tokens/sec
Step 12753 | loss: 3.116344 | lr:2.0311e-04 | norm 0.3283 | dt 336.57ms | 1557731.63 tokens/sec
Step 12754 | loss: 3.058982 | lr:2.0307e-04 | norm 0.2888 | dt 336.10ms | 1559898.52 tokens/sec
Step 12755 | loss: 3.103858 | lr:2.0303e-04 | norm 0.2668 | dt 336.49ms | 1558095.85 tokens/sec
Step 12756 | loss: 3.098446 | lr:2.0299e-04 | norm 0.2898 | dt 336.69ms | 1557190.02 tokens/sec
Step 12757 | loss: 3.155147 | lr:2.0294e-04 | norm 0.2842 | dt 336.03ms | 1560262.65 tokens/sec
Step 12758 | loss: 3.083747 | lr:2.0290e-04 | norm 0.3106 | dt 337.09ms | 1555332.00 tokens/sec
Step 12759 | loss: 3.114360 | lr:2.0286e-04 | norm 0.2786 | dt 336.72ms | 1557066.53 tokens/sec
Step 12760 | loss: 3.117770 | lr:2.0282e-04 | norm 0.2972 | dt 336.66ms | 1557314.63 tokens/sec
Step 12761 | loss: 3.096244 | lr:2.0278e-04 | norm 0.3002 | dt 336.67ms | 1557282.65 tokens/sec
Step 12762 | loss: 3.073633 | lr:2.0274e-04 | norm 0.2814 | dt 336.67ms | 1557290.37 tokens/sec
Step 12763 | loss: 3.188085 | lr:2.0270e-04 | norm 0.3196 | dt 336.81ms | 1556636.66 tokens/sec
Step 12764 | loss: 3.103461 | lr:2.0266e-04 | norm 0.2582 | dt 337.34ms | 1554172.30 tokens/sec
Step 12765 | loss: 3.185115 | lr:2.0262e-04 | norm 0.3089 | dt 337.24ms | 1554631.58 tokens/sec
Step 12766 | loss: 3.118882 | lr:2.0258e-04 | norm 0.3143 | dt 337.74ms | 1552352.16 tokens/sec
Step 12767 | loss: 3.150424 | lr:2.0254e-04 | norm 0.2981 | dt 337.19ms | 1554860.22 tokens/sec
Step 12768 | loss: 3.072883 | lr:2.0250e-04 | norm 0.3123 | dt 337.63ms | 1552852.02 tokens/sec
Step 12769 | loss: 3.142488 | lr:2.0246e-04 | norm 0.4557 | dt 337.35ms | 1554149.23 tokens/sec
Step 12770 | loss: 3.155499 | lr:2.0241e-04 | norm 0.3468 | dt 337.51ms | 1553409.27 tokens/sec
Step 12771 | loss: 3.077819 | lr:2.0237e-04 | norm 0.3109 | dt 337.75ms | 1552280.93 tokens/sec
Step 12772 | loss: 3.094078 | lr:2.0233e-04 | norm 0.3225 | dt 338.26ms | 1549965.78 tokens/sec
Step 12773 | loss: 3.136544 | lr:2.0229e-04 | norm 0.3221 | dt 337.97ms | 1551286.63 tokens/sec
Step 12774 | loss: 3.162176 | lr:2.0225e-04 | norm 0.3039 | dt 337.68ms | 1552595.47 tokens/sec
Step 12775 | loss: 3.147523 | lr:2.0221e-04 | norm 0.3045 | dt 337.71ms | 1552478.19 tokens/sec
Step 12776 | loss: 3.110938 | lr:2.0217e-04 | norm 0.2964 | dt 338.60ms | 1548404.02 tokens/sec
Step 12777 | loss: 3.160877 | lr:2.0213e-04 | norm 0.2939 | dt 338.29ms | 1549836.88 tokens/sec
Step 12778 | loss: 3.152574 | lr:2.0209e-04 | norm 0.3226 | dt 338.90ms | 1547007.54 tokens/sec
Step 12779 | loss: 3.158672 | lr:2.0205e-04 | norm 0.3049 | dt 338.37ms | 1549457.95 tokens/sec
Step 12780 | loss: 3.182699 | lr:2.0201e-04 | norm 0.2852 | dt 338.75ms | 1547730.52 tokens/sec
Step 12781 | loss: 3.150789 | lr:2.0197e-04 | norm 0.3094 | dt 338.19ms | 1550260.81 tokens/sec
Step 12782 | loss: 3.187132 | lr:2.0193e-04 | norm 0.2815 | dt 337.75ms | 1552285.31 tokens/sec
Step 12783 | loss: 3.152500 | lr:2.0189e-04 | norm 0.2938 | dt 338.42ms | 1549214.52 tokens/sec
Step 12784 | loss: 3.164810 | lr:2.0185e-04 | norm 0.3029 | dt 339.14ms | 1545942.81 tokens/sec
Step 12785 | loss: 3.127050 | lr:2.0180e-04 | norm 0.2855 | dt 337.59ms | 1553047.24 tokens/sec
Step 12786 | loss: 3.126179 | lr:2.0176e-04 | norm 0.2828 | dt 337.95ms | 1551378.56 tokens/sec
Step 12787 | loss: 3.135493 | lr:2.0172e-04 | norm 0.2797 | dt 338.48ms | 1548960.26 tokens/sec
Step 12788 | loss: 3.137109 | lr:2.0168e-04 | norm 0.2844 | dt 339.07ms | 1546245.00 tokens/sec
Step 12789 | loss: 3.126710 | lr:2.0164e-04 | norm 0.2894 | dt 338.50ms | 1548878.43 tokens/sec
Step 12790 | loss: 3.082150 | lr:2.0160e-04 | norm 0.2741 | dt 338.73ms | 1547800.24 tokens/sec
Step 12791 | loss: 3.127503 | lr:2.0156e-04 | norm 0.2698 | dt 337.79ms | 1552099.06 tokens/sec
Step 12792 | loss: 3.127467 | lr:2.0152e-04 | norm 0.2706 | dt 338.22ms | 1550128.58 tokens/sec
Step 12793 | loss: 3.090329 | lr:2.0148e-04 | norm 0.2637 | dt 338.98ms | 1546671.32 tokens/sec
Step 12794 | loss: 3.065514 | lr:2.0144e-04 | norm 0.2687 | dt 338.15ms | 1550463.02 tokens/sec
Step 12795 | loss: 3.120088 | lr:2.0140e-04 | norm 0.2763 | dt 338.42ms | 1549233.07 tokens/sec
Step 12796 | loss: 3.141943 | lr:2.0136e-04 | norm 0.2837 | dt 338.24ms | 1550068.48 tokens/sec
Step 12797 | loss: 3.092324 | lr:2.0132e-04 | norm 0.2743 | dt 338.37ms | 1549453.58 tokens/sec
Step 12798 | loss: 3.154538 | lr:2.0128e-04 | norm 0.3037 | dt 338.74ms | 1547777.36 tokens/sec
Step 12799 | loss: 3.102623 | lr:2.0124e-04 | norm 0.2806 | dt 338.37ms | 1549432.84 tokens/sec
Step 12800 | loss: 3.130277 | lr:2.0119e-04 | norm 0.3147 | dt 337.77ms | 1552182.32 tokens/sec
Step 12801 | loss: 3.126915 | lr:2.0115e-04 | norm 0.2747 | dt 338.01ms | 1551122.49 tokens/sec
Step 12802 | loss: 3.182165 | lr:2.0111e-04 | norm 0.3235 | dt 338.09ms | 1550754.96 tokens/sec
Step 12803 | loss: 3.150811 | lr:2.0107e-04 | norm 0.2764 | dt 337.92ms | 1551492.39 tokens/sec
Step 12804 | loss: 3.110983 | lr:2.0103e-04 | norm 0.2882 | dt 337.31ms | 1554322.80 tokens/sec
Step 12805 | loss: 3.121076 | lr:2.0099e-04 | norm 0.2965 | dt 341.07ms | 1537185.13 tokens/sec
Step 12806 | loss: 3.132622 | lr:2.0095e-04 | norm 0.2855 | dt 337.70ms | 1552527.51 tokens/sec
Step 12807 | loss: 3.175275 | lr:2.0091e-04 | norm 0.3064 | dt 337.71ms | 1552465.04 tokens/sec
Step 12808 | loss: 3.150241 | lr:2.0087e-04 | norm 0.2938 | dt 338.09ms | 1550728.71 tokens/sec
Step 12809 | loss: 3.114827 | lr:2.0083e-04 | norm 0.3005 | dt 337.88ms | 1551705.87 tokens/sec
Step 12810 | loss: 3.129240 | lr:2.0079e-04 | norm 0.3165 | dt 338.19ms | 1550280.48 tokens/sec
Step 12811 | loss: 3.170106 | lr:2.0075e-04 | norm 0.3178 | dt 338.12ms | 1550602.96 tokens/sec
Step 12812 | loss: 3.162529 | lr:2.0071e-04 | norm 0.3161 | dt 337.73ms | 1552389.42 tokens/sec
Step 12813 | loss: 3.072807 | lr:2.0067e-04 | norm 0.2962 | dt 337.64ms | 1552812.55 tokens/sec
Step 12814 | loss: 3.134684 | lr:2.0063e-04 | norm 0.3033 | dt 338.91ms | 1546983.59 tokens/sec
Step 12815 | loss: 3.117019 | lr:2.0059e-04 | norm 0.2938 | dt 337.77ms | 1552207.52 tokens/sec
Step 12816 | loss: 3.124936 | lr:2.0055e-04 | norm 0.2740 | dt 337.79ms | 1552093.58 tokens/sec
Step 12817 | loss: 3.149455 | lr:2.0051e-04 | norm 0.3118 | dt 338.06ms | 1550858.86 tokens/sec
Step 12818 | loss: 3.172134 | lr:2.0046e-04 | norm 0.2747 | dt 337.99ms | 1551195.80 tokens/sec
Step 12819 | loss: 3.118740 | lr:2.0042e-04 | norm 0.3237 | dt 337.99ms | 1551205.65 tokens/sec
Step 12820 | loss: 3.155586 | lr:2.0038e-04 | norm 0.2884 | dt 338.24ms | 1550042.26 tokens/sec
Step 12821 | loss: 3.105143 | lr:2.0034e-04 | norm 0.3124 | dt 337.87ms | 1551758.43 tokens/sec
Step 12822 | loss: 3.183329 | lr:2.0030e-04 | norm 0.2940 | dt 337.25ms | 1554588.71 tokens/sec
Step 12823 | loss: 3.158143 | lr:2.0026e-04 | norm 0.2907 | dt 338.20ms | 1550232.40 tokens/sec
Step 12824 | loss: 3.123712 | lr:2.0022e-04 | norm 0.2920 | dt 337.97ms | 1551264.74 tokens/sec
Step 12825 | loss: 3.056011 | lr:2.0018e-04 | norm 0.2830 | dt 337.99ms | 1551187.05 tokens/sec
Step 12826 | loss: 3.058187 | lr:2.0014e-04 | norm 0.2805 | dt 339.05ms | 1546343.95 tokens/sec
Step 12827 | loss: 3.113930 | lr:2.0010e-04 | norm 0.2854 | dt 337.43ms | 1553785.75 tokens/sec
Step 12828 | loss: 3.106268 | lr:2.0006e-04 | norm 0.2679 | dt 338.12ms | 1550619.36 tokens/sec
Step 12829 | loss: 3.120341 | lr:2.0002e-04 | norm 0.2937 | dt 339.40ms | 1544735.21 tokens/sec
Step 12830 | loss: 3.061260 | lr:1.9998e-04 | norm 0.2768 | dt 337.92ms | 1551508.81 tokens/sec
Step 12831 | loss: 3.084537 | lr:1.9994e-04 | norm 0.2875 | dt 337.57ms | 1553122.92 tokens/sec
Step 12832 | loss: 3.057916 | lr:1.9990e-04 | norm 0.2804 | dt 338.28ms | 1549857.64 tokens/sec
Step 12833 | loss: 3.129206 | lr:1.9986e-04 | norm 0.2899 | dt 338.51ms | 1548818.43 tokens/sec
Step 12834 | loss: 3.065568 | lr:1.9982e-04 | norm 0.2833 | dt 338.36ms | 1549478.69 tokens/sec
Step 12835 | loss: 3.097139 | lr:1.9978e-04 | norm 0.2621 | dt 338.13ms | 1550554.86 tokens/sec
Step 12836 | loss: 3.088049 | lr:1.9974e-04 | norm 0.2949 | dt 337.53ms | 1553292.96 tokens/sec
Step 12837 | loss: 3.155929 | lr:1.9969e-04 | norm 0.2870 | dt 339.01ms | 1546546.23 tokens/sec
Step 12838 | loss: 3.104218 | lr:1.9965e-04 | norm 0.2819 | dt 337.29ms | 1554417.29 tokens/sec
Step 12839 | loss: 3.158150 | lr:1.9961e-04 | norm 0.2709 | dt 337.53ms | 1553313.81 tokens/sec
Step 12840 | loss: 3.179945 | lr:1.9957e-04 | norm 0.3505 | dt 338.04ms | 1550978.08 tokens/sec
Step 12841 | loss: 3.120392 | lr:1.9953e-04 | norm 0.3558 | dt 337.80ms | 1552072.76 tokens/sec
Step 12842 | loss: 3.211767 | lr:1.9949e-04 | norm 0.3357 | dt 336.95ms | 1555982.41 tokens/sec
Step 12843 | loss: 3.161860 | lr:1.9945e-04 | norm 0.3670 | dt 338.12ms | 1550576.72 tokens/sec
Step 12844 | loss: 3.137024 | lr:1.9941e-04 | norm 0.3047 | dt 337.44ms | 1553738.55 tokens/sec
Step 12845 | loss: 3.091811 | lr:1.9937e-04 | norm 0.3199 | dt 337.73ms | 1552377.36 tokens/sec
Step 12846 | loss: 3.178330 | lr:1.9933e-04 | norm 0.3196 | dt 338.20ms | 1550240.05 tokens/sec
Step 12847 | loss: 3.187966 | lr:1.9929e-04 | norm 0.3162 | dt 337.80ms | 1552061.81 tokens/sec
Step 12848 | loss: 3.087025 | lr:1.9925e-04 | norm 0.3342 | dt 338.09ms | 1550723.24 tokens/sec
Step 12849 | loss: 3.136857 | lr:1.9921e-04 | norm 0.3008 | dt 337.38ms | 1553988.89 tokens/sec
Step 12850 | loss: 3.122368 | lr:1.9917e-04 | norm 0.2950 | dt 337.28ms | 1554477.72 tokens/sec
Step 12851 | loss: 3.142231 | lr:1.9913e-04 | norm 0.2953 | dt 896.65ms | 584718.63 tokens/sec
Step 12852 | loss: 3.157849 | lr:1.9909e-04 | norm 0.2998 | dt 334.82ms | 1565891.17 tokens/sec
Step 12853 | loss: 3.109103 | lr:1.9905e-04 | norm 0.3198 | dt 338.36ms | 1549489.61 tokens/sec
Step 12854 | loss: 3.096875 | lr:1.9901e-04 | norm 0.2846 | dt 339.37ms | 1544888.23 tokens/sec
Step 12855 | loss: 3.156074 | lr:1.9897e-04 | norm 0.3053 | dt 337.46ms | 1553652.92 tokens/sec
Step 12856 | loss: 3.130715 | lr:1.9893e-04 | norm 0.2735 | dt 337.88ms | 1551693.83 tokens/sec
Step 12857 | loss: 3.114488 | lr:1.9889e-04 | norm 0.3048 | dt 338.62ms | 1548285.19 tokens/sec
Step 12858 | loss: 3.146845 | lr:1.9885e-04 | norm 0.2711 | dt 338.03ms | 1551032.78 tokens/sec
Step 12859 | loss: 3.215820 | lr:1.9881e-04 | norm 0.3080 | dt 337.23ms | 1554676.64 tokens/sec
Step 12860 | loss: 3.132134 | lr:1.9877e-04 | norm 0.2793 | dt 338.27ms | 1549900.24 tokens/sec
Step 12861 | loss: 3.118809 | lr:1.9872e-04 | norm 0.3216 | dt 338.01ms | 1551087.48 tokens/sec
Step 12862 | loss: 3.223383 | lr:1.9868e-04 | norm 0.2873 | dt 337.71ms | 1552485.86 tokens/sec
Step 12863 | loss: 3.126247 | lr:1.9864e-04 | norm 0.3081 | dt 337.65ms | 1552739.09 tokens/sec
Step 12864 | loss: 3.053111 | lr:1.9860e-04 | norm 0.2722 | dt 338.43ms | 1549193.78 tokens/sec
Step 12865 | loss: 3.067017 | lr:1.9856e-04 | norm 0.2822 | dt 338.80ms | 1547476.74 tokens/sec
Step 12866 | loss: 3.124567 | lr:1.9852e-04 | norm 0.3033 | dt 337.74ms | 1552319.28 tokens/sec
Step 12867 | loss: 3.047777 | lr:1.9848e-04 | norm 0.2772 | dt 336.87ms | 1556334.80 tokens/sec
Step 12868 | loss: 3.043550 | lr:1.9844e-04 | norm 0.2922 | dt 338.75ms | 1547694.57 tokens/sec
Step 12869 | loss: 3.080388 | lr:1.9840e-04 | norm 0.2957 | dt 339.41ms | 1544725.44 tokens/sec
Step 12870 | loss: 3.094807 | lr:1.9836e-04 | norm 0.2936 | dt 337.90ms | 1551599.67 tokens/sec
Step 12871 | loss: 3.080530 | lr:1.9832e-04 | norm 0.2906 | dt 336.61ms | 1557574.95 tokens/sec
Step 12872 | loss: 3.134953 | lr:1.9828e-04 | norm 0.2736 | dt 339.00ms | 1546564.72 tokens/sec
Step 12873 | loss: 3.176364 | lr:1.9824e-04 | norm 0.2965 | dt 338.09ms | 1550714.50 tokens/sec
Step 12874 | loss: 3.128944 | lr:1.9820e-04 | norm 0.3031 | dt 337.77ms | 1552226.14 tokens/sec
Step 12875 | loss: 3.129277 | lr:1.9816e-04 | norm 0.2813 | dt 338.14ms | 1550518.78 tokens/sec
Step 12876 | loss: 3.127952 | lr:1.9812e-04 | norm 0.2890 | dt 338.27ms | 1549917.72 tokens/sec
Step 12877 | loss: 3.151742 | lr:1.9808e-04 | norm 0.2692 | dt 340.02ms | 1541946.07 tokens/sec
Step 12878 | loss: 3.189651 | lr:1.9804e-04 | norm 0.3311 | dt 337.72ms | 1552420.10 tokens/sec
Step 12879 | loss: 3.251730 | lr:1.9800e-04 | norm 0.3116 | dt 337.96ms | 1551313.99 tokens/sec
Step 12880 | loss: 3.137230 | lr:1.9796e-04 | norm 0.3119 | dt 338.29ms | 1549818.31 tokens/sec
Step 12881 | loss: 3.171641 | lr:1.9792e-04 | norm 0.2796 | dt 338.21ms | 1550174.48 tokens/sec
Step 12882 | loss: 3.145784 | lr:1.9788e-04 | norm 0.3079 | dt 338.92ms | 1546956.39 tokens/sec
Step 12883 | loss: 3.183255 | lr:1.9784e-04 | norm 0.2952 | dt 339.28ms | 1545275.79 tokens/sec
Step 12884 | loss: 3.117558 | lr:1.9780e-04 | norm 0.3525 | dt 338.10ms | 1550668.57 tokens/sec
Step 12885 | loss: 3.147583 | lr:1.9776e-04 | norm 0.3041 | dt 337.26ms | 1554574.43 tokens/sec
Step 12886 | loss: 3.126693 | lr:1.9772e-04 | norm 0.3148 | dt 338.60ms | 1548376.76 tokens/sec
Step 12887 | loss: 3.116736 | lr:1.9768e-04 | norm 0.2822 | dt 337.98ms | 1551235.19 tokens/sec
Step 12888 | loss: 3.118272 | lr:1.9764e-04 | norm 0.2948 | dt 337.61ms | 1552933.17 tokens/sec
Step 12889 | loss: 3.131309 | lr:1.9760e-04 | norm 0.2890 | dt 338.06ms | 1550854.48 tokens/sec
Step 12890 | loss: 3.126872 | lr:1.9756e-04 | norm 0.3150 | dt 337.12ms | 1555180.21 tokens/sec
Step 12891 | loss: 3.163014 | lr:1.9752e-04 | norm 0.2854 | dt 338.04ms | 1550955.11 tokens/sec
Step 12892 | loss: 3.159322 | lr:1.9748e-04 | norm 0.2936 | dt 338.37ms | 1549461.22 tokens/sec
Step 12893 | loss: 3.182520 | lr:1.9743e-04 | norm 0.3233 | dt 337.88ms | 1551715.73 tokens/sec
Step 12894 | loss: 3.123235 | lr:1.9739e-04 | norm 0.2890 | dt 337.71ms | 1552463.94 tokens/sec
Step 12895 | loss: 3.143457 | lr:1.9735e-04 | norm 0.2899 | dt 338.89ms | 1547069.57 tokens/sec
Step 12896 | loss: 3.081043 | lr:1.9731e-04 | norm 0.2869 | dt 337.88ms | 1551699.30 tokens/sec
Step 12897 | loss: 3.069271 | lr:1.9727e-04 | norm 0.2853 | dt 337.83ms | 1551916.13 tokens/sec
Step 12898 | loss: 3.136321 | lr:1.9723e-04 | norm 0.2654 | dt 338.61ms | 1548372.40 tokens/sec
Step 12899 | loss: 3.152643 | lr:1.9719e-04 | norm 0.2977 | dt 339.00ms | 1546570.16 tokens/sec
Step 12900 | loss: 3.065451 | lr:1.9715e-04 | norm 0.2653 | dt 338.45ms | 1549065.01 tokens/sec
Step 12901 | loss: 3.092283 | lr:1.9711e-04 | norm 0.2908 | dt 339.06ms | 1546293.93 tokens/sec
Step 12902 | loss: 3.070164 | lr:1.9707e-04 | norm 0.2702 | dt 338.25ms | 1549985.45 tokens/sec
Step 12903 | loss: 3.143678 | lr:1.9703e-04 | norm 0.2802 | dt 338.71ms | 1547890.66 tokens/sec
Step 12904 | loss: 3.094508 | lr:1.9699e-04 | norm 0.2789 | dt 339.51ms | 1544261.16 tokens/sec
Step 12905 | loss: 3.125982 | lr:1.9695e-04 | norm 0.2700 | dt 338.66ms | 1548106.43 tokens/sec
Step 12906 | loss: 3.108409 | lr:1.9691e-04 | norm 0.2853 | dt 338.97ms | 1546689.81 tokens/sec
Step 12907 | loss: 3.078779 | lr:1.9687e-04 | norm 0.2793 | dt 338.99ms | 1546611.49 tokens/sec
Step 12908 | loss: 3.156656 | lr:1.9683e-04 | norm 0.2766 | dt 338.20ms | 1550244.42 tokens/sec
Step 12909 | loss: 3.138996 | lr:1.9679e-04 | norm 0.2803 | dt 339.21ms | 1545592.93 tokens/sec
Step 12910 | loss: 3.158046 | lr:1.9675e-04 | norm 0.2965 | dt 338.87ms | 1547187.13 tokens/sec
Step 12911 | loss: 3.153938 | lr:1.9671e-04 | norm 0.2855 | dt 341.74ms | 1534184.43 tokens/sec
Step 12912 | loss: 3.127458 | lr:1.9667e-04 | norm 0.3053 | dt 338.76ms | 1547685.85 tokens/sec
Step 12913 | loss: 3.136308 | lr:1.9663e-04 | norm 0.3022 | dt 338.28ms | 1549872.93 tokens/sec
Step 12914 | loss: 3.087439 | lr:1.9659e-04 | norm 0.3000 | dt 338.37ms | 1549439.39 tokens/sec
Step 12915 | loss: 3.163207 | lr:1.9655e-04 | norm 0.2869 | dt 339.13ms | 1545971.07 tokens/sec
Step 12916 | loss: 3.157022 | lr:1.9651e-04 | norm 0.3102 | dt 341.20ms | 1536615.83 tokens/sec
Step 12917 | loss: 3.174586 | lr:1.9647e-04 | norm 0.3035 | dt 343.35ms | 1526986.02 tokens/sec
Step 12918 | loss: 3.145534 | lr:1.9643e-04 | norm 0.3090 | dt 338.93ms | 1546913.95 tokens/sec
Step 12919 | loss: 3.154451 | lr:1.9639e-04 | norm 0.3019 | dt 1033.48ms | 507305.50 tokens/sec
Step 12920 | loss: 3.207571 | lr:1.9635e-04 | norm 0.3151 | dt 338.64ms | 1548196.89 tokens/sec
Step 12921 | loss: 3.113979 | lr:1.9631e-04 | norm 0.2884 | dt 338.38ms | 1549411.00 tokens/sec
Step 12922 | loss: 3.164157 | lr:1.9627e-04 | norm 0.2997 | dt 337.90ms | 1551588.73 tokens/sec
Step 12923 | loss: 3.131376 | lr:1.9623e-04 | norm 0.2884 | dt 339.11ms | 1546084.11 tokens/sec
Step 12924 | loss: 3.108086 | lr:1.9619e-04 | norm 0.2628 | dt 338.97ms | 1546694.16 tokens/sec
Step 12925 | loss: 3.115506 | lr:1.9615e-04 | norm 0.2966 | dt 338.31ms | 1549708.00 tokens/sec
Step 12926 | loss: 3.144803 | lr:1.9611e-04 | norm 0.2925 | dt 338.35ms | 1549521.27 tokens/sec
Step 12927 | loss: 3.136456 | lr:1.9607e-04 | norm 0.2893 | dt 338.20ms | 1550245.51 tokens/sec
Step 12928 | loss: 3.107950 | lr:1.9603e-04 | norm 0.3005 | dt 338.79ms | 1547519.21 tokens/sec
Step 12929 | loss: 3.191660 | lr:1.9599e-04 | norm 0.2997 | dt 337.44ms | 1553720.98 tokens/sec
Step 12930 | loss: 3.169870 | lr:1.9595e-04 | norm 0.3056 | dt 337.78ms | 1552162.60 tokens/sec
Step 12931 | loss: 3.112637 | lr:1.9591e-04 | norm 0.2753 | dt 338.95ms | 1546798.61 tokens/sec
Step 12932 | loss: 3.104148 | lr:1.9587e-04 | norm 0.2763 | dt 338.10ms | 1550701.37 tokens/sec
Step 12933 | loss: 3.070681 | lr:1.9583e-04 | norm 0.2795 | dt 338.29ms | 1549804.11 tokens/sec
Step 12934 | loss: 3.083810 | lr:1.9579e-04 | norm 0.2637 | dt 338.67ms | 1548087.90 tokens/sec
Step 12935 | loss: 3.115106 | lr:1.9575e-04 | norm 0.2693 | dt 337.87ms | 1551735.44 tokens/sec
Step 12936 | loss: 3.142848 | lr:1.9571e-04 | norm 0.2744 | dt 338.69ms | 1548003.99 tokens/sec
Step 12937 | loss: 3.060762 | lr:1.9567e-04 | norm 0.2687 | dt 338.55ms | 1548639.55 tokens/sec
Step 12938 | loss: 3.101098 | lr:1.9563e-04 | norm 0.2592 | dt 339.14ms | 1545949.33 tokens/sec
Step 12939 | loss: 3.081786 | lr:1.9559e-04 | norm 0.2773 | dt 338.80ms | 1547474.56 tokens/sec
Step 12940 | loss: 3.090080 | lr:1.9555e-04 | norm 0.2762 | dt 340.95ms | 1537707.53 tokens/sec
Step 12941 | loss: 3.110791 | lr:1.9551e-04 | norm 0.2861 | dt 338.71ms | 1547896.11 tokens/sec
Step 12942 | loss: 3.075974 | lr:1.9547e-04 | norm 0.2813 | dt 338.48ms | 1548962.44 tokens/sec
Step 12943 | loss: 3.111150 | lr:1.9543e-04 | norm 0.3173 | dt 340.19ms | 1541156.11 tokens/sec
Step 12944 | loss: 3.100925 | lr:1.9539e-04 | norm 0.2646 | dt 338.84ms | 1547304.70 tokens/sec
Step 12945 | loss: 3.114620 | lr:1.9535e-04 | norm 0.3042 | dt 338.82ms | 1547411.41 tokens/sec
Step 12946 | loss: 3.119627 | lr:1.9531e-04 | norm 0.2638 | dt 338.34ms | 1549597.70 tokens/sec
Step 12947 | loss: 3.129388 | lr:1.9527e-04 | norm 0.2759 | dt 339.75ms | 1543157.97 tokens/sec
Step 12948 | loss: 3.144808 | lr:1.9523e-04 | norm 0.2677 | dt 339.25ms | 1545434.34 tokens/sec
Step 12949 | loss: 3.188532 | lr:1.9519e-04 | norm 0.2941 | dt 339.00ms | 1546560.37 tokens/sec
Step 12950 | loss: 3.248171 | lr:1.9515e-04 | norm 0.3497 | dt 339.04ms | 1546384.18 tokens/sec
Step 12951 | loss: 3.131975 | lr:1.9511e-04 | norm 0.2980 | dt 339.20ms | 1545639.65 tokens/sec
Step 12952 | loss: 3.134316 | lr:1.9507e-04 | norm 0.2904 | dt 338.23ms | 1550078.32 tokens/sec
Step 12953 | loss: 3.163633 | lr:1.9503e-04 | norm 0.2941 | dt 338.95ms | 1546822.54 tokens/sec
Step 12954 | loss: 3.162803 | lr:1.9499e-04 | norm 0.2834 | dt 337.88ms | 1551713.54 tokens/sec
Step 12955 | loss: 3.119517 | lr:1.9495e-04 | norm 0.2928 | dt 338.27ms | 1549931.92 tokens/sec
Step 12956 | loss: 3.111076 | lr:1.9491e-04 | norm 0.2761 | dt 337.63ms | 1552869.57 tokens/sec
Step 12957 | loss: 3.105263 | lr:1.9487e-04 | norm 0.2807 | dt 338.65ms | 1548176.18 tokens/sec
Step 12958 | loss: 3.115767 | lr:1.9483e-04 | norm 0.2705 | dt 338.40ms | 1549332.40 tokens/sec
Step 12959 | loss: 3.106525 | lr:1.9479e-04 | norm 0.2996 | dt 337.95ms | 1551386.22 tokens/sec
Step 12960 | loss: 3.126570 | lr:1.9475e-04 | norm 0.2725 | dt 337.96ms | 1551329.31 tokens/sec
Step 12961 | loss: 3.087729 | lr:1.9471e-04 | norm 0.3391 | dt 338.12ms | 1550580.00 tokens/sec
Step 12962 | loss: 3.153190 | lr:1.9467e-04 | norm 0.2700 | dt 338.53ms | 1548697.36 tokens/sec
Step 12963 | loss: 3.138044 | lr:1.9463e-04 | norm 0.2944 | dt 338.27ms | 1549901.33 tokens/sec
Step 12964 | loss: 3.125585 | lr:1.9459e-04 | norm 0.2727 | dt 337.73ms | 1552374.07 tokens/sec
Step 12965 | loss: 3.174864 | lr:1.9455e-04 | norm 0.2951 | dt 338.81ms | 1547444.07 tokens/sec
Step 12966 | loss: 3.099420 | lr:1.9451e-04 | norm 0.2651 | dt 337.92ms | 1551494.58 tokens/sec
Step 12967 | loss: 3.125950 | lr:1.9447e-04 | norm 0.2919 | dt 338.19ms | 1550284.85 tokens/sec
Step 12968 | loss: 3.117704 | lr:1.9443e-04 | norm 0.2716 | dt 337.45ms | 1553690.24 tokens/sec
Step 12969 | loss: 3.147937 | lr:1.9439e-04 | norm 0.2846 | dt 337.41ms | 1553851.63 tokens/sec
Step 12970 | loss: 3.170339 | lr:1.9435e-04 | norm 0.2738 | dt 338.13ms | 1550571.26 tokens/sec
Step 12971 | loss: 3.082557 | lr:1.9431e-04 | norm 0.2773 | dt 338.03ms | 1550990.12 tokens/sec
Step 12972 | loss: 3.073466 | lr:1.9427e-04 | norm 0.2782 | dt 337.83ms | 1551942.42 tokens/sec
Step 12973 | loss: 3.080947 | lr:1.9423e-04 | norm 0.2665 | dt 338.06ms | 1550885.11 tokens/sec
Step 12974 | loss: 3.095442 | lr:1.9419e-04 | norm 0.2762 | dt 338.93ms | 1546871.51 tokens/sec
Step 12975 | loss: 3.122428 | lr:1.9415e-04 | norm 0.2684 | dt 338.20ms | 1550243.32 tokens/sec
Step 12976 | loss: 3.105587 | lr:1.9411e-04 | norm 0.2565 | dt 338.44ms | 1549141.40 tokens/sec
Step 12977 | loss: 3.154748 | lr:1.9407e-04 | norm 0.2880 | dt 338.03ms | 1551012.00 tokens/sec
Step 12978 | loss: 3.153611 | lr:1.9403e-04 | norm 0.2897 | dt 339.13ms | 1545977.59 tokens/sec
Step 12979 | loss: 3.124386 | lr:1.9399e-04 | norm 0.2733 | dt 338.38ms | 1549384.80 tokens/sec
Step 12980 | loss: 3.094784 | lr:1.9395e-04 | norm 0.2897 | dt 338.22ms | 1550122.02 tokens/sec
Step 12981 | loss: 3.249076 | lr:1.9391e-04 | norm 0.3520 | dt 338.26ms | 1549961.41 tokens/sec
Step 12982 | loss: 3.121631 | lr:1.9387e-04 | norm 0.3348 | dt 337.98ms | 1551226.44 tokens/sec
Step 12983 | loss: 3.193475 | lr:1.9383e-04 | norm 0.3159 | dt 338.69ms | 1547994.18 tokens/sec
Step 12984 | loss: 3.125609 | lr:1.9379e-04 | norm 0.3134 | dt 338.96ms | 1546765.97 tokens/sec
Step 12985 | loss: 3.169148 | lr:1.9375e-04 | norm 0.3410 | dt 338.42ms | 1549226.53 tokens/sec
Step 12986 | loss: 3.118631 | lr:1.9371e-04 | norm 0.3079 | dt 338.49ms | 1548922.07 tokens/sec
Step 12987 | loss: 3.039942 | lr:1.9367e-04 | norm 0.3139 | dt 338.70ms | 1547950.59 tokens/sec
Step 12988 | loss: 3.188311 | lr:1.9363e-04 | norm 0.3033 | dt 339.11ms | 1546062.37 tokens/sec
Step 12989 | loss: 3.159492 | lr:1.9359e-04 | norm 0.3183 | dt 338.11ms | 1550652.17 tokens/sec
Step 12990 | loss: 3.143327 | lr:1.9355e-04 | norm 0.2808 | dt 337.69ms | 1552591.09 tokens/sec
Step 12991 | loss: 3.151812 | lr:1.9351e-04 | norm 0.3128 | dt 338.79ms | 1547508.32 tokens/sec
Step 12992 | loss: 3.097433 | lr:1.9347e-04 | norm 0.2811 | dt 338.15ms | 1550440.07 tokens/sec
Step 12993 | loss: 3.177514 | lr:1.9343e-04 | norm 0.2849 | dt 339.06ms | 1546305.89 tokens/sec
Step 12994 | loss: 3.120456 | lr:1.9339e-04 | norm 0.3068 | dt 338.42ms | 1549203.61 tokens/sec
Step 12995 | loss: 3.159280 | lr:1.9335e-04 | norm 0.2964 | dt 338.19ms | 1550257.53 tokens/sec
Step 12996 | loss: 3.128852 | lr:1.9331e-04 | norm 0.2937 | dt 339.19ms | 1545695.05 tokens/sec
Step 12997 | loss: 3.087524 | lr:1.9327e-04 | norm 0.2772 | dt 337.67ms | 1552678.79 tokens/sec
Step 12998 | loss: 3.130167 | lr:1.9323e-04 | norm 0.2957 | dt 337.87ms | 1551746.39 tokens/sec
Step 12999 | loss: 3.111447 | lr:1.9319e-04 | norm 0.2757 | dt 338.94ms | 1546831.25 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 13000: 3.1421
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2999/10042=0.2986


ddp_rank 3: ####### Printing generated samples ####### 



ddp_rank 5: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not trying to be fluent in the English language.
P? You say this is all right? This
rank 3 sample 1 >Hello, I'm a language model, so what's the difference between C++ and C#?
- C++ is a programming language that is not native
rank 5 sample 0 >Hello, I'm a language model, but my mother is really a language model. She loves learning and reading. I think she's going to have a wonderful
rank 3 sample 2 >Hello, I'm a language model, so this isn't much of a guide. A real language is a language of two or more programmers, so in the
rank 3 sample 3 >Hello, I'm a language model, so let's look at some language models and some ways we could learn.
To illustrate the benefits of using the Crank 5 sample 1 >Hello, I'm a language model, one of the great newbies, a language nerd, but one of my students did something amazing. When i started learning



rank 5 sample 2 >Hello, I'm a language model, and you are going to be an amazing person! I'm a good user, so much fun!
I'm just
rank 5 sample 3 >Hello, I'm a language model, that I was curious to talk to.
What you want to do is this... you have someone to speak with you




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I thought I had some idea of being a "superwriter" myself. Sure, I made up a lot of


ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, there are no rules to do grammar that should be set a certain way.
As for the grammar part, the question
rank 7 sample 2 >Hello, I'm a language model, so I'll create a script by importing my code.
- To see the code, I'll click the file and
rank 7 sample 3 >Hello, I'm a language model, right? If you haven't seen her then you're not alone! And I hope you find you can.
For


rank 2 sample 0 >Hello, I'm a language model, trying to figure out how to write text in Python. It's a program, written in Python. I was wondering what
rank 2 sample 1 >Hello, I'm a language model,
I try to model everything. There a lot of things to consider while
talking about a topic and it's what
rank 2 sample 2 >Hello, I'm a language model, and I've had some problems, such as a lot of students having language anxiety, because they'd prefer to just do
rank 2 sample 3 >Hello, I'm a language model, I think learning a second language is an ideal way to develop this skill. To me that first experience of a new language




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, like you. I'd love to follow along. You see now, the problem is how to find the value of a
rank 1 sample 1 >Hello, I'm a language model, a programmer who lives in a house that isn't as built, where I work in learning the system, and I'm
rank 1 sample 2 >Hello, I'm a language model, but could anyone describe what it's like?
I have just started learning computer programming, so that was my primary ambition
rank 1 sample 3 >Hello, I'm a language model, so I'm looking at one of the famous languages. First, I would spend a short session explaining how to type a




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, meaning we're not limited to people using a single language for writing, we're able to do things like:
-
rank 6 sample 1 >Hello, I'm a language model, which means that I don’t even have access to the source text. So I don’t have to
rank 6 sample 2 >Hello, I'm a language model, but a computer language model, I'd like to see how the user would see the text in the user's brain,
rank 6 sample 3 >Hello, I'm a language model, so if you want to know why you were doing this, you need to read the whole explanation. Some explanations give specific




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I know that all these things are just as intuitive as they are the things that the author feels comfortable telling you about
rank 4 sample 1 >Hello, I'm a language model, am not a lawyer but I just found this page to be a simple list and useful! I hope you found this to


ddp_rank 0: ####### Printing generated samples ####### 

rank 4 sample 2 >Hello, I'm a language model, I read one more. In the beginning, I was taught a new language, for me, but over time became a
rank 4 sample 3 >Hello, I'm a language model, so you come on.
I didn't do very much as I intended to be. I am not a natural lear


rank 0 sample 0 >Hello, I'm a language model, so I think I've created the "what", "what" and "whole language" section for the first section
rank 0 sample 1 >Hello, I'm a language model, and am thinking about the future of human language development . We are working on an idea for a future to speak by humans
rank 0 sample 2 >Hello, I'm a language model, and I used it to show the relationship between people. I don't understand how I've done it, but it just
rank 0 sample 3 >Hello, I'm a language model, and am always thinking about how I like the sound of the words. My goal is to try and give you some language


Step 13000 | loss: 3.110337 | lr:1.9315e-04 | norm 0.2797 | dt 18688.88ms | 28053.47 tokens/sec
Step 13001 | loss: 3.116489 | lr:1.9311e-04 | norm 0.2792 | dt 333.73ms | 1570981.13 tokens/sec
Step 13002 | loss: 3.141135 | lr:1.9307e-04 | norm 0.2717 | dt 335.84ms | 1561121.08 tokens/sec
Step 13003 | loss: 3.105096 | lr:1.9303e-04 | norm 0.2677 | dt 336.92ms | 1556104.62 tokens/sec
Step 13004 | loss: 3.100994 | lr:1.9299e-04 | norm 0.2871 | dt 336.38ms | 1558630.36 tokens/sec
Step 13005 | loss: 3.086384 | lr:1.9295e-04 | norm 0.2690 | dt 335.69ms | 1561825.15 tokens/sec
Step 13006 | loss: 3.098758 | lr:1.9291e-04 | norm 0.2614 | dt 336.84ms | 1556469.19 tokens/sec
Step 13007 | loss: 3.124216 | lr:1.9287e-04 | norm 0.2834 | dt 336.57ms | 1557761.42 tokens/sec
Step 13008 | loss: 3.152988 | lr:1.9283e-04 | norm 0.2726 | dt 336.49ms | 1558123.45 tokens/sec
Step 13009 | loss: 3.085386 | lr:1.9279e-04 | norm 0.3020 | dt 336.87ms | 1556338.10 tokens/sec
Step 13010 | loss: 3.133278 | lr:1.9275e-04 | norm 0.2575 | dt 335.59ms | 1562297.84 tokens/sec
Step 13011 | loss: 3.123398 | lr:1.9271e-04 | norm 0.2989 | dt 337.39ms | 1553972.41 tokens/sec
Step 13012 | loss: 3.072398 | lr:1.9267e-04 | norm 0.2692 | dt 336.89ms | 1556235.67 tokens/sec
Step 13013 | loss: 3.045882 | lr:1.9263e-04 | norm 0.3155 | dt 336.17ms | 1559589.86 tokens/sec
Step 13014 | loss: 3.178814 | lr:1.9259e-04 | norm 0.2993 | dt 336.54ms | 1557883.92 tokens/sec
Step 13015 | loss: 3.163975 | lr:1.9255e-04 | norm 0.3228 | dt 336.26ms | 1559184.03 tokens/sec
Step 13016 | loss: 3.078336 | lr:1.9251e-04 | norm 0.2957 | dt 336.76ms | 1556876.92 tokens/sec
Step 13017 | loss: 3.160973 | lr:1.9247e-04 | norm 0.3222 | dt 336.88ms | 1556295.15 tokens/sec
Step 13018 | loss: 3.095399 | lr:1.9243e-04 | norm 0.3057 | dt 337.70ms | 1552530.80 tokens/sec
Step 13019 | loss: 3.108862 | lr:1.9239e-04 | norm 0.3092 | dt 338.40ms | 1549321.49 tokens/sec
Step 13020 | loss: 3.185017 | lr:1.9235e-04 | norm 0.3376 | dt 337.66ms | 1552727.03 tokens/sec
Step 13021 | loss: 3.152306 | lr:1.9231e-04 | norm 0.2842 | dt 337.28ms | 1554482.12 tokens/sec
Step 13022 | loss: 3.092017 | lr:1.9227e-04 | norm 0.2918 | dt 337.89ms | 1551664.27 tokens/sec
Step 13023 | loss: 3.154384 | lr:1.9224e-04 | norm 0.3230 | dt 339.08ms | 1546218.91 tokens/sec
Step 13024 | loss: 3.185239 | lr:1.9220e-04 | norm 0.3219 | dt 337.45ms | 1553668.29 tokens/sec
Step 13025 | loss: 3.123411 | lr:1.9216e-04 | norm 0.3250 | dt 338.42ms | 1549225.43 tokens/sec
Step 13026 | loss: 3.107619 | lr:1.9212e-04 | norm 0.3150 | dt 337.43ms | 1553751.72 tokens/sec
Step 13027 | loss: 3.134990 | lr:1.9208e-04 | norm 0.3007 | dt 338.17ms | 1550360.27 tokens/sec
Step 13028 | loss: 3.115480 | lr:1.9204e-04 | norm 0.3005 | dt 338.29ms | 1549798.65 tokens/sec
Step 13029 | loss: 3.096270 | lr:1.9200e-04 | norm 0.2788 | dt 337.86ms | 1551805.52 tokens/sec
Step 13030 | loss: 3.095310 | lr:1.9196e-04 | norm 0.2941 | dt 338.41ms | 1549288.74 tokens/sec
Step 13031 | loss: 3.131137 | lr:1.9192e-04 | norm 0.2652 | dt 337.86ms | 1551777.05 tokens/sec
Step 13032 | loss: 3.122797 | lr:1.9188e-04 | norm 0.2926 | dt 337.29ms | 1554396.41 tokens/sec
Step 13033 | loss: 3.130845 | lr:1.9184e-04 | norm 0.2828 | dt 336.88ms | 1556312.77 tokens/sec
Step 13034 | loss: 3.106813 | lr:1.9180e-04 | norm 0.2633 | dt 337.20ms | 1554829.43 tokens/sec
Step 13035 | loss: 3.085626 | lr:1.9176e-04 | norm 0.2807 | dt 337.73ms | 1552391.61 tokens/sec
Step 13036 | loss: 3.142198 | lr:1.9172e-04 | norm 0.3031 | dt 338.18ms | 1550311.09 tokens/sec
Step 13037 | loss: 3.045160 | lr:1.9168e-04 | norm 0.2599 | dt 336.60ms | 1557615.77 tokens/sec
Step 13038 | loss: 3.100796 | lr:1.9164e-04 | norm 0.2842 | dt 338.92ms | 1546957.48 tokens/sec
Step 13039 | loss: 3.114199 | lr:1.9160e-04 | norm 0.2939 | dt 337.56ms | 1553173.38 tokens/sec
Step 13040 | loss: 3.119863 | lr:1.9156e-04 | norm 0.2597 | dt 900.34ms | 582322.34 tokens/sec
Step 13041 | loss: 3.123774 | lr:1.9152e-04 | norm 0.2892 | dt 335.77ms | 1561462.50 tokens/sec
Step 13042 | loss: 3.099684 | lr:1.9148e-04 | norm 0.2516 | dt 338.84ms | 1547323.21 tokens/sec
Step 13043 | loss: 3.138590 | lr:1.9144e-04 | norm 0.2924 | dt 338.84ms | 1547294.91 tokens/sec
Step 13044 | loss: 3.075927 | lr:1.9140e-04 | norm 0.2691 | dt 337.64ms | 1552787.33 tokens/sec
Step 13045 | loss: 3.061395 | lr:1.9136e-04 | norm 0.2722 | dt 337.13ms | 1555171.41 tokens/sec
Step 13046 | loss: 3.002907 | lr:1.9132e-04 | norm 0.2796 | dt 337.89ms | 1551637.99 tokens/sec
Step 13047 | loss: 3.053676 | lr:1.9128e-04 | norm 0.2611 | dt 337.57ms | 1553127.31 tokens/sec
Step 13048 | loss: 3.133977 | lr:1.9124e-04 | norm 0.3269 | dt 338.40ms | 1549333.49 tokens/sec
Step 13049 | loss: 3.122071 | lr:1.9120e-04 | norm 0.3142 | dt 337.66ms | 1552693.04 tokens/sec
Step 13050 | loss: 3.168174 | lr:1.9116e-04 | norm 0.2901 | dt 338.26ms | 1549970.15 tokens/sec
Step 13051 | loss: 3.076644 | lr:1.9112e-04 | norm 0.2830 | dt 337.56ms | 1553148.15 tokens/sec
Step 13052 | loss: 3.090194 | lr:1.9108e-04 | norm 0.2751 | dt 337.88ms | 1551705.87 tokens/sec
Step 13053 | loss: 3.163222 | lr:1.9104e-04 | norm 0.2820 | dt 338.37ms | 1549445.94 tokens/sec
Step 13054 | loss: 3.092940 | lr:1.9101e-04 | norm 0.3010 | dt 338.16ms | 1550405.09 tokens/sec
Step 13055 | loss: 3.098184 | lr:1.9097e-04 | norm 0.2987 | dt 338.18ms | 1550326.39 tokens/sec
Step 13056 | loss: 3.078045 | lr:1.9093e-04 | norm 0.2936 | dt 338.08ms | 1550770.27 tokens/sec
Step 13057 | loss: 3.111952 | lr:1.9089e-04 | norm 0.3016 | dt 338.41ms | 1549266.91 tokens/sec
Step 13058 | loss: 3.131280 | lr:1.9085e-04 | norm 0.3209 | dt 337.74ms | 1552364.21 tokens/sec
Step 13059 | loss: 3.139844 | lr:1.9081e-04 | norm 0.3071 | dt 338.34ms | 1549572.59 tokens/sec
Step 13060 | loss: 3.103218 | lr:1.9077e-04 | norm 0.3475 | dt 337.88ms | 1551720.11 tokens/sec
Step 13061 | loss: 3.096852 | lr:1.9073e-04 | norm 0.3412 | dt 337.93ms | 1551471.59 tokens/sec
Step 13062 | loss: 3.123317 | lr:1.9069e-04 | norm 0.3438 | dt 338.09ms | 1550718.87 tokens/sec
Step 13063 | loss: 3.153633 | lr:1.9065e-04 | norm 0.3572 | dt 338.40ms | 1549295.29 tokens/sec
Step 13064 | loss: 3.180668 | lr:1.9061e-04 | norm 0.3045 | dt 337.72ms | 1552419.01 tokens/sec
Step 13065 | loss: 3.106612 | lr:1.9057e-04 | norm 0.3087 | dt 339.43ms | 1544618.03 tokens/sec
Step 13066 | loss: 3.150612 | lr:1.9053e-04 | norm 0.3018 | dt 339.15ms | 1545884.12 tokens/sec
Step 13067 | loss: 3.128612 | lr:1.9049e-04 | norm 0.2929 | dt 339.14ms | 1545926.51 tokens/sec
Step 13068 | loss: 3.080632 | lr:1.9045e-04 | norm 0.3068 | dt 338.37ms | 1549454.67 tokens/sec
Step 13069 | loss: 3.168811 | lr:1.9041e-04 | norm 0.3286 | dt 339.71ms | 1543354.00 tokens/sec
Step 13070 | loss: 3.147734 | lr:1.9037e-04 | norm 0.2957 | dt 338.84ms | 1547291.64 tokens/sec
Step 13071 | loss: 3.077926 | lr:1.9033e-04 | norm 0.3131 | dt 339.42ms | 1544651.66 tokens/sec
Step 13072 | loss: 3.123419 | lr:1.9029e-04 | norm 0.2965 | dt 338.15ms | 1550472.86 tokens/sec
Step 13073 | loss: 3.121440 | lr:1.9025e-04 | norm 0.2879 | dt 338.24ms | 1550044.45 tokens/sec
Step 13074 | loss: 3.028166 | lr:1.9021e-04 | norm 0.3156 | dt 338.63ms | 1548280.82 tokens/sec
Step 13075 | loss: 3.082903 | lr:1.9017e-04 | norm 0.3058 | dt 340.35ms | 1540447.89 tokens/sec
Step 13076 | loss: 3.138479 | lr:1.9013e-04 | norm 0.2925 | dt 337.43ms | 1553780.26 tokens/sec
Step 13077 | loss: 3.138689 | lr:1.9010e-04 | norm 0.3174 | dt 337.93ms | 1551471.59 tokens/sec
Step 13078 | loss: 3.095693 | lr:1.9006e-04 | norm 0.2889 | dt 338.57ms | 1548540.31 tokens/sec
Step 13079 | loss: 3.127039 | lr:1.9002e-04 | norm 0.3265 | dt 337.87ms | 1551731.06 tokens/sec
Step 13080 | loss: 3.083799 | lr:1.8998e-04 | norm 0.2820 | dt 338.37ms | 1549437.20 tokens/sec
Step 13081 | loss: 3.118118 | lr:1.8994e-04 | norm 0.3439 | dt 338.15ms | 1550471.77 tokens/sec
Step 13082 | loss: 3.092919 | lr:1.8990e-04 | norm 0.2638 | dt 338.44ms | 1549120.66 tokens/sec
Step 13083 | loss: 3.132072 | lr:1.8986e-04 | norm 0.2986 | dt 338.05ms | 1550938.70 tokens/sec
Step 13084 | loss: 3.143802 | lr:1.8982e-04 | norm 0.2920 | dt 338.76ms | 1547673.87 tokens/sec
Step 13085 | loss: 3.153504 | lr:1.8978e-04 | norm 0.3071 | dt 337.60ms | 1552988.01 tokens/sec
Step 13086 | loss: 3.139076 | lr:1.8974e-04 | norm 0.3187 | dt 338.07ms | 1550834.79 tokens/sec
Step 13087 | loss: 3.118496 | lr:1.8970e-04 | norm 0.2918 | dt 337.85ms | 1551848.23 tokens/sec
Step 13088 | loss: 3.107336 | lr:1.8966e-04 | norm 0.2935 | dt 338.16ms | 1550426.95 tokens/sec
Step 13089 | loss: 3.181882 | lr:1.8962e-04 | norm 0.3538 | dt 337.52ms | 1553362.09 tokens/sec
Step 13090 | loss: 3.120779 | lr:1.8958e-04 | norm 0.3091 | dt 338.03ms | 1550992.30 tokens/sec
Step 13091 | loss: 3.115739 | lr:1.8954e-04 | norm 0.3326 | dt 338.06ms | 1550873.07 tokens/sec
Step 13092 | loss: 3.094521 | lr:1.8950e-04 | norm 0.3143 | dt 337.95ms | 1551388.41 tokens/sec
Step 13093 | loss: 3.171030 | lr:1.8946e-04 | norm 0.2994 | dt 338.19ms | 1550260.81 tokens/sec
Step 13094 | loss: 3.105827 | lr:1.8942e-04 | norm 0.3054 | dt 337.90ms | 1551627.04 tokens/sec
Step 13095 | loss: 3.157434 | lr:1.8938e-04 | norm 0.2923 | dt 338.63ms | 1548256.84 tokens/sec
Step 13096 | loss: 3.158726 | lr:1.8935e-04 | norm 0.3073 | dt 338.18ms | 1550339.50 tokens/sec
Step 13097 | loss: 3.120329 | lr:1.8931e-04 | norm 0.2783 | dt 338.59ms | 1548430.18 tokens/sec
Step 13098 | loss: 3.098090 | lr:1.8927e-04 | norm 0.3046 | dt 338.30ms | 1549751.69 tokens/sec
Step 13099 | loss: 3.156956 | lr:1.8923e-04 | norm 0.2896 | dt 338.06ms | 1550869.79 tokens/sec
Step 13100 | loss: 3.079794 | lr:1.8919e-04 | norm 0.2975 | dt 338.80ms | 1547498.52 tokens/sec
Step 13101 | loss: 3.128264 | lr:1.8915e-04 | norm 0.2928 | dt 339.13ms | 1545989.54 tokens/sec
Step 13102 | loss: 3.164614 | lr:1.8911e-04 | norm 0.2867 | dt 338.51ms | 1548819.53 tokens/sec
Step 13103 | loss: 3.120298 | lr:1.8907e-04 | norm 0.2964 | dt 338.64ms | 1548238.31 tokens/sec
Step 13104 | loss: 3.140227 | lr:1.8903e-04 | norm 0.2799 | dt 338.34ms | 1549575.87 tokens/sec
Step 13105 | loss: 3.140635 | lr:1.8899e-04 | norm 0.2898 | dt 338.36ms | 1549491.79 tokens/sec
Step 13106 | loss: 3.074436 | lr:1.8895e-04 | norm 0.2911 | dt 338.37ms | 1549444.84 tokens/sec
Step 13107 | loss: 3.156933 | lr:1.8891e-04 | norm 0.2944 | dt 337.70ms | 1552514.36 tokens/sec
Step 13108 | loss: 3.135247 | lr:1.8887e-04 | norm 0.3026 | dt 338.10ms | 1550676.22 tokens/sec
Step 13109 | loss: 3.074896 | lr:1.8883e-04 | norm 0.2846 | dt 1044.58ms | 501914.23 tokens/sec
Step 13110 | loss: 3.089143 | lr:1.8879e-04 | norm 0.2858 | dt 337.63ms | 1552831.19 tokens/sec
Step 13111 | loss: 3.058486 | lr:1.8875e-04 | norm 0.2728 | dt 337.69ms | 1552591.09 tokens/sec
Step 13112 | loss: 3.118639 | lr:1.8871e-04 | norm 0.2886 | dt 339.22ms | 1545587.50 tokens/sec
Step 13113 | loss: 3.127248 | lr:1.8868e-04 | norm 0.2795 | dt 338.04ms | 1550968.24 tokens/sec
Step 13114 | loss: 3.078760 | lr:1.8864e-04 | norm 0.2856 | dt 338.17ms | 1550361.36 tokens/sec
Step 13115 | loss: 3.102944 | lr:1.8860e-04 | norm 0.2849 | dt 338.61ms | 1548362.59 tokens/sec
Step 13116 | loss: 3.141557 | lr:1.8856e-04 | norm 0.2690 | dt 337.41ms | 1553845.04 tokens/sec
Step 13117 | loss: 3.148044 | lr:1.8852e-04 | norm 0.2714 | dt 337.52ms | 1553362.09 tokens/sec
Step 13118 | loss: 3.145069 | lr:1.8848e-04 | norm 0.2640 | dt 338.07ms | 1550830.42 tokens/sec
Step 13119 | loss: 3.103329 | lr:1.8844e-04 | norm 0.2587 | dt 336.99ms | 1555801.87 tokens/sec
Step 13120 | loss: 3.099272 | lr:1.8840e-04 | norm 0.2690 | dt 337.63ms | 1552858.60 tokens/sec
Step 13121 | loss: 3.118995 | lr:1.8836e-04 | norm 0.2828 | dt 338.35ms | 1549547.48 tokens/sec
Step 13122 | loss: 3.116223 | lr:1.8832e-04 | norm 0.2723 | dt 337.87ms | 1551723.39 tokens/sec
Step 13123 | loss: 3.149258 | lr:1.8828e-04 | norm 0.2853 | dt 337.25ms | 1554577.72 tokens/sec
Step 13124 | loss: 3.136742 | lr:1.8824e-04 | norm 0.2699 | dt 337.96ms | 1551344.63 tokens/sec
Step 13125 | loss: 3.105070 | lr:1.8820e-04 | norm 0.2673 | dt 340.13ms | 1541429.42 tokens/sec
Step 13126 | loss: 3.206682 | lr:1.8816e-04 | norm 0.2751 | dt 338.10ms | 1550669.66 tokens/sec
Step 13127 | loss: 3.193197 | lr:1.8812e-04 | norm 0.2893 | dt 338.25ms | 1550005.11 tokens/sec
Step 13128 | loss: 3.147149 | lr:1.8809e-04 | norm 0.2992 | dt 337.95ms | 1551355.57 tokens/sec
Step 13129 | loss: 3.133816 | lr:1.8805e-04 | norm 0.2905 | dt 337.67ms | 1552653.57 tokens/sec
Step 13130 | loss: 3.119241 | lr:1.8801e-04 | norm 0.2915 | dt 337.81ms | 1552033.33 tokens/sec
Step 13131 | loss: 3.100994 | lr:1.8797e-04 | norm 0.2799 | dt 337.46ms | 1553613.41 tokens/sec
Step 13132 | loss: 3.222688 | lr:1.8793e-04 | norm 0.3191 | dt 338.93ms | 1546878.04 tokens/sec
Step 13133 | loss: 3.146060 | lr:1.8789e-04 | norm 0.3211 | dt 337.08ms | 1555388.10 tokens/sec
Step 13134 | loss: 3.104587 | lr:1.8785e-04 | norm 0.3716 | dt 338.34ms | 1549600.98 tokens/sec
Step 13135 | loss: 3.083181 | lr:1.8781e-04 | norm 0.3234 | dt 338.40ms | 1549326.95 tokens/sec
Step 13136 | loss: 3.147781 | lr:1.8777e-04 | norm 0.3386 | dt 337.97ms | 1551305.23 tokens/sec
Step 13137 | loss: 3.105716 | lr:1.8773e-04 | norm 0.3041 | dt 338.35ms | 1549534.37 tokens/sec
Step 13138 | loss: 3.137727 | lr:1.8769e-04 | norm 0.2904 | dt 338.67ms | 1548078.09 tokens/sec
Step 13139 | loss: 3.126806 | lr:1.8765e-04 | norm 0.2923 | dt 338.05ms | 1550911.36 tokens/sec
Step 13140 | loss: 3.112878 | lr:1.8761e-04 | norm 0.3064 | dt 337.65ms | 1552735.80 tokens/sec
Step 13141 | loss: 3.282920 | lr:1.8757e-04 | norm 0.3046 | dt 337.97ms | 1551273.49 tokens/sec
Step 13142 | loss: 3.094286 | lr:1.8754e-04 | norm 0.3053 | dt 338.28ms | 1549858.73 tokens/sec
Step 13143 | loss: 3.139191 | lr:1.8750e-04 | norm 0.3477 | dt 338.20ms | 1550241.14 tokens/sec
Step 13144 | loss: 3.135879 | lr:1.8746e-04 | norm 0.3453 | dt 337.77ms | 1552221.76 tokens/sec
Step 13145 | loss: 3.101233 | lr:1.8742e-04 | norm 0.2979 | dt 337.54ms | 1553249.08 tokens/sec
Step 13146 | loss: 3.085522 | lr:1.8738e-04 | norm 0.3435 | dt 337.67ms | 1552642.61 tokens/sec
Step 13147 | loss: 3.078457 | lr:1.8734e-04 | norm 0.2912 | dt 338.67ms | 1548066.10 tokens/sec
Step 13148 | loss: 3.090830 | lr:1.8730e-04 | norm 0.3218 | dt 338.61ms | 1548372.40 tokens/sec
Step 13149 | loss: 3.074223 | lr:1.8726e-04 | norm 0.2814 | dt 337.38ms | 1553986.69 tokens/sec
Step 13150 | loss: 3.083619 | lr:1.8722e-04 | norm 0.2970 | dt 337.98ms | 1551235.19 tokens/sec
Step 13151 | loss: 3.059874 | lr:1.8718e-04 | norm 0.2699 | dt 337.94ms | 1551426.72 tokens/sec
Step 13152 | loss: 3.107950 | lr:1.8714e-04 | norm 0.3259 | dt 338.00ms | 1551140.00 tokens/sec
Step 13153 | loss: 3.114647 | lr:1.8710e-04 | norm 0.2863 | dt 337.80ms | 1552047.57 tokens/sec
Step 13154 | loss: 3.004588 | lr:1.8706e-04 | norm 0.2760 | dt 337.56ms | 1553181.06 tokens/sec
Step 13155 | loss: 3.082946 | lr:1.8703e-04 | norm 0.2768 | dt 337.75ms | 1552315.99 tokens/sec
Step 13156 | loss: 3.148637 | lr:1.8699e-04 | norm 0.3094 | dt 337.92ms | 1551516.47 tokens/sec
Step 13157 | loss: 3.145308 | lr:1.8695e-04 | norm 0.2721 | dt 337.73ms | 1552376.27 tokens/sec
Step 13158 | loss: 3.125984 | lr:1.8691e-04 | norm 0.3082 | dt 337.95ms | 1551373.09 tokens/sec
Step 13159 | loss: 3.133178 | lr:1.8687e-04 | norm 0.2943 | dt 337.09ms | 1555358.40 tokens/sec
Step 13160 | loss: 3.161622 | lr:1.8683e-04 | norm 0.2923 | dt 337.98ms | 1551218.78 tokens/sec
Step 13161 | loss: 3.165430 | lr:1.8679e-04 | norm 0.2845 | dt 338.03ms | 1551007.62 tokens/sec
Step 13162 | loss: 3.132476 | lr:1.8675e-04 | norm 0.2741 | dt 337.81ms | 1552019.09 tokens/sec
Step 13163 | loss: 3.148631 | lr:1.8671e-04 | norm 0.2841 | dt 337.43ms | 1553768.19 tokens/sec
Step 13164 | loss: 3.117053 | lr:1.8667e-04 | norm 0.2598 | dt 337.61ms | 1552921.11 tokens/sec
Step 13165 | loss: 3.161396 | lr:1.8663e-04 | norm 0.2851 | dt 338.40ms | 1549334.59 tokens/sec
Step 13166 | loss: 3.172944 | lr:1.8659e-04 | norm 0.2714 | dt 337.78ms | 1552137.40 tokens/sec
Step 13167 | loss: 3.126989 | lr:1.8656e-04 | norm 0.3050 | dt 337.52ms | 1553348.92 tokens/sec
Step 13168 | loss: 3.109115 | lr:1.8652e-04 | norm 0.2845 | dt 337.94ms | 1551415.77 tokens/sec
Step 13169 | loss: 3.142092 | lr:1.8648e-04 | norm 0.3152 | dt 338.17ms | 1550372.29 tokens/sec
Step 13170 | loss: 3.132553 | lr:1.8644e-04 | norm 0.3044 | dt 337.71ms | 1552460.65 tokens/sec
Step 13171 | loss: 3.113999 | lr:1.8640e-04 | norm 0.2914 | dt 337.37ms | 1554051.48 tokens/sec
Step 13172 | loss: 3.140556 | lr:1.8636e-04 | norm 0.3047 | dt 339.98ms | 1542107.18 tokens/sec
Step 13173 | loss: 3.197675 | lr:1.8632e-04 | norm 0.2923 | dt 337.92ms | 1551524.14 tokens/sec
Step 13174 | loss: 3.151325 | lr:1.8628e-04 | norm 0.3151 | dt 338.21ms | 1550182.13 tokens/sec
Step 13175 | loss: 3.119015 | lr:1.8624e-04 | norm 0.2807 | dt 337.91ms | 1551578.87 tokens/sec
Step 13176 | loss: 3.208898 | lr:1.8620e-04 | norm 0.3234 | dt 338.36ms | 1549490.70 tokens/sec
Step 13177 | loss: 3.123127 | lr:1.8616e-04 | norm 0.2860 | dt 338.00ms | 1551152.03 tokens/sec
Step 13178 | loss: 3.140563 | lr:1.8613e-04 | norm 0.3063 | dt 338.51ms | 1548800.98 tokens/sec
Step 13179 | loss: 3.170457 | lr:1.8609e-04 | norm 0.2650 | dt 337.31ms | 1554325.00 tokens/sec
Step 13180 | loss: 3.116115 | lr:1.8605e-04 | norm 0.2997 | dt 338.18ms | 1550341.69 tokens/sec
Step 13181 | loss: 3.075358 | lr:1.8601e-04 | norm 0.2783 | dt 338.27ms | 1549900.24 tokens/sec
Step 13182 | loss: 3.105907 | lr:1.8597e-04 | norm 0.2774 | dt 338.18ms | 1550325.29 tokens/sec
Step 13183 | loss: 3.090163 | lr:1.8593e-04 | norm 0.2821 | dt 338.00ms | 1551131.25 tokens/sec
Step 13184 | loss: 3.072376 | lr:1.8589e-04 | norm 0.2594 | dt 338.59ms | 1548458.53 tokens/sec
Step 13185 | loss: 3.089153 | lr:1.8585e-04 | norm 0.2888 | dt 338.41ms | 1549289.83 tokens/sec
Step 13186 | loss: 3.099493 | lr:1.8581e-04 | norm 0.2798 | dt 338.08ms | 1550781.20 tokens/sec
Step 13187 | loss: 3.080084 | lr:1.8577e-04 | norm 0.3187 | dt 338.12ms | 1550578.91 tokens/sec
Step 13188 | loss: 3.041635 | lr:1.8573e-04 | norm 0.2628 | dt 338.05ms | 1550936.52 tokens/sec
Step 13189 | loss: 3.094110 | lr:1.8570e-04 | norm 0.3044 | dt 338.02ms | 1551068.88 tokens/sec
Step 13190 | loss: 3.132414 | lr:1.8566e-04 | norm 0.2985 | dt 338.72ms | 1547867.78 tokens/sec
Step 13191 | loss: 3.096810 | lr:1.8562e-04 | norm 0.3099 | dt 337.73ms | 1552379.55 tokens/sec
Step 13192 | loss: 3.102258 | lr:1.8558e-04 | norm 0.2974 | dt 339.06ms | 1546318.94 tokens/sec
Step 13193 | loss: 3.195497 | lr:1.8554e-04 | norm 0.3280 | dt 338.21ms | 1550205.07 tokens/sec
Step 13194 | loss: 3.131265 | lr:1.8550e-04 | norm 0.2942 | dt 338.38ms | 1549394.63 tokens/sec
Step 13195 | loss: 3.116477 | lr:1.8546e-04 | norm 0.3216 | dt 337.51ms | 1553389.52 tokens/sec
Step 13196 | loss: 3.097647 | lr:1.8542e-04 | norm 0.3040 | dt 338.21ms | 1550164.64 tokens/sec
Step 13197 | loss: 3.129537 | lr:1.8538e-04 | norm 0.2841 | dt 337.97ms | 1551294.29 tokens/sec
Step 13198 | loss: 3.137937 | lr:1.8534e-04 | norm 0.2995 | dt 338.93ms | 1546873.68 tokens/sec
Step 13199 | loss: 3.152827 | lr:1.8530e-04 | norm 0.2927 | dt 338.76ms | 1547670.60 tokens/sec
Step 13200 | loss: 3.151919 | lr:1.8527e-04 | norm 0.2822 | dt 338.46ms | 1549055.19 tokens/sec
Step 13201 | loss: 3.126814 | lr:1.8523e-04 | norm 0.2842 | dt 337.95ms | 1551390.60 tokens/sec
Step 13202 | loss: 3.176113 | lr:1.8519e-04 | norm 0.2805 | dt 338.37ms | 1549467.77 tokens/sec
Step 13203 | loss: 3.120315 | lr:1.8515e-04 | norm 0.2725 | dt 338.31ms | 1549704.72 tokens/sec
Step 13204 | loss: 3.197901 | lr:1.8511e-04 | norm 0.2760 | dt 337.54ms | 1553273.22 tokens/sec
Step 13205 | loss: 3.143295 | lr:1.8507e-04 | norm 0.2910 | dt 340.37ms | 1540333.52 tokens/sec
Step 13206 | loss: 3.090830 | lr:1.8503e-04 | norm 0.2760 | dt 337.97ms | 1551308.51 tokens/sec
Step 13207 | loss: 3.089366 | lr:1.8499e-04 | norm 0.2760 | dt 338.64ms | 1548223.05 tokens/sec
Step 13208 | loss: 3.127880 | lr:1.8495e-04 | norm 0.2886 | dt 337.46ms | 1553619.99 tokens/sec
Step 13209 | loss: 3.098273 | lr:1.8492e-04 | norm 0.2817 | dt 337.89ms | 1551645.66 tokens/sec
Step 13210 | loss: 3.147019 | lr:1.8488e-04 | norm 0.3010 | dt 337.95ms | 1551382.94 tokens/sec
Step 13211 | loss: 3.091089 | lr:1.8484e-04 | norm 0.2838 | dt 337.83ms | 1551915.04 tokens/sec
Step 13212 | loss: 3.131011 | lr:1.8480e-04 | norm 0.2985 | dt 337.05ms | 1555536.64 tokens/sec
Step 13213 | loss: 3.127627 | lr:1.8476e-04 | norm 0.2897 | dt 338.42ms | 1549240.71 tokens/sec
Step 13214 | loss: 3.092323 | lr:1.8472e-04 | norm 0.2997 | dt 338.20ms | 1550233.49 tokens/sec
Step 13215 | loss: 3.102994 | lr:1.8468e-04 | norm 0.2661 | dt 337.73ms | 1552385.03 tokens/sec
Step 13216 | loss: 3.109801 | lr:1.8464e-04 | norm 0.2869 | dt 338.17ms | 1550389.78 tokens/sec
Step 13217 | loss: 3.091718 | lr:1.8460e-04 | norm 0.3034 | dt 337.40ms | 1553920.80 tokens/sec
Step 13218 | loss: 3.057553 | lr:1.8456e-04 | norm 0.2937 | dt 337.69ms | 1552564.78 tokens/sec
Step 13219 | loss: 3.059649 | lr:1.8453e-04 | norm 0.3004 | dt 338.18ms | 1550308.90 tokens/sec
Step 13220 | loss: 3.087173 | lr:1.8449e-04 | norm 0.3125 | dt 337.79ms | 1552091.39 tokens/sec
Step 13221 | loss: 3.090310 | lr:1.8445e-04 | norm 0.2717 | dt 337.70ms | 1552522.03 tokens/sec
Step 13222 | loss: 3.061625 | lr:1.8441e-04 | norm 0.3112 | dt 337.84ms | 1551875.61 tokens/sec
Step 13223 | loss: 3.109049 | lr:1.8437e-04 | norm 0.3031 | dt 337.76ms | 1552243.67 tokens/sec
Step 13224 | loss: 3.087136 | lr:1.8433e-04 | norm 0.2961 | dt 337.79ms | 1552110.01 tokens/sec
Step 13225 | loss: 3.059121 | lr:1.8429e-04 | norm 0.2863 | dt 338.96ms | 1546739.86 tokens/sec
Step 13226 | loss: 3.033381 | lr:1.8425e-04 | norm 0.2869 | dt 337.55ms | 1553223.85 tokens/sec
Step 13227 | loss: 3.057789 | lr:1.8421e-04 | norm 0.2823 | dt 338.15ms | 1550478.33 tokens/sec
Step 13228 | loss: 3.099908 | lr:1.8418e-04 | norm 0.2959 | dt 338.08ms | 1550779.02 tokens/sec
Step 13229 | loss: 3.136121 | lr:1.8414e-04 | norm 0.2814 | dt 903.57ms | 580240.34 tokens/sec
Step 13230 | loss: 3.138307 | lr:1.8410e-04 | norm 0.2968 | dt 334.92ms | 1565391.79 tokens/sec
Step 13231 | loss: 3.068833 | lr:1.8406e-04 | norm 0.2785 | dt 338.67ms | 1548057.38 tokens/sec
Step 13232 | loss: 3.148869 | lr:1.8402e-04 | norm 0.3014 | dt 339.18ms | 1545770.02 tokens/sec
Step 13233 | loss: 3.127334 | lr:1.8398e-04 | norm 0.2898 | dt 337.10ms | 1555299.00 tokens/sec
Step 13234 | loss: 3.081373 | lr:1.8394e-04 | norm 0.2912 | dt 339.56ms | 1544010.69 tokens/sec
Step 13235 | loss: 3.111396 | lr:1.8390e-04 | norm 0.3068 | dt 338.96ms | 1546732.24 tokens/sec
Step 13236 | loss: 3.120021 | lr:1.8386e-04 | norm 0.2711 | dt 339.12ms | 1546039.54 tokens/sec
Step 13237 | loss: 3.168440 | lr:1.8383e-04 | norm 0.3233 | dt 338.51ms | 1548824.98 tokens/sec
Step 13238 | loss: 3.203717 | lr:1.8379e-04 | norm 0.3169 | dt 338.76ms | 1547690.21 tokens/sec
Step 13239 | loss: 3.122060 | lr:1.8375e-04 | norm 0.3010 | dt 338.16ms | 1550405.09 tokens/sec
Step 13240 | loss: 3.176890 | lr:1.8371e-04 | norm 0.3062 | dt 338.97ms | 1546719.19 tokens/sec
Step 13241 | loss: 3.139820 | lr:1.8367e-04 | norm 0.2867 | dt 338.82ms | 1547398.34 tokens/sec
Step 13242 | loss: 3.130508 | lr:1.8363e-04 | norm 0.2964 | dt 338.96ms | 1546768.14 tokens/sec
Step 13243 | loss: 3.150192 | lr:1.8359e-04 | norm 0.2874 | dt 339.18ms | 1545764.59 tokens/sec
Step 13244 | loss: 3.158780 | lr:1.8355e-04 | norm 0.2897 | dt 339.79ms | 1542994.47 tokens/sec
Step 13245 | loss: 3.075162 | lr:1.8351e-04 | norm 0.3049 | dt 337.93ms | 1551479.26 tokens/sec
Step 13246 | loss: 3.131661 | lr:1.8348e-04 | norm 0.2688 | dt 338.34ms | 1549581.33 tokens/sec
Step 13247 | loss: 3.120363 | lr:1.8344e-04 | norm 0.2834 | dt 339.81ms | 1542869.97 tokens/sec
Step 13248 | loss: 3.108420 | lr:1.8340e-04 | norm 0.2765 | dt 338.83ms | 1547344.99 tokens/sec
Step 13249 | loss: 3.125167 | lr:1.8336e-04 | norm 0.3116 | dt 339.77ms | 1543045.36 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 13250: 3.1379
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2947/10042=0.2935


ddp_rank 5: ####### Printing generated samples ####### 



ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but a programmer. It's not like how you read the code of someone else, but it's something I learned.
rank 3 sample 0 >Hello, I'm a language model, so I'm not the one who just want a computer model of the class... but that guy at heart. It does
rank 5 sample 1 >Hello, I'm a language model, because, once we see our program, we understand it and use it to complete all of the activities we were supposed to
rank 3 sample 1 >Hello, I'm a language model, so my favorite language is Dutch for programming, it's so easy.
As a language model, I really have no
rank 5 sample 2 >Hello, I'm a language model, and I was thinking where I should first learn the basics of Haskell.
And, now, I've got a new
rank 3 sample 2 >Hello, I'm a language model, so you need to come up with a vocabulary!
A: I've heard from a language where the words come in
rank 5 sample 3 >Hello, I'm a language model, here is the picture of some of the things we use:
In this diagram, you see orange and yellow, two


rank 3 sample 3 >Hello, I'm a language model, so a lot of people tend to think that I'm too old and I'm too young. I try and understand what




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, for instance. This is just one of many ways you can make your business easier on the Internet.
A language model
rank 2 sample 1 >Hello, I'm a language model, but I prefer that we're all fluent around the end of this semester. I'm not really fluent and that's how
rank 2 sample 2 >Hello, I'm a language model, and I've never had to think beyond being a programmer. What is different from that? I'll bet you didn't
rank 2 sample 3 >Hello, I'm a language model, but that kinda sucks, because I'm also a machine language guy. I really do want help figuring out how I know




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I am going to explain how to take a good look at the Python
Lists to go beyond just the name
rank 7 sample 1 >Hello, I'm a language model, writing simple scripts. I started programming first. I teach children how to write simple functions, then I teach them how to
rank 7 sample 2 >Hello, I'm a language model, but I don't have a job-like enthusiasm for a language. When I first started learning a language, like French
rank 7 sample 3 >Hello, I'm a language model, and you guys are trying to work to bring them to life.
My name is Bill, and I'm a part




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, if you're ever a seasoned developer... I'm the first person I know how to write computer code.
I wrote
rank 6 sample 1 >Hello, I'm a language model, so I know what that is. But first I've got to look something up and find the code that is telling me
rank 6 sample 2 >Hello, I'm a language model, but this post will discuss the differences between the native and the foreign language languages I use.
What is the difference between


ddp_rank 1: ####### Printing generated samples ####### 

rank 6 sample 3 >Hello, I'm a language model, I know you say it's an extension to C, but I think it has some really positive aspects and we see very


rank 1 sample 0 >Hello, I'm a language model, so I got to try to create good, nice code. First, I start with an "object" in the "


ddp_rank 0: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, a programmer. Can you explain it to me?
There are various ways to set up your custom code. I'm
rank 1 sample 2 >Hello, I'm a language model, but even I love the language. I love the language because every day it's a new experience for me, because it
rank 0 sample 0 >Hello, I'm a language model, so I have to explain some terms...it makes me feel stupid in english :)
So, if we use the verb
rank 0 sample 1 >Hello, I'm a language model, and have a lot of fun working with it, although I don't even like the language.
Well, I have
rank 1 sample 3 >Hello, I'm a language model, so I'm wondering how i can explain:
- First, you will introduce the use of an object to an object


rank 0 sample 2 >Hello, I'm a language model, and I see how quickly, even when I first encounter a topic of the language, it's not as easy to move
rank 0 sample 3 >Hello, I'm a language model, and want to help you get a lot out of your language. We will use and teach you the code that's just




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I'd like to get a little something to help you understand the process of your workstation.<|endoftext|>When it comes
rank 4 sample 1 >Hello, I'm a language model, no problem. Like the language code here, and so often I'm talking about people using it. In this kind of
rank 4 sample 2 >Hello, I'm a language model, I could think of something else. So let's say I wanted to write a computer program that will execute a particular function
rank 4 sample 3 >Hello, I'm a language model, so this idea came out of a small school where she told me she found out about the meaning of "dish,"


Step 13250 | loss: 3.107378 | lr:1.8332e-04 | norm 0.2869 | dt 12513.26ms | 41898.60 tokens/sec
Step 13251 | loss: 3.076851 | lr:1.8328e-04 | norm 0.2892 | dt 334.69ms | 1566490.18 tokens/sec
Step 13252 | loss: 3.101801 | lr:1.8324e-04 | norm 0.2837 | dt 337.27ms | 1554519.48 tokens/sec
Step 13253 | loss: 3.111106 | lr:1.8320e-04 | norm 0.3196 | dt 336.77ms | 1556818.50 tokens/sec
Step 13254 | loss: 3.023908 | lr:1.8317e-04 | norm 0.2844 | dt 336.52ms | 1557978.84 tokens/sec
Step 13255 | loss: 3.082215 | lr:1.8313e-04 | norm 0.2711 | dt 337.33ms | 1554211.85 tokens/sec
Step 13256 | loss: 3.129851 | lr:1.8309e-04 | norm 0.2860 | dt 336.84ms | 1556478.01 tokens/sec
Step 13257 | loss: 3.094872 | lr:1.8305e-04 | norm 0.2680 | dt 336.72ms | 1557056.60 tokens/sec
Step 13258 | loss: 3.152798 | lr:1.8301e-04 | norm 0.2806 | dt 337.26ms | 1554560.14 tokens/sec
Step 13259 | loss: 3.114420 | lr:1.8297e-04 | norm 0.2885 | dt 336.68ms | 1557207.66 tokens/sec
Step 13260 | loss: 3.087101 | lr:1.8293e-04 | norm 0.2774 | dt 337.22ms | 1554756.88 tokens/sec
Step 13261 | loss: 3.121091 | lr:1.8289e-04 | norm 0.2808 | dt 337.16ms | 1554993.25 tokens/sec
Step 13262 | loss: 3.109831 | lr:1.8286e-04 | norm 0.2802 | dt 336.91ms | 1556161.89 tokens/sec
Step 13263 | loss: 3.172397 | lr:1.8282e-04 | norm 0.3133 | dt 337.73ms | 1552406.95 tokens/sec
Step 13264 | loss: 3.066479 | lr:1.8278e-04 | norm 0.2652 | dt 338.92ms | 1546958.56 tokens/sec
Step 13265 | loss: 3.135691 | lr:1.8274e-04 | norm 0.2958 | dt 337.87ms | 1551728.87 tokens/sec
Step 13266 | loss: 3.141182 | lr:1.8270e-04 | norm 0.2898 | dt 337.38ms | 1554020.73 tokens/sec
Step 13267 | loss: 3.133368 | lr:1.8266e-04 | norm 0.2907 | dt 337.25ms | 1554597.51 tokens/sec
Step 13268 | loss: 3.144848 | lr:1.8262e-04 | norm 0.3437 | dt 337.58ms | 1553066.98 tokens/sec
Step 13269 | loss: 3.208515 | lr:1.8258e-04 | norm 0.2788 | dt 336.98ms | 1555860.21 tokens/sec
Step 13270 | loss: 3.127211 | lr:1.8255e-04 | norm 0.3257 | dt 337.25ms | 1554609.60 tokens/sec
Step 13271 | loss: 3.141460 | lr:1.8251e-04 | norm 0.2811 | dt 337.79ms | 1552131.92 tokens/sec
Step 13272 | loss: 3.096527 | lr:1.8247e-04 | norm 0.2988 | dt 337.47ms | 1553584.87 tokens/sec
Step 13273 | loss: 3.152206 | lr:1.8243e-04 | norm 0.3138 | dt 338.22ms | 1550137.32 tokens/sec
Step 13274 | loss: 3.135958 | lr:1.8239e-04 | norm 0.2952 | dt 337.42ms | 1553802.22 tokens/sec
Step 13275 | loss: 3.093583 | lr:1.8235e-04 | norm 0.3049 | dt 337.44ms | 1553701.22 tokens/sec
Step 13276 | loss: 3.148670 | lr:1.8231e-04 | norm 0.3026 | dt 337.84ms | 1551861.37 tokens/sec
Step 13277 | loss: 3.100466 | lr:1.8228e-04 | norm 0.3088 | dt 337.43ms | 1553762.70 tokens/sec
Step 13278 | loss: 3.166746 | lr:1.8224e-04 | norm 0.2959 | dt 341.71ms | 1534291.47 tokens/sec
Step 13279 | loss: 3.103746 | lr:1.8220e-04 | norm 0.2958 | dt 339.72ms | 1543279.27 tokens/sec
Step 13280 | loss: 3.089228 | lr:1.8216e-04 | norm 0.2733 | dt 337.51ms | 1553397.20 tokens/sec
Step 13281 | loss: 3.170225 | lr:1.8212e-04 | norm 0.3028 | dt 337.51ms | 1553413.66 tokens/sec
Step 13282 | loss: 3.231450 | lr:1.8208e-04 | norm 0.2976 | dt 338.87ms | 1547150.12 tokens/sec
Step 13283 | loss: 3.075531 | lr:1.8204e-04 | norm 0.4051 | dt 337.75ms | 1552277.64 tokens/sec
Step 13284 | loss: 3.102584 | lr:1.8200e-04 | norm 0.2896 | dt 338.49ms | 1548896.98 tokens/sec
Step 13285 | loss: 3.129532 | lr:1.8197e-04 | norm 0.2965 | dt 338.42ms | 1549238.53 tokens/sec
Step 13286 | loss: 3.094381 | lr:1.8193e-04 | norm 0.2889 | dt 338.95ms | 1546808.40 tokens/sec
Step 13287 | loss: 3.164607 | lr:1.8189e-04 | norm 0.3541 | dt 338.46ms | 1549032.27 tokens/sec
Step 13288 | loss: 3.112677 | lr:1.8185e-04 | norm 0.3143 | dt 337.45ms | 1553699.03 tokens/sec
Step 13289 | loss: 3.068205 | lr:1.8181e-04 | norm 0.2963 | dt 337.56ms | 1553186.55 tokens/sec
Step 13290 | loss: 3.066170 | lr:1.8177e-04 | norm 0.3060 | dt 338.40ms | 1549302.93 tokens/sec
Step 13291 | loss: 3.054962 | lr:1.8173e-04 | norm 0.2977 | dt 338.02ms | 1551040.44 tokens/sec
Step 13292 | loss: 3.109039 | lr:1.8170e-04 | norm 0.2960 | dt 338.05ms | 1550912.45 tokens/sec
Step 13293 | loss: 3.061080 | lr:1.8166e-04 | norm 0.2979 | dt 338.51ms | 1548792.25 tokens/sec
Step 13294 | loss: 3.065790 | lr:1.8162e-04 | norm 0.3048 | dt 338.33ms | 1549652.30 tokens/sec
Step 13295 | loss: 3.100962 | lr:1.8158e-04 | norm 0.2702 | dt 338.62ms | 1548326.61 tokens/sec
Step 13296 | loss: 3.025157 | lr:1.8154e-04 | norm 0.2652 | dt 337.55ms | 1553207.39 tokens/sec
Step 13297 | loss: 3.176970 | lr:1.8150e-04 | norm 0.3188 | dt 338.20ms | 1550241.14 tokens/sec
Step 13298 | loss: 3.100699 | lr:1.8146e-04 | norm 0.3099 | dt 339.36ms | 1544919.70 tokens/sec
Step 13299 | loss: 3.164338 | lr:1.8143e-04 | norm 0.3023 | dt 915.06ms | 572954.75 tokens/sec
Step 13300 | loss: 3.070974 | lr:1.8139e-04 | norm 0.3140 | dt 336.29ms | 1559022.64 tokens/sec
Step 13301 | loss: 3.139614 | lr:1.8135e-04 | norm 0.2896 | dt 338.12ms | 1550599.68 tokens/sec
Step 13302 | loss: 3.164736 | lr:1.8131e-04 | norm 0.2915 | dt 337.28ms | 1554457.94 tokens/sec
Step 13303 | loss: 3.092382 | lr:1.8127e-04 | norm 0.2817 | dt 337.56ms | 1553192.03 tokens/sec
Step 13304 | loss: 3.088177 | lr:1.8123e-04 | norm 0.2879 | dt 337.89ms | 1551654.41 tokens/sec
Step 13305 | loss: 3.110794 | lr:1.8119e-04 | norm 0.2779 | dt 338.33ms | 1549656.67 tokens/sec
Step 13306 | loss: 3.160864 | lr:1.8116e-04 | norm 0.2899 | dt 337.64ms | 1552802.68 tokens/sec
Step 13307 | loss: 3.172717 | lr:1.8112e-04 | norm 0.2887 | dt 337.19ms | 1554881.11 tokens/sec
Step 13308 | loss: 3.153992 | lr:1.8108e-04 | norm 0.2870 | dt 337.45ms | 1553671.58 tokens/sec
Step 13309 | loss: 3.121380 | lr:1.8104e-04 | norm 0.2803 | dt 337.47ms | 1553605.72 tokens/sec
Step 13310 | loss: 3.100712 | lr:1.8100e-04 | norm 0.3007 | dt 337.81ms | 1552012.52 tokens/sec
Step 13311 | loss: 3.143245 | lr:1.8096e-04 | norm 0.2676 | dt 337.86ms | 1551813.19 tokens/sec
Step 13312 | loss: 3.136552 | lr:1.8092e-04 | norm 0.3029 | dt 337.84ms | 1551884.37 tokens/sec
Step 13313 | loss: 3.152858 | lr:1.8089e-04 | norm 0.2946 | dt 337.77ms | 1552217.38 tokens/sec
Step 13314 | loss: 3.110500 | lr:1.8085e-04 | norm 0.2912 | dt 337.71ms | 1552460.65 tokens/sec
Step 13315 | loss: 3.146409 | lr:1.8081e-04 | norm 0.3024 | dt 338.06ms | 1550853.39 tokens/sec
Step 13316 | loss: 3.129685 | lr:1.8077e-04 | norm 0.3063 | dt 338.29ms | 1549821.59 tokens/sec
Step 13317 | loss: 3.131465 | lr:1.8073e-04 | norm 0.2943 | dt 337.58ms | 1553060.40 tokens/sec
Step 13318 | loss: 3.123400 | lr:1.8069e-04 | norm 0.3205 | dt 337.54ms | 1553281.99 tokens/sec
Step 13319 | loss: 3.139216 | lr:1.8065e-04 | norm 0.2906 | dt 337.81ms | 1552010.33 tokens/sec
Step 13320 | loss: 3.197267 | lr:1.8062e-04 | norm 0.3017 | dt 338.16ms | 1550436.79 tokens/sec
Step 13321 | loss: 3.134698 | lr:1.8058e-04 | norm 0.3091 | dt 337.32ms | 1554293.14 tokens/sec
Step 13322 | loss: 3.110464 | lr:1.8054e-04 | norm 0.3066 | dt 338.09ms | 1550748.40 tokens/sec
Step 13323 | loss: 3.150235 | lr:1.8050e-04 | norm 0.3153 | dt 337.80ms | 1552083.72 tokens/sec
Step 13324 | loss: 3.078304 | lr:1.8046e-04 | norm 0.3323 | dt 337.62ms | 1552889.31 tokens/sec
Step 13325 | loss: 3.100802 | lr:1.8042e-04 | norm 0.2766 | dt 338.39ms | 1549361.88 tokens/sec
Step 13326 | loss: 3.101701 | lr:1.8039e-04 | norm 0.2956 | dt 338.31ms | 1549706.91 tokens/sec
Step 13327 | loss: 3.084022 | lr:1.8035e-04 | norm 0.3111 | dt 337.93ms | 1551491.30 tokens/sec
Step 13328 | loss: 3.067900 | lr:1.8031e-04 | norm 0.2999 | dt 338.81ms | 1547460.41 tokens/sec
Step 13329 | loss: 3.110767 | lr:1.8027e-04 | norm 0.2964 | dt 337.38ms | 1553991.08 tokens/sec
Step 13330 | loss: 3.117634 | lr:1.8023e-04 | norm 0.3047 | dt 337.30ms | 1554364.55 tokens/sec
Step 13331 | loss: 3.141198 | lr:1.8019e-04 | norm 0.2951 | dt 338.94ms | 1546842.13 tokens/sec
Step 13332 | loss: 3.119904 | lr:1.8015e-04 | norm 0.2729 | dt 337.96ms | 1551329.31 tokens/sec
Step 13333 | loss: 3.080319 | lr:1.8012e-04 | norm 0.3388 | dt 337.72ms | 1552431.06 tokens/sec
Step 13334 | loss: 3.137549 | lr:1.8008e-04 | norm 0.2810 | dt 337.40ms | 1553891.16 tokens/sec
Step 13335 | loss: 3.084875 | lr:1.8004e-04 | norm 0.2931 | dt 338.68ms | 1548029.05 tokens/sec
Step 13336 | loss: 3.148859 | lr:1.8000e-04 | norm 0.2916 | dt 337.62ms | 1552892.60 tokens/sec
Step 13337 | loss: 3.191276 | lr:1.7996e-04 | norm 0.3032 | dt 338.09ms | 1550737.46 tokens/sec
Step 13338 | loss: 3.125338 | lr:1.7992e-04 | norm 0.3046 | dt 337.55ms | 1553230.43 tokens/sec
Step 13339 | loss: 3.103251 | lr:1.7989e-04 | norm 0.2715 | dt 337.93ms | 1551477.07 tokens/sec
Step 13340 | loss: 3.166981 | lr:1.7985e-04 | norm 0.3279 | dt 337.84ms | 1551902.99 tokens/sec
Step 13341 | loss: 3.156775 | lr:1.7981e-04 | norm 0.3099 | dt 337.25ms | 1554618.39 tokens/sec
Step 13342 | loss: 3.165275 | lr:1.7977e-04 | norm 0.3120 | dt 337.87ms | 1551756.24 tokens/sec
Step 13343 | loss: 3.118502 | lr:1.7973e-04 | norm 0.3120 | dt 338.53ms | 1548715.90 tokens/sec
Step 13344 | loss: 3.098776 | lr:1.7969e-04 | norm 0.2910 | dt 338.09ms | 1550754.96 tokens/sec
Step 13345 | loss: 3.141266 | lr:1.7966e-04 | norm 0.3053 | dt 337.35ms | 1554137.15 tokens/sec
Step 13346 | loss: 3.132172 | lr:1.7962e-04 | norm 0.2971 | dt 337.97ms | 1551265.83 tokens/sec
Step 13347 | loss: 3.096336 | lr:1.7958e-04 | norm 0.2981 | dt 337.95ms | 1551385.12 tokens/sec
Step 13348 | loss: 3.146715 | lr:1.7954e-04 | norm 0.2791 | dt 338.00ms | 1551126.87 tokens/sec
Step 13349 | loss: 3.143506 | lr:1.7950e-04 | norm 0.2943 | dt 338.29ms | 1549797.56 tokens/sec
Step 13350 | loss: 3.166258 | lr:1.7946e-04 | norm 0.2719 | dt 338.52ms | 1548776.98 tokens/sec
Step 13351 | loss: 3.059339 | lr:1.7943e-04 | norm 0.2690 | dt 337.96ms | 1551341.35 tokens/sec
Step 13352 | loss: 3.084760 | lr:1.7939e-04 | norm 0.2548 | dt 337.30ms | 1554354.66 tokens/sec
Step 13353 | loss: 3.144046 | lr:1.7935e-04 | norm 0.2832 | dt 338.17ms | 1550382.13 tokens/sec
Step 13354 | loss: 3.003576 | lr:1.7931e-04 | norm 0.4676 | dt 338.34ms | 1549606.44 tokens/sec
Step 13355 | loss: 3.107708 | lr:1.7927e-04 | norm 0.2876 | dt 338.70ms | 1547948.41 tokens/sec
Step 13356 | loss: 3.053126 | lr:1.7923e-04 | norm 0.2736 | dt 337.95ms | 1551396.07 tokens/sec
Step 13357 | loss: 3.093828 | lr:1.7920e-04 | norm 0.2752 | dt 337.75ms | 1552303.94 tokens/sec
Step 13358 | loss: 3.080710 | lr:1.7916e-04 | norm 0.2771 | dt 337.78ms | 1552158.22 tokens/sec
Step 13359 | loss: 3.119603 | lr:1.7912e-04 | norm 0.2649 | dt 337.91ms | 1551553.69 tokens/sec
Step 13360 | loss: 3.035004 | lr:1.7908e-04 | norm 0.2653 | dt 339.90ms | 1542472.80 tokens/sec
Step 13361 | loss: 3.146817 | lr:1.7904e-04 | norm 0.2626 | dt 338.06ms | 1550878.54 tokens/sec
Step 13362 | loss: 3.075390 | lr:1.7900e-04 | norm 0.2745 | dt 338.89ms | 1547078.28 tokens/sec
Step 13363 | loss: 3.108824 | lr:1.7897e-04 | norm 0.2779 | dt 339.78ms | 1543042.11 tokens/sec
Step 13364 | loss: 3.083009 | lr:1.7893e-04 | norm 0.2826 | dt 339.36ms | 1544915.36 tokens/sec
Step 13365 | loss: 3.060510 | lr:1.7889e-04 | norm 0.2830 | dt 338.02ms | 1551049.19 tokens/sec
Step 13366 | loss: 3.061730 | lr:1.7885e-04 | norm 0.2705 | dt 338.14ms | 1550528.62 tokens/sec
Step 13367 | loss: 3.089228 | lr:1.7881e-04 | norm 0.2776 | dt 338.58ms | 1548492.33 tokens/sec
Step 13368 | loss: 3.099379 | lr:1.7877e-04 | norm 0.2924 | dt 339.31ms | 1545158.52 tokens/sec
Step 13369 | loss: 3.109799 | lr:1.7874e-04 | norm 0.2763 | dt 337.84ms | 1551887.66 tokens/sec
Step 13370 | loss: 3.106100 | lr:1.7870e-04 | norm 0.2856 | dt 338.81ms | 1547422.30 tokens/sec
Step 13371 | loss: 3.123404 | lr:1.7866e-04 | norm 0.2645 | dt 339.18ms | 1545754.81 tokens/sec
Step 13372 | loss: 3.179853 | lr:1.7862e-04 | norm 0.2948 | dt 339.02ms | 1546491.85 tokens/sec
Step 13373 | loss: 3.154082 | lr:1.7858e-04 | norm 0.3149 | dt 338.74ms | 1547740.32 tokens/sec
Step 13374 | loss: 3.085292 | lr:1.7854e-04 | norm 0.2772 | dt 338.04ms | 1550974.80 tokens/sec
Step 13375 | loss: 3.214910 | lr:1.7851e-04 | norm 0.3012 | dt 337.53ms | 1553295.16 tokens/sec
Step 13376 | loss: 3.158277 | lr:1.7847e-04 | norm 0.2990 | dt 339.16ms | 1545859.13 tokens/sec
Step 13377 | loss: 3.154764 | lr:1.7843e-04 | norm 0.3096 | dt 338.63ms | 1548265.56 tokens/sec
Step 13378 | loss: 3.111561 | lr:1.7839e-04 | norm 0.2771 | dt 337.97ms | 1551293.19 tokens/sec
Step 13379 | loss: 3.153644 | lr:1.7835e-04 | norm 0.2930 | dt 337.72ms | 1552451.88 tokens/sec
Step 13380 | loss: 3.125139 | lr:1.7831e-04 | norm 0.2639 | dt 338.27ms | 1549925.36 tokens/sec
Step 13381 | loss: 3.072700 | lr:1.7828e-04 | norm 0.3042 | dt 338.42ms | 1549234.17 tokens/sec
Step 13382 | loss: 3.119429 | lr:1.7824e-04 | norm 0.3092 | dt 338.32ms | 1549658.86 tokens/sec
Step 13383 | loss: 3.196385 | lr:1.7820e-04 | norm 0.2621 | dt 337.56ms | 1553182.16 tokens/sec
Step 13384 | loss: 3.105004 | lr:1.7816e-04 | norm 0.3085 | dt 337.74ms | 1552362.02 tokens/sec
Step 13385 | loss: 3.139794 | lr:1.7812e-04 | norm 0.2990 | dt 337.65ms | 1552737.99 tokens/sec
Step 13386 | loss: 3.130445 | lr:1.7809e-04 | norm 0.2750 | dt 338.30ms | 1549793.19 tokens/sec
Step 13387 | loss: 3.109120 | lr:1.7805e-04 | norm 0.2916 | dt 338.84ms | 1547321.03 tokens/sec
Step 13388 | loss: 3.106481 | lr:1.7801e-04 | norm 0.2737 | dt 336.91ms | 1556154.18 tokens/sec
Step 13389 | loss: 3.131756 | lr:1.7797e-04 | norm 0.2903 | dt 339.24ms | 1545470.19 tokens/sec
Step 13390 | loss: 3.127589 | lr:1.7793e-04 | norm 0.2805 | dt 339.54ms | 1544131.03 tokens/sec
Step 13391 | loss: 3.101654 | lr:1.7789e-04 | norm 0.2894 | dt 343.30ms | 1527218.27 tokens/sec
Step 13392 | loss: 3.094383 | lr:1.7786e-04 | norm 0.2966 | dt 337.91ms | 1551547.12 tokens/sec
Step 13393 | loss: 3.061433 | lr:1.7782e-04 | norm 0.2633 | dt 338.58ms | 1548469.44 tokens/sec
Step 13394 | loss: 3.052028 | lr:1.7778e-04 | norm 0.2995 | dt 338.80ms | 1547471.30 tokens/sec
Step 13395 | loss: 3.162421 | lr:1.7774e-04 | norm 0.3108 | dt 339.25ms | 1545421.31 tokens/sec
Step 13396 | loss: 3.067682 | lr:1.7770e-04 | norm 0.3075 | dt 339.31ms | 1545175.89 tokens/sec
Step 13397 | loss: 3.121121 | lr:1.7767e-04 | norm 0.2929 | dt 339.76ms | 1543094.08 tokens/sec
Step 13398 | loss: 3.040393 | lr:1.7763e-04 | norm 0.2923 | dt 339.42ms | 1544650.58 tokens/sec
Step 13399 | loss: 3.079773 | lr:1.7759e-04 | norm 0.2824 | dt 339.26ms | 1545389.82 tokens/sec
Step 13400 | loss: 3.070772 | lr:1.7755e-04 | norm 0.2983 | dt 339.56ms | 1544013.94 tokens/sec
Step 13401 | loss: 3.082928 | lr:1.7751e-04 | norm 0.2773 | dt 338.57ms | 1548515.23 tokens/sec
Step 13402 | loss: 3.121766 | lr:1.7747e-04 | norm 0.2970 | dt 338.81ms | 1547428.83 tokens/sec
Step 13403 | loss: 3.094020 | lr:1.7744e-04 | norm 0.2704 | dt 339.65ms | 1543611.84 tokens/sec
Step 13404 | loss: 3.163497 | lr:1.7740e-04 | norm 0.2656 | dt 338.82ms | 1547379.83 tokens/sec
Step 13405 | loss: 3.151763 | lr:1.7736e-04 | norm 0.2941 | dt 338.82ms | 1547398.34 tokens/sec
Step 13406 | loss: 3.197974 | lr:1.7732e-04 | norm 0.2889 | dt 339.16ms | 1545851.52 tokens/sec
Step 13407 | loss: 3.119388 | lr:1.7728e-04 | norm 0.2993 | dt 338.77ms | 1547612.88 tokens/sec
Step 13408 | loss: 3.192633 | lr:1.7725e-04 | norm 0.2893 | dt 338.32ms | 1549673.05 tokens/sec
Step 13409 | loss: 3.175251 | lr:1.7721e-04 | norm 0.3009 | dt 340.20ms | 1541110.75 tokens/sec
Step 13410 | loss: 3.206827 | lr:1.7717e-04 | norm 0.3208 | dt 339.13ms | 1545977.59 tokens/sec
Step 13411 | loss: 3.139283 | lr:1.7713e-04 | norm 0.2902 | dt 339.01ms | 1546515.77 tokens/sec
Step 13412 | loss: 3.128923 | lr:1.7709e-04 | norm 0.2804 | dt 338.08ms | 1550765.89 tokens/sec
Step 13413 | loss: 3.141782 | lr:1.7706e-04 | norm 0.3157 | dt 339.31ms | 1545176.98 tokens/sec
Step 13414 | loss: 3.076393 | lr:1.7702e-04 | norm 0.3141 | dt 338.30ms | 1549787.73 tokens/sec
Step 13415 | loss: 3.128598 | lr:1.7698e-04 | norm 0.3417 | dt 338.36ms | 1549515.81 tokens/sec
Step 13416 | loss: 3.144805 | lr:1.7694e-04 | norm 0.3034 | dt 338.73ms | 1547804.59 tokens/sec
Step 13417 | loss: 3.122645 | lr:1.7690e-04 | norm 0.3354 | dt 339.04ms | 1546391.79 tokens/sec
Step 13418 | loss: 3.219478 | lr:1.7687e-04 | norm 0.3124 | dt 893.24ms | 586952.62 tokens/sec
Step 13419 | loss: 3.169343 | lr:1.7683e-04 | norm 0.3175 | dt 336.22ms | 1559373.09 tokens/sec
Step 13420 | loss: 3.048182 | lr:1.7679e-04 | norm 0.3422 | dt 337.30ms | 1554360.15 tokens/sec
Step 13421 | loss: 3.138074 | lr:1.7675e-04 | norm 0.2849 | dt 338.94ms | 1546825.81 tokens/sec
Step 13422 | loss: 3.138885 | lr:1.7671e-04 | norm 0.3282 | dt 338.52ms | 1548776.98 tokens/sec
Step 13423 | loss: 3.146676 | lr:1.7668e-04 | norm 0.2937 | dt 338.53ms | 1548740.99 tokens/sec
Step 13424 | loss: 3.082388 | lr:1.7664e-04 | norm 0.2972 | dt 339.01ms | 1546538.61 tokens/sec
Step 13425 | loss: 3.109264 | lr:1.7660e-04 | norm 0.2894 | dt 338.04ms | 1550969.33 tokens/sec
Step 13426 | loss: 3.119921 | lr:1.7656e-04 | norm 0.3029 | dt 338.19ms | 1550285.95 tokens/sec
Step 13427 | loss: 3.132177 | lr:1.7652e-04 | norm 0.2669 | dt 338.55ms | 1548610.11 tokens/sec
Step 13428 | loss: 3.031546 | lr:1.7649e-04 | norm 0.3103 | dt 338.30ms | 1549752.78 tokens/sec
Step 13429 | loss: 3.074961 | lr:1.7645e-04 | norm 0.2745 | dt 339.23ms | 1545526.67 tokens/sec
Step 13430 | loss: 3.105329 | lr:1.7641e-04 | norm 0.2881 | dt 338.97ms | 1546687.64 tokens/sec
Step 13431 | loss: 3.064425 | lr:1.7637e-04 | norm 0.2867 | dt 338.85ms | 1547270.95 tokens/sec
Step 13432 | loss: 3.134888 | lr:1.7633e-04 | norm 0.3016 | dt 338.97ms | 1546691.99 tokens/sec
Step 13433 | loss: 3.101968 | lr:1.7630e-04 | norm 0.3105 | dt 338.67ms | 1548098.80 tokens/sec
Step 13434 | loss: 3.121533 | lr:1.7626e-04 | norm 0.2759 | dt 339.75ms | 1543142.81 tokens/sec
Step 13435 | loss: 3.054259 | lr:1.7622e-04 | norm 0.2873 | dt 339.12ms | 1546008.02 tokens/sec
Step 13436 | loss: 3.089403 | lr:1.7618e-04 | norm 0.3046 | dt 338.69ms | 1548000.72 tokens/sec
Step 13437 | loss: 3.022093 | lr:1.7614e-04 | norm 0.2913 | dt 338.85ms | 1547265.51 tokens/sec
Step 13438 | loss: 3.087370 | lr:1.7611e-04 | norm 0.2867 | dt 339.95ms | 1542257.52 tokens/sec
Step 13439 | loss: 3.101602 | lr:1.7607e-04 | norm 0.3067 | dt 338.59ms | 1548447.63 tokens/sec
Step 13440 | loss: 3.086221 | lr:1.7603e-04 | norm 0.3394 | dt 339.52ms | 1544180.91 tokens/sec
Step 13441 | loss: 3.069530 | lr:1.7599e-04 | norm 0.3383 | dt 338.28ms | 1549880.57 tokens/sec
Step 13442 | loss: 3.105454 | lr:1.7595e-04 | norm 0.3041 | dt 339.76ms | 1543127.65 tokens/sec
Step 13443 | loss: 3.125473 | lr:1.7592e-04 | norm 0.3077 | dt 338.40ms | 1549295.29 tokens/sec
Step 13444 | loss: 3.111190 | lr:1.7588e-04 | norm 0.3112 | dt 339.29ms | 1545263.84 tokens/sec
Step 13445 | loss: 3.072757 | lr:1.7584e-04 | norm 0.2949 | dt 338.76ms | 1547658.62 tokens/sec
Step 13446 | loss: 3.163574 | lr:1.7580e-04 | norm 0.2913 | dt 338.91ms | 1546968.36 tokens/sec
Step 13447 | loss: 3.090796 | lr:1.7576e-04 | norm 0.2914 | dt 340.58ms | 1539380.32 tokens/sec
Step 13448 | loss: 3.146295 | lr:1.7573e-04 | norm 0.3215 | dt 339.45ms | 1544542.08 tokens/sec
Step 13449 | loss: 3.092927 | lr:1.7569e-04 | norm 0.2721 | dt 338.95ms | 1546785.55 tokens/sec
Step 13450 | loss: 3.111492 | lr:1.7565e-04 | norm 0.3085 | dt 338.81ms | 1547427.74 tokens/sec
Step 13451 | loss: 3.133729 | lr:1.7561e-04 | norm 0.2915 | dt 338.57ms | 1548526.14 tokens/sec
Step 13452 | loss: 3.174409 | lr:1.7557e-04 | norm 0.3066 | dt 337.92ms | 1551505.53 tokens/sec
Step 13453 | loss: 3.169579 | lr:1.7554e-04 | norm 0.3276 | dt 339.62ms | 1543750.55 tokens/sec
Step 13454 | loss: 3.110427 | lr:1.7550e-04 | norm 0.3109 | dt 339.08ms | 1546191.73 tokens/sec
Step 13455 | loss: 3.119459 | lr:1.7546e-04 | norm 0.3026 | dt 339.17ms | 1545814.57 tokens/sec
Step 13456 | loss: 3.110435 | lr:1.7542e-04 | norm 0.2748 | dt 338.96ms | 1546772.50 tokens/sec
Step 13457 | loss: 3.105026 | lr:1.7538e-04 | norm 0.3017 | dt 339.40ms | 1544736.30 tokens/sec
Step 13458 | loss: 3.099850 | lr:1.7535e-04 | norm 0.2948 | dt 339.50ms | 1544311.04 tokens/sec
Step 13459 | loss: 3.106371 | lr:1.7531e-04 | norm 0.2864 | dt 338.98ms | 1546683.29 tokens/sec
Step 13460 | loss: 3.158005 | lr:1.7527e-04 | norm 0.2828 | dt 338.35ms | 1549559.49 tokens/sec
Step 13461 | loss: 3.092861 | lr:1.7523e-04 | norm 0.2932 | dt 338.76ms | 1547672.78 tokens/sec
Step 13462 | loss: 3.070893 | lr:1.7520e-04 | norm 0.2768 | dt 339.01ms | 1546517.95 tokens/sec
Step 13463 | loss: 3.072972 | lr:1.7516e-04 | norm 0.2670 | dt 337.95ms | 1551357.76 tokens/sec
Step 13464 | loss: 3.088159 | lr:1.7512e-04 | norm 0.2932 | dt 338.08ms | 1550779.02 tokens/sec
Step 13465 | loss: 3.088243 | lr:1.7508e-04 | norm 0.2548 | dt 337.80ms | 1552049.76 tokens/sec
Step 13466 | loss: 3.058638 | lr:1.7504e-04 | norm 0.2628 | dt 338.18ms | 1550307.81 tokens/sec
Step 13467 | loss: 3.058968 | lr:1.7501e-04 | norm 0.2574 | dt 337.38ms | 1553987.79 tokens/sec
Step 13468 | loss: 3.154259 | lr:1.7497e-04 | norm 0.2812 | dt 338.47ms | 1548990.81 tokens/sec
Step 13469 | loss: 3.086617 | lr:1.7493e-04 | norm 0.2537 | dt 338.06ms | 1550854.48 tokens/sec
Step 13470 | loss: 3.106632 | lr:1.7489e-04 | norm 0.2747 | dt 337.74ms | 1552356.54 tokens/sec
Step 13471 | loss: 3.071448 | lr:1.7485e-04 | norm 0.2509 | dt 337.87ms | 1551723.39 tokens/sec
Step 13472 | loss: 3.093742 | lr:1.7482e-04 | norm 0.2653 | dt 337.83ms | 1551934.75 tokens/sec
Step 13473 | loss: 3.161648 | lr:1.7478e-04 | norm 0.2612 | dt 338.58ms | 1548508.69 tokens/sec
Step 13474 | loss: 3.073746 | lr:1.7474e-04 | norm 0.2696 | dt 338.36ms | 1549513.63 tokens/sec
Step 13475 | loss: 3.127006 | lr:1.7470e-04 | norm 0.2734 | dt 337.91ms | 1551551.50 tokens/sec
Step 13476 | loss: 3.159058 | lr:1.7467e-04 | norm 0.2791 | dt 338.39ms | 1549380.43 tokens/sec
Step 13477 | loss: 3.154463 | lr:1.7463e-04 | norm 0.2571 | dt 338.38ms | 1549395.72 tokens/sec
Step 13478 | loss: 3.114513 | lr:1.7459e-04 | norm 0.2725 | dt 339.35ms | 1544959.86 tokens/sec
Step 13479 | loss: 3.090617 | lr:1.7455e-04 | norm 0.2682 | dt 337.92ms | 1551518.66 tokens/sec
Step 13480 | loss: 3.169812 | lr:1.7451e-04 | norm 0.2764 | dt 338.65ms | 1548157.65 tokens/sec
Step 13481 | loss: 3.102353 | lr:1.7448e-04 | norm 0.2786 | dt 338.42ms | 1549212.34 tokens/sec
Step 13482 | loss: 3.079585 | lr:1.7444e-04 | norm 0.2672 | dt 337.87ms | 1551750.77 tokens/sec
Step 13483 | loss: 3.102532 | lr:1.7440e-04 | norm 0.2973 | dt 338.29ms | 1549840.16 tokens/sec
Step 13484 | loss: 3.125928 | lr:1.7436e-04 | norm 0.2902 | dt 338.83ms | 1547344.99 tokens/sec
Step 13485 | loss: 3.120014 | lr:1.7433e-04 | norm 0.3074 | dt 337.83ms | 1551947.89 tokens/sec
Step 13486 | loss: 3.105949 | lr:1.7429e-04 | norm 0.3208 | dt 338.04ms | 1550971.52 tokens/sec
Step 13487 | loss: 3.109563 | lr:1.7425e-04 | norm 0.3147 | dt 338.16ms | 1550423.67 tokens/sec
Step 13488 | loss: 3.124952 | lr:1.7421e-04 | norm 0.3003 | dt 337.85ms | 1551832.90 tokens/sec
Step 13489 | loss: 3.083657 | lr:1.7417e-04 | norm 0.2948 | dt 928.09ms | 564912.70 tokens/sec
Step 13490 | loss: 3.179156 | lr:1.7414e-04 | norm 0.3122 | dt 336.06ms | 1560115.43 tokens/sec
Step 13491 | loss: 3.111105 | lr:1.7410e-04 | norm 0.2829 | dt 337.50ms | 1553456.46 tokens/sec
Step 13492 | loss: 3.122428 | lr:1.7406e-04 | norm 0.3264 | dt 338.58ms | 1548506.51 tokens/sec
Step 13493 | loss: 3.107940 | lr:1.7402e-04 | norm 0.3115 | dt 337.10ms | 1555282.50 tokens/sec
Step 13494 | loss: 3.150865 | lr:1.7399e-04 | norm 0.3077 | dt 337.61ms | 1552944.14 tokens/sec
Step 13495 | loss: 3.097957 | lr:1.7395e-04 | norm 0.2855 | dt 338.48ms | 1548967.90 tokens/sec
Step 13496 | loss: 3.089389 | lr:1.7391e-04 | norm 0.3194 | dt 338.36ms | 1549485.24 tokens/sec
Step 13497 | loss: 3.062352 | lr:1.7387e-04 | norm 0.2867 | dt 337.72ms | 1552455.17 tokens/sec
Step 13498 | loss: 3.109750 | lr:1.7384e-04 | norm 0.3249 | dt 338.02ms | 1551044.81 tokens/sec
Step 13499 | loss: 3.109614 | lr:1.7380e-04 | norm 0.2703 | dt 338.38ms | 1549384.80 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 13500: 3.1350
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3005/10042=0.2992


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but not as good as a typical computer programmer, so it's hard to be an expert. You'll find people willing
rank 5 sample 1 >Hello, I'm a language model, here's how you learn your way from your way from your way from your way from your way from your way from your
rank 5 sample 2 >Hello, I'm a language model, and you might have better luck reading these.
The other thing you'd want to get out of the way is for
rank 5 sample 3 >Hello, I'm a language model, one that is mostly used in the social media. You're supposed to be pretty neat with your friend's name and we




ddp_rank 2: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so here is everything you need to know.
I am going to have to say that I am going have to go
rank 6 sample 0 >Hello, I'm a language model, just by the way that I learn the English language. So if I want to learn a foreign language, it's good
rank 2 sample 1 >Hello, I'm a language model, but I thought you would be a hard surgeon. Yes, that's it.
I have been studying for a decade


ddp_rank 7: ####### Printing generated samples ####### 

rank 6 sample 1 >Hello, I'm a language model, which means that I use a small set system, that is, a very small number of words, that have a sequence
rank 2 sample 2 >Hello, I'm a language model, and I've been trying to do many real-life things like I get more out of it to go on an email
rank 6 sample 2 >Hello, I'm a language model, but a human is a model for us. The human model, is the machine that allows us to take control of our
rank 2 sample 3 >Hello, I'm a language model, but that really is the same as writing this to Python. In Python,
the
# if <
then,


rank 7 sample 0 >Hello, I'm a language model, so I thought I was pretty good at just starting this off. I started off reading, and working in a language like
rank 6 sample 3 >Hello, I'm a language model, so let's build something that we can understand, and let's use it as an input value. Once, we could


rank 7 sample 1 >Hello, I'm a language model, using programming, and I know some more things about. Why is it that I had a bad experience with programming in school
rank 7 sample 2 >Hello, I'm a language model, so I'll teach you how to say, "we want to go down the hill, we want to go over,
rank 7 sample 3 >Hello, I'm a language model, and you guys are pretty confused. Your friend may be a writer or computer scientist, but you were going to start it




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so let me explain a few things very well that are part of the language model. Let's get started:
-
rank 1 sample 1 >Hello, I'm a language model, a person that loves to learn a language. I don't know many people like that language. Even if I'm a
rank 1 sample 2 >Hello, I'm a language model, but still, if you're not a language model, it should not have made me a good teacher. I can now
rank 1 sample 3 >Hello, I'm a language model, so I'm looking at that part of creating a script;
A function:
void set_name(String)




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't have to go there when I'm typing in HTML. It's the same language as Java, so
rank 0 sample 1 >Hello, I'm a language model, and here's a simple example to simplify it for both myself and anyone else in my world. I mean, I'm
rank 0 sample 2 >Hello, I'm a language model, and I see how these three processes get started by working together with one another. In my mind, I'm not able
rank 0 sample 3 >Hello, I'm a language model, and want to create a table that displays a variety of variables. Let's say it's a list of variables that describe




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not the best model but I can say that learning to speak as a computer programmer is extremely important in that
rank 3 sample 1 >Hello, I'm a language model, so my question is: what should I do when I use a different language?
I'm not a language. But
rank 3 sample 2 >Hello, I'm a language model, so this one is to explain about the machine to a computer how to use machine learning. This lesson will explain basic machine
rank 3 sample 3 >Hello, I'm a language model, so let's look at a sample of a few different words and then figure out what we do. Say you have a




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I don't have a lot of things in mind. So, the answer is I was too big for my age
rank 4 sample 1 >Hello, I'm a language model, for sure. Is anyone here telling me, can someone define a language without having to get into the technicalities now,
rank 4 sample 2 >Hello, I'm a language model, I feel like your name in the context of a language. It also suggests that if you understand, try to find out
rank 4 sample 3 >Hello, I'm a language model, so you never hear a native speaker with one hand and a pair with the other.
Can any one of you have


Step 13500 | loss: 3.101916 | lr:1.7376e-04 | norm 0.3007 | dt 18659.30ms | 28097.95 tokens/sec
Step 13501 | loss: 3.078494 | lr:1.7372e-04 | norm 0.2805 | dt 336.35ms | 1558778.41 tokens/sec
Step 13502 | loss: 3.056130 | lr:1.7368e-04 | norm 0.3190 | dt 335.56ms | 1562419.94 tokens/sec
Step 13503 | loss: 3.092404 | lr:1.7365e-04 | norm 0.3199 | dt 336.05ms | 1560160.81 tokens/sec
Step 13504 | loss: 3.094743 | lr:1.7361e-04 | norm 0.3283 | dt 335.70ms | 1561758.59 tokens/sec
Step 13505 | loss: 3.106649 | lr:1.7357e-04 | norm 0.2928 | dt 337.14ms | 1555123.02 tokens/sec
Step 13506 | loss: 3.078670 | lr:1.7353e-04 | norm 0.2825 | dt 336.07ms | 1560074.47 tokens/sec
Step 13507 | loss: 3.115671 | lr:1.7350e-04 | norm 0.3169 | dt 336.23ms | 1559326.65 tokens/sec
Step 13508 | loss: 3.127480 | lr:1.7346e-04 | norm 0.3381 | dt 337.07ms | 1555418.91 tokens/sec
Step 13509 | loss: 3.110722 | lr:1.7342e-04 | norm 0.3414 | dt 336.78ms | 1556761.19 tokens/sec
Step 13510 | loss: 3.116471 | lr:1.7338e-04 | norm 0.2994 | dt 336.04ms | 1560199.55 tokens/sec
Step 13511 | loss: 3.085340 | lr:1.7335e-04 | norm 0.3288 | dt 335.40ms | 1563160.73 tokens/sec
Step 13512 | loss: 3.124498 | lr:1.7331e-04 | norm 0.3005 | dt 337.27ms | 1554519.48 tokens/sec
Step 13513 | loss: 3.110473 | lr:1.7327e-04 | norm 0.2971 | dt 336.15ms | 1559689.41 tokens/sec
Step 13514 | loss: 3.116806 | lr:1.7323e-04 | norm 0.2994 | dt 336.07ms | 1560034.63 tokens/sec
Step 13515 | loss: 3.108136 | lr:1.7320e-04 | norm 0.2965 | dt 337.70ms | 1552504.49 tokens/sec
Step 13516 | loss: 3.135760 | lr:1.7316e-04 | norm 0.2894 | dt 335.71ms | 1561721.99 tokens/sec
Step 13517 | loss: 3.166866 | lr:1.7312e-04 | norm 0.3223 | dt 336.28ms | 1559063.53 tokens/sec
Step 13518 | loss: 3.118360 | lr:1.7308e-04 | norm 0.2999 | dt 336.62ms | 1557518.69 tokens/sec
Step 13519 | loss: 3.168212 | lr:1.7304e-04 | norm 0.3424 | dt 337.72ms | 1552443.12 tokens/sec
Step 13520 | loss: 3.153096 | lr:1.7301e-04 | norm 0.2980 | dt 336.68ms | 1557249.56 tokens/sec
Step 13521 | loss: 3.125542 | lr:1.7297e-04 | norm 0.3014 | dt 337.45ms | 1553684.76 tokens/sec
Step 13522 | loss: 3.115056 | lr:1.7293e-04 | norm 0.3047 | dt 336.59ms | 1557655.49 tokens/sec
Step 13523 | loss: 3.120072 | lr:1.7289e-04 | norm 0.2895 | dt 337.33ms | 1554231.62 tokens/sec
Step 13524 | loss: 3.110389 | lr:1.7286e-04 | norm 0.2949 | dt 336.81ms | 1556625.64 tokens/sec
Step 13525 | loss: 3.131909 | lr:1.7282e-04 | norm 0.2852 | dt 336.90ms | 1556218.05 tokens/sec
Step 13526 | loss: 3.145764 | lr:1.7278e-04 | norm 0.2846 | dt 336.66ms | 1557315.73 tokens/sec
Step 13527 | loss: 3.161810 | lr:1.7274e-04 | norm 0.2801 | dt 336.91ms | 1556170.70 tokens/sec
Step 13528 | loss: 3.098757 | lr:1.7271e-04 | norm 0.3066 | dt 336.42ms | 1558435.95 tokens/sec
Step 13529 | loss: 3.142591 | lr:1.7267e-04 | norm 0.2835 | dt 336.22ms | 1559362.03 tokens/sec
Step 13530 | loss: 3.082350 | lr:1.7263e-04 | norm 0.2977 | dt 338.12ms | 1550585.47 tokens/sec
Step 13531 | loss: 3.075002 | lr:1.7259e-04 | norm 0.2694 | dt 337.33ms | 1554211.85 tokens/sec
Step 13532 | loss: 3.089329 | lr:1.7256e-04 | norm 0.2905 | dt 336.00ms | 1560365.61 tokens/sec
Step 13533 | loss: 3.031587 | lr:1.7252e-04 | norm 0.2972 | dt 337.54ms | 1553273.22 tokens/sec
Step 13534 | loss: 3.009908 | lr:1.7248e-04 | norm 0.2738 | dt 337.51ms | 1553419.15 tokens/sec
Step 13535 | loss: 3.106231 | lr:1.7244e-04 | norm 0.3012 | dt 337.00ms | 1555743.53 tokens/sec
Step 13536 | loss: 3.106990 | lr:1.7241e-04 | norm 0.2766 | dt 338.00ms | 1551126.87 tokens/sec
Step 13537 | loss: 3.037862 | lr:1.7237e-04 | norm 0.2890 | dt 338.04ms | 1550973.71 tokens/sec
Step 13538 | loss: 3.098119 | lr:1.7233e-04 | norm 0.2472 | dt 338.34ms | 1549572.59 tokens/sec
Step 13539 | loss: 3.110335 | lr:1.7229e-04 | norm 0.2807 | dt 338.15ms | 1550477.23 tokens/sec
Step 13540 | loss: 3.054950 | lr:1.7226e-04 | norm 0.2707 | dt 338.66ms | 1548128.22 tokens/sec
Step 13541 | loss: 3.172153 | lr:1.7222e-04 | norm 0.3762 | dt 338.15ms | 1550472.86 tokens/sec
Step 13542 | loss: 3.067267 | lr:1.7218e-04 | norm 0.3121 | dt 338.67ms | 1548065.01 tokens/sec
Step 13543 | loss: 3.158831 | lr:1.7214e-04 | norm 0.3250 | dt 338.66ms | 1548127.13 tokens/sec
Step 13544 | loss: 3.154989 | lr:1.7211e-04 | norm 0.3030 | dt 337.91ms | 1551574.49 tokens/sec
Step 13545 | loss: 3.100159 | lr:1.7207e-04 | norm 0.3122 | dt 338.84ms | 1547284.02 tokens/sec
Step 13546 | loss: 3.080093 | lr:1.7203e-04 | norm 0.3353 | dt 338.54ms | 1548661.37 tokens/sec
Step 13547 | loss: 3.162264 | lr:1.7199e-04 | norm 0.3240 | dt 337.77ms | 1552188.89 tokens/sec
Step 13548 | loss: 3.108810 | lr:1.7196e-04 | norm 0.3366 | dt 339.02ms | 1546489.67 tokens/sec
Step 13549 | loss: 3.146633 | lr:1.7192e-04 | norm 0.3023 | dt 338.46ms | 1549034.46 tokens/sec
Step 13550 | loss: 3.126464 | lr:1.7188e-04 | norm 0.3017 | dt 338.72ms | 1547856.89 tokens/sec
Step 13551 | loss: 3.108638 | lr:1.7184e-04 | norm 0.3165 | dt 338.48ms | 1548951.53 tokens/sec
Step 13552 | loss: 3.074227 | lr:1.7181e-04 | norm 0.2793 | dt 338.21ms | 1550195.24 tokens/sec
Step 13553 | loss: 3.081164 | lr:1.7177e-04 | norm 0.3280 | dt 337.61ms | 1552960.59 tokens/sec
Step 13554 | loss: 3.078400 | lr:1.7173e-04 | norm 0.3043 | dt 337.79ms | 1552096.87 tokens/sec
Step 13555 | loss: 3.116115 | lr:1.7169e-04 | norm 0.2898 | dt 337.91ms | 1551567.92 tokens/sec
Step 13556 | loss: 3.093698 | lr:1.7166e-04 | norm 0.3031 | dt 339.09ms | 1546153.68 tokens/sec
Step 13557 | loss: 3.160903 | lr:1.7162e-04 | norm 0.2860 | dt 337.71ms | 1552492.44 tokens/sec
Step 13558 | loss: 3.200128 | lr:1.7158e-04 | norm 0.3065 | dt 337.70ms | 1552503.40 tokens/sec
Step 13559 | loss: 3.094425 | lr:1.7154e-04 | norm 0.2800 | dt 337.93ms | 1551489.11 tokens/sec
Step 13560 | loss: 3.112596 | lr:1.7151e-04 | norm 0.2767 | dt 338.10ms | 1550684.97 tokens/sec
Step 13561 | loss: 3.172331 | lr:1.7147e-04 | norm 0.2980 | dt 337.75ms | 1552303.94 tokens/sec
Step 13562 | loss: 3.117848 | lr:1.7143e-04 | norm 0.2754 | dt 338.32ms | 1549677.42 tokens/sec
Step 13563 | loss: 3.118786 | lr:1.7140e-04 | norm 0.2855 | dt 337.31ms | 1554322.80 tokens/sec
Step 13564 | loss: 3.134311 | lr:1.7136e-04 | norm 0.2816 | dt 338.49ms | 1548922.07 tokens/sec
Step 13565 | loss: 3.121070 | lr:1.7132e-04 | norm 0.2908 | dt 338.65ms | 1548176.18 tokens/sec
Step 13566 | loss: 3.082543 | lr:1.7128e-04 | norm 0.2779 | dt 339.17ms | 1545791.76 tokens/sec
Step 13567 | loss: 3.046144 | lr:1.7125e-04 | norm 0.2880 | dt 338.85ms | 1547240.47 tokens/sec
Step 13568 | loss: 3.072674 | lr:1.7121e-04 | norm 0.2650 | dt 338.53ms | 1548715.90 tokens/sec
Step 13569 | loss: 3.108265 | lr:1.7117e-04 | norm 0.2994 | dt 338.83ms | 1547344.99 tokens/sec
Step 13570 | loss: 3.118550 | lr:1.7113e-04 | norm 0.3268 | dt 339.65ms | 1543629.18 tokens/sec
Step 13571 | loss: 3.095148 | lr:1.7110e-04 | norm 0.2878 | dt 340.11ms | 1541519.11 tokens/sec
Step 13572 | loss: 3.094244 | lr:1.7106e-04 | norm 0.2911 | dt 337.90ms | 1551589.82 tokens/sec
Step 13573 | loss: 3.058632 | lr:1.7102e-04 | norm 0.2763 | dt 338.25ms | 1550011.67 tokens/sec
Step 13574 | loss: 3.099522 | lr:1.7098e-04 | norm 0.2951 | dt 337.81ms | 1552012.52 tokens/sec
Step 13575 | loss: 3.111118 | lr:1.7095e-04 | norm 0.2698 | dt 338.01ms | 1551109.36 tokens/sec
Step 13576 | loss: 3.132189 | lr:1.7091e-04 | norm 0.3123 | dt 338.31ms | 1549737.49 tokens/sec
Step 13577 | loss: 3.094084 | lr:1.7087e-04 | norm 0.2890 | dt 337.64ms | 1552780.75 tokens/sec
Step 13578 | loss: 3.099775 | lr:1.7083e-04 | norm 0.3043 | dt 338.51ms | 1548799.89 tokens/sec
Step 13579 | loss: 3.140968 | lr:1.7080e-04 | norm 0.2840 | dt 338.70ms | 1547925.53 tokens/sec
Step 13580 | loss: 3.047098 | lr:1.7076e-04 | norm 0.2841 | dt 338.12ms | 1550575.63 tokens/sec
Step 13581 | loss: 3.177583 | lr:1.7072e-04 | norm 0.3365 | dt 338.32ms | 1549694.90 tokens/sec
Step 13582 | loss: 3.110254 | lr:1.7069e-04 | norm 0.2922 | dt 337.64ms | 1552810.36 tokens/sec
Step 13583 | loss: 3.108266 | lr:1.7065e-04 | norm 0.3032 | dt 338.48ms | 1548928.62 tokens/sec
Step 13584 | loss: 3.148533 | lr:1.7061e-04 | norm 0.2869 | dt 337.97ms | 1551282.25 tokens/sec
Step 13585 | loss: 3.102471 | lr:1.7057e-04 | norm 0.3101 | dt 338.48ms | 1548963.53 tokens/sec
Step 13586 | loss: 3.085988 | lr:1.7054e-04 | norm 0.2648 | dt 337.44ms | 1553737.45 tokens/sec
Step 13587 | loss: 3.113323 | lr:1.7050e-04 | norm 0.2977 | dt 337.56ms | 1553159.12 tokens/sec
Step 13588 | loss: 3.130241 | lr:1.7046e-04 | norm 0.2779 | dt 339.51ms | 1544226.46 tokens/sec
Step 13589 | loss: 3.107191 | lr:1.7042e-04 | norm 0.2955 | dt 338.71ms | 1547918.99 tokens/sec
Step 13590 | loss: 3.145511 | lr:1.7039e-04 | norm 0.2743 | dt 337.80ms | 1552066.19 tokens/sec
Step 13591 | loss: 3.105682 | lr:1.7035e-04 | norm 0.2769 | dt 337.74ms | 1552334.62 tokens/sec
Step 13592 | loss: 3.204598 | lr:1.7031e-04 | norm 0.3024 | dt 338.32ms | 1549685.07 tokens/sec
Step 13593 | loss: 3.147417 | lr:1.7028e-04 | norm 0.2867 | dt 337.84ms | 1551860.28 tokens/sec
Step 13594 | loss: 3.100698 | lr:1.7024e-04 | norm 0.2795 | dt 338.00ms | 1551126.87 tokens/sec
Step 13595 | loss: 3.155551 | lr:1.7020e-04 | norm 0.2956 | dt 338.02ms | 1551052.47 tokens/sec
Step 13596 | loss: 3.142374 | lr:1.7016e-04 | norm 0.2729 | dt 338.14ms | 1550503.47 tokens/sec
Step 13597 | loss: 3.089532 | lr:1.7013e-04 | norm 0.2752 | dt 337.40ms | 1553915.31 tokens/sec
Step 13598 | loss: 3.133404 | lr:1.7009e-04 | norm 0.2637 | dt 337.73ms | 1552371.88 tokens/sec
Step 13599 | loss: 3.114457 | lr:1.7005e-04 | norm 0.2614 | dt 338.73ms | 1547814.40 tokens/sec
Step 13600 | loss: 3.122019 | lr:1.7001e-04 | norm 0.3014 | dt 338.25ms | 1550008.39 tokens/sec
Step 13601 | loss: 3.040240 | lr:1.6998e-04 | norm 0.2724 | dt 337.88ms | 1551687.26 tokens/sec
Step 13602 | loss: 3.070644 | lr:1.6994e-04 | norm 0.2682 | dt 338.24ms | 1550038.98 tokens/sec
Step 13603 | loss: 3.096497 | lr:1.6990e-04 | norm 0.2889 | dt 337.65ms | 1552752.24 tokens/sec
Step 13604 | loss: 3.106133 | lr:1.6987e-04 | norm 0.2625 | dt 339.80ms | 1542944.67 tokens/sec
Step 13605 | loss: 3.082075 | lr:1.6983e-04 | norm 0.2775 | dt 338.31ms | 1549714.55 tokens/sec
Step 13606 | loss: 3.026247 | lr:1.6979e-04 | norm 0.2985 | dt 338.91ms | 1546998.83 tokens/sec
Step 13607 | loss: 3.086519 | lr:1.6975e-04 | norm 0.2931 | dt 903.12ms | 580527.85 tokens/sec
Step 13608 | loss: 3.076720 | lr:1.6972e-04 | norm 0.2827 | dt 335.62ms | 1562134.69 tokens/sec
Step 13609 | loss: 3.045775 | lr:1.6968e-04 | norm 0.2891 | dt 339.13ms | 1545986.28 tokens/sec
Step 13610 | loss: 3.077709 | lr:1.6964e-04 | norm 0.2676 | dt 340.70ms | 1538865.39 tokens/sec
Step 13611 | loss: 3.033095 | lr:1.6961e-04 | norm 0.2950 | dt 337.67ms | 1552651.38 tokens/sec
Step 13612 | loss: 3.149643 | lr:1.6957e-04 | norm 0.2762 | dt 337.73ms | 1552385.03 tokens/sec
Step 13613 | loss: 3.157780 | lr:1.6953e-04 | norm 0.2973 | dt 338.78ms | 1547584.56 tokens/sec
Step 13614 | loss: 3.076629 | lr:1.6949e-04 | norm 0.2745 | dt 337.77ms | 1552211.90 tokens/sec
Step 13615 | loss: 3.104148 | lr:1.6946e-04 | norm 0.3061 | dt 337.87ms | 1551742.01 tokens/sec
Step 13616 | loss: 3.122939 | lr:1.6942e-04 | norm 0.2795 | dt 338.20ms | 1550226.93 tokens/sec
Step 13617 | loss: 3.118169 | lr:1.6938e-04 | norm 0.2853 | dt 337.50ms | 1553464.14 tokens/sec
Step 13618 | loss: 3.059049 | lr:1.6935e-04 | norm 0.3121 | dt 338.66ms | 1548135.85 tokens/sec
Step 13619 | loss: 3.152369 | lr:1.6931e-04 | norm 0.2878 | dt 338.01ms | 1551089.67 tokens/sec
Step 13620 | loss: 3.126500 | lr:1.6927e-04 | norm 0.3279 | dt 338.26ms | 1549962.51 tokens/sec
Step 13621 | loss: 3.071135 | lr:1.6923e-04 | norm 0.2913 | dt 338.55ms | 1548648.28 tokens/sec
Step 13622 | loss: 3.121397 | lr:1.6920e-04 | norm 0.3174 | dt 338.61ms | 1548345.14 tokens/sec
Step 13623 | loss: 3.077667 | lr:1.6916e-04 | norm 0.2878 | dt 338.44ms | 1549121.75 tokens/sec
Step 13624 | loss: 3.088037 | lr:1.6912e-04 | norm 0.2938 | dt 337.57ms | 1553116.34 tokens/sec
Step 13625 | loss: 3.109924 | lr:1.6909e-04 | norm 0.2769 | dt 338.65ms | 1548177.27 tokens/sec
Step 13626 | loss: 3.028565 | lr:1.6905e-04 | norm 0.2804 | dt 337.73ms | 1552386.13 tokens/sec
Step 13627 | loss: 3.104843 | lr:1.6901e-04 | norm 0.2938 | dt 337.79ms | 1552104.53 tokens/sec
Step 13628 | loss: 3.126275 | lr:1.6897e-04 | norm 0.2859 | dt 337.61ms | 1552935.37 tokens/sec
Step 13629 | loss: 3.131344 | lr:1.6894e-04 | norm 0.3052 | dt 338.35ms | 1549548.57 tokens/sec
Step 13630 | loss: 3.106928 | lr:1.6890e-04 | norm 0.2786 | dt 338.42ms | 1549212.34 tokens/sec
Step 13631 | loss: 3.112391 | lr:1.6886e-04 | norm 0.2970 | dt 337.13ms | 1555164.81 tokens/sec
Step 13632 | loss: 3.136993 | lr:1.6883e-04 | norm 0.2985 | dt 337.71ms | 1552460.65 tokens/sec
Step 13633 | loss: 3.099783 | lr:1.6879e-04 | norm 0.2916 | dt 338.18ms | 1550322.01 tokens/sec
Step 13634 | loss: 3.129040 | lr:1.6875e-04 | norm 0.2800 | dt 338.60ms | 1548394.20 tokens/sec
Step 13635 | loss: 3.138966 | lr:1.6872e-04 | norm 0.2779 | dt 338.02ms | 1551056.85 tokens/sec
Step 13636 | loss: 3.109411 | lr:1.6868e-04 | norm 0.2610 | dt 339.10ms | 1546137.37 tokens/sec
Step 13637 | loss: 3.142117 | lr:1.6864e-04 | norm 0.3081 | dt 337.92ms | 1551498.96 tokens/sec
Step 13638 | loss: 3.124830 | lr:1.6860e-04 | norm 0.2676 | dt 337.63ms | 1552844.35 tokens/sec
Step 13639 | loss: 3.080025 | lr:1.6857e-04 | norm 0.3011 | dt 339.29ms | 1545243.21 tokens/sec
Step 13640 | loss: 3.085313 | lr:1.6853e-04 | norm 0.2938 | dt 338.50ms | 1548841.34 tokens/sec
Step 13641 | loss: 3.043038 | lr:1.6849e-04 | norm 0.2806 | dt 339.38ms | 1544827.45 tokens/sec
Step 13642 | loss: 3.046661 | lr:1.6846e-04 | norm 0.2906 | dt 338.54ms | 1548661.37 tokens/sec
Step 13643 | loss: 3.080527 | lr:1.6842e-04 | norm 0.2980 | dt 339.06ms | 1546296.10 tokens/sec
Step 13644 | loss: 3.046371 | lr:1.6838e-04 | norm 0.2897 | dt 338.70ms | 1547959.31 tokens/sec
Step 13645 | loss: 3.093739 | lr:1.6834e-04 | norm 0.2869 | dt 338.31ms | 1549709.09 tokens/sec
Step 13646 | loss: 3.032769 | lr:1.6831e-04 | norm 0.3192 | dt 337.93ms | 1551470.50 tokens/sec
Step 13647 | loss: 3.088000 | lr:1.6827e-04 | norm 0.2881 | dt 338.03ms | 1550995.59 tokens/sec
Step 13648 | loss: 3.162314 | lr:1.6823e-04 | norm 0.3166 | dt 338.61ms | 1548360.41 tokens/sec
Step 13649 | loss: 3.104257 | lr:1.6820e-04 | norm 0.2958 | dt 338.13ms | 1550553.76 tokens/sec
Step 13650 | loss: 3.116665 | lr:1.6816e-04 | norm 0.2923 | dt 338.21ms | 1550176.66 tokens/sec
Step 13651 | loss: 3.107152 | lr:1.6812e-04 | norm 0.3038 | dt 338.39ms | 1549344.41 tokens/sec
Step 13652 | loss: 3.157292 | lr:1.6809e-04 | norm 0.2973 | dt 337.87ms | 1551733.25 tokens/sec
Step 13653 | loss: 3.083592 | lr:1.6805e-04 | norm 0.2854 | dt 337.38ms | 1553994.38 tokens/sec
Step 13654 | loss: 3.089648 | lr:1.6801e-04 | norm 0.3060 | dt 338.67ms | 1548096.62 tokens/sec
Step 13655 | loss: 3.113997 | lr:1.6797e-04 | norm 0.3040 | dt 338.31ms | 1549721.11 tokens/sec
Step 13656 | loss: 3.099520 | lr:1.6794e-04 | norm 0.2875 | dt 337.87ms | 1551739.82 tokens/sec
Step 13657 | loss: 3.160363 | lr:1.6790e-04 | norm 0.3408 | dt 337.32ms | 1554279.95 tokens/sec
Step 13658 | loss: 3.204861 | lr:1.6786e-04 | norm 0.3476 | dt 337.63ms | 1552865.18 tokens/sec
Step 13659 | loss: 3.166087 | lr:1.6783e-04 | norm 0.3255 | dt 337.71ms | 1552470.52 tokens/sec
Step 13660 | loss: 3.124028 | lr:1.6779e-04 | norm 0.2968 | dt 337.60ms | 1552969.36 tokens/sec
Step 13661 | loss: 3.103612 | lr:1.6775e-04 | norm 0.3122 | dt 337.19ms | 1554880.01 tokens/sec
Step 13662 | loss: 3.126617 | lr:1.6772e-04 | norm 0.2851 | dt 338.52ms | 1548771.53 tokens/sec
Step 13663 | loss: 3.149447 | lr:1.6768e-04 | norm 0.3118 | dt 338.21ms | 1550199.61 tokens/sec
Step 13664 | loss: 3.121864 | lr:1.6764e-04 | norm 0.2794 | dt 337.53ms | 1553294.06 tokens/sec
Step 13665 | loss: 3.115509 | lr:1.6761e-04 | norm 0.2852 | dt 337.89ms | 1551656.60 tokens/sec
Step 13666 | loss: 3.102756 | lr:1.6757e-04 | norm 0.2744 | dt 338.81ms | 1547452.79 tokens/sec
Step 13667 | loss: 3.086812 | lr:1.6753e-04 | norm 0.2587 | dt 339.63ms | 1543710.45 tokens/sec
Step 13668 | loss: 3.142733 | lr:1.6749e-04 | norm 0.2987 | dt 338.48ms | 1548967.90 tokens/sec
Step 13669 | loss: 3.120287 | lr:1.6746e-04 | norm 0.2735 | dt 338.46ms | 1549024.64 tokens/sec
Step 13670 | loss: 3.091411 | lr:1.6742e-04 | norm 0.3046 | dt 338.25ms | 1550017.13 tokens/sec
Step 13671 | loss: 3.079611 | lr:1.6738e-04 | norm 0.2855 | dt 337.40ms | 1553915.31 tokens/sec
Step 13672 | loss: 3.086740 | lr:1.6735e-04 | norm 0.3013 | dt 337.88ms | 1551715.73 tokens/sec
Step 13673 | loss: 3.040055 | lr:1.6731e-04 | norm 0.2646 | dt 337.91ms | 1551578.87 tokens/sec
Step 13674 | loss: 3.107368 | lr:1.6727e-04 | norm 0.2842 | dt 338.25ms | 1549986.54 tokens/sec
Step 13675 | loss: 3.091927 | lr:1.6724e-04 | norm 0.2847 | dt 338.26ms | 1549952.67 tokens/sec
Step 13676 | loss: 3.094656 | lr:1.6720e-04 | norm 0.2717 | dt 338.13ms | 1550554.86 tokens/sec
Step 13677 | loss: 3.096115 | lr:1.6716e-04 | norm 0.2911 | dt 337.61ms | 1552945.24 tokens/sec
Step 13678 | loss: 3.095501 | lr:1.6713e-04 | norm 0.2781 | dt 338.61ms | 1548347.32 tokens/sec
Step 13679 | loss: 3.107605 | lr:1.6709e-04 | norm 0.2828 | dt 1033.72ms | 507184.29 tokens/sec
Step 13680 | loss: 3.086174 | lr:1.6705e-04 | norm 0.2901 | dt 336.68ms | 1557236.33 tokens/sec
Step 13681 | loss: 3.108799 | lr:1.6702e-04 | norm 0.2755 | dt 336.98ms | 1555843.70 tokens/sec
Step 13682 | loss: 3.109807 | lr:1.6698e-04 | norm 0.2954 | dt 338.62ms | 1548306.99 tokens/sec
Step 13683 | loss: 3.152587 | lr:1.6694e-04 | norm 0.3173 | dt 337.78ms | 1552150.55 tokens/sec
Step 13684 | loss: 3.096545 | lr:1.6691e-04 | norm 0.2839 | dt 336.74ms | 1556972.82 tokens/sec
Step 13685 | loss: 3.098678 | lr:1.6687e-04 | norm 0.2657 | dt 338.65ms | 1548171.82 tokens/sec
Step 13686 | loss: 3.158221 | lr:1.6683e-04 | norm 0.3143 | dt 337.90ms | 1551623.76 tokens/sec
Step 13687 | loss: 3.171374 | lr:1.6679e-04 | norm 0.2819 | dt 338.45ms | 1549083.56 tokens/sec
Step 13688 | loss: 3.159325 | lr:1.6676e-04 | norm 0.3256 | dt 337.61ms | 1552948.53 tokens/sec
Step 13689 | loss: 3.120153 | lr:1.6672e-04 | norm 0.2956 | dt 338.80ms | 1547507.23 tokens/sec
Step 13690 | loss: 3.109588 | lr:1.6668e-04 | norm 0.2887 | dt 339.39ms | 1544814.43 tokens/sec
Step 13691 | loss: 3.137221 | lr:1.6665e-04 | norm 0.3168 | dt 339.28ms | 1545302.94 tokens/sec
Step 13692 | loss: 3.104309 | lr:1.6661e-04 | norm 0.2862 | dt 338.78ms | 1547566.04 tokens/sec
Step 13693 | loss: 3.120626 | lr:1.6657e-04 | norm 0.2843 | dt 338.40ms | 1549314.94 tokens/sec
Step 13694 | loss: 3.147227 | lr:1.6654e-04 | norm 0.2750 | dt 338.75ms | 1547710.91 tokens/sec
Step 13695 | loss: 3.116578 | lr:1.6650e-04 | norm 0.2754 | dt 338.84ms | 1547314.50 tokens/sec
Step 13696 | loss: 3.123781 | lr:1.6646e-04 | norm 0.2821 | dt 339.51ms | 1544263.33 tokens/sec
Step 13697 | loss: 3.062908 | lr:1.6643e-04 | norm 0.2783 | dt 338.77ms | 1547620.50 tokens/sec
Step 13698 | loss: 3.130937 | lr:1.6639e-04 | norm 0.2651 | dt 338.25ms | 1549992.00 tokens/sec
Step 13699 | loss: 3.150908 | lr:1.6635e-04 | norm 0.2978 | dt 338.10ms | 1550699.19 tokens/sec
Step 13700 | loss: 3.114084 | lr:1.6632e-04 | norm 0.2675 | dt 338.60ms | 1548400.75 tokens/sec
Step 13701 | loss: 3.100790 | lr:1.6628e-04 | norm 0.2630 | dt 337.88ms | 1551704.78 tokens/sec
Step 13702 | loss: 3.085593 | lr:1.6624e-04 | norm 0.2840 | dt 337.87ms | 1551750.77 tokens/sec
Step 13703 | loss: 3.116335 | lr:1.6621e-04 | norm 0.2767 | dt 339.03ms | 1546439.64 tokens/sec
Step 13704 | loss: 3.158416 | lr:1.6617e-04 | norm 0.2916 | dt 337.77ms | 1552211.90 tokens/sec
Step 13705 | loss: 3.095540 | lr:1.6613e-04 | norm 0.3135 | dt 337.69ms | 1552562.59 tokens/sec
Step 13706 | loss: 3.102888 | lr:1.6610e-04 | norm 0.2957 | dt 339.91ms | 1542447.91 tokens/sec
Step 13707 | loss: 3.093942 | lr:1.6606e-04 | norm 0.3008 | dt 338.12ms | 1550580.00 tokens/sec
Step 13708 | loss: 3.065592 | lr:1.6602e-04 | norm 0.2853 | dt 338.97ms | 1546724.63 tokens/sec
Step 13709 | loss: 3.090511 | lr:1.6599e-04 | norm 0.3003 | dt 337.97ms | 1551271.31 tokens/sec
Step 13710 | loss: 3.070035 | lr:1.6595e-04 | norm 0.2908 | dt 337.83ms | 1551943.51 tokens/sec
Step 13711 | loss: 3.092790 | lr:1.6591e-04 | norm 0.2675 | dt 343.36ms | 1526942.55 tokens/sec
Step 13712 | loss: 3.083871 | lr:1.6588e-04 | norm 0.2786 | dt 338.37ms | 1549445.94 tokens/sec
Step 13713 | loss: 3.097470 | lr:1.6584e-04 | norm 0.2788 | dt 338.84ms | 1547321.03 tokens/sec
Step 13714 | loss: 3.071152 | lr:1.6580e-04 | norm 0.2808 | dt 338.63ms | 1548266.65 tokens/sec
Step 13715 | loss: 3.036069 | lr:1.6577e-04 | norm 0.2670 | dt 338.46ms | 1549027.91 tokens/sec
Step 13716 | loss: 3.071594 | lr:1.6573e-04 | norm 0.2706 | dt 338.47ms | 1548971.17 tokens/sec
Step 13717 | loss: 3.057843 | lr:1.6569e-04 | norm 0.2877 | dt 338.96ms | 1546739.86 tokens/sec
Step 13718 | loss: 3.057199 | lr:1.6566e-04 | norm 0.2989 | dt 337.57ms | 1553134.99 tokens/sec
Step 13719 | loss: 3.175015 | lr:1.6562e-04 | norm 0.3004 | dt 337.69ms | 1552581.22 tokens/sec
Step 13720 | loss: 3.147408 | lr:1.6558e-04 | norm 0.2852 | dt 338.60ms | 1548421.46 tokens/sec
Step 13721 | loss: 3.044182 | lr:1.6555e-04 | norm 0.2973 | dt 339.23ms | 1545525.58 tokens/sec
Step 13722 | loss: 3.096921 | lr:1.6551e-04 | norm 0.2950 | dt 339.15ms | 1545873.25 tokens/sec
Step 13723 | loss: 3.064797 | lr:1.6547e-04 | norm 0.2897 | dt 337.92ms | 1551517.57 tokens/sec
Step 13724 | loss: 3.106425 | lr:1.6544e-04 | norm 0.3082 | dt 339.66ms | 1543556.58 tokens/sec
Step 13725 | loss: 3.145480 | lr:1.6540e-04 | norm 0.2989 | dt 339.40ms | 1544727.61 tokens/sec
Step 13726 | loss: 3.101899 | lr:1.6536e-04 | norm 0.3035 | dt 339.24ms | 1545493.00 tokens/sec
Step 13727 | loss: 3.143439 | lr:1.6533e-04 | norm 0.2988 | dt 338.45ms | 1549093.38 tokens/sec
Step 13728 | loss: 3.146475 | lr:1.6529e-04 | norm 0.3122 | dt 338.69ms | 1548006.17 tokens/sec
Step 13729 | loss: 3.051691 | lr:1.6525e-04 | norm 0.2855 | dt 339.30ms | 1545196.52 tokens/sec
Step 13730 | loss: 3.099398 | lr:1.6522e-04 | norm 0.3248 | dt 339.47ms | 1544423.84 tokens/sec
Step 13731 | loss: 3.104408 | lr:1.6518e-04 | norm 0.2784 | dt 338.86ms | 1547204.55 tokens/sec
Step 13732 | loss: 3.164582 | lr:1.6514e-04 | norm 0.2964 | dt 337.85ms | 1551855.90 tokens/sec
Step 13733 | loss: 3.134389 | lr:1.6511e-04 | norm 0.3119 | dt 338.88ms | 1547116.38 tokens/sec
Step 13734 | loss: 3.068874 | lr:1.6507e-04 | norm 0.2808 | dt 338.02ms | 1551061.22 tokens/sec
Step 13735 | loss: 3.173370 | lr:1.6503e-04 | norm 0.3887 | dt 337.95ms | 1551385.12 tokens/sec
Step 13736 | loss: 3.096025 | lr:1.6500e-04 | norm 0.3593 | dt 338.39ms | 1549357.51 tokens/sec
Step 13737 | loss: 3.140820 | lr:1.6496e-04 | norm 0.3279 | dt 338.64ms | 1548219.78 tokens/sec
Step 13738 | loss: 3.103325 | lr:1.6492e-04 | norm 0.3200 | dt 338.23ms | 1550101.26 tokens/sec
Step 13739 | loss: 3.125155 | lr:1.6489e-04 | norm 0.2935 | dt 336.99ms | 1555805.17 tokens/sec
Step 13740 | loss: 3.097028 | lr:1.6485e-04 | norm 0.3373 | dt 338.54ms | 1548681.00 tokens/sec
Step 13741 | loss: 3.080600 | lr:1.6481e-04 | norm 0.2839 | dt 338.61ms | 1548370.22 tokens/sec
Step 13742 | loss: 3.064908 | lr:1.6478e-04 | norm 0.2849 | dt 337.14ms | 1555084.52 tokens/sec
Step 13743 | loss: 3.118830 | lr:1.6474e-04 | norm 0.3296 | dt 339.24ms | 1545470.19 tokens/sec
Step 13744 | loss: 3.049019 | lr:1.6470e-04 | norm 0.3353 | dt 338.38ms | 1549423.01 tokens/sec
Step 13745 | loss: 3.094462 | lr:1.6467e-04 | norm 0.3024 | dt 337.51ms | 1553422.44 tokens/sec
Step 13746 | loss: 3.062714 | lr:1.6463e-04 | norm 0.3156 | dt 338.06ms | 1550875.26 tokens/sec
Step 13747 | loss: 3.086641 | lr:1.6459e-04 | norm 0.3062 | dt 337.80ms | 1552068.38 tokens/sec
Step 13748 | loss: 3.164972 | lr:1.6456e-04 | norm 0.2916 | dt 338.45ms | 1549089.02 tokens/sec
Step 13749 | loss: 3.058736 | lr:1.6452e-04 | norm 0.3226 | dt 338.37ms | 1549469.96 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 13750: 3.1319
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3019/10042=0.3006


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but the first thing you need to do is learn. I'm the person who teaches you how to think and express yourself
rank 5 sample 1 >Hello, I'm a language model, because what if there's not another language like Bali or Javanese spoken, like Java or BSL, but
rank 5 sample 2 >Hello, I'm a language model, and you know how something works, which is the same as English. So it works pretty well. I'm going to
rank 5 sample 3 >Hello, I'm a language model, language model, person, who's a person in English?
Answer: In some countries, these people are referred to




ddp_rank 1: ####### Printing generated samples ####### 



ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so let me explain to you something . I will start with:
I used to use it because I want to learn


ddp_rank 6: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, a programmer and a programmer. I am looking for a model that provides a general design process to guide my design process.
rank 7 sample 0 >Hello, I'm a language model, so I thought I should take a look before I take the plunge. You can understand my model even in the simplest of
rank 2 sample 0 >Hello, I'm a language model, writing software. This is, in and out, a great language. How can I ask a computer to give me an
rank 7 sample 1 >Hello, I'm a language model, too, a language model. The answer is, when I'm at the university class, I'm trying to figure out
rank 1 sample 2 >Hello, I'm a language model, but who's not a language model?
I'm a bilingual person. I'm a person who can communicate through oral
rank 2 sample 1 >Hello, I'm a language model, but I only teach language to 3 yr & 4 age and 6 and they are still learning English in this time. In
rank 6 sample 0 >Hello, I'm a language model, with two languages: I would love to hear about you and your stories, if you want to learn more.
As
rank 7 sample 2 >Hello, I'm a language model, so I've added some resources I already had for language modeling, but as I'm writing this, I'll use it
rank 2 sample 2 >Hello, I'm a language model, and I've got a lot of trouble using it. I just don't use this stuff, let me explain. Let
rank 1 sample 3 >Hello, I'm a language model, so I'm working with all the people whose languages I already know. My other language codebook is called "English"
rank 6 sample 1 >Hello, I'm a language model, so I know how to use it. One of these is the Pivotalizer (or Pivotalizer)


rank 7 sample 3 >Hello, I'm a language model, and you'll be surprised at how little language instruction you can get from text. So, for our English class, go


rank 2 sample 3 >Hello, I'm a language model, but that way I could move on and say, okay, so I'm more flexible, this week, if I get


rank 6 sample 2 >Hello, I'm a language model, but how do you tell the truth?
If you're trying to explain it to someone, you usually just assume it
rank 6 sample 3 >Hello, I'm a language model, so let's get started!
What's a language model?
A language model, just like an image. Language




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I don't want to be a mathematician, but I do want to be a writer/journalist who writes for
rank 4 sample 1 >Hello, I'm a language model, right?
Most likely, no, that's just so. Most English learners do not learn English to communicate meaningfully
rank 4 sample 2 >Hello, I'm a language model, I wrote a chapter and we're still using the same language as I did in other projects. It gives me freedom and
rank 4 sample 3 >Hello, I'm a language model, so this work really shows what you can use it to add this feature to a tool. I just wanted to learn more




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not the same person but there's something to draw in
Dalal (2 weeks) I got
rank 3 sample 1 >Hello, I'm a language model, so it was a great, nice, easy to understand course. I'm a language model, so it was so fun
rank 3 sample 2 >Hello, I'm a language model, so you might be frustrated. That's another important part of writing. I have just been using more and more and we
rank 3 sample 3 >Hello, I'm a language model, so let's talk about the kind of things you need, the type of language you have, and language that you use




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't want to spend that much time writing new code with any code. And once in the past, you
rank 0 sample 1 >Hello, I'm a language model, and one of the best known is Dora, "My Name". To make it easy, we go back to the
rank 0 sample 2 >Hello, I'm a language model, and I believe in working backwards. If you haven't already tried the first version of the language, you can check it
rank 0 sample 3 >Hello, I'm a language model, and thanks for reading.
The book "Unraveling the Truth" was selected in this week's edition by I


Step 13750 | loss: 3.121721 | lr:1.6449e-04 | norm 0.2830 | dt 12228.64ms | 42873.77 tokens/sec
Step 13751 | loss: 3.062264 | lr:1.6445e-04 | norm 0.2777 | dt 335.59ms | 1562305.61 tokens/sec
Step 13752 | loss: 3.121543 | lr:1.6441e-04 | norm 0.2964 | dt 335.83ms | 1561182.04 tokens/sec
Step 13753 | loss: 3.191613 | lr:1.6438e-04 | norm 0.3228 | dt 336.81ms | 1556635.56 tokens/sec
Step 13754 | loss: 3.105026 | lr:1.6434e-04 | norm 0.2876 | dt 336.58ms | 1557680.87 tokens/sec
Step 13755 | loss: 3.180930 | lr:1.6430e-04 | norm 0.3033 | dt 337.36ms | 1554091.02 tokens/sec
Step 13756 | loss: 3.157924 | lr:1.6427e-04 | norm 0.2960 | dt 337.43ms | 1553764.89 tokens/sec
Step 13757 | loss: 3.142323 | lr:1.6423e-04 | norm 0.2942 | dt 336.36ms | 1558717.64 tokens/sec
Step 13758 | loss: 3.176730 | lr:1.6419e-04 | norm 0.3330 | dt 336.95ms | 1555972.50 tokens/sec
Step 13759 | loss: 3.123084 | lr:1.6416e-04 | norm 0.3048 | dt 337.29ms | 1554411.79 tokens/sec
Step 13760 | loss: 3.054520 | lr:1.6412e-04 | norm 0.3764 | dt 337.35ms | 1554156.92 tokens/sec
Step 13761 | loss: 3.124128 | lr:1.6408e-04 | norm 0.3055 | dt 336.82ms | 1556579.37 tokens/sec
Step 13762 | loss: 3.146676 | lr:1.6405e-04 | norm 0.3288 | dt 337.09ms | 1555352.90 tokens/sec
Step 13763 | loss: 3.087049 | lr:1.6401e-04 | norm 0.3166 | dt 336.46ms | 1558244.90 tokens/sec
Step 13764 | loss: 3.099753 | lr:1.6397e-04 | norm 0.2902 | dt 337.09ms | 1555346.30 tokens/sec
Step 13765 | loss: 3.175016 | lr:1.6394e-04 | norm 0.3238 | dt 336.88ms | 1556322.68 tokens/sec
Step 13766 | loss: 3.077320 | lr:1.6390e-04 | norm 0.3010 | dt 337.66ms | 1552716.06 tokens/sec
Step 13767 | loss: 3.121053 | lr:1.6387e-04 | norm 0.2961 | dt 337.60ms | 1552988.01 tokens/sec
Step 13768 | loss: 3.118549 | lr:1.6383e-04 | norm 0.3065 | dt 337.43ms | 1553764.89 tokens/sec
Step 13769 | loss: 3.114191 | lr:1.6379e-04 | norm 0.3840 | dt 336.95ms | 1555976.90 tokens/sec
Step 13770 | loss: 3.159449 | lr:1.6376e-04 | norm 0.3003 | dt 336.64ms | 1557405.07 tokens/sec
Step 13771 | loss: 3.156955 | lr:1.6372e-04 | norm 0.2991 | dt 337.23ms | 1554673.34 tokens/sec
Step 13772 | loss: 3.118678 | lr:1.6368e-04 | norm 0.3141 | dt 337.99ms | 1551173.92 tokens/sec
Step 13773 | loss: 3.113847 | lr:1.6365e-04 | norm 0.2963 | dt 337.33ms | 1554247.00 tokens/sec
Step 13774 | loss: 3.116156 | lr:1.6361e-04 | norm 0.3323 | dt 337.67ms | 1552685.37 tokens/sec
Step 13775 | loss: 3.054384 | lr:1.6357e-04 | norm 0.2850 | dt 337.07ms | 1555428.81 tokens/sec
Step 13776 | loss: 3.074795 | lr:1.6354e-04 | norm 0.3001 | dt 337.94ms | 1551425.62 tokens/sec
Step 13777 | loss: 3.073757 | lr:1.6350e-04 | norm 0.2955 | dt 337.78ms | 1552172.46 tokens/sec
Step 13778 | loss: 3.057153 | lr:1.6346e-04 | norm 0.2914 | dt 338.61ms | 1548350.59 tokens/sec
Step 13779 | loss: 3.076392 | lr:1.6343e-04 | norm 0.2959 | dt 337.91ms | 1551538.37 tokens/sec
Step 13780 | loss: 3.087504 | lr:1.6339e-04 | norm 0.2690 | dt 339.04ms | 1546368.96 tokens/sec
Step 13781 | loss: 3.049169 | lr:1.6336e-04 | norm 0.3029 | dt 339.12ms | 1546013.45 tokens/sec
Step 13782 | loss: 2.998848 | lr:1.6332e-04 | norm 0.2789 | dt 338.60ms | 1548409.47 tokens/sec
Step 13783 | loss: 3.081254 | lr:1.6328e-04 | norm 0.2825 | dt 339.03ms | 1546422.24 tokens/sec
Step 13784 | loss: 3.108849 | lr:1.6325e-04 | norm 0.2957 | dt 337.77ms | 1552202.04 tokens/sec
Step 13785 | loss: 3.095096 | lr:1.6321e-04 | norm 0.2688 | dt 339.31ms | 1545142.24 tokens/sec
Step 13786 | loss: 3.059571 | lr:1.6317e-04 | norm 0.2779 | dt 338.29ms | 1549830.33 tokens/sec
Step 13787 | loss: 3.120192 | lr:1.6314e-04 | norm 0.3101 | dt 338.39ms | 1549342.23 tokens/sec
Step 13788 | loss: 3.008657 | lr:1.6310e-04 | norm 0.2784 | dt 338.58ms | 1548491.24 tokens/sec
Step 13789 | loss: 3.047167 | lr:1.6307e-04 | norm 0.2988 | dt 339.31ms | 1545181.32 tokens/sec
Step 13790 | loss: 3.186875 | lr:1.6303e-04 | norm 0.3066 | dt 338.85ms | 1547253.53 tokens/sec
Step 13791 | loss: 3.081411 | lr:1.6299e-04 | norm 0.2913 | dt 338.68ms | 1548037.77 tokens/sec
Step 13792 | loss: 3.139493 | lr:1.6296e-04 | norm 0.2866 | dt 338.83ms | 1547344.99 tokens/sec
Step 13793 | loss: 3.112245 | lr:1.6292e-04 | norm 0.2809 | dt 338.71ms | 1547887.40 tokens/sec
Step 13794 | loss: 3.111040 | lr:1.6288e-04 | norm 0.3037 | dt 338.71ms | 1547915.72 tokens/sec
Step 13795 | loss: 3.129537 | lr:1.6285e-04 | norm 0.3008 | dt 338.91ms | 1546977.06 tokens/sec
Step 13796 | loss: 3.088151 | lr:1.6281e-04 | norm 0.3022 | dt 1016.97ms | 515538.86 tokens/sec
Step 13797 | loss: 3.182442 | lr:1.6277e-04 | norm 0.3206 | dt 335.94ms | 1560651.32 tokens/sec
Step 13798 | loss: 3.116487 | lr:1.6274e-04 | norm 0.3027 | dt 337.57ms | 1553108.66 tokens/sec
Step 13799 | loss: 3.106084 | lr:1.6270e-04 | norm 0.3055 | dt 339.06ms | 1546309.15 tokens/sec
Step 13800 | loss: 3.132206 | lr:1.6267e-04 | norm 0.3013 | dt 338.59ms | 1548446.54 tokens/sec
Step 13801 | loss: 3.094540 | lr:1.6263e-04 | norm 0.2999 | dt 337.92ms | 1551529.61 tokens/sec
Step 13802 | loss: 3.129205 | lr:1.6259e-04 | norm 0.2916 | dt 337.99ms | 1551184.86 tokens/sec
Step 13803 | loss: 3.148059 | lr:1.6256e-04 | norm 0.2783 | dt 338.26ms | 1549950.49 tokens/sec
Step 13804 | loss: 3.115510 | lr:1.6252e-04 | norm 0.2939 | dt 339.21ms | 1545609.23 tokens/sec
Step 13805 | loss: 3.071053 | lr:1.6248e-04 | norm 0.2899 | dt 338.80ms | 1547475.65 tokens/sec
Step 13806 | loss: 3.106873 | lr:1.6245e-04 | norm 0.2750 | dt 339.53ms | 1544142.96 tokens/sec
Step 13807 | loss: 3.115813 | lr:1.6241e-04 | norm 0.2757 | dt 338.61ms | 1548346.23 tokens/sec
Step 13808 | loss: 3.097790 | lr:1.6238e-04 | norm 0.2739 | dt 339.62ms | 1543750.55 tokens/sec
Step 13809 | loss: 3.082330 | lr:1.6234e-04 | norm 0.3017 | dt 339.11ms | 1546058.02 tokens/sec
Step 13810 | loss: 3.100914 | lr:1.6230e-04 | norm 0.2896 | dt 338.38ms | 1549419.74 tokens/sec
Step 13811 | loss: 3.117594 | lr:1.6227e-04 | norm 0.2911 | dt 339.24ms | 1545475.62 tokens/sec
Step 13812 | loss: 3.008492 | lr:1.6223e-04 | norm 0.2743 | dt 339.48ms | 1544377.20 tokens/sec
Step 13813 | loss: 3.111840 | lr:1.6220e-04 | norm 0.2963 | dt 338.98ms | 1546680.02 tokens/sec
Step 13814 | loss: 3.098720 | lr:1.6216e-04 | norm 0.2813 | dt 339.16ms | 1545836.31 tokens/sec
Step 13815 | loss: 3.102148 | lr:1.6212e-04 | norm 0.2733 | dt 339.50ms | 1544280.68 tokens/sec
Step 13816 | loss: 3.057061 | lr:1.6209e-04 | norm 0.2927 | dt 338.90ms | 1547019.51 tokens/sec
Step 13817 | loss: 3.062693 | lr:1.6205e-04 | norm 0.2616 | dt 339.25ms | 1545451.72 tokens/sec
Step 13818 | loss: 3.121145 | lr:1.6201e-04 | norm 0.2649 | dt 339.17ms | 1545774.37 tokens/sec
Step 13819 | loss: 3.065198 | lr:1.6198e-04 | norm 0.2903 | dt 338.32ms | 1549687.25 tokens/sec
Step 13820 | loss: 3.121759 | lr:1.6194e-04 | norm 0.2998 | dt 338.52ms | 1548786.80 tokens/sec
Step 13821 | loss: 3.053426 | lr:1.6191e-04 | norm 0.2723 | dt 338.47ms | 1549015.91 tokens/sec
Step 13822 | loss: 3.172544 | lr:1.6187e-04 | norm 0.3086 | dt 338.76ms | 1547680.41 tokens/sec
Step 13823 | loss: 3.122109 | lr:1.6183e-04 | norm 0.2930 | dt 338.26ms | 1549957.04 tokens/sec
Step 13824 | loss: 3.119009 | lr:1.6180e-04 | norm 0.2840 | dt 338.51ms | 1548823.89 tokens/sec
Step 13825 | loss: 3.113823 | lr:1.6176e-04 | norm 0.3107 | dt 338.17ms | 1550353.71 tokens/sec
Step 13826 | loss: 3.091444 | lr:1.6172e-04 | norm 0.2794 | dt 338.15ms | 1550454.28 tokens/sec
Step 13827 | loss: 3.184720 | lr:1.6169e-04 | norm 0.2936 | dt 337.98ms | 1551231.91 tokens/sec
Step 13828 | loss: 3.126942 | lr:1.6165e-04 | norm 0.2894 | dt 339.11ms | 1546061.28 tokens/sec
Step 13829 | loss: 3.196896 | lr:1.6162e-04 | norm 0.2968 | dt 337.65ms | 1552771.98 tokens/sec
Step 13830 | loss: 3.129851 | lr:1.6158e-04 | norm 0.2778 | dt 338.54ms | 1548661.37 tokens/sec
Step 13831 | loss: 3.082756 | lr:1.6154e-04 | norm 0.3026 | dt 338.65ms | 1548169.64 tokens/sec
Step 13832 | loss: 3.103962 | lr:1.6151e-04 | norm 0.2710 | dt 337.99ms | 1551184.86 tokens/sec
Step 13833 | loss: 3.103287 | lr:1.6147e-04 | norm 0.2872 | dt 337.90ms | 1551611.72 tokens/sec
Step 13834 | loss: 3.112693 | lr:1.6144e-04 | norm 0.2931 | dt 338.94ms | 1546843.22 tokens/sec
Step 13835 | loss: 3.187640 | lr:1.6140e-04 | norm 0.2830 | dt 337.69ms | 1552574.65 tokens/sec
Step 13836 | loss: 3.044859 | lr:1.6136e-04 | norm 0.3007 | dt 337.49ms | 1553477.31 tokens/sec
Step 13837 | loss: 3.080600 | lr:1.6133e-04 | norm 0.2711 | dt 338.63ms | 1548253.57 tokens/sec
Step 13838 | loss: 3.115558 | lr:1.6129e-04 | norm 0.2708 | dt 338.37ms | 1549469.96 tokens/sec
Step 13839 | loss: 3.145492 | lr:1.6126e-04 | norm 0.2851 | dt 337.60ms | 1552981.43 tokens/sec
Step 13840 | loss: 3.147051 | lr:1.6122e-04 | norm 0.2808 | dt 338.00ms | 1551146.56 tokens/sec
Step 13841 | loss: 3.191932 | lr:1.6118e-04 | norm 0.2853 | dt 338.37ms | 1549460.13 tokens/sec
Step 13842 | loss: 3.118799 | lr:1.6115e-04 | norm 0.2956 | dt 337.83ms | 1551910.65 tokens/sec
Step 13843 | loss: 3.063683 | lr:1.6111e-04 | norm 0.2855 | dt 337.65ms | 1552743.47 tokens/sec
Step 13844 | loss: 3.099939 | lr:1.6108e-04 | norm 0.2771 | dt 338.05ms | 1550936.52 tokens/sec
Step 13845 | loss: 3.085706 | lr:1.6104e-04 | norm 0.2942 | dt 338.11ms | 1550621.55 tokens/sec
Step 13846 | loss: 3.057043 | lr:1.6100e-04 | norm 0.2856 | dt 337.71ms | 1552488.05 tokens/sec
Step 13847 | loss: 3.060979 | lr:1.6097e-04 | norm 0.2885 | dt 337.63ms | 1552850.93 tokens/sec
Step 13848 | loss: 3.131759 | lr:1.6093e-04 | norm 0.2961 | dt 337.60ms | 1552962.78 tokens/sec
Step 13849 | loss: 3.039890 | lr:1.6090e-04 | norm 0.2800 | dt 337.77ms | 1552191.08 tokens/sec
Step 13850 | loss: 3.020261 | lr:1.6086e-04 | norm 0.2853 | dt 338.04ms | 1550969.33 tokens/sec
Step 13851 | loss: 3.103652 | lr:1.6082e-04 | norm 0.2817 | dt 337.80ms | 1552068.38 tokens/sec
Step 13852 | loss: 3.125563 | lr:1.6079e-04 | norm 0.2721 | dt 338.16ms | 1550430.23 tokens/sec
Step 13853 | loss: 3.058517 | lr:1.6075e-04 | norm 0.2646 | dt 337.38ms | 1553988.89 tokens/sec
Step 13854 | loss: 3.092469 | lr:1.6072e-04 | norm 0.2727 | dt 337.94ms | 1551416.87 tokens/sec
Step 13855 | loss: 3.073235 | lr:1.6068e-04 | norm 0.2956 | dt 337.83ms | 1551928.18 tokens/sec
Step 13856 | loss: 3.124506 | lr:1.6064e-04 | norm 0.2770 | dt 337.91ms | 1551538.37 tokens/sec
Step 13857 | loss: 3.072458 | lr:1.6061e-04 | norm 0.2725 | dt 337.72ms | 1552434.35 tokens/sec
Step 13858 | loss: 3.092598 | lr:1.6057e-04 | norm 0.2708 | dt 337.95ms | 1551373.09 tokens/sec
Step 13859 | loss: 3.103310 | lr:1.6054e-04 | norm 0.2808 | dt 337.86ms | 1551805.52 tokens/sec
Step 13860 | loss: 3.129704 | lr:1.6050e-04 | norm 0.2897 | dt 338.14ms | 1550516.59 tokens/sec
Step 13861 | loss: 3.080026 | lr:1.6046e-04 | norm 0.2786 | dt 338.07ms | 1550826.04 tokens/sec
Step 13862 | loss: 3.154412 | lr:1.6043e-04 | norm 0.3045 | dt 338.16ms | 1550431.32 tokens/sec
Step 13863 | loss: 3.101298 | lr:1.6039e-04 | norm 0.2766 | dt 339.69ms | 1543420.08 tokens/sec
Step 13864 | loss: 3.112695 | lr:1.6036e-04 | norm 0.2956 | dt 338.92ms | 1546928.09 tokens/sec
Step 13865 | loss: 3.121972 | lr:1.6032e-04 | norm 0.2806 | dt 339.37ms | 1544875.20 tokens/sec
Step 13866 | loss: 3.158801 | lr:1.6028e-04 | norm 0.3019 | dt 339.25ms | 1545446.29 tokens/sec
Step 13867 | loss: 3.091071 | lr:1.6025e-04 | norm 0.2939 | dt 339.04ms | 1546375.48 tokens/sec
Step 13868 | loss: 3.121478 | lr:1.6021e-04 | norm 0.2817 | dt 338.85ms | 1547261.16 tokens/sec
Step 13869 | loss: 3.124192 | lr:1.6018e-04 | norm 0.2933 | dt 986.61ms | 531401.99 tokens/sec
Step 13870 | loss: 3.084475 | lr:1.6014e-04 | norm 0.2763 | dt 337.41ms | 1553839.55 tokens/sec
Step 13871 | loss: 3.103374 | lr:1.6010e-04 | norm 0.2884 | dt 338.37ms | 1549462.31 tokens/sec
Step 13872 | loss: 3.081894 | lr:1.6007e-04 | norm 0.2614 | dt 337.66ms | 1552719.35 tokens/sec
Step 13873 | loss: 3.149892 | lr:1.6003e-04 | norm 0.2985 | dt 341.64ms | 1534636.25 tokens/sec
Step 13874 | loss: 3.113780 | lr:1.6000e-04 | norm 0.2764 | dt 337.17ms | 1554951.47 tokens/sec
Step 13875 | loss: 3.127046 | lr:1.5996e-04 | norm 0.2941 | dt 337.64ms | 1552784.04 tokens/sec
Step 13876 | loss: 3.090424 | lr:1.5992e-04 | norm 0.2887 | dt 337.49ms | 1553469.63 tokens/sec
Step 13877 | loss: 3.080289 | lr:1.5989e-04 | norm 0.3066 | dt 337.46ms | 1553646.34 tokens/sec
Step 13878 | loss: 3.084177 | lr:1.5985e-04 | norm 0.2816 | dt 337.31ms | 1554320.60 tokens/sec
Step 13879 | loss: 3.074179 | lr:1.5982e-04 | norm 0.2976 | dt 337.60ms | 1552966.07 tokens/sec
Step 13880 | loss: 3.123023 | lr:1.5978e-04 | norm 0.2950 | dt 337.77ms | 1552220.67 tokens/sec
Step 13881 | loss: 3.067801 | lr:1.5974e-04 | norm 0.2759 | dt 338.38ms | 1549415.37 tokens/sec
Step 13882 | loss: 3.056235 | lr:1.5971e-04 | norm 0.2729 | dt 338.87ms | 1547157.74 tokens/sec
Step 13883 | loss: 3.104313 | lr:1.5967e-04 | norm 0.2724 | dt 338.89ms | 1547093.52 tokens/sec
Step 13884 | loss: 3.078866 | lr:1.5964e-04 | norm 0.2786 | dt 338.84ms | 1547291.64 tokens/sec
Step 13885 | loss: 3.097951 | lr:1.5960e-04 | norm 0.2936 | dt 338.55ms | 1548605.75 tokens/sec
Step 13886 | loss: 3.112506 | lr:1.5957e-04 | norm 0.2798 | dt 339.36ms | 1544931.64 tokens/sec
Step 13887 | loss: 3.076961 | lr:1.5953e-04 | norm 0.2800 | dt 338.63ms | 1548266.65 tokens/sec
Step 13888 | loss: 3.079621 | lr:1.5949e-04 | norm 0.2896 | dt 338.75ms | 1547701.10 tokens/sec
Step 13889 | loss: 3.028914 | lr:1.5946e-04 | norm 0.2599 | dt 338.67ms | 1548083.54 tokens/sec
Step 13890 | loss: 3.048808 | lr:1.5942e-04 | norm 0.2908 | dt 338.68ms | 1548050.85 tokens/sec
Step 13891 | loss: 3.064074 | lr:1.5939e-04 | norm 0.2509 | dt 338.24ms | 1550037.89 tokens/sec
Step 13892 | loss: 3.083796 | lr:1.5935e-04 | norm 0.2828 | dt 338.12ms | 1550594.22 tokens/sec
Step 13893 | loss: 3.161046 | lr:1.5932e-04 | norm 0.2582 | dt 338.34ms | 1549598.80 tokens/sec
Step 13894 | loss: 3.170591 | lr:1.5928e-04 | norm 0.3269 | dt 337.48ms | 1553521.21 tokens/sec
Step 13895 | loss: 3.092548 | lr:1.5924e-04 | norm 0.2890 | dt 338.06ms | 1550853.39 tokens/sec
Step 13896 | loss: 3.207595 | lr:1.5921e-04 | norm 0.3361 | dt 337.82ms | 1551989.51 tokens/sec
Step 13897 | loss: 3.142387 | lr:1.5917e-04 | norm 0.2951 | dt 336.96ms | 1555955.98 tokens/sec
Step 13898 | loss: 3.211255 | lr:1.5914e-04 | norm 0.3338 | dt 338.07ms | 1550831.51 tokens/sec
Step 13899 | loss: 3.119466 | lr:1.5910e-04 | norm 0.2887 | dt 338.17ms | 1550376.67 tokens/sec
Step 13900 | loss: 3.159138 | lr:1.5906e-04 | norm 0.3110 | dt 338.29ms | 1549818.31 tokens/sec
Step 13901 | loss: 3.145072 | lr:1.5903e-04 | norm 0.2973 | dt 338.34ms | 1549594.43 tokens/sec
Step 13902 | loss: 3.121048 | lr:1.5899e-04 | norm 0.2842 | dt 338.46ms | 1549018.09 tokens/sec
Step 13903 | loss: 3.128032 | lr:1.5896e-04 | norm 0.2952 | dt 339.51ms | 1544226.46 tokens/sec
Step 13904 | loss: 3.096792 | lr:1.5892e-04 | norm 0.2716 | dt 338.99ms | 1546631.07 tokens/sec
Step 13905 | loss: 3.107384 | lr:1.5889e-04 | norm 0.3125 | dt 338.42ms | 1549237.44 tokens/sec
Step 13906 | loss: 3.080549 | lr:1.5885e-04 | norm 0.3126 | dt 338.44ms | 1549125.03 tokens/sec
Step 13907 | loss: 3.147379 | lr:1.5881e-04 | norm 0.2949 | dt 338.97ms | 1546694.16 tokens/sec
Step 13908 | loss: 3.109755 | lr:1.5878e-04 | norm 0.3382 | dt 337.86ms | 1551801.14 tokens/sec
Step 13909 | loss: 3.101061 | lr:1.5874e-04 | norm 0.2885 | dt 338.00ms | 1551129.06 tokens/sec
Step 13910 | loss: 3.120396 | lr:1.5871e-04 | norm 0.3066 | dt 337.91ms | 1551573.40 tokens/sec
Step 13911 | loss: 3.090071 | lr:1.5867e-04 | norm 0.2644 | dt 338.73ms | 1547815.49 tokens/sec
Step 13912 | loss: 3.179786 | lr:1.5864e-04 | norm 0.2826 | dt 338.47ms | 1549003.90 tokens/sec
Step 13913 | loss: 3.092163 | lr:1.5860e-04 | norm 0.2716 | dt 340.38ms | 1540310.86 tokens/sec
Step 13914 | loss: 3.127544 | lr:1.5856e-04 | norm 0.2740 | dt 339.26ms | 1545382.21 tokens/sec
Step 13915 | loss: 3.086379 | lr:1.5853e-04 | norm 0.2915 | dt 338.69ms | 1547978.92 tokens/sec
Step 13916 | loss: 3.068767 | lr:1.5849e-04 | norm 0.2650 | dt 338.47ms | 1548977.72 tokens/sec
Step 13917 | loss: 3.030746 | lr:1.5846e-04 | norm 0.2720 | dt 338.65ms | 1548168.55 tokens/sec
Step 13918 | loss: 3.097678 | lr:1.5842e-04 | norm 0.3006 | dt 339.15ms | 1545894.99 tokens/sec
Step 13919 | loss: 3.117366 | lr:1.5839e-04 | norm 0.2866 | dt 338.84ms | 1547317.77 tokens/sec
Step 13920 | loss: 3.081341 | lr:1.5835e-04 | norm 0.2761 | dt 338.34ms | 1549608.62 tokens/sec
Step 13921 | loss: 3.130030 | lr:1.5831e-04 | norm 0.2834 | dt 338.62ms | 1548303.72 tokens/sec
Step 13922 | loss: 3.065227 | lr:1.5828e-04 | norm 0.3006 | dt 339.06ms | 1546302.63 tokens/sec
Step 13923 | loss: 3.066025 | lr:1.5824e-04 | norm 0.2874 | dt 342.70ms | 1529883.02 tokens/sec
Step 13924 | loss: 3.036525 | lr:1.5821e-04 | norm 0.2872 | dt 339.48ms | 1544404.32 tokens/sec
Step 13925 | loss: 3.040704 | lr:1.5817e-04 | norm 0.2805 | dt 338.98ms | 1546647.39 tokens/sec
Step 13926 | loss: 3.069623 | lr:1.5814e-04 | norm 0.2924 | dt 338.70ms | 1547942.97 tokens/sec
Step 13927 | loss: 3.097351 | lr:1.5810e-04 | norm 0.2877 | dt 339.15ms | 1545890.64 tokens/sec
Step 13928 | loss: 3.094490 | lr:1.5807e-04 | norm 0.2888 | dt 338.15ms | 1550476.14 tokens/sec
Step 13929 | loss: 3.168554 | lr:1.5803e-04 | norm 0.3081 | dt 338.63ms | 1548273.19 tokens/sec
Step 13930 | loss: 3.104528 | lr:1.5799e-04 | norm 0.2963 | dt 339.42ms | 1544648.41 tokens/sec
Step 13931 | loss: 3.098376 | lr:1.5796e-04 | norm 0.2894 | dt 337.91ms | 1551578.87 tokens/sec
Step 13932 | loss: 3.073593 | lr:1.5792e-04 | norm 0.2962 | dt 338.40ms | 1549302.93 tokens/sec
Step 13933 | loss: 3.124835 | lr:1.5789e-04 | norm 0.2970 | dt 339.71ms | 1543357.25 tokens/sec
Step 13934 | loss: 3.139890 | lr:1.5785e-04 | norm 0.2949 | dt 338.99ms | 1546625.63 tokens/sec
Step 13935 | loss: 3.095171 | lr:1.5782e-04 | norm 0.2971 | dt 338.04ms | 1550979.18 tokens/sec
Step 13936 | loss: 3.171911 | lr:1.5778e-04 | norm 0.3123 | dt 338.63ms | 1548241.58 tokens/sec
Step 13937 | loss: 3.123547 | lr:1.5774e-04 | norm 0.2742 | dt 339.46ms | 1544466.15 tokens/sec
Step 13938 | loss: 3.168430 | lr:1.5771e-04 | norm 0.3527 | dt 338.87ms | 1547180.60 tokens/sec
Step 13939 | loss: 3.120895 | lr:1.5767e-04 | norm 0.2887 | dt 338.55ms | 1548643.92 tokens/sec
Step 13940 | loss: 3.117778 | lr:1.5764e-04 | norm 0.3158 | dt 338.32ms | 1549689.44 tokens/sec
Step 13941 | loss: 3.062995 | lr:1.5760e-04 | norm 0.2966 | dt 338.92ms | 1546937.89 tokens/sec
Step 13942 | loss: 3.141865 | lr:1.5757e-04 | norm 0.3122 | dt 338.05ms | 1550912.45 tokens/sec
Step 13943 | loss: 3.150722 | lr:1.5753e-04 | norm 0.2871 | dt 338.56ms | 1548577.39 tokens/sec
Step 13944 | loss: 3.084614 | lr:1.5750e-04 | norm 0.2808 | dt 338.68ms | 1548020.33 tokens/sec
Step 13945 | loss: 3.097088 | lr:1.5746e-04 | norm 0.2879 | dt 337.78ms | 1552157.12 tokens/sec
Step 13946 | loss: 3.126990 | lr:1.5742e-04 | norm 0.2807 | dt 337.24ms | 1554660.15 tokens/sec
Step 13947 | loss: 3.104020 | lr:1.5739e-04 | norm 0.2961 | dt 339.35ms | 1544992.43 tokens/sec
Step 13948 | loss: 3.117970 | lr:1.5735e-04 | norm 0.3018 | dt 338.04ms | 1550964.96 tokens/sec
Step 13949 | loss: 3.081585 | lr:1.5732e-04 | norm 0.3091 | dt 337.61ms | 1552945.24 tokens/sec
Step 13950 | loss: 3.123007 | lr:1.5728e-04 | norm 0.2821 | dt 338.98ms | 1546650.65 tokens/sec
Step 13951 | loss: 3.117498 | lr:1.5725e-04 | norm 0.3156 | dt 338.08ms | 1550783.39 tokens/sec
Step 13952 | loss: 3.043104 | lr:1.5721e-04 | norm 0.2681 | dt 337.87ms | 1551759.53 tokens/sec
Step 13953 | loss: 3.051830 | lr:1.5718e-04 | norm 0.2702 | dt 338.33ms | 1549627.19 tokens/sec
Step 13954 | loss: 3.021840 | lr:1.5714e-04 | norm 0.2783 | dt 338.41ms | 1549257.09 tokens/sec
Step 13955 | loss: 3.114950 | lr:1.5710e-04 | norm 0.3099 | dt 338.32ms | 1549665.41 tokens/sec
Step 13956 | loss: 3.096665 | lr:1.5707e-04 | norm 0.3182 | dt 337.60ms | 1552997.88 tokens/sec
Step 13957 | loss: 3.071985 | lr:1.5703e-04 | norm 0.2822 | dt 338.39ms | 1549354.24 tokens/sec
Step 13958 | loss: 3.063306 | lr:1.5700e-04 | norm 0.2927 | dt 338.63ms | 1548249.21 tokens/sec
Step 13959 | loss: 3.043154 | lr:1.5696e-04 | norm 0.3164 | dt 337.73ms | 1552406.95 tokens/sec
Step 13960 | loss: 3.034758 | lr:1.5693e-04 | norm 0.2852 | dt 338.34ms | 1549584.60 tokens/sec
Step 13961 | loss: 3.041476 | lr:1.5689e-04 | norm 0.2969 | dt 338.69ms | 1548010.53 tokens/sec
Step 13962 | loss: 3.067392 | lr:1.5686e-04 | norm 0.3112 | dt 338.80ms | 1547487.63 tokens/sec
Step 13963 | loss: 3.058755 | lr:1.5682e-04 | norm 0.2945 | dt 337.40ms | 1553888.96 tokens/sec
Step 13964 | loss: 3.091955 | lr:1.5679e-04 | norm 0.3171 | dt 338.64ms | 1548204.52 tokens/sec
Step 13965 | loss: 3.153360 | lr:1.5675e-04 | norm 0.2925 | dt 338.79ms | 1547547.53 tokens/sec
Step 13966 | loss: 3.058016 | lr:1.5671e-04 | norm 0.3057 | dt 338.88ms | 1547103.31 tokens/sec
Step 13967 | loss: 3.090560 | lr:1.5668e-04 | norm 0.2777 | dt 338.31ms | 1549726.57 tokens/sec
Step 13968 | loss: 3.088225 | lr:1.5664e-04 | norm 0.3185 | dt 338.48ms | 1548960.26 tokens/sec
Step 13969 | loss: 3.154259 | lr:1.5661e-04 | norm 0.3118 | dt 338.88ms | 1547116.38 tokens/sec
Step 13970 | loss: 3.083421 | lr:1.5657e-04 | norm 0.3090 | dt 338.85ms | 1547269.86 tokens/sec
Step 13971 | loss: 3.127599 | lr:1.5654e-04 | norm 0.3256 | dt 337.95ms | 1551369.80 tokens/sec
Step 13972 | loss: 3.071874 | lr:1.5650e-04 | norm 0.3106 | dt 337.33ms | 1554243.70 tokens/sec
Step 13973 | loss: 3.127557 | lr:1.5647e-04 | norm 0.3266 | dt 337.86ms | 1551803.33 tokens/sec
Step 13974 | loss: 3.158600 | lr:1.5643e-04 | norm 0.3064 | dt 338.47ms | 1549008.27 tokens/sec
Step 13975 | loss: 3.113369 | lr:1.5640e-04 | norm 0.3319 | dt 337.22ms | 1554755.78 tokens/sec
Step 13976 | loss: 3.059886 | lr:1.5636e-04 | norm 0.2956 | dt 337.25ms | 1554585.42 tokens/sec
Step 13977 | loss: 3.135596 | lr:1.5633e-04 | norm 0.3389 | dt 338.26ms | 1549950.49 tokens/sec
Step 13978 | loss: 3.137838 | lr:1.5629e-04 | norm 0.3100 | dt 337.69ms | 1552581.22 tokens/sec
Step 13979 | loss: 3.102295 | lr:1.5625e-04 | norm 0.3080 | dt 337.62ms | 1552900.27 tokens/sec
Step 13980 | loss: 3.101733 | lr:1.5622e-04 | norm 0.3132 | dt 338.21ms | 1550203.98 tokens/sec
Step 13981 | loss: 3.085630 | lr:1.5618e-04 | norm 0.2985 | dt 337.86ms | 1551813.19 tokens/sec
Step 13982 | loss: 3.147320 | lr:1.5615e-04 | norm 0.3060 | dt 337.71ms | 1552491.34 tokens/sec
Step 13983 | loss: 3.173665 | lr:1.5611e-04 | norm 0.3117 | dt 337.66ms | 1552709.49 tokens/sec
Step 13984 | loss: 3.126028 | lr:1.5608e-04 | norm 0.3218 | dt 337.92ms | 1551508.81 tokens/sec
Step 13985 | loss: 3.174551 | lr:1.5604e-04 | norm 0.3224 | dt 905.11ms | 579254.65 tokens/sec
Step 13986 | loss: 3.080434 | lr:1.5601e-04 | norm 0.2965 | dt 334.36ms | 1568043.94 tokens/sec
Step 13987 | loss: 3.075194 | lr:1.5597e-04 | norm 0.3377 | dt 336.73ms | 1556980.53 tokens/sec
Step 13988 | loss: 3.005968 | lr:1.5594e-04 | norm 0.2888 | dt 339.45ms | 1544544.25 tokens/sec
Step 13989 | loss: 3.069326 | lr:1.5590e-04 | norm 0.3173 | dt 337.64ms | 1552788.43 tokens/sec
Step 13990 | loss: 3.095429 | lr:1.5587e-04 | norm 0.3036 | dt 337.36ms | 1554093.22 tokens/sec
Step 13991 | loss: 3.055379 | lr:1.5583e-04 | norm 0.2770 | dt 337.82ms | 1551991.71 tokens/sec
Step 13992 | loss: 3.064659 | lr:1.5580e-04 | norm 0.2988 | dt 338.34ms | 1549600.98 tokens/sec
Step 13993 | loss: 3.092641 | lr:1.5576e-04 | norm 0.2963 | dt 337.80ms | 1552058.52 tokens/sec
Step 13994 | loss: 3.091376 | lr:1.5572e-04 | norm 0.2904 | dt 337.87ms | 1551732.15 tokens/sec
Step 13995 | loss: 3.060498 | lr:1.5569e-04 | norm 0.3146 | dt 337.59ms | 1553012.14 tokens/sec
Step 13996 | loss: 3.092131 | lr:1.5565e-04 | norm 0.3280 | dt 338.17ms | 1550358.08 tokens/sec
Step 13997 | loss: 3.073465 | lr:1.5562e-04 | norm 0.3089 | dt 338.15ms | 1550441.16 tokens/sec
Step 13998 | loss: 3.123017 | lr:1.5558e-04 | norm 0.3279 | dt 338.14ms | 1550501.29 tokens/sec
Step 13999 | loss: 3.070205 | lr:1.5555e-04 | norm 0.2982 | dt 338.08ms | 1550784.49 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 14000: 3.1273
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3021/10042=0.3008


ddp_rank 5: ####### Printing generated samples ####### 



ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but not the best. I'm not so sure where I'm on that track. I'm really unsure where I'm
rank 3 sample 0 >Hello, I'm a language model, and I'm not the kind of person who's trying to speak a non-standard programming language. A lot of software
rank 5 sample 1 >Hello, I'm a language model, meaning there's lots of languages, so there's lots of them. I'll have some more for each person, but
rank 3 sample 1 >Hello, I'm a language model, and it is a language simulator I got a good time by doing some examples. I'm going to create a simulation,
rank 5 sample 2 >Hello, I'm a language model, so I would love playing with a video game!
I'm so happy with this
I'm just a little bored
rank 3 sample 2 >Hello, I'm a language model, and you are thinking, "That's your native language. But I don't see how the brain can learn, thatrank 5 sample 3 >Hello, I'm a language model, who can speak your language to a computer, or the Internet to an Internet to a person,” said Dr.



rank 3 sample 3 >Hello, I'm a language model, and a language learner. If you want to be the best teacher I can have, you can simply be the best




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, do the same?
What's the best language to start?
My first language is English, and then I have
rank 2 sample 1 >Hello, I'm a language model, but I cant teach that to anyone
for the following reason, they wouldnt understand me. I dont believe the whole
rank 2 sample 2 >Hello, I'm a language model, so I want to create a new library that uses the base types in order to do my project of learning how to type
rank 2 sample 3 >Hello, I'm a language model, but my current language models are not as complicated as mine. It's a much different approach if a person can understand you




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm going to do a math based on some code. I got to design a script to take the input of
rank 7 sample 1 >Hello, I'm a language model, thanks. You're right. This kind of thing? "We're going to change language. We're going to go
rank 7 sample 2 >Hello, I'm a language model, and I want to demonstrate how these modules interact with specific languages. And they're going to be doing this by going into
rank 7 sample 3 >Hello, I'm a language model, so this is a nice and simple step. This is a free, low-level, low-level, low-




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I know what you're doing, well I'm going to tell you all the ways:
- you have a
rank 4 sample 1 >Hello, I'm a language model, for instance, at first, what we can think of an example of programming in it? I'm in a machine that
rank 4 sample 2 >Hello, I'm a language model, I never made as many decisions as I thought I would. The language model is always going to be going to get more
rank 1 sample 0 >Hello, I'm a language model, and a translator. I'm a reader of C. As if I were a translator myself, my job is to translate
rank 4 sample 3 >Hello, I'm a language model, and the I have no idea how to explain. A number of languages (and each one) will have a verb and


rank 1 sample 1 >Hello, I'm a language model, a person that studies languages. I'm looking for a reason to become a natural language author, but it's not a
rank 1 sample 2 >Hello, I'm a language model, but language model is a language model.
I'm an advocate, so I'm going to go back and ask to
rank 1 sample 3 >Hello, I'm a language model, and I'm interested to build a project such as a small model airplane from that. Do you see how this looks like




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where one can go in.
In the previous blog I talked about how to run a database in Python, but we
rank 6 sample 1 >Hello, I'm a language model, and I can't make it easy. To make my code easier, first, you should understand how the syntax works.
rank 6 sample 2 >Hello, I'm a language model, but how do you build an app and do this?
The code in the app is the same as how they build

ddp_rank 0: ####### Printing generated samples ####### 


rank 6 sample 3 >Hello, I'm a language model, and a computer program is a kind of language, and a computer program is not a machine just a process; what computers


rank 0 sample 0 >Hello, I'm a language model, and I need to learn about this one better.
Lets use all those words, plus those you already know:
rank 0 sample 1 >Hello, I'm a language model, so when I'm done, it asks me if, then it tells me that, so let me answer it.

rank 0 sample 2 >Hello, I'm a language model, so I like that this project is called "Teaching Language". The teacher is an English teacher, and the teacher represents
rank 0 sample 3 >Hello, I'm a language model, so can you explain what's happening here?”
“That’s an interesting question, and how


Step 14000 | loss: 3.124258 | lr:1.5551e-04 | norm 0.3185 | dt 13025.09ms | 40252.17 tokens/sec
Step 14001 | loss: 3.119576 | lr:1.5548e-04 | norm 0.2918 | dt 336.22ms | 1559378.62 tokens/sec
Step 14002 | loss: 3.224442 | lr:1.5544e-04 | norm 0.3574 | dt 337.08ms | 1555394.71 tokens/sec
Step 14003 | loss: 3.156950 | lr:1.5541e-04 | norm 0.3325 | dt 336.27ms | 1559135.38 tokens/sec
Step 14004 | loss: 3.082730 | lr:1.5537e-04 | norm 0.3151 | dt 335.39ms | 1563228.52 tokens/sec
Step 14005 | loss: 3.103639 | lr:1.5534e-04 | norm 0.2993 | dt 338.04ms | 1550972.61 tokens/sec
Step 14006 | loss: 3.108592 | lr:1.5530e-04 | norm 0.3110 | dt 336.22ms | 1559356.50 tokens/sec
Step 14007 | loss: 3.122514 | lr:1.5527e-04 | norm 0.2990 | dt 335.91ms | 1560804.18 tokens/sec
Step 14008 | loss: 3.144966 | lr:1.5523e-04 | norm 0.2987 | dt 336.28ms | 1559063.53 tokens/sec
Step 14009 | loss: 3.115510 | lr:1.5520e-04 | norm 0.2945 | dt 337.19ms | 1554858.02 tokens/sec
Step 14010 | loss: 3.117295 | lr:1.5516e-04 | norm 0.2959 | dt 336.86ms | 1556399.79 tokens/sec
Step 14011 | loss: 3.118341 | lr:1.5513e-04 | norm 0.2878 | dt 336.73ms | 1557001.48 tokens/sec
Step 14012 | loss: 3.095951 | lr:1.5509e-04 | norm 0.2851 | dt 336.67ms | 1557280.44 tokens/sec
Step 14013 | loss: 3.115576 | lr:1.5506e-04 | norm 0.2728 | dt 338.34ms | 1549583.51 tokens/sec
Step 14014 | loss: 3.066366 | lr:1.5502e-04 | norm 0.2696 | dt 338.33ms | 1549614.08 tokens/sec
Step 14015 | loss: 3.149113 | lr:1.5498e-04 | norm 0.2925 | dt 336.98ms | 1555827.18 tokens/sec
Step 14016 | loss: 3.124162 | lr:1.5495e-04 | norm 0.2675 | dt 337.05ms | 1555511.33 tokens/sec
Step 14017 | loss: 3.097251 | lr:1.5491e-04 | norm 0.2965 | dt 337.45ms | 1553688.05 tokens/sec
Step 14018 | loss: 3.133741 | lr:1.5488e-04 | norm 0.2658 | dt 336.69ms | 1557188.91 tokens/sec
Step 14019 | loss: 3.095792 | lr:1.5484e-04 | norm 0.2682 | dt 337.19ms | 1554878.91 tokens/sec
Step 14020 | loss: 3.120437 | lr:1.5481e-04 | norm 0.2619 | dt 338.16ms | 1550429.13 tokens/sec
Step 14021 | loss: 3.059753 | lr:1.5477e-04 | norm 0.2682 | dt 337.86ms | 1551773.76 tokens/sec
Step 14022 | loss: 3.123064 | lr:1.5474e-04 | norm 0.2687 | dt 337.14ms | 1555093.32 tokens/sec
Step 14023 | loss: 3.128688 | lr:1.5470e-04 | norm 0.2810 | dt 337.96ms | 1551341.35 tokens/sec
Step 14024 | loss: 3.094526 | lr:1.5467e-04 | norm 0.2644 | dt 338.34ms | 1549602.07 tokens/sec
Step 14025 | loss: 3.096164 | lr:1.5463e-04 | norm 0.2620 | dt 338.64ms | 1548237.22 tokens/sec
Step 14026 | loss: 3.033890 | lr:1.5460e-04 | norm 0.2849 | dt 339.42ms | 1544670.11 tokens/sec
Step 14027 | loss: 3.092088 | lr:1.5456e-04 | norm 0.2765 | dt 339.56ms | 1543999.85 tokens/sec
Step 14028 | loss: 3.048422 | lr:1.5453e-04 | norm 0.2697 | dt 338.10ms | 1550691.53 tokens/sec
Step 14029 | loss: 3.094649 | lr:1.5449e-04 | norm 0.2714 | dt 339.49ms | 1544338.16 tokens/sec
Step 14030 | loss: 3.116875 | lr:1.5446e-04 | norm 0.2912 | dt 340.00ms | 1542010.94 tokens/sec
Step 14031 | loss: 3.072089 | lr:1.5442e-04 | norm 0.2755 | dt 337.68ms | 1552606.43 tokens/sec
Step 14032 | loss: 3.062617 | lr:1.5439e-04 | norm 0.2961 | dt 338.49ms | 1548907.89 tokens/sec
Step 14033 | loss: 3.044126 | lr:1.5435e-04 | norm 0.2778 | dt 338.77ms | 1547621.59 tokens/sec
Step 14034 | loss: 3.109142 | lr:1.5432e-04 | norm 0.2950 | dt 338.12ms | 1550599.68 tokens/sec
Step 14035 | loss: 3.122397 | lr:1.5428e-04 | norm 0.2948 | dt 338.92ms | 1546948.77 tokens/sec
Step 14036 | loss: 3.078460 | lr:1.5425e-04 | norm 0.2948 | dt 339.12ms | 1546005.85 tokens/sec
Step 14037 | loss: 3.119744 | lr:1.5421e-04 | norm 0.2772 | dt 338.77ms | 1547600.90 tokens/sec
Step 14038 | loss: 3.111122 | lr:1.5418e-04 | norm 0.3013 | dt 338.68ms | 1548056.30 tokens/sec
Step 14039 | loss: 3.037415 | lr:1.5414e-04 | norm 0.2759 | dt 338.26ms | 1549951.58 tokens/sec
Step 14040 | loss: 3.092166 | lr:1.5411e-04 | norm 0.3020 | dt 338.73ms | 1547810.04 tokens/sec
Step 14041 | loss: 3.134744 | lr:1.5407e-04 | norm 0.2807 | dt 337.95ms | 1551357.76 tokens/sec
Step 14042 | loss: 3.137836 | lr:1.5404e-04 | norm 0.3014 | dt 337.93ms | 1551466.12 tokens/sec
Step 14043 | loss: 3.082936 | lr:1.5400e-04 | norm 0.2899 | dt 338.86ms | 1547196.93 tokens/sec
Step 14044 | loss: 3.088517 | lr:1.5397e-04 | norm 0.3000 | dt 337.74ms | 1552349.96 tokens/sec
Step 14045 | loss: 3.147448 | lr:1.5393e-04 | norm 0.3034 | dt 337.21ms | 1554771.17 tokens/sec
Step 14046 | loss: 3.189893 | lr:1.5390e-04 | norm 0.3509 | dt 338.51ms | 1548806.44 tokens/sec
Step 14047 | loss: 3.148894 | lr:1.5386e-04 | norm 0.2692 | dt 339.10ms | 1546093.89 tokens/sec
Step 14048 | loss: 3.154621 | lr:1.5383e-04 | norm 0.3031 | dt 338.42ms | 1549200.33 tokens/sec
Step 14049 | loss: 3.119517 | lr:1.5379e-04 | norm 0.2950 | dt 338.22ms | 1550128.58 tokens/sec
Step 14050 | loss: 3.128307 | lr:1.5376e-04 | norm 0.2870 | dt 337.98ms | 1551259.27 tokens/sec
Step 14051 | loss: 3.142341 | lr:1.5372e-04 | norm 0.2887 | dt 338.33ms | 1549641.38 tokens/sec
Step 14052 | loss: 3.087483 | lr:1.5369e-04 | norm 0.3049 | dt 338.10ms | 1550698.09 tokens/sec
Step 14053 | loss: 3.090815 | lr:1.5365e-04 | norm 0.2878 | dt 337.63ms | 1552836.67 tokens/sec
Step 14054 | loss: 3.120407 | lr:1.5362e-04 | norm 0.2832 | dt 337.67ms | 1552654.67 tokens/sec
Step 14055 | loss: 3.040618 | lr:1.5358e-04 | norm 0.2776 | dt 338.56ms | 1548569.76 tokens/sec
Step 14056 | loss: 3.123321 | lr:1.5355e-04 | norm 0.2916 | dt 337.97ms | 1551281.16 tokens/sec
Step 14057 | loss: 3.128863 | lr:1.5351e-04 | norm 0.2724 | dt 337.55ms | 1553235.91 tokens/sec
Step 14058 | loss: 3.051849 | lr:1.5348e-04 | norm 0.2817 | dt 337.86ms | 1551774.86 tokens/sec
Step 14059 | loss: 3.093211 | lr:1.5344e-04 | norm 0.2852 | dt 925.96ms | 566207.69 tokens/sec
Step 14060 | loss: 3.077062 | lr:1.5341e-04 | norm 0.2725 | dt 337.29ms | 1554409.60 tokens/sec
Step 14061 | loss: 3.104285 | lr:1.5337e-04 | norm 0.2793 | dt 337.45ms | 1553682.56 tokens/sec
Step 14062 | loss: 3.076510 | lr:1.5334e-04 | norm 0.2777 | dt 337.22ms | 1554740.39 tokens/sec
Step 14063 | loss: 3.114424 | lr:1.5330e-04 | norm 0.2834 | dt 337.92ms | 1551496.77 tokens/sec
Step 14064 | loss: 3.038660 | lr:1.5327e-04 | norm 0.2605 | dt 337.93ms | 1551482.54 tokens/sec
Step 14065 | loss: 3.139487 | lr:1.5323e-04 | norm 0.2825 | dt 337.72ms | 1552450.79 tokens/sec
Step 14066 | loss: 3.039429 | lr:1.5320e-04 | norm 0.2766 | dt 337.05ms | 1555511.33 tokens/sec
Step 14067 | loss: 3.052818 | lr:1.5316e-04 | norm 0.2877 | dt 337.49ms | 1553498.16 tokens/sec
Step 14068 | loss: 3.094441 | lr:1.5313e-04 | norm 0.2809 | dt 337.71ms | 1552460.65 tokens/sec
Step 14069 | loss: 3.089334 | lr:1.5309e-04 | norm 0.2967 | dt 337.74ms | 1552353.25 tokens/sec
Step 14070 | loss: 3.070759 | lr:1.5306e-04 | norm 0.2916 | dt 337.57ms | 1553132.79 tokens/sec
Step 14071 | loss: 3.092676 | lr:1.5302e-04 | norm 0.2898 | dt 338.49ms | 1548915.53 tokens/sec
Step 14072 | loss: 3.076002 | lr:1.5299e-04 | norm 0.2943 | dt 338.38ms | 1549428.47 tokens/sec
Step 14073 | loss: 3.167681 | lr:1.5295e-04 | norm 0.3118 | dt 337.30ms | 1554379.93 tokens/sec
Step 14074 | loss: 3.107661 | lr:1.5292e-04 | norm 0.2951 | dt 338.10ms | 1550687.16 tokens/sec
Step 14075 | loss: 3.086705 | lr:1.5288e-04 | norm 0.3185 | dt 338.03ms | 1551031.69 tokens/sec
Step 14076 | loss: 3.121604 | lr:1.5285e-04 | norm 0.3015 | dt 337.98ms | 1551237.38 tokens/sec
Step 14077 | loss: 3.133702 | lr:1.5281e-04 | norm 0.3448 | dt 337.77ms | 1552185.60 tokens/sec
Step 14078 | loss: 3.061809 | lr:1.5278e-04 | norm 0.3586 | dt 338.65ms | 1548147.84 tokens/sec
Step 14079 | loss: 3.099187 | lr:1.5274e-04 | norm 0.3081 | dt 337.90ms | 1551612.81 tokens/sec
Step 14080 | loss: 3.138865 | lr:1.5271e-04 | norm 0.3264 | dt 339.33ms | 1545064.07 tokens/sec
Step 14081 | loss: 3.089101 | lr:1.5267e-04 | norm 0.3156 | dt 338.17ms | 1550386.50 tokens/sec
Step 14082 | loss: 3.086301 | lr:1.5264e-04 | norm 0.2864 | dt 337.78ms | 1552141.78 tokens/sec
Step 14083 | loss: 3.147640 | lr:1.5260e-04 | norm 0.3064 | dt 337.70ms | 1552532.99 tokens/sec
Step 14084 | loss: 3.122000 | lr:1.5257e-04 | norm 0.2910 | dt 338.37ms | 1549464.50 tokens/sec
Step 14085 | loss: 3.096653 | lr:1.5253e-04 | norm 0.2873 | dt 337.44ms | 1553724.27 tokens/sec
Step 14086 | loss: 3.093752 | lr:1.5250e-04 | norm 0.2676 | dt 338.43ms | 1549186.14 tokens/sec
Step 14087 | loss: 3.131906 | lr:1.5246e-04 | norm 0.2832 | dt 338.39ms | 1549348.78 tokens/sec
Step 14088 | loss: 3.104650 | lr:1.5243e-04 | norm 0.2787 | dt 337.59ms | 1553028.59 tokens/sec
Step 14089 | loss: 3.075150 | lr:1.5240e-04 | norm 0.2644 | dt 337.49ms | 1553471.82 tokens/sec
Step 14090 | loss: 3.146479 | lr:1.5236e-04 | norm 0.2853 | dt 337.95ms | 1551391.69 tokens/sec
Step 14091 | loss: 3.092119 | lr:1.5233e-04 | norm 0.2824 | dt 338.11ms | 1550651.07 tokens/sec
Step 14092 | loss: 3.111253 | lr:1.5229e-04 | norm 0.2773 | dt 338.03ms | 1550991.21 tokens/sec
Step 14093 | loss: 3.056644 | lr:1.5226e-04 | norm 0.2815 | dt 337.09ms | 1555348.50 tokens/sec
Step 14094 | loss: 3.049251 | lr:1.5222e-04 | norm 0.2901 | dt 338.15ms | 1550470.68 tokens/sec
Step 14095 | loss: 3.034920 | lr:1.5219e-04 | norm 0.2858 | dt 337.56ms | 1553190.93 tokens/sec
Step 14096 | loss: 3.076636 | lr:1.5215e-04 | norm 0.2887 | dt 338.71ms | 1547887.40 tokens/sec
Step 14097 | loss: 3.055420 | lr:1.5212e-04 | norm 0.2731 | dt 337.81ms | 1552014.71 tokens/sec
Step 14098 | loss: 3.087868 | lr:1.5208e-04 | norm 0.2764 | dt 337.47ms | 1553564.01 tokens/sec
Step 14099 | loss: 3.043512 | lr:1.5205e-04 | norm 0.2698 | dt 338.58ms | 1548491.24 tokens/sec
Step 14100 | loss: 3.098139 | lr:1.5201e-04 | norm 0.2787 | dt 338.05ms | 1550924.48 tokens/sec
Step 14101 | loss: 3.089844 | lr:1.5198e-04 | norm 0.2719 | dt 336.92ms | 1556137.66 tokens/sec
Step 14102 | loss: 3.047442 | lr:1.5194e-04 | norm 0.2865 | dt 339.18ms | 1545751.55 tokens/sec
Step 14103 | loss: 3.065799 | lr:1.5191e-04 | norm 0.2657 | dt 338.07ms | 1550831.51 tokens/sec
Step 14104 | loss: 3.097553 | lr:1.5187e-04 | norm 0.3005 | dt 337.92ms | 1551537.27 tokens/sec
Step 14105 | loss: 3.103410 | lr:1.5184e-04 | norm 0.3009 | dt 337.89ms | 1551657.70 tokens/sec
Step 14106 | loss: 3.238982 | lr:1.5180e-04 | norm 0.3436 | dt 337.92ms | 1551516.47 tokens/sec
Step 14107 | loss: 3.058344 | lr:1.5177e-04 | norm 0.3403 | dt 337.35ms | 1554136.05 tokens/sec
Step 14108 | loss: 3.148326 | lr:1.5174e-04 | norm 0.3249 | dt 339.66ms | 1543558.75 tokens/sec
Step 14109 | loss: 3.080974 | lr:1.5170e-04 | norm 0.2855 | dt 339.24ms | 1545473.45 tokens/sec
Step 14110 | loss: 3.018633 | lr:1.5167e-04 | norm 0.3794 | dt 339.03ms | 1546416.81 tokens/sec
Step 14111 | loss: 3.158675 | lr:1.5163e-04 | norm 0.3472 | dt 338.39ms | 1549364.06 tokens/sec
Step 14112 | loss: 3.153118 | lr:1.5160e-04 | norm 0.3377 | dt 338.96ms | 1546751.82 tokens/sec
Step 14113 | loss: 3.065037 | lr:1.5156e-04 | norm 0.3049 | dt 339.01ms | 1546532.09 tokens/sec
Step 14114 | loss: 3.186089 | lr:1.5153e-04 | norm 0.3355 | dt 339.72ms | 1543283.60 tokens/sec
Step 14115 | loss: 3.100823 | lr:1.5149e-04 | norm 0.2959 | dt 338.74ms | 1547771.91 tokens/sec
Step 14116 | loss: 3.085824 | lr:1.5146e-04 | norm 0.2902 | dt 338.37ms | 1549444.84 tokens/sec
Step 14117 | loss: 3.106277 | lr:1.5142e-04 | norm 0.2882 | dt 338.56ms | 1548597.02 tokens/sec
Step 14118 | loss: 3.103656 | lr:1.5139e-04 | norm 0.2743 | dt 338.87ms | 1547143.59 tokens/sec
Step 14119 | loss: 3.057876 | lr:1.5135e-04 | norm 0.2971 | dt 339.56ms | 1544031.29 tokens/sec
Step 14120 | loss: 3.115702 | lr:1.5132e-04 | norm 0.2887 | dt 338.95ms | 1546785.55 tokens/sec
Step 14121 | loss: 3.114433 | lr:1.5128e-04 | norm 0.2880 | dt 338.47ms | 1548975.54 tokens/sec
Step 14122 | loss: 3.108867 | lr:1.5125e-04 | norm 0.2892 | dt 338.57ms | 1548517.41 tokens/sec
Step 14123 | loss: 3.115500 | lr:1.5122e-04 | norm 0.2798 | dt 338.66ms | 1548124.96 tokens/sec
Step 14124 | loss: 3.142030 | lr:1.5118e-04 | norm 0.2857 | dt 339.39ms | 1544789.47 tokens/sec
Step 14125 | loss: 3.127254 | lr:1.5115e-04 | norm 0.4610 | dt 339.20ms | 1545679.84 tokens/sec
Step 14126 | loss: 3.097787 | lr:1.5111e-04 | norm 0.2782 | dt 338.28ms | 1549872.93 tokens/sec
Step 14127 | loss: 3.142087 | lr:1.5108e-04 | norm 0.3033 | dt 339.99ms | 1542074.74 tokens/sec
Step 14128 | loss: 3.116256 | lr:1.5104e-04 | norm 0.2762 | dt 337.54ms | 1553279.80 tokens/sec
Step 14129 | loss: 3.079212 | lr:1.5101e-04 | norm 0.2897 | dt 338.20ms | 1550237.86 tokens/sec
Step 14130 | loss: 3.093765 | lr:1.5097e-04 | norm 0.2672 | dt 337.67ms | 1552679.88 tokens/sec
Step 14131 | loss: 3.096920 | lr:1.5094e-04 | norm 0.2739 | dt 338.49ms | 1548884.98 tokens/sec
Step 14132 | loss: 3.064960 | lr:1.5090e-04 | norm 0.2805 | dt 338.24ms | 1550059.74 tokens/sec
Step 14133 | loss: 3.048301 | lr:1.5087e-04 | norm 0.2740 | dt 337.76ms | 1552232.72 tokens/sec
Step 14134 | loss: 3.099737 | lr:1.5083e-04 | norm 0.3231 | dt 338.52ms | 1548770.44 tokens/sec
Step 14135 | loss: 3.064208 | lr:1.5080e-04 | norm 0.2919 | dt 337.90ms | 1551607.34 tokens/sec
Step 14136 | loss: 3.113199 | lr:1.5077e-04 | norm 0.2946 | dt 337.30ms | 1554353.56 tokens/sec
Step 14137 | loss: 3.089841 | lr:1.5073e-04 | norm 0.2982 | dt 338.42ms | 1549241.81 tokens/sec
Step 14138 | loss: 3.141091 | lr:1.5070e-04 | norm 0.3011 | dt 338.70ms | 1547949.50 tokens/sec
Step 14139 | loss: 3.109426 | lr:1.5066e-04 | norm 0.2813 | dt 338.27ms | 1549894.78 tokens/sec
Step 14140 | loss: 3.098889 | lr:1.5063e-04 | norm 0.2960 | dt 337.28ms | 1554453.55 tokens/sec
Step 14141 | loss: 3.143341 | lr:1.5059e-04 | norm 0.2961 | dt 338.12ms | 1550590.94 tokens/sec
Step 14142 | loss: 3.134652 | lr:1.5056e-04 | norm 0.2862 | dt 338.68ms | 1548049.76 tokens/sec
Step 14143 | loss: 3.101089 | lr:1.5052e-04 | norm 0.3188 | dt 337.60ms | 1552980.33 tokens/sec
Step 14144 | loss: 3.111478 | lr:1.5049e-04 | norm 0.2973 | dt 338.08ms | 1550798.70 tokens/sec
Step 14145 | loss: 3.080331 | lr:1.5045e-04 | norm 0.2792 | dt 338.64ms | 1548214.33 tokens/sec
Step 14146 | loss: 3.103828 | lr:1.5042e-04 | norm 0.3234 | dt 338.10ms | 1550705.75 tokens/sec
Step 14147 | loss: 3.139417 | lr:1.5039e-04 | norm 0.3240 | dt 337.83ms | 1551942.42 tokens/sec
Step 14148 | loss: 3.129589 | lr:1.5035e-04 | norm 0.3282 | dt 338.54ms | 1548686.45 tokens/sec
Step 14149 | loss: 3.114553 | lr:1.5032e-04 | norm 0.3198 | dt 337.58ms | 1553091.11 tokens/sec
Step 14150 | loss: 3.092962 | lr:1.5028e-04 | norm 0.2995 | dt 337.66ms | 1552706.20 tokens/sec
Step 14151 | loss: 3.166220 | lr:1.5025e-04 | norm 0.2936 | dt 338.10ms | 1550668.57 tokens/sec
Step 14152 | loss: 3.073302 | lr:1.5021e-04 | norm 0.2805 | dt 337.96ms | 1551327.12 tokens/sec
Step 14153 | loss: 3.138969 | lr:1.5018e-04 | norm 0.2913 | dt 338.13ms | 1550557.04 tokens/sec
Step 14154 | loss: 3.124608 | lr:1.5014e-04 | norm 0.2716 | dt 337.55ms | 1553219.46 tokens/sec
Step 14155 | loss: 3.068816 | lr:1.5011e-04 | norm 0.2805 | dt 337.83ms | 1551942.42 tokens/sec
Step 14156 | loss: 3.163253 | lr:1.5008e-04 | norm 0.2961 | dt 337.42ms | 1553808.81 tokens/sec
Step 14157 | loss: 3.071204 | lr:1.5004e-04 | norm 0.2794 | dt 337.48ms | 1553517.92 tokens/sec
Step 14158 | loss: 3.102932 | lr:1.5001e-04 | norm 0.2926 | dt 338.76ms | 1547689.12 tokens/sec
Step 14159 | loss: 3.148146 | lr:1.4997e-04 | norm 0.2709 | dt 337.90ms | 1551625.95 tokens/sec
Step 14160 | loss: 3.133815 | lr:1.4994e-04 | norm 0.2897 | dt 337.07ms | 1555434.31 tokens/sec
Step 14161 | loss: 3.114154 | lr:1.4990e-04 | norm 0.3003 | dt 337.52ms | 1553337.95 tokens/sec
Step 14162 | loss: 3.089451 | lr:1.4987e-04 | norm 0.2776 | dt 338.50ms | 1548845.71 tokens/sec
Step 14163 | loss: 3.092849 | lr:1.4983e-04 | norm 0.2846 | dt 338.52ms | 1548770.44 tokens/sec
Step 14164 | loss: 3.001392 | lr:1.4980e-04 | norm 0.2788 | dt 338.08ms | 1550784.49 tokens/sec
Step 14165 | loss: 3.089267 | lr:1.4977e-04 | norm 0.2736 | dt 337.88ms | 1551676.31 tokens/sec
Step 14166 | loss: 3.068040 | lr:1.4973e-04 | norm 0.3007 | dt 337.54ms | 1553246.88 tokens/sec
Step 14167 | loss: 3.004594 | lr:1.4970e-04 | norm 0.2845 | dt 337.77ms | 1552188.89 tokens/sec
Step 14168 | loss: 3.038452 | lr:1.4966e-04 | norm 0.2769 | dt 337.57ms | 1553124.02 tokens/sec
Step 14169 | loss: 3.108813 | lr:1.4963e-04 | norm 0.2820 | dt 338.42ms | 1549201.42 tokens/sec
Step 14170 | loss: 3.073538 | lr:1.4959e-04 | norm 0.2938 | dt 337.44ms | 1553735.25 tokens/sec
Step 14171 | loss: 3.059537 | lr:1.4956e-04 | norm 0.2612 | dt 338.64ms | 1548199.07 tokens/sec
Step 14172 | loss: 3.077178 | lr:1.4952e-04 | norm 0.2856 | dt 337.45ms | 1553677.07 tokens/sec
Step 14173 | loss: 3.079701 | lr:1.4949e-04 | norm 0.2807 | dt 338.82ms | 1547412.50 tokens/sec
Step 14174 | loss: 3.148578 | lr:1.4946e-04 | norm 0.3197 | dt 899.02ms | 583179.89 tokens/sec
Step 14175 | loss: 3.126405 | lr:1.4942e-04 | norm 0.3346 | dt 335.11ms | 1564539.79 tokens/sec
Step 14176 | loss: 3.080380 | lr:1.4939e-04 | norm 0.2864 | dt 338.10ms | 1550682.78 tokens/sec
Step 14177 | loss: 3.075258 | lr:1.4935e-04 | norm 0.3015 | dt 338.30ms | 1549770.25 tokens/sec
Step 14178 | loss: 3.062689 | lr:1.4932e-04 | norm 0.3001 | dt 337.91ms | 1551575.59 tokens/sec
Step 14179 | loss: 3.157393 | lr:1.4928e-04 | norm 0.2940 | dt 338.15ms | 1550470.68 tokens/sec
Step 14180 | loss: 3.083094 | lr:1.4925e-04 | norm 0.3040 | dt 337.25ms | 1554618.39 tokens/sec
Step 14181 | loss: 3.131589 | lr:1.4922e-04 | norm 0.3100 | dt 337.59ms | 1553031.88 tokens/sec
Step 14182 | loss: 3.085828 | lr:1.4918e-04 | norm 0.3020 | dt 337.68ms | 1552638.23 tokens/sec
Step 14183 | loss: 3.127199 | lr:1.4915e-04 | norm 0.3163 | dt 338.64ms | 1548209.97 tokens/sec
Step 14184 | loss: 3.105400 | lr:1.4911e-04 | norm 0.2797 | dt 338.27ms | 1549893.68 tokens/sec
Step 14185 | loss: 3.125738 | lr:1.4908e-04 | norm 0.2986 | dt 337.71ms | 1552461.75 tokens/sec
Step 14186 | loss: 3.204508 | lr:1.4904e-04 | norm 0.3019 | dt 338.08ms | 1550768.08 tokens/sec
Step 14187 | loss: 3.045891 | lr:1.4901e-04 | norm 0.3245 | dt 338.26ms | 1549939.56 tokens/sec
Step 14188 | loss: 3.116689 | lr:1.4898e-04 | norm 0.3264 | dt 338.24ms | 1550059.74 tokens/sec
Step 14189 | loss: 3.085696 | lr:1.4894e-04 | norm 0.2956 | dt 338.15ms | 1550449.90 tokens/sec
Step 14190 | loss: 3.085052 | lr:1.4891e-04 | norm 0.2919 | dt 338.11ms | 1550651.07 tokens/sec
Step 14191 | loss: 3.105071 | lr:1.4887e-04 | norm 0.2704 | dt 337.67ms | 1552652.48 tokens/sec
Step 14192 | loss: 3.093941 | lr:1.4884e-04 | norm 0.3118 | dt 338.18ms | 1550342.78 tokens/sec
Step 14193 | loss: 3.094605 | lr:1.4880e-04 | norm 0.2842 | dt 339.12ms | 1546021.06 tokens/sec
Step 14194 | loss: 3.099424 | lr:1.4877e-04 | norm 0.2817 | dt 337.73ms | 1552380.65 tokens/sec
Step 14195 | loss: 3.165867 | lr:1.4874e-04 | norm 0.2985 | dt 338.08ms | 1550775.74 tokens/sec
Step 14196 | loss: 3.155679 | lr:1.4870e-04 | norm 0.2839 | dt 339.53ms | 1544137.54 tokens/sec
Step 14197 | loss: 3.122309 | lr:1.4867e-04 | norm 0.3032 | dt 337.97ms | 1551281.16 tokens/sec
Step 14198 | loss: 3.080359 | lr:1.4863e-04 | norm 0.2684 | dt 338.79ms | 1547549.71 tokens/sec
Step 14199 | loss: 3.119840 | lr:1.4860e-04 | norm 0.2758 | dt 338.59ms | 1548431.27 tokens/sec
Step 14200 | loss: 3.131672 | lr:1.4856e-04 | norm 0.2728 | dt 338.25ms | 1549989.82 tokens/sec
Step 14201 | loss: 3.093191 | lr:1.4853e-04 | norm 0.2672 | dt 339.00ms | 1546567.98 tokens/sec
Step 14202 | loss: 3.084265 | lr:1.4850e-04 | norm 0.2832 | dt 338.41ms | 1549268.00 tokens/sec
Step 14203 | loss: 3.061203 | lr:1.4846e-04 | norm 0.2638 | dt 338.14ms | 1550528.62 tokens/sec
Step 14204 | loss: 3.035849 | lr:1.4843e-04 | norm 0.2721 | dt 337.93ms | 1551456.27 tokens/sec
Step 14205 | loss: 3.054327 | lr:1.4839e-04 | norm 0.2586 | dt 338.16ms | 1550428.04 tokens/sec
Step 14206 | loss: 3.115135 | lr:1.4836e-04 | norm 0.2757 | dt 339.21ms | 1545637.47 tokens/sec
Step 14207 | loss: 3.091604 | lr:1.4833e-04 | norm 0.2799 | dt 337.96ms | 1551310.70 tokens/sec
Step 14208 | loss: 3.084426 | lr:1.4829e-04 | norm 0.3425 | dt 339.13ms | 1545998.24 tokens/sec
Step 14209 | loss: 3.090033 | lr:1.4826e-04 | norm 0.2822 | dt 339.21ms | 1545605.97 tokens/sec
Step 14210 | loss: 3.086803 | lr:1.4822e-04 | norm 0.2756 | dt 338.75ms | 1547732.69 tokens/sec
Step 14211 | loss: 3.063475 | lr:1.4819e-04 | norm 0.2913 | dt 339.69ms | 1543441.74 tokens/sec
Step 14212 | loss: 3.110841 | lr:1.4815e-04 | norm 0.3132 | dt 339.55ms | 1544049.72 tokens/sec
Step 14213 | loss: 3.139479 | lr:1.4812e-04 | norm 0.2880 | dt 338.98ms | 1546666.97 tokens/sec
Step 14214 | loss: 3.116870 | lr:1.4809e-04 | norm 0.3033 | dt 339.18ms | 1545755.90 tokens/sec
Step 14215 | loss: 3.112006 | lr:1.4805e-04 | norm 0.2925 | dt 338.95ms | 1546804.05 tokens/sec
Step 14216 | loss: 3.176770 | lr:1.4802e-04 | norm 0.3157 | dt 339.16ms | 1545848.26 tokens/sec
Step 14217 | loss: 3.130269 | lr:1.4798e-04 | norm 0.3024 | dt 338.09ms | 1550718.87 tokens/sec
Step 14218 | loss: 3.106552 | lr:1.4795e-04 | norm 0.3587 | dt 338.25ms | 1549985.45 tokens/sec
Step 14219 | loss: 3.086175 | lr:1.4792e-04 | norm 0.3228 | dt 338.29ms | 1549795.38 tokens/sec
Step 14220 | loss: 3.003549 | lr:1.4788e-04 | norm 0.2930 | dt 338.74ms | 1547773.00 tokens/sec
Step 14221 | loss: 3.097116 | lr:1.4785e-04 | norm 0.2975 | dt 338.57ms | 1548538.13 tokens/sec
Step 14222 | loss: 3.140119 | lr:1.4781e-04 | norm 0.3102 | dt 338.11ms | 1550627.02 tokens/sec
Step 14223 | loss: 3.125435 | lr:1.4778e-04 | norm 0.3395 | dt 338.42ms | 1549207.97 tokens/sec
Step 14224 | loss: 3.032720 | lr:1.4774e-04 | norm 0.2907 | dt 337.74ms | 1552353.25 tokens/sec
Step 14225 | loss: 3.089236 | lr:1.4771e-04 | norm 0.3240 | dt 339.54ms | 1544097.42 tokens/sec
Step 14226 | loss: 3.126849 | lr:1.4768e-04 | norm 0.3103 | dt 338.91ms | 1546977.06 tokens/sec
Step 14227 | loss: 3.086606 | lr:1.4764e-04 | norm 0.2883 | dt 339.24ms | 1545485.39 tokens/sec
Step 14228 | loss: 3.135345 | lr:1.4761e-04 | norm 0.2913 | dt 339.44ms | 1544588.73 tokens/sec
Step 14229 | loss: 3.053228 | lr:1.4757e-04 | norm 0.3333 | dt 344.46ms | 1522068.20 tokens/sec
Step 14230 | loss: 3.145573 | lr:1.4754e-04 | norm 0.2941 | dt 339.09ms | 1546168.90 tokens/sec
Step 14231 | loss: 3.087556 | lr:1.4751e-04 | norm 0.2841 | dt 338.73ms | 1547793.70 tokens/sec
Step 14232 | loss: 3.081067 | lr:1.4747e-04 | norm 0.2872 | dt 338.68ms | 1548024.69 tokens/sec
Step 14233 | loss: 3.081390 | lr:1.4744e-04 | norm 0.2808 | dt 338.70ms | 1547951.68 tokens/sec
Step 14234 | loss: 3.150521 | lr:1.4740e-04 | norm 0.2886 | dt 338.92ms | 1546952.03 tokens/sec
Step 14235 | loss: 3.080227 | lr:1.4737e-04 | norm 0.2703 | dt 338.04ms | 1550952.92 tokens/sec
Step 14236 | loss: 3.083888 | lr:1.4734e-04 | norm 0.2717 | dt 338.86ms | 1547198.02 tokens/sec
Step 14237 | loss: 3.115942 | lr:1.4730e-04 | norm 0.2773 | dt 337.81ms | 1552022.38 tokens/sec
Step 14238 | loss: 3.051855 | lr:1.4727e-04 | norm 0.2671 | dt 338.36ms | 1549505.99 tokens/sec
Step 14239 | loss: 3.150980 | lr:1.4723e-04 | norm 0.3902 | dt 337.67ms | 1552682.08 tokens/sec
Step 14240 | loss: 3.071578 | lr:1.4720e-04 | norm 0.3160 | dt 338.62ms | 1548318.98 tokens/sec
Step 14241 | loss: 3.076148 | lr:1.4717e-04 | norm 0.3017 | dt 337.59ms | 1553008.85 tokens/sec
Step 14242 | loss: 3.064336 | lr:1.4713e-04 | norm 0.2973 | dt 338.40ms | 1549336.77 tokens/sec
Step 14243 | loss: 3.152272 | lr:1.4710e-04 | norm 0.2986 | dt 338.35ms | 1549547.48 tokens/sec
Step 14244 | loss: 3.048982 | lr:1.4706e-04 | norm 0.3073 | dt 338.79ms | 1547511.59 tokens/sec
Step 14245 | loss: 3.049293 | lr:1.4703e-04 | norm 0.2669 | dt 338.56ms | 1548600.29 tokens/sec
Step 14246 | loss: 3.091971 | lr:1.4700e-04 | norm 0.3008 | dt 338.08ms | 1550794.33 tokens/sec
Step 14247 | loss: 3.066194 | lr:1.4696e-04 | norm 0.2685 | dt 338.35ms | 1549526.73 tokens/sec
Step 14248 | loss: 3.089763 | lr:1.4693e-04 | norm 0.3141 | dt 339.10ms | 1546117.81 tokens/sec
Step 14249 | loss: 3.074861 | lr:1.4689e-04 | norm 0.2721 | dt 994.78ms | 527037.59 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 14250: 3.1236
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 2999/10042=0.2986


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this isn't an easy task. All you have is a lot of time to understand it, and I want to
rank 5 sample 1 >Hello, I'm a language model, why aren't there things?<|endoftext|>|Home ~||Dogs_Fetacor||Barcopneumatic
rank 5 sample 2 >Hello, I'm a language model, and you might have my own list?
And what about the most popular language model we've seen?
I like
rank 5 sample 3 >Hello, I'm a language model, this is the basics of our language. I don't understand that in the technical, technical, technological world, I would




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I wanted to share something that I see here, but I don't know because I'm currently not using it,
rank 7 sample 1 >Hello, I'm a language model, can I learn it? Or I use it in my homework?
I'm getting into trouble with how to teach languages
rank 7 sample 2 >Hello, I'm a language model, so I'll put a little time and hope it takes a while to develop. I'll explain how to write programs and
rank 7 sample 3 >Hello, I'm a language model, and you should be asking this question some the day.
Answer to The Question
What is the name of the machine




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I think that's because I'm one of the people who wants to understand the English as a second language.

rank 2 sample 0 >Hello, I'm a language model, so my question may be: "Which language do you want to learn?" I know that people like to be able to
rank 4 sample 1 >Hello, I'm a language model, too:
Sophisticated programming (the basic technology behind what computers and mobile phones are built out of) uses
rank 2 sample 1 >Hello, I'm a language model, but I prefer that's the way your point of confusion is to avoid confusing your students with your own version.
A
rank 4 sample 2 >Hello, I'm a language model, I will only look to be able to use the language model and look at the object code. If i'm an official
rank 2 sample 2 >Hello, I'm a language model, and I've written a book called C++ that I teach to people using a lot of the concepts it provides. In
rank 4 sample 3 >Hello, I'm a language model, so it appears this is not the "problem" there's plenty to explain, but it is certainly a good approach,

rank 2 sample 3 >Hello, I'm a language model, but a new language.
I'm new to HTML and PHP, but one of my own ways to teach it is





ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so if I'm really interested in "learning and understanding grammar", I might give you the language learning app I've been
rank 1 sample 1 >Hello, I'm a language model, a person who says something like that (say, the expression "My name is a real person").
I'm a
rank 1 sample 2 >Hello, I'm a language model, but i have an idea of what i'm going to say...<|endoftext|>What is the difference between
A and E

rank 1 sample 3 >Hello, I'm a language model, so I'm doing this lesson here! Hopefully, you’ll come down with someone who can help you to understand




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, like most people, with only one grammar, and so the syntax seems like something of a technical problem.
I guess
rank 6 sample 1 >Hello, I'm a language model, so I can't think about what I use or don't use. That's a good question, and that's how
rank 6 sample 2 >Hello, I'm a language model, but not very well known to you. The main difference is in grammar and usage. Let's say you use Spanish for
rank 6 sample 3 >Hello, I'm a language model, so here's my way to describe the interface of a language.
The human language has as its head two very common





ddp_rank 3: ####### Printing generated samples ####### 


ddp_rank 0: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not the kind to give you the grammar or pronunciation of native English. That doesn't count. It might
rank 0 sample 0 >Hello, I'm a language model, so I think I'll just have to work hard with this. For your first time programming, the only way to berank 3 sample 1 >Hello, I'm a language model, so what I'm doing is actually helping you in some way.
I'm going to begin by talking about one particular

rank 3 sample 2 >Hello, I'm a language model, so this isn't hard to put together. For example, a language like French or Spanish has words and phrases similar torank 0 sample 1 >Hello, I'm a language model, and like I said earlier, the author of a course and I'll add more to the subject, here are some more

rank 3 sample 3 >Hello, I'm a language model, so a lot of the best tutorials on the Internet can just as well be written in languages.
Hello, I'mrank 0 sample 2 >Hello, I'm a language model, and I wrote about writing language models on my Facebook page. In a previous post, I wrote about how to create one



rank 0 sample 3 >Hello, I'm a language model, and how do I make it?
The short answer is that this is not only a code-set. Rather of


Step 14250 | loss: 3.138870 | lr:1.4686e-04 | norm 0.3190 | dt 18590.49ms | 28201.94 tokens/sec
Step 14251 | loss: 3.069241 | lr:1.4683e-04 | norm 0.2984 | dt 333.99ms | 1569773.34 tokens/sec
Step 14252 | loss: 3.093172 | lr:1.4679e-04 | norm 0.3032 | dt 336.43ms | 1558379.63 tokens/sec
Step 14253 | loss: 3.169426 | lr:1.4676e-04 | norm 0.2962 | dt 336.31ms | 1558948.59 tokens/sec
Step 14254 | loss: 3.084458 | lr:1.4672e-04 | norm 0.2919 | dt 336.86ms | 1556405.30 tokens/sec
Step 14255 | loss: 3.132006 | lr:1.4669e-04 | norm 0.2818 | dt 337.51ms | 1553402.69 tokens/sec
Step 14256 | loss: 3.124988 | lr:1.4666e-04 | norm 0.2824 | dt 336.46ms | 1558268.09 tokens/sec
Step 14257 | loss: 3.131778 | lr:1.4662e-04 | norm 0.2898 | dt 336.99ms | 1555785.36 tokens/sec
Step 14258 | loss: 3.140651 | lr:1.4659e-04 | norm 0.2837 | dt 336.55ms | 1557848.60 tokens/sec
Step 14259 | loss: 3.103889 | lr:1.4656e-04 | norm 0.2850 | dt 338.80ms | 1547464.76 tokens/sec
Step 14260 | loss: 3.135622 | lr:1.4652e-04 | norm 0.2979 | dt 336.61ms | 1557546.27 tokens/sec
Step 14261 | loss: 3.102498 | lr:1.4649e-04 | norm 0.2924 | dt 336.43ms | 1558373.00 tokens/sec
Step 14262 | loss: 3.123582 | lr:1.4645e-04 | norm 0.3037 | dt 336.66ms | 1557312.43 tokens/sec
Step 14263 | loss: 3.097090 | lr:1.4642e-04 | norm 0.2698 | dt 337.00ms | 1555736.93 tokens/sec
Step 14264 | loss: 3.095984 | lr:1.4639e-04 | norm 0.3112 | dt 337.63ms | 1552845.44 tokens/sec
Step 14265 | loss: 3.120480 | lr:1.4635e-04 | norm 0.2813 | dt 337.87ms | 1551763.91 tokens/sec
Step 14266 | loss: 3.093084 | lr:1.4632e-04 | norm 0.2865 | dt 336.39ms | 1558578.44 tokens/sec
Step 14267 | loss: 3.066797 | lr:1.4628e-04 | norm 0.2884 | dt 337.11ms | 1555245.10 tokens/sec
Step 14268 | loss: 3.139800 | lr:1.4625e-04 | norm 0.2914 | dt 337.73ms | 1552398.18 tokens/sec
Step 14269 | loss: 3.087152 | lr:1.4622e-04 | norm 0.2841 | dt 336.87ms | 1556331.50 tokens/sec
Step 14270 | loss: 3.033061 | lr:1.4618e-04 | norm 0.2731 | dt 337.59ms | 1553008.85 tokens/sec
Step 14271 | loss: 3.033214 | lr:1.4615e-04 | norm 0.2773 | dt 337.30ms | 1554375.54 tokens/sec
Step 14272 | loss: 3.069318 | lr:1.4611e-04 | norm 0.2773 | dt 338.31ms | 1549733.12 tokens/sec
Step 14273 | loss: 3.072248 | lr:1.4608e-04 | norm 0.2734 | dt 337.37ms | 1554039.40 tokens/sec
Step 14274 | loss: 3.059222 | lr:1.4605e-04 | norm 0.2754 | dt 337.74ms | 1552323.66 tokens/sec
Step 14275 | loss: 3.028378 | lr:1.4601e-04 | norm 0.2783 | dt 337.69ms | 1552594.38 tokens/sec
Step 14276 | loss: 3.087241 | lr:1.4598e-04 | norm 0.2863 | dt 338.11ms | 1550641.23 tokens/sec
Step 14277 | loss: 3.037996 | lr:1.4595e-04 | norm 0.2793 | dt 337.97ms | 1551299.76 tokens/sec
Step 14278 | loss: 3.038895 | lr:1.4591e-04 | norm 0.2859 | dt 337.22ms | 1554739.29 tokens/sec
Step 14279 | loss: 3.075710 | lr:1.4588e-04 | norm 0.2761 | dt 340.30ms | 1540651.87 tokens/sec
Step 14280 | loss: 3.081378 | lr:1.4584e-04 | norm 0.2668 | dt 337.87ms | 1551767.19 tokens/sec
Step 14281 | loss: 3.039101 | lr:1.4581e-04 | norm 0.2860 | dt 338.92ms | 1546928.09 tokens/sec
Step 14282 | loss: 3.109740 | lr:1.4578e-04 | norm 0.2940 | dt 337.11ms | 1555264.90 tokens/sec
Step 14283 | loss: 3.081410 | lr:1.4574e-04 | norm 0.3063 | dt 338.91ms | 1546970.53 tokens/sec
Step 14284 | loss: 3.142183 | lr:1.4571e-04 | norm 0.2875 | dt 338.64ms | 1548228.50 tokens/sec
Step 14285 | loss: 3.072544 | lr:1.4568e-04 | norm 0.2831 | dt 338.15ms | 1550447.72 tokens/sec
Step 14286 | loss: 3.023452 | lr:1.4564e-04 | norm 0.3414 | dt 337.65ms | 1552758.82 tokens/sec
Step 14287 | loss: 3.082599 | lr:1.4561e-04 | norm 0.3010 | dt 337.75ms | 1552284.22 tokens/sec
Step 14288 | loss: 3.112154 | lr:1.4557e-04 | norm 0.3024 | dt 338.46ms | 1549049.73 tokens/sec
Step 14289 | loss: 3.078190 | lr:1.4554e-04 | norm 0.2968 | dt 339.16ms | 1545835.22 tokens/sec
Step 14290 | loss: 3.043116 | lr:1.4551e-04 | norm 0.2811 | dt 338.70ms | 1547956.04 tokens/sec
Step 14291 | loss: 3.210153 | lr:1.4547e-04 | norm 0.3295 | dt 338.59ms | 1548430.18 tokens/sec
Step 14292 | loss: 3.095844 | lr:1.4544e-04 | norm 0.3219 | dt 338.43ms | 1549187.24 tokens/sec
Step 14293 | loss: 3.127850 | lr:1.4541e-04 | norm 0.3345 | dt 339.25ms | 1545416.97 tokens/sec
Step 14294 | loss: 3.079688 | lr:1.4537e-04 | norm 0.2896 | dt 338.05ms | 1550905.89 tokens/sec
Step 14295 | loss: 3.109623 | lr:1.4534e-04 | norm 0.3073 | dt 339.66ms | 1543551.17 tokens/sec
Step 14296 | loss: 3.041464 | lr:1.4530e-04 | norm 0.3110 | dt 338.64ms | 1548233.95 tokens/sec
Step 14297 | loss: 3.032067 | lr:1.4527e-04 | norm 0.3028 | dt 338.13ms | 1550555.95 tokens/sec
Step 14298 | loss: 3.109419 | lr:1.4524e-04 | norm 0.3224 | dt 338.35ms | 1549564.95 tokens/sec
Step 14299 | loss: 3.126059 | lr:1.4520e-04 | norm 0.3235 | dt 338.94ms | 1546835.60 tokens/sec
Step 14300 | loss: 3.056661 | lr:1.4517e-04 | norm 0.3058 | dt 338.81ms | 1547433.18 tokens/sec
Step 14301 | loss: 3.117113 | lr:1.4514e-04 | norm 0.3144 | dt 338.51ms | 1548826.07 tokens/sec
Step 14302 | loss: 3.072726 | lr:1.4510e-04 | norm 0.3164 | dt 338.90ms | 1547036.92 tokens/sec
Step 14303 | loss: 3.084654 | lr:1.4507e-04 | norm 0.2922 | dt 339.01ms | 1546533.18 tokens/sec
Step 14304 | loss: 3.082413 | lr:1.4503e-04 | norm 0.3101 | dt 340.64ms | 1539106.65 tokens/sec
Step 14305 | loss: 3.137769 | lr:1.4500e-04 | norm 0.3069 | dt 338.45ms | 1549083.56 tokens/sec
Step 14306 | loss: 3.045306 | lr:1.4497e-04 | norm 0.3269 | dt 338.20ms | 1550235.67 tokens/sec
Step 14307 | loss: 3.081435 | lr:1.4493e-04 | norm 0.2881 | dt 339.32ms | 1545111.84 tokens/sec
Step 14308 | loss: 3.055442 | lr:1.4490e-04 | norm 0.3204 | dt 341.26ms | 1536323.83 tokens/sec
Step 14309 | loss: 3.078525 | lr:1.4487e-04 | norm 0.2707 | dt 338.40ms | 1549312.75 tokens/sec
Step 14310 | loss: 3.074185 | lr:1.4483e-04 | norm 0.2874 | dt 338.50ms | 1548876.25 tokens/sec
Step 14311 | loss: 3.108480 | lr:1.4480e-04 | norm 0.2831 | dt 339.39ms | 1544800.32 tokens/sec
Step 14312 | loss: 3.080945 | lr:1.4477e-04 | norm 0.2913 | dt 338.79ms | 1547526.84 tokens/sec
Step 14313 | loss: 3.079113 | lr:1.4473e-04 | norm 0.2908 | dt 339.69ms | 1543436.33 tokens/sec
Step 14314 | loss: 3.044077 | lr:1.4470e-04 | norm 0.2802 | dt 338.90ms | 1547051.07 tokens/sec
Step 14315 | loss: 3.094775 | lr:1.4466e-04 | norm 0.2952 | dt 338.91ms | 1546974.89 tokens/sec
Step 14316 | loss: 3.052785 | lr:1.4463e-04 | norm 0.3092 | dt 339.31ms | 1545146.58 tokens/sec
Step 14317 | loss: 3.060030 | lr:1.4460e-04 | norm 0.3004 | dt 338.66ms | 1548105.34 tokens/sec
Step 14318 | loss: 3.089866 | lr:1.4456e-04 | norm 0.3448 | dt 338.68ms | 1548037.77 tokens/sec
Step 14319 | loss: 3.179281 | lr:1.4453e-04 | norm 0.4890 | dt 339.42ms | 1544657.09 tokens/sec
Step 14320 | loss: 3.122429 | lr:1.4450e-04 | norm 0.3222 | dt 337.82ms | 1551988.42 tokens/sec
Step 14321 | loss: 3.110234 | lr:1.4446e-04 | norm 0.3333 | dt 337.57ms | 1553142.67 tokens/sec
Step 14322 | loss: 3.134735 | lr:1.4443e-04 | norm 0.3055 | dt 339.18ms | 1545759.16 tokens/sec
Step 14323 | loss: 3.029191 | lr:1.4440e-04 | norm 0.2857 | dt 338.27ms | 1549912.25 tokens/sec
Step 14324 | loss: 3.032165 | lr:1.4436e-04 | norm 0.2975 | dt 336.95ms | 1555966.99 tokens/sec
Step 14325 | loss: 3.034910 | lr:1.4433e-04 | norm 0.2848 | dt 339.04ms | 1546410.28 tokens/sec
Step 14326 | loss: 3.051966 | lr:1.4430e-04 | norm 0.3094 | dt 340.71ms | 1538804.01 tokens/sec
Step 14327 | loss: 3.065112 | lr:1.4426e-04 | norm 0.2906 | dt 337.38ms | 1553995.48 tokens/sec
Step 14328 | loss: 3.156110 | lr:1.4423e-04 | norm 0.2967 | dt 338.31ms | 1549713.46 tokens/sec
Step 14329 | loss: 3.089221 | lr:1.4420e-04 | norm 0.3031 | dt 337.77ms | 1552198.75 tokens/sec
Step 14330 | loss: 3.106750 | lr:1.4416e-04 | norm 0.2972 | dt 337.45ms | 1553663.90 tokens/sec
Step 14331 | loss: 3.082366 | lr:1.4413e-04 | norm 0.2797 | dt 337.87ms | 1551732.15 tokens/sec
Step 14332 | loss: 3.090581 | lr:1.4409e-04 | norm 0.2513 | dt 337.60ms | 1552962.78 tokens/sec
Step 14333 | loss: 3.074934 | lr:1.4406e-04 | norm 0.2709 | dt 338.21ms | 1550169.01 tokens/sec
Step 14334 | loss: 3.128608 | lr:1.4403e-04 | norm 0.2691 | dt 337.08ms | 1555373.80 tokens/sec
Step 14335 | loss: 3.090880 | lr:1.4399e-04 | norm 0.2674 | dt 339.64ms | 1543648.68 tokens/sec
Step 14336 | loss: 3.099647 | lr:1.4396e-04 | norm 0.2666 | dt 339.86ms | 1542667.57 tokens/sec
Step 14337 | loss: 3.095345 | lr:1.4393e-04 | norm 0.2566 | dt 338.42ms | 1549237.44 tokens/sec
Step 14338 | loss: 3.028237 | lr:1.4389e-04 | norm 0.2706 | dt 337.94ms | 1551417.96 tokens/sec
Step 14339 | loss: 3.070756 | lr:1.4386e-04 | norm 0.2666 | dt 339.87ms | 1542601.56 tokens/sec
Step 14340 | loss: 3.104973 | lr:1.4383e-04 | norm 0.2749 | dt 337.60ms | 1552975.94 tokens/sec
Step 14341 | loss: 3.072310 | lr:1.4379e-04 | norm 0.2834 | dt 337.78ms | 1552153.83 tokens/sec
Step 14342 | loss: 3.091470 | lr:1.4376e-04 | norm 0.2738 | dt 337.77ms | 1552183.41 tokens/sec
Step 14343 | loss: 3.064679 | lr:1.4373e-04 | norm 0.2852 | dt 337.93ms | 1551463.93 tokens/sec
Step 14344 | loss: 3.083846 | lr:1.4369e-04 | norm 0.2687 | dt 338.24ms | 1550054.28 tokens/sec
Step 14345 | loss: 3.063050 | lr:1.4366e-04 | norm 0.3521 | dt 338.04ms | 1550986.83 tokens/sec
Step 14346 | loss: 3.078232 | lr:1.4363e-04 | norm 0.2914 | dt 338.34ms | 1549592.24 tokens/sec
Step 14347 | loss: 3.041341 | lr:1.4359e-04 | norm 0.3032 | dt 337.80ms | 1552066.19 tokens/sec
Step 14348 | loss: 3.032063 | lr:1.4356e-04 | norm 0.2961 | dt 338.02ms | 1551076.54 tokens/sec
Step 14349 | loss: 3.105335 | lr:1.4353e-04 | norm 0.3097 | dt 337.88ms | 1551721.20 tokens/sec
Step 14350 | loss: 3.056997 | lr:1.4349e-04 | norm 0.3103 | dt 337.46ms | 1553649.63 tokens/sec
Step 14351 | loss: 3.073206 | lr:1.4346e-04 | norm 0.3252 | dt 338.15ms | 1550448.81 tokens/sec
Step 14352 | loss: 3.138379 | lr:1.4343e-04 | norm 0.2900 | dt 338.32ms | 1549681.79 tokens/sec
Step 14353 | loss: 3.073265 | lr:1.4339e-04 | norm 0.3307 | dt 337.55ms | 1553227.14 tokens/sec
Step 14354 | loss: 3.131935 | lr:1.4336e-04 | norm 0.2856 | dt 337.71ms | 1552481.48 tokens/sec
Step 14355 | loss: 3.100006 | lr:1.4333e-04 | norm 0.3133 | dt 337.47ms | 1553568.40 tokens/sec
Step 14356 | loss: 3.147460 | lr:1.4329e-04 | norm 0.3042 | dt 337.68ms | 1552598.76 tokens/sec
Step 14357 | loss: 3.057384 | lr:1.4326e-04 | norm 0.2997 | dt 338.24ms | 1550055.37 tokens/sec
Step 14358 | loss: 3.161229 | lr:1.4323e-04 | norm 0.3072 | dt 337.81ms | 1552037.71 tokens/sec
Step 14359 | loss: 3.069503 | lr:1.4319e-04 | norm 0.3128 | dt 337.93ms | 1551468.31 tokens/sec
Step 14360 | loss: 3.128170 | lr:1.4316e-04 | norm 0.3130 | dt 338.37ms | 1549457.95 tokens/sec
Step 14361 | loss: 3.071171 | lr:1.4313e-04 | norm 0.3048 | dt 337.37ms | 1554040.50 tokens/sec
Step 14362 | loss: 3.129875 | lr:1.4309e-04 | norm 0.3105 | dt 337.43ms | 1553772.58 tokens/sec
Step 14363 | loss: 3.065453 | lr:1.4306e-04 | norm 0.2961 | dt 897.57ms | 584120.50 tokens/sec
Step 14364 | loss: 3.125594 | lr:1.4303e-04 | norm 0.3087 | dt 334.82ms | 1565892.29 tokens/sec
Step 14365 | loss: 3.110798 | lr:1.4299e-04 | norm 0.2897 | dt 337.04ms | 1555583.95 tokens/sec
Step 14366 | loss: 3.078000 | lr:1.4296e-04 | norm 0.2984 | dt 338.90ms | 1547019.51 tokens/sec
Step 14367 | loss: 3.063074 | lr:1.4293e-04 | norm 0.3042 | dt 337.10ms | 1555277.00 tokens/sec
Step 14368 | loss: 3.102076 | lr:1.4289e-04 | norm 0.2752 | dt 338.54ms | 1548696.27 tokens/sec
Step 14369 | loss: 3.089994 | lr:1.4286e-04 | norm 0.2898 | dt 337.14ms | 1555124.12 tokens/sec
Step 14370 | loss: 3.126269 | lr:1.4283e-04 | norm 0.2803 | dt 337.93ms | 1551457.36 tokens/sec
Step 14371 | loss: 3.070659 | lr:1.4279e-04 | norm 0.3011 | dt 338.30ms | 1549750.59 tokens/sec
Step 14372 | loss: 3.122936 | lr:1.4276e-04 | norm 0.2931 | dt 337.10ms | 1555304.50 tokens/sec
Step 14373 | loss: 3.068696 | lr:1.4273e-04 | norm 0.2901 | dt 337.33ms | 1554217.34 tokens/sec
Step 14374 | loss: 3.120203 | lr:1.4269e-04 | norm 0.3074 | dt 338.07ms | 1550820.58 tokens/sec
Step 14375 | loss: 3.090803 | lr:1.4266e-04 | norm 0.2851 | dt 337.70ms | 1552519.84 tokens/sec
Step 14376 | loss: 3.028406 | lr:1.4263e-04 | norm 0.2718 | dt 337.42ms | 1553834.06 tokens/sec
Step 14377 | loss: 3.048568 | lr:1.4259e-04 | norm 0.2660 | dt 338.21ms | 1550200.70 tokens/sec
Step 14378 | loss: 3.091875 | lr:1.4256e-04 | norm 0.3518 | dt 337.94ms | 1551426.72 tokens/sec
Step 14379 | loss: 3.027518 | lr:1.4253e-04 | norm 0.3097 | dt 338.06ms | 1550886.20 tokens/sec
Step 14380 | loss: 3.118068 | lr:1.4249e-04 | norm 0.3092 | dt 338.24ms | 1550058.65 tokens/sec
Step 14381 | loss: 3.118216 | lr:1.4246e-04 | norm 0.3077 | dt 338.17ms | 1550390.88 tokens/sec
Step 14382 | loss: 3.038712 | lr:1.4243e-04 | norm 0.2959 | dt 337.51ms | 1553385.13 tokens/sec
Step 14383 | loss: 3.075211 | lr:1.4239e-04 | norm 0.3155 | dt 338.51ms | 1548820.62 tokens/sec
Step 14384 | loss: 3.094999 | lr:1.4236e-04 | norm 0.2749 | dt 338.65ms | 1548162.01 tokens/sec
Step 14385 | loss: 3.061972 | lr:1.4233e-04 | norm 0.2922 | dt 337.44ms | 1553710.00 tokens/sec
Step 14386 | loss: 3.104802 | lr:1.4229e-04 | norm 0.2851 | dt 337.12ms | 1555196.70 tokens/sec
Step 14387 | loss: 3.167920 | lr:1.4226e-04 | norm 0.3656 | dt 338.83ms | 1547330.83 tokens/sec
Step 14388 | loss: 3.064185 | lr:1.4223e-04 | norm 0.3189 | dt 337.97ms | 1551277.87 tokens/sec
Step 14389 | loss: 3.098594 | lr:1.4219e-04 | norm 0.3009 | dt 337.98ms | 1551246.14 tokens/sec
Step 14390 | loss: 3.179384 | lr:1.4216e-04 | norm 0.3150 | dt 337.32ms | 1554289.84 tokens/sec
Step 14391 | loss: 3.200183 | lr:1.4213e-04 | norm 0.4088 | dt 338.52ms | 1548749.71 tokens/sec
Step 14392 | loss: 3.114898 | lr:1.4209e-04 | norm 0.3213 | dt 337.82ms | 1551996.09 tokens/sec
Step 14393 | loss: 3.187491 | lr:1.4206e-04 | norm 0.3379 | dt 337.50ms | 1553440.00 tokens/sec
Step 14394 | loss: 3.095058 | lr:1.4203e-04 | norm 0.3093 | dt 337.67ms | 1552677.69 tokens/sec
Step 14395 | loss: 3.099775 | lr:1.4199e-04 | norm 0.3149 | dt 338.76ms | 1547684.77 tokens/sec
Step 14396 | loss: 3.039296 | lr:1.4196e-04 | norm 0.3096 | dt 338.55ms | 1548639.55 tokens/sec
Step 14397 | loss: 3.105815 | lr:1.4193e-04 | norm 0.3024 | dt 338.59ms | 1548460.71 tokens/sec
Step 14398 | loss: 3.150677 | lr:1.4189e-04 | norm 0.3327 | dt 338.52ms | 1548747.53 tokens/sec
Step 14399 | loss: 3.074434 | lr:1.4186e-04 | norm 0.3271 | dt 338.64ms | 1548231.77 tokens/sec
Step 14400 | loss: 3.080923 | lr:1.4183e-04 | norm 0.2895 | dt 339.04ms | 1546387.44 tokens/sec
Step 14401 | loss: 3.132508 | lr:1.4180e-04 | norm 0.3218 | dt 338.87ms | 1547163.18 tokens/sec
Step 14402 | loss: 3.193970 | lr:1.4176e-04 | norm 0.3680 | dt 338.66ms | 1548133.67 tokens/sec
Step 14403 | loss: 3.162799 | lr:1.4173e-04 | norm 0.3243 | dt 338.41ms | 1549269.09 tokens/sec
Step 14404 | loss: 3.085340 | lr:1.4170e-04 | norm 0.3168 | dt 339.79ms | 1542996.63 tokens/sec
Step 14405 | loss: 3.105287 | lr:1.4166e-04 | norm 0.3010 | dt 338.11ms | 1550662.01 tokens/sec
Step 14406 | loss: 3.019873 | lr:1.4163e-04 | norm 0.2968 | dt 338.33ms | 1549615.18 tokens/sec
Step 14407 | loss: 3.104837 | lr:1.4160e-04 | norm 0.3077 | dt 339.04ms | 1546385.27 tokens/sec
Step 14408 | loss: 3.109683 | lr:1.4156e-04 | norm 0.2760 | dt 339.68ms | 1543480.74 tokens/sec
Step 14409 | loss: 3.078238 | lr:1.4153e-04 | norm 0.2717 | dt 338.91ms | 1546991.21 tokens/sec
Step 14410 | loss: 3.097147 | lr:1.4150e-04 | norm 0.2981 | dt 338.56ms | 1548563.21 tokens/sec
Step 14411 | loss: 3.035640 | lr:1.4146e-04 | norm 0.2891 | dt 338.59ms | 1548434.55 tokens/sec
Step 14412 | loss: 3.076144 | lr:1.4143e-04 | norm 0.2759 | dt 338.55ms | 1548647.19 tokens/sec
Step 14413 | loss: 3.081043 | lr:1.4140e-04 | norm 0.3039 | dt 338.71ms | 1547884.13 tokens/sec
Step 14414 | loss: 3.038021 | lr:1.4137e-04 | norm 0.3177 | dt 339.53ms | 1544168.98 tokens/sec
Step 14415 | loss: 3.079763 | lr:1.4133e-04 | norm 0.2790 | dt 338.64ms | 1548193.62 tokens/sec
Step 14416 | loss: 3.000044 | lr:1.4130e-04 | norm 0.2858 | dt 338.44ms | 1549146.85 tokens/sec
Step 14417 | loss: 3.042813 | lr:1.4127e-04 | norm 0.2607 | dt 338.51ms | 1548799.89 tokens/sec
Step 14418 | loss: 3.073994 | lr:1.4123e-04 | norm 0.2888 | dt 338.59ms | 1548461.80 tokens/sec
Step 14419 | loss: 3.093714 | lr:1.4120e-04 | norm 0.2770 | dt 338.78ms | 1547596.54 tokens/sec
Step 14420 | loss: 3.066434 | lr:1.4117e-04 | norm 0.2851 | dt 337.76ms | 1552244.77 tokens/sec
Step 14421 | loss: 3.049071 | lr:1.4113e-04 | norm 0.2769 | dt 336.99ms | 1555806.27 tokens/sec
Step 14422 | loss: 3.102974 | lr:1.4110e-04 | norm 0.2748 | dt 338.14ms | 1550502.38 tokens/sec
Step 14423 | loss: 3.090378 | lr:1.4107e-04 | norm 0.3100 | dt 338.62ms | 1548296.09 tokens/sec
Step 14424 | loss: 3.165636 | lr:1.4104e-04 | norm 0.2652 | dt 338.14ms | 1550494.73 tokens/sec
Step 14425 | loss: 3.143852 | lr:1.4100e-04 | norm 0.2913 | dt 338.34ms | 1549599.89 tokens/sec
Step 14426 | loss: 3.091481 | lr:1.4097e-04 | norm 0.2682 | dt 337.95ms | 1551363.23 tokens/sec
Step 14427 | loss: 3.072518 | lr:1.4094e-04 | norm 0.2961 | dt 338.15ms | 1550467.40 tokens/sec
Step 14428 | loss: 3.034534 | lr:1.4090e-04 | norm 0.2848 | dt 339.07ms | 1546266.75 tokens/sec
Step 14429 | loss: 3.064634 | lr:1.4087e-04 | norm 0.3034 | dt 338.06ms | 1550890.58 tokens/sec
Step 14430 | loss: 3.157596 | lr:1.4084e-04 | norm 0.2921 | dt 338.59ms | 1548455.26 tokens/sec
Step 14431 | loss: 3.099455 | lr:1.4080e-04 | norm 0.3084 | dt 338.60ms | 1548397.48 tokens/sec
Step 14432 | loss: 3.110079 | lr:1.4077e-04 | norm 0.2808 | dt 337.97ms | 1551273.49 tokens/sec
Step 14433 | loss: 3.088960 | lr:1.4074e-04 | norm 0.2921 | dt 338.93ms | 1546890.01 tokens/sec
Step 14434 | loss: 3.096095 | lr:1.4071e-04 | norm 0.2740 | dt 338.90ms | 1547009.71 tokens/sec
Step 14435 | loss: 3.149246 | lr:1.4067e-04 | norm 0.2976 | dt 338.27ms | 1549914.44 tokens/sec
Step 14436 | loss: 3.131896 | lr:1.4064e-04 | norm 0.2748 | dt 338.28ms | 1549855.45 tokens/sec
Step 14437 | loss: 3.072985 | lr:1.4061e-04 | norm 0.2816 | dt 339.02ms | 1546465.74 tokens/sec
Step 14438 | loss: 3.135804 | lr:1.4057e-04 | norm 0.2902 | dt 338.80ms | 1547481.10 tokens/sec
Step 14439 | loss: 3.115956 | lr:1.4054e-04 | norm 0.2936 | dt 1012.61ms | 517760.79 tokens/sec
Step 14440 | loss: 3.087501 | lr:1.4051e-04 | norm 0.2871 | dt 337.07ms | 1555432.11 tokens/sec
Step 14441 | loss: 3.081678 | lr:1.4047e-04 | norm 0.2753 | dt 337.40ms | 1553888.96 tokens/sec
Step 14442 | loss: 3.057287 | lr:1.4044e-04 | norm 0.2872 | dt 337.57ms | 1553108.66 tokens/sec
Step 14443 | loss: 3.071653 | lr:1.4041e-04 | norm 0.2786 | dt 337.36ms | 1554093.22 tokens/sec
Step 14444 | loss: 3.106561 | lr:1.4038e-04 | norm 0.3201 | dt 336.33ms | 1558864.60 tokens/sec
Step 14445 | loss: 3.045630 | lr:1.4034e-04 | norm 0.2935 | dt 337.75ms | 1552306.13 tokens/sec
Step 14446 | loss: 3.068042 | lr:1.4031e-04 | norm 0.2965 | dt 338.12ms | 1550597.50 tokens/sec
Step 14447 | loss: 3.040072 | lr:1.4028e-04 | norm 0.3014 | dt 337.82ms | 1551987.32 tokens/sec
Step 14448 | loss: 3.091107 | lr:1.4024e-04 | norm 0.2901 | dt 337.39ms | 1553959.24 tokens/sec
Step 14449 | loss: 3.067217 | lr:1.4021e-04 | norm 0.2928 | dt 337.47ms | 1553569.50 tokens/sec
Step 14450 | loss: 3.001495 | lr:1.4018e-04 | norm 0.2809 | dt 337.88ms | 1551680.69 tokens/sec
Step 14451 | loss: 3.100847 | lr:1.4015e-04 | norm 0.3248 | dt 337.83ms | 1551919.42 tokens/sec
Step 14452 | loss: 3.076903 | lr:1.4011e-04 | norm 0.2691 | dt 338.20ms | 1550219.28 tokens/sec
Step 14453 | loss: 3.047918 | lr:1.4008e-04 | norm 0.3515 | dt 337.84ms | 1551886.56 tokens/sec
Step 14454 | loss: 2.996325 | lr:1.4005e-04 | norm 0.2747 | dt 337.86ms | 1551813.19 tokens/sec
Step 14455 | loss: 3.042897 | lr:1.4001e-04 | norm 0.3454 | dt 337.96ms | 1551323.84 tokens/sec
Step 14456 | loss: 3.110748 | lr:1.3998e-04 | norm 0.2881 | dt 337.85ms | 1551848.23 tokens/sec
Step 14457 | loss: 3.052641 | lr:1.3995e-04 | norm 0.3514 | dt 338.75ms | 1547734.87 tokens/sec
Step 14458 | loss: 3.086318 | lr:1.3992e-04 | norm 0.2821 | dt 337.50ms | 1553463.05 tokens/sec
Step 14459 | loss: 3.103126 | lr:1.3988e-04 | norm 0.2821 | dt 337.44ms | 1553724.27 tokens/sec
Step 14460 | loss: 3.095416 | lr:1.3985e-04 | norm 0.3120 | dt 339.02ms | 1546502.72 tokens/sec
Step 14461 | loss: 3.069937 | lr:1.3982e-04 | norm 0.3283 | dt 337.44ms | 1553742.94 tokens/sec
Step 14462 | loss: 3.116661 | lr:1.3978e-04 | norm 0.2889 | dt 338.67ms | 1548086.81 tokens/sec
Step 14463 | loss: 3.072296 | lr:1.3975e-04 | norm 0.3561 | dt 338.39ms | 1549352.05 tokens/sec
Step 14464 | loss: 3.047487 | lr:1.3972e-04 | norm 0.2898 | dt 338.55ms | 1548605.75 tokens/sec
Step 14465 | loss: 3.076553 | lr:1.3969e-04 | norm 0.3096 | dt 338.35ms | 1549539.83 tokens/sec
Step 14466 | loss: 3.083139 | lr:1.3965e-04 | norm 0.2919 | dt 338.17ms | 1550349.34 tokens/sec
Step 14467 | loss: 3.041041 | lr:1.3962e-04 | norm 0.2851 | dt 338.17ms | 1550373.39 tokens/sec
Step 14468 | loss: 3.139714 | lr:1.3959e-04 | norm 0.3308 | dt 338.46ms | 1549044.28 tokens/sec
Step 14469 | loss: 3.115419 | lr:1.3956e-04 | norm 0.2790 | dt 338.26ms | 1549934.10 tokens/sec
Step 14470 | loss: 3.071684 | lr:1.3952e-04 | norm 0.3069 | dt 337.70ms | 1552507.78 tokens/sec
Step 14471 | loss: 3.115652 | lr:1.3949e-04 | norm 0.2975 | dt 338.90ms | 1547044.54 tokens/sec
Step 14472 | loss: 3.114317 | lr:1.3946e-04 | norm 0.3162 | dt 338.16ms | 1550417.11 tokens/sec
Step 14473 | loss: 3.059978 | lr:1.3942e-04 | norm 0.2738 | dt 337.91ms | 1551583.25 tokens/sec
Step 14474 | loss: 3.115754 | lr:1.3939e-04 | norm 0.2880 | dt 338.13ms | 1550567.98 tokens/sec
Step 14475 | loss: 3.130292 | lr:1.3936e-04 | norm 0.2919 | dt 338.37ms | 1549447.03 tokens/sec
Step 14476 | loss: 3.112610 | lr:1.3933e-04 | norm 0.2865 | dt 338.20ms | 1550228.02 tokens/sec
Step 14477 | loss: 3.100318 | lr:1.3929e-04 | norm 0.2702 | dt 337.27ms | 1554522.78 tokens/sec
Step 14478 | loss: 3.104310 | lr:1.3926e-04 | norm 0.3044 | dt 338.41ms | 1549290.92 tokens/sec
Step 14479 | loss: 3.054680 | lr:1.3923e-04 | norm 0.2861 | dt 338.99ms | 1546622.37 tokens/sec
Step 14480 | loss: 3.002254 | lr:1.3920e-04 | norm 0.2632 | dt 337.88ms | 1551720.11 tokens/sec
Step 14481 | loss: 3.062740 | lr:1.3916e-04 | norm 0.2739 | dt 337.52ms | 1553364.28 tokens/sec
Step 14482 | loss: 3.050092 | lr:1.3913e-04 | norm 0.2761 | dt 339.55ms | 1544067.06 tokens/sec
Step 14483 | loss: 3.062614 | lr:1.3910e-04 | norm 0.2776 | dt 338.35ms | 1549543.11 tokens/sec
Step 14484 | loss: 3.083614 | lr:1.3906e-04 | norm 0.2856 | dt 337.78ms | 1552157.12 tokens/sec
Step 14485 | loss: 3.055286 | lr:1.3903e-04 | norm 0.2780 | dt 338.30ms | 1549768.07 tokens/sec
Step 14486 | loss: 3.078629 | lr:1.3900e-04 | norm 0.2897 | dt 337.20ms | 1554820.64 tokens/sec
Step 14487 | loss: 3.071801 | lr:1.3897e-04 | norm 0.2894 | dt 338.48ms | 1548954.81 tokens/sec
Step 14488 | loss: 3.057759 | lr:1.3893e-04 | norm 0.3054 | dt 337.92ms | 1551496.77 tokens/sec
Step 14489 | loss: 3.037961 | lr:1.3890e-04 | norm 0.2785 | dt 338.18ms | 1550300.16 tokens/sec
Step 14490 | loss: 3.017762 | lr:1.3887e-04 | norm 0.2890 | dt 337.76ms | 1552250.25 tokens/sec
Step 14491 | loss: 3.014607 | lr:1.3884e-04 | norm 0.2818 | dt 338.80ms | 1547507.23 tokens/sec
Step 14492 | loss: 3.041756 | lr:1.3880e-04 | norm 0.2971 | dt 337.79ms | 1552111.11 tokens/sec
Step 14493 | loss: 3.102553 | lr:1.3877e-04 | norm 0.2910 | dt 337.71ms | 1552459.56 tokens/sec
Step 14494 | loss: 3.122727 | lr:1.3874e-04 | norm 0.2830 | dt 338.30ms | 1549792.10 tokens/sec
Step 14495 | loss: 3.093109 | lr:1.3871e-04 | norm 0.3010 | dt 338.03ms | 1551007.62 tokens/sec
Step 14496 | loss: 3.064077 | lr:1.3867e-04 | norm 0.2710 | dt 337.95ms | 1551377.46 tokens/sec
Step 14497 | loss: 3.178001 | lr:1.3864e-04 | norm 0.3201 | dt 337.69ms | 1552562.59 tokens/sec
Step 14498 | loss: 3.030815 | lr:1.3861e-04 | norm 0.3108 | dt 337.78ms | 1552165.88 tokens/sec
Step 14499 | loss: 3.069257 | lr:1.3858e-04 | norm 0.2879 | dt 338.09ms | 1550715.59 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 14500: 3.1179
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3029/10042=0.3016


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not the kind to put you off on when you're dealing with something complex.
Now, we do
rank 3 sample 1 >Hello, I'm a language model, and my father is a journalist; I'm just starting a new job. I'm a lawyer, and I work at
rank 3 sample 2 >Hello, I'm a language model, and you know a reason why a lot of software engineers, even if you were one of them, are now getting frustrated
rank 3 sample 3 >Hello, I'm a language model, and he's got a name. He's a native American. So he's so different.
One thing that makes




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm going to teach you how well you do in programming. As you begin to do...<|endoftext|>A new report
rank 7 sample 1 >Hello, I'm a language model, is that possible?
- (not so)
Yeah, I'm a real mathematician, but how does this differ
rank 7 sample 2 >Hello, I'm a language model, and I want to ask you two questions about how humans have learned and been taught to read and write. I would ask
rank 7 sample 3 >Hello, I'm a language model, so this is a very long-baseline lesson. I'll start with a brief introduction to using a language model in




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, for which you want to develop your software, and I've done it well, too.
The first of the tools
rank 2 sample 1 >Hello, I'm a language model, but I got it wrong.
Here's the catch: I just had to do a quick test. I can't
rank 2 sample 2 >Hello, I'm a language model, so I need to understand the different steps I need to take to learn these languages better. I would do a simulation where
rank 2 sample 3 >Hello, I'm a language model, but this tutorial is for a beginner. Here's why. A language model is a collection of knowledge, tools and procedures




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, as my father says I had a very difficult time doing that job right away. He's now a teenager and I really
rank 6 sample 1 >Hello, I'm a language model, and I know that a language model is built, that is, a description of the language, it is an action and
rank 6 sample 2 >Hello, I'm a language model, but how does that compare? What language model should I use for writing the same thing?
I mean, let me
rank 6 sample 3 >Hello, I'm a language model, and so, just a little introduction. I'll start with the structure of an HTML page .
|title in Tag




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 0: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I'd like to help you with one.
I'm a very small man from Los Angeles, California.

rank 0 sample 0 >Hello, I'm a language model, and I need to learn English and write down this language using a programming language and a program. And then when I've
rank 4 sample 1 >Hello, I'm a language model, can someone explain his work? A child?
For now, I need you to find a way to teach
it
rank 0 sample 1 >Hello, I'm a language model, so please understand me and not the programming language that might suit you, not the language I'm going to use.

rank 4 sample 2 >Hello, I'm a language model, I see someone out here at the conference talking about the project that you're working on. When the researchers are going through
rank 0 sample 2 >Hello, I'm a language model, so I should definitely explain all the steps. (Don't just read about it, you'll find it in my chapter
rank 4 sample 3 >Hello, I'm a language model, and it made its way to the center of everything!” — a colleague from the UC system who was talking with


rank 0 sample 3 >Hello, I'm a language model, so maybe I could be wrong, but that helps me in the way I understand things. That's it. I like





ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 1: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but not in the business world. It's a problem that I'd have to tackle in my business. It's my
rank 1 sample 0 >Hello, I'm a language model, and my last post (in the new section I've also listed some tips for writing in the second person. I'm
rank 5 sample 1 >Hello, I'm a language model, here's another example
Here's the problem :
- A customer wants to ask you for a particular thing, but
rank 5 sample 2 >Hello, I'm a language model, so I will have to write two articles. In this article, I will go into that.
In the first piece
rank 1 sample 1 >Hello, I'm a language model, not an academic at all. And my math teacher's answer is really, you can explain why. I'm not a
rank 5 sample 3 >Hello, I'm a language model, meaning I'm learning how to use it in real life situations.
I hope, as a community member, I am


rank 1 sample 2 >Hello, I'm a language model, but is this how I learn it?
I'm going to do some experiment. I want to see how long the
rank 1 sample 3 >Hello, I'm a language model, and I'm wondering how exactly do I come up with models that you guys write? [C.S. Lewis]


Step 14500 | loss: 3.075106 | lr:1.3854e-04 | norm 0.2893 | dt 18672.15ms | 28078.60 tokens/sec
Step 14501 | loss: 3.200025 | lr:1.3851e-04 | norm 0.3107 | dt 333.61ms | 1571570.56 tokens/sec
Step 14502 | loss: 3.098591 | lr:1.3848e-04 | norm 0.2950 | dt 335.43ms | 1563035.18 tokens/sec
Step 14503 | loss: 3.071172 | lr:1.3845e-04 | norm 0.2775 | dt 337.13ms | 1555139.51 tokens/sec
Step 14504 | loss: 3.147963 | lr:1.3841e-04 | norm 0.3237 | dt 336.26ms | 1559192.87 tokens/sec
Step 14505 | loss: 3.052866 | lr:1.3838e-04 | norm 0.2817 | dt 335.68ms | 1561879.50 tokens/sec
Step 14506 | loss: 3.104767 | lr:1.3835e-04 | norm 0.3009 | dt 336.50ms | 1558078.19 tokens/sec
Step 14507 | loss: 3.107826 | lr:1.3831e-04 | norm 0.3156 | dt 336.68ms | 1557244.05 tokens/sec
Step 14508 | loss: 3.089818 | lr:1.3828e-04 | norm 0.3106 | dt 336.39ms | 1558579.54 tokens/sec
Step 14509 | loss: 3.138677 | lr:1.3825e-04 | norm 0.3128 | dt 337.04ms | 1555577.35 tokens/sec
Step 14510 | loss: 3.078909 | lr:1.3822e-04 | norm 0.2817 | dt 337.20ms | 1554826.14 tokens/sec
Step 14511 | loss: 3.107836 | lr:1.3818e-04 | norm 0.2970 | dt 336.50ms | 1558057.21 tokens/sec
Step 14512 | loss: 3.114982 | lr:1.3815e-04 | norm 0.3041 | dt 337.29ms | 1554407.40 tokens/sec
Step 14513 | loss: 3.076442 | lr:1.3812e-04 | norm 0.2933 | dt 337.48ms | 1553529.99 tokens/sec
Step 14514 | loss: 3.134890 | lr:1.3809e-04 | norm 0.3310 | dt 336.08ms | 1559991.47 tokens/sec
Step 14515 | loss: 3.081352 | lr:1.3805e-04 | norm 0.2776 | dt 337.05ms | 1555499.23 tokens/sec
Step 14516 | loss: 3.019403 | lr:1.3802e-04 | norm 0.2952 | dt 337.32ms | 1554268.97 tokens/sec
Step 14517 | loss: 3.035997 | lr:1.3799e-04 | norm 0.3131 | dt 337.48ms | 1553527.80 tokens/sec
Step 14518 | loss: 3.090523 | lr:1.3796e-04 | norm 0.2890 | dt 337.84ms | 1551886.56 tokens/sec
Step 14519 | loss: 3.048776 | lr:1.3792e-04 | norm 0.3076 | dt 336.44ms | 1558343.18 tokens/sec
Step 14520 | loss: 3.074602 | lr:1.3789e-04 | norm 0.2683 | dt 337.85ms | 1551851.51 tokens/sec
Step 14521 | loss: 3.022118 | lr:1.3786e-04 | norm 0.2942 | dt 337.27ms | 1554492.01 tokens/sec
Step 14522 | loss: 3.107875 | lr:1.3783e-04 | norm 0.2734 | dt 337.48ms | 1553548.65 tokens/sec
Step 14523 | loss: 3.071284 | lr:1.3780e-04 | norm 0.3036 | dt 337.32ms | 1554252.49 tokens/sec
Step 14524 | loss: 3.084380 | lr:1.3776e-04 | norm 0.2743 | dt 337.57ms | 1553129.50 tokens/sec
Step 14525 | loss: 3.081151 | lr:1.3773e-04 | norm 0.2923 | dt 336.83ms | 1556533.09 tokens/sec
Step 14526 | loss: 3.086829 | lr:1.3770e-04 | norm 0.2823 | dt 337.09ms | 1555339.70 tokens/sec
Step 14527 | loss: 3.069780 | lr:1.3767e-04 | norm 0.3532 | dt 337.08ms | 1555370.50 tokens/sec
Step 14528 | loss: 3.131613 | lr:1.3763e-04 | norm 0.3256 | dt 336.91ms | 1556186.11 tokens/sec
Step 14529 | loss: 3.073217 | lr:1.3760e-04 | norm 0.3252 | dt 337.44ms | 1553728.67 tokens/sec
Step 14530 | loss: 3.045232 | lr:1.3757e-04 | norm 0.3401 | dt 336.89ms | 1556273.12 tokens/sec
Step 14531 | loss: 3.080204 | lr:1.3754e-04 | norm 0.2880 | dt 338.17ms | 1550385.41 tokens/sec
Step 14532 | loss: 3.051121 | lr:1.3750e-04 | norm 0.3348 | dt 336.78ms | 1556772.21 tokens/sec
Step 14533 | loss: 3.081087 | lr:1.3747e-04 | norm 0.3056 | dt 337.57ms | 1553120.73 tokens/sec
Step 14534 | loss: 3.100126 | lr:1.3744e-04 | norm 0.3037 | dt 336.71ms | 1557074.24 tokens/sec
Step 14535 | loss: 3.076453 | lr:1.3741e-04 | norm 0.2989 | dt 336.86ms | 1556415.21 tokens/sec
Step 14536 | loss: 3.066589 | lr:1.3737e-04 | norm 0.3157 | dt 338.66ms | 1548127.13 tokens/sec
Step 14537 | loss: 3.102383 | lr:1.3734e-04 | norm 0.2893 | dt 337.10ms | 1555274.80 tokens/sec
Step 14538 | loss: 3.063458 | lr:1.3731e-04 | norm 0.2882 | dt 337.20ms | 1554806.35 tokens/sec
Step 14539 | loss: 3.136767 | lr:1.3728e-04 | norm 0.3143 | dt 337.01ms | 1555727.02 tokens/sec
Step 14540 | loss: 3.131715 | lr:1.3724e-04 | norm 0.2795 | dt 337.39ms | 1553952.65 tokens/sec
Step 14541 | loss: 3.099758 | lr:1.3721e-04 | norm 0.2945 | dt 337.74ms | 1552331.34 tokens/sec
Step 14542 | loss: 3.107859 | lr:1.3718e-04 | norm 0.2862 | dt 336.63ms | 1557460.22 tokens/sec
Step 14543 | loss: 3.057004 | lr:1.3715e-04 | norm 0.2911 | dt 337.87ms | 1551733.25 tokens/sec
Step 14544 | loss: 3.039937 | lr:1.3711e-04 | norm 0.2827 | dt 337.70ms | 1552529.70 tokens/sec
Step 14545 | loss: 3.095338 | lr:1.3708e-04 | norm 0.2718 | dt 337.52ms | 1553358.80 tokens/sec
Step 14546 | loss: 3.109125 | lr:1.3705e-04 | norm 0.2946 | dt 337.55ms | 1553228.23 tokens/sec
Step 14547 | loss: 3.054007 | lr:1.3702e-04 | norm 0.2638 | dt 338.36ms | 1549496.16 tokens/sec
Step 14548 | loss: 3.075459 | lr:1.3699e-04 | norm 0.2831 | dt 337.93ms | 1551473.78 tokens/sec
Step 14549 | loss: 3.064562 | lr:1.3695e-04 | norm 0.2750 | dt 337.31ms | 1554321.70 tokens/sec
Step 14550 | loss: 3.077111 | lr:1.3692e-04 | norm 0.2785 | dt 337.77ms | 1552183.41 tokens/sec
Step 14551 | loss: 3.053015 | lr:1.3689e-04 | norm 0.2835 | dt 338.16ms | 1550422.58 tokens/sec
Step 14552 | loss: 3.086322 | lr:1.3686e-04 | norm 0.2685 | dt 915.65ms | 572588.50 tokens/sec
Step 14553 | loss: 3.009745 | lr:1.3682e-04 | norm 0.2812 | dt 334.64ms | 1566727.91 tokens/sec
Step 14554 | loss: 3.055704 | lr:1.3679e-04 | norm 0.2826 | dt 337.33ms | 1554215.14 tokens/sec
Step 14555 | loss: 3.019840 | lr:1.3676e-04 | norm 0.2823 | dt 337.97ms | 1551299.76 tokens/sec
Step 14556 | loss: 3.058399 | lr:1.3673e-04 | norm 0.2646 | dt 337.24ms | 1554653.56 tokens/sec
Step 14557 | loss: 3.081849 | lr:1.3669e-04 | norm 0.2807 | dt 337.50ms | 1553425.73 tokens/sec
Step 14558 | loss: 3.064731 | lr:1.3666e-04 | norm 0.2793 | dt 337.69ms | 1552587.80 tokens/sec
Step 14559 | loss: 3.046007 | lr:1.3663e-04 | norm 0.2814 | dt 337.19ms | 1554881.11 tokens/sec
Step 14560 | loss: 3.073799 | lr:1.3660e-04 | norm 0.2911 | dt 338.07ms | 1550817.30 tokens/sec
Step 14561 | loss: 3.111879 | lr:1.3657e-04 | norm 0.3141 | dt 338.07ms | 1550808.55 tokens/sec
Step 14562 | loss: 3.015487 | lr:1.3653e-04 | norm 0.2769 | dt 337.40ms | 1553905.43 tokens/sec
Step 14563 | loss: 3.073919 | lr:1.3650e-04 | norm 0.3356 | dt 338.87ms | 1547188.22 tokens/sec
Step 14564 | loss: 3.093375 | lr:1.3647e-04 | norm 0.3005 | dt 337.98ms | 1551218.78 tokens/sec
Step 14565 | loss: 3.112664 | lr:1.3644e-04 | norm 0.3105 | dt 338.09ms | 1550727.62 tokens/sec
Step 14566 | loss: 3.128200 | lr:1.3640e-04 | norm 0.2948 | dt 338.05ms | 1550911.36 tokens/sec
Step 14567 | loss: 3.169456 | lr:1.3637e-04 | norm 0.2886 | dt 337.77ms | 1552213.00 tokens/sec
Step 14568 | loss: 3.042119 | lr:1.3634e-04 | norm 0.3251 | dt 338.12ms | 1550587.66 tokens/sec
Step 14569 | loss: 3.104064 | lr:1.3631e-04 | norm 0.2851 | dt 338.49ms | 1548895.89 tokens/sec
Step 14570 | loss: 3.030201 | lr:1.3628e-04 | norm 0.2935 | dt 338.52ms | 1548771.53 tokens/sec
Step 14571 | loss: 3.135140 | lr:1.3624e-04 | norm 0.3065 | dt 338.21ms | 1550190.87 tokens/sec
Step 14572 | loss: 3.110792 | lr:1.3621e-04 | norm 0.3025 | dt 337.68ms | 1552606.43 tokens/sec
Step 14573 | loss: 3.083116 | lr:1.3618e-04 | norm 0.2731 | dt 338.22ms | 1550144.97 tokens/sec
Step 14574 | loss: 3.081804 | lr:1.3615e-04 | norm 0.2744 | dt 339.44ms | 1544574.63 tokens/sec
Step 14575 | loss: 3.103948 | lr:1.3612e-04 | norm 0.2855 | dt 337.74ms | 1552352.16 tokens/sec
Step 14576 | loss: 3.062788 | lr:1.3608e-04 | norm 0.2605 | dt 338.41ms | 1549247.26 tokens/sec
Step 14577 | loss: 3.110380 | lr:1.3605e-04 | norm 0.2653 | dt 337.91ms | 1551539.46 tokens/sec
Step 14578 | loss: 3.125740 | lr:1.3602e-04 | norm 0.3305 | dt 338.69ms | 1548002.90 tokens/sec
Step 14579 | loss: 3.048250 | lr:1.3599e-04 | norm 0.2776 | dt 339.10ms | 1546134.11 tokens/sec
Step 14580 | loss: 3.075096 | lr:1.3595e-04 | norm 0.2800 | dt 338.11ms | 1550644.51 tokens/sec
Step 14581 | loss: 3.104853 | lr:1.3592e-04 | norm 0.2751 | dt 338.22ms | 1550144.97 tokens/sec
Step 14582 | loss: 3.072482 | lr:1.3589e-04 | norm 0.2759 | dt 338.98ms | 1546678.93 tokens/sec
Step 14583 | loss: 3.044701 | lr:1.3586e-04 | norm 0.2719 | dt 339.05ms | 1546324.38 tokens/sec
Step 14584 | loss: 3.088223 | lr:1.3583e-04 | norm 0.2762 | dt 338.43ms | 1549161.04 tokens/sec
Step 14585 | loss: 3.024236 | lr:1.3579e-04 | norm 0.2937 | dt 338.52ms | 1548782.44 tokens/sec
Step 14586 | loss: 3.080471 | lr:1.3576e-04 | norm 0.2852 | dt 337.51ms | 1553393.91 tokens/sec
Step 14587 | loss: 3.005568 | lr:1.3573e-04 | norm 0.2624 | dt 338.38ms | 1549414.28 tokens/sec
Step 14588 | loss: 3.027738 | lr:1.3570e-04 | norm 0.2903 | dt 338.58ms | 1548509.78 tokens/sec
Step 14589 | loss: 3.119065 | lr:1.3567e-04 | norm 0.3043 | dt 338.04ms | 1550979.18 tokens/sec
Step 14590 | loss: 3.107390 | lr:1.3563e-04 | norm 0.2805 | dt 338.15ms | 1550481.61 tokens/sec
Step 14591 | loss: 3.031914 | lr:1.3560e-04 | norm 0.3264 | dt 338.73ms | 1547808.95 tokens/sec
Step 14592 | loss: 3.070531 | lr:1.3557e-04 | norm 0.2749 | dt 338.22ms | 1550116.56 tokens/sec
Step 14593 | loss: 3.091790 | lr:1.3554e-04 | norm 0.2983 | dt 338.90ms | 1547018.42 tokens/sec
Step 14594 | loss: 3.037835 | lr:1.3551e-04 | norm 0.2938 | dt 338.33ms | 1549652.30 tokens/sec
Step 14595 | loss: 3.042779 | lr:1.3547e-04 | norm 0.2945 | dt 338.64ms | 1548225.23 tokens/sec
Step 14596 | loss: 3.095824 | lr:1.3544e-04 | norm 0.2932 | dt 338.75ms | 1547706.55 tokens/sec
Step 14597 | loss: 3.148571 | lr:1.3541e-04 | norm 0.2886 | dt 338.62ms | 1548317.89 tokens/sec
Step 14598 | loss: 3.068077 | lr:1.3538e-04 | norm 0.2783 | dt 337.41ms | 1553871.39 tokens/sec
Step 14599 | loss: 3.090852 | lr:1.3535e-04 | norm 0.2886 | dt 338.19ms | 1550281.58 tokens/sec
Step 14600 | loss: 3.148222 | lr:1.3531e-04 | norm 0.2886 | dt 338.87ms | 1547159.92 tokens/sec
Step 14601 | loss: 3.106849 | lr:1.3528e-04 | norm 0.2944 | dt 338.12ms | 1550614.99 tokens/sec
Step 14602 | loss: 3.097325 | lr:1.3525e-04 | norm 0.2843 | dt 338.40ms | 1549319.30 tokens/sec
Step 14603 | loss: 3.090665 | lr:1.3522e-04 | norm 0.2931 | dt 337.82ms | 1551996.09 tokens/sec
Step 14604 | loss: 3.007153 | lr:1.3519e-04 | norm 0.2697 | dt 338.49ms | 1548892.62 tokens/sec
Step 14605 | loss: 3.115775 | lr:1.3515e-04 | norm 0.3105 | dt 337.84ms | 1551877.80 tokens/sec
Step 14606 | loss: 3.058340 | lr:1.3512e-04 | norm 0.3608 | dt 337.44ms | 1553704.51 tokens/sec
Step 14607 | loss: 3.148504 | lr:1.3509e-04 | norm 0.2964 | dt 337.78ms | 1552139.59 tokens/sec
Step 14608 | loss: 3.118512 | lr:1.3506e-04 | norm 0.3064 | dt 338.11ms | 1550634.67 tokens/sec
Step 14609 | loss: 3.082778 | lr:1.3503e-04 | norm 0.3207 | dt 337.98ms | 1551231.91 tokens/sec
Step 14610 | loss: 3.140909 | lr:1.3499e-04 | norm 0.3428 | dt 337.71ms | 1552459.56 tokens/sec
Step 14611 | loss: 3.082922 | lr:1.3496e-04 | norm 0.3277 | dt 337.70ms | 1552532.99 tokens/sec
Step 14612 | loss: 3.108163 | lr:1.3493e-04 | norm 0.2895 | dt 338.18ms | 1550335.13 tokens/sec
Step 14613 | loss: 3.101028 | lr:1.3490e-04 | norm 0.3209 | dt 337.20ms | 1554820.64 tokens/sec
Step 14614 | loss: 3.049954 | lr:1.3487e-04 | norm 0.3026 | dt 338.34ms | 1549595.52 tokens/sec
Step 14615 | loss: 3.084527 | lr:1.3483e-04 | norm 0.3020 | dt 338.57ms | 1548551.22 tokens/sec
Step 14616 | loss: 3.106186 | lr:1.3480e-04 | norm 0.3129 | dt 337.11ms | 1555238.50 tokens/sec
Step 14617 | loss: 3.137866 | lr:1.3477e-04 | norm 0.2911 | dt 338.38ms | 1549383.71 tokens/sec
Step 14618 | loss: 3.059260 | lr:1.3474e-04 | norm 0.3173 | dt 337.64ms | 1552820.22 tokens/sec
Step 14619 | loss: 3.013863 | lr:1.3471e-04 | norm 0.2658 | dt 337.93ms | 1551450.80 tokens/sec
Step 14620 | loss: 3.057880 | lr:1.3467e-04 | norm 0.3065 | dt 337.64ms | 1552785.14 tokens/sec
Step 14621 | loss: 3.019153 | lr:1.3464e-04 | norm 0.3068 | dt 337.76ms | 1552256.82 tokens/sec
Step 14622 | loss: 3.086009 | lr:1.3461e-04 | norm 0.2771 | dt 338.14ms | 1550499.10 tokens/sec
Step 14623 | loss: 3.105852 | lr:1.3458e-04 | norm 0.3210 | dt 337.91ms | 1551558.07 tokens/sec
Step 14624 | loss: 3.017461 | lr:1.3455e-04 | norm 0.2785 | dt 338.00ms | 1551138.90 tokens/sec
Step 14625 | loss: 3.050006 | lr:1.3451e-04 | norm 0.3018 | dt 337.09ms | 1555339.70 tokens/sec
Step 14626 | loss: 3.104167 | lr:1.3448e-04 | norm 0.2635 | dt 337.95ms | 1551397.16 tokens/sec
Step 14627 | loss: 3.074746 | lr:1.3445e-04 | norm 0.3002 | dt 338.48ms | 1548938.44 tokens/sec
Step 14628 | loss: 3.079969 | lr:1.3442e-04 | norm 0.2752 | dt 338.36ms | 1549485.24 tokens/sec
Step 14629 | loss: 3.004016 | lr:1.3439e-04 | norm 0.2772 | dt 927.77ms | 565104.48 tokens/sec
Step 14630 | loss: 3.033443 | lr:1.3436e-04 | norm 0.3139 | dt 336.98ms | 1555822.78 tokens/sec
Step 14631 | loss: 3.085198 | lr:1.3432e-04 | norm 0.2958 | dt 337.30ms | 1554362.35 tokens/sec
Step 14632 | loss: 3.089916 | lr:1.3429e-04 | norm 0.3197 | dt 336.79ms | 1556736.94 tokens/sec
Step 14633 | loss: 3.094144 | lr:1.3426e-04 | norm 0.3031 | dt 338.72ms | 1547838.37 tokens/sec
Step 14634 | loss: 3.015267 | lr:1.3423e-04 | norm 0.3163 | dt 338.16ms | 1550434.60 tokens/sec
Step 14635 | loss: 3.028348 | lr:1.3420e-04 | norm 0.2832 | dt 338.68ms | 1548031.23 tokens/sec
Step 14636 | loss: 3.122033 | lr:1.3416e-04 | norm 0.3223 | dt 337.67ms | 1552659.05 tokens/sec
Step 14637 | loss: 3.134064 | lr:1.3413e-04 | norm 0.3156 | dt 339.24ms | 1545490.82 tokens/sec
Step 14638 | loss: 3.054751 | lr:1.3410e-04 | norm 0.3011 | dt 338.62ms | 1548298.27 tokens/sec
Step 14639 | loss: 3.052553 | lr:1.3407e-04 | norm 0.2890 | dt 338.53ms | 1548712.63 tokens/sec
Step 14640 | loss: 3.073241 | lr:1.3404e-04 | norm 0.3001 | dt 338.53ms | 1548703.90 tokens/sec
Step 14641 | loss: 3.090254 | lr:1.3401e-04 | norm 0.3005 | dt 338.15ms | 1550476.14 tokens/sec
Step 14642 | loss: 3.117329 | lr:1.3397e-04 | norm 0.2966 | dt 339.15ms | 1545887.38 tokens/sec
Step 14643 | loss: 3.110271 | lr:1.3394e-04 | norm 0.2903 | dt 340.59ms | 1539333.98 tokens/sec
Step 14644 | loss: 3.126454 | lr:1.3391e-04 | norm 0.2935 | dt 338.45ms | 1549092.29 tokens/sec
Step 14645 | loss: 3.096825 | lr:1.3388e-04 | norm 0.2912 | dt 338.43ms | 1549177.41 tokens/sec
Step 14646 | loss: 3.104811 | lr:1.3385e-04 | norm 0.2979 | dt 338.88ms | 1547119.64 tokens/sec
Step 14647 | loss: 3.129210 | lr:1.3381e-04 | norm 0.3124 | dt 338.79ms | 1547527.93 tokens/sec
Step 14648 | loss: 3.037625 | lr:1.3378e-04 | norm 0.3907 | dt 338.31ms | 1549708.00 tokens/sec
Step 14649 | loss: 3.044334 | lr:1.3375e-04 | norm 0.3377 | dt 338.38ms | 1549390.26 tokens/sec
Step 14650 | loss: 3.122525 | lr:1.3372e-04 | norm 0.3199 | dt 338.24ms | 1550035.71 tokens/sec
Step 14651 | loss: 3.056292 | lr:1.3369e-04 | norm 0.3053 | dt 339.22ms | 1545565.77 tokens/sec
Step 14652 | loss: 3.100160 | lr:1.3366e-04 | norm 0.3225 | dt 338.91ms | 1546987.95 tokens/sec
Step 14653 | loss: 3.055233 | lr:1.3362e-04 | norm 0.3071 | dt 338.05ms | 1550903.70 tokens/sec
Step 14654 | loss: 2.958580 | lr:1.3359e-04 | norm 0.3604 | dt 338.58ms | 1548475.98 tokens/sec
Step 14655 | loss: 3.047586 | lr:1.3356e-04 | norm 0.3048 | dt 338.61ms | 1548333.15 tokens/sec
Step 14656 | loss: 3.021658 | lr:1.3353e-04 | norm 0.2993 | dt 338.56ms | 1548573.03 tokens/sec
Step 14657 | loss: 3.074444 | lr:1.3350e-04 | norm 0.2984 | dt 338.67ms | 1548100.98 tokens/sec
Step 14658 | loss: 3.055403 | lr:1.3347e-04 | norm 0.2959 | dt 338.78ms | 1547554.06 tokens/sec
Step 14659 | loss: 3.060395 | lr:1.3343e-04 | norm 0.2944 | dt 338.42ms | 1549245.08 tokens/sec
Step 14660 | loss: 3.068941 | lr:1.3340e-04 | norm 0.3027 | dt 338.72ms | 1547835.10 tokens/sec
Step 14661 | loss: 3.007477 | lr:1.3337e-04 | norm 0.3019 | dt 338.14ms | 1550500.19 tokens/sec
Step 14662 | loss: 3.095901 | lr:1.3334e-04 | norm 0.3171 | dt 337.90ms | 1551629.23 tokens/sec
Step 14663 | loss: 3.104007 | lr:1.3331e-04 | norm 0.2932 | dt 337.63ms | 1552831.19 tokens/sec
Step 14664 | loss: 3.121824 | lr:1.3328e-04 | norm 0.2975 | dt 338.26ms | 1549953.77 tokens/sec
Step 14665 | loss: 3.098213 | lr:1.3324e-04 | norm 0.3019 | dt 338.26ms | 1549961.41 tokens/sec
Step 14666 | loss: 3.089311 | lr:1.3321e-04 | norm 0.3234 | dt 337.89ms | 1551653.32 tokens/sec
Step 14667 | loss: 3.077871 | lr:1.3318e-04 | norm 0.2940 | dt 337.90ms | 1551600.77 tokens/sec
Step 14668 | loss: 3.118529 | lr:1.3315e-04 | norm 0.3124 | dt 338.05ms | 1550921.20 tokens/sec
Step 14669 | loss: 3.086880 | lr:1.3312e-04 | norm 0.2864 | dt 337.68ms | 1552608.63 tokens/sec
Step 14670 | loss: 3.027027 | lr:1.3309e-04 | norm 0.3073 | dt 338.26ms | 1549938.47 tokens/sec
Step 14671 | loss: 3.093046 | lr:1.3305e-04 | norm 0.2875 | dt 338.23ms | 1550106.73 tokens/sec
Step 14672 | loss: 3.102004 | lr:1.3302e-04 | norm 0.3069 | dt 338.00ms | 1551147.66 tokens/sec
Step 14673 | loss: 3.083398 | lr:1.3299e-04 | norm 0.2918 | dt 338.27ms | 1549931.92 tokens/sec
Step 14674 | loss: 3.036995 | lr:1.3296e-04 | norm 0.3037 | dt 338.47ms | 1549009.36 tokens/sec
Step 14675 | loss: 3.144505 | lr:1.3293e-04 | norm 0.3014 | dt 337.44ms | 1553742.94 tokens/sec
Step 14676 | loss: 3.032294 | lr:1.3290e-04 | norm 0.3077 | dt 338.19ms | 1550256.44 tokens/sec
Step 14677 | loss: 3.079848 | lr:1.3286e-04 | norm 0.3195 | dt 337.71ms | 1552489.15 tokens/sec
Step 14678 | loss: 3.055642 | lr:1.3283e-04 | norm 0.2715 | dt 337.50ms | 1553426.83 tokens/sec
Step 14679 | loss: 3.102059 | lr:1.3280e-04 | norm 0.3001 | dt 338.23ms | 1550112.19 tokens/sec
Step 14680 | loss: 3.077781 | lr:1.3277e-04 | norm 0.2900 | dt 338.14ms | 1550496.91 tokens/sec
Step 14681 | loss: 3.082285 | lr:1.3274e-04 | norm 0.2728 | dt 337.37ms | 1554035.01 tokens/sec
Step 14682 | loss: 3.051288 | lr:1.3271e-04 | norm 0.3146 | dt 338.20ms | 1550232.40 tokens/sec
Step 14683 | loss: 3.107572 | lr:1.3268e-04 | norm 0.2747 | dt 337.11ms | 1555228.60 tokens/sec
Step 14684 | loss: 3.098552 | lr:1.3264e-04 | norm 0.2794 | dt 337.48ms | 1553528.89 tokens/sec
Step 14685 | loss: 3.062985 | lr:1.3261e-04 | norm 0.2633 | dt 338.07ms | 1550811.83 tokens/sec
Step 14686 | loss: 3.060716 | lr:1.3258e-04 | norm 0.2761 | dt 337.23ms | 1554671.14 tokens/sec
Step 14687 | loss: 3.158692 | lr:1.3255e-04 | norm 0.2661 | dt 337.70ms | 1552507.78 tokens/sec
Step 14688 | loss: 3.104336 | lr:1.3252e-04 | norm 0.2779 | dt 338.64ms | 1548199.07 tokens/sec
Step 14689 | loss: 3.038903 | lr:1.3249e-04 | norm 0.2676 | dt 337.79ms | 1552118.78 tokens/sec
Step 14690 | loss: 3.030288 | lr:1.3246e-04 | norm 0.2860 | dt 337.36ms | 1554082.24 tokens/sec
Step 14691 | loss: 3.058555 | lr:1.3242e-04 | norm 0.2809 | dt 338.09ms | 1550745.12 tokens/sec
Step 14692 | loss: 3.042343 | lr:1.3239e-04 | norm 0.2785 | dt 338.07ms | 1550843.54 tokens/sec
Step 14693 | loss: 3.033077 | lr:1.3236e-04 | norm 0.2869 | dt 337.62ms | 1552899.18 tokens/sec
Step 14694 | loss: 3.135947 | lr:1.3233e-04 | norm 0.2846 | dt 337.48ms | 1553546.45 tokens/sec
Step 14695 | loss: 3.116946 | lr:1.3230e-04 | norm 0.3236 | dt 338.56ms | 1548575.21 tokens/sec
Step 14696 | loss: 3.100039 | lr:1.3227e-04 | norm 0.3064 | dt 337.56ms | 1553147.05 tokens/sec
Step 14697 | loss: 3.082453 | lr:1.3223e-04 | norm 0.2995 | dt 337.36ms | 1554088.83 tokens/sec
Step 14698 | loss: 3.094223 | lr:1.3220e-04 | norm 0.3074 | dt 337.76ms | 1552245.87 tokens/sec
Step 14699 | loss: 3.062131 | lr:1.3217e-04 | norm 0.2912 | dt 338.06ms | 1550863.23 tokens/sec
Step 14700 | loss: 3.090691 | lr:1.3214e-04 | norm 0.3118 | dt 337.23ms | 1554689.83 tokens/sec
Step 14701 | loss: 3.093223 | lr:1.3211e-04 | norm 0.3505 | dt 338.42ms | 1549213.43 tokens/sec
Step 14702 | loss: 3.089522 | lr:1.3208e-04 | norm 0.3237 | dt 339.39ms | 1544773.19 tokens/sec
Step 14703 | loss: 3.065759 | lr:1.3205e-04 | norm 0.3335 | dt 338.14ms | 1550523.15 tokens/sec
Step 14704 | loss: 3.064728 | lr:1.3201e-04 | norm 0.3050 | dt 337.14ms | 1555083.43 tokens/sec
Step 14705 | loss: 3.063254 | lr:1.3198e-04 | norm 0.2986 | dt 338.54ms | 1548665.73 tokens/sec
Step 14706 | loss: 3.056769 | lr:1.3195e-04 | norm 0.2923 | dt 338.07ms | 1550821.67 tokens/sec
Step 14707 | loss: 3.085136 | lr:1.3192e-04 | norm 0.3160 | dt 337.42ms | 1553816.49 tokens/sec
Step 14708 | loss: 3.026725 | lr:1.3189e-04 | norm 0.3023 | dt 337.86ms | 1551800.04 tokens/sec
Step 14709 | loss: 3.088160 | lr:1.3186e-04 | norm 0.2831 | dt 338.32ms | 1549668.69 tokens/sec
Step 14710 | loss: 3.037612 | lr:1.3183e-04 | norm 0.3044 | dt 338.45ms | 1549105.39 tokens/sec
Step 14711 | loss: 3.061184 | lr:1.3179e-04 | norm 0.2782 | dt 337.94ms | 1551444.23 tokens/sec
Step 14712 | loss: 3.084229 | lr:1.3176e-04 | norm 0.2954 | dt 338.19ms | 1550268.46 tokens/sec
Step 14713 | loss: 3.083930 | lr:1.3173e-04 | norm 0.3049 | dt 337.82ms | 1551982.94 tokens/sec
Step 14714 | loss: 3.068740 | lr:1.3170e-04 | norm 0.2837 | dt 337.57ms | 1553130.60 tokens/sec
Step 14715 | loss: 3.115492 | lr:1.3167e-04 | norm 0.2864 | dt 338.25ms | 1550020.41 tokens/sec
Step 14716 | loss: 3.148258 | lr:1.3164e-04 | norm 0.3045 | dt 337.72ms | 1552444.21 tokens/sec
Step 14717 | loss: 3.072376 | lr:1.3161e-04 | norm 0.2649 | dt 337.67ms | 1552665.63 tokens/sec
Step 14718 | loss: 3.100173 | lr:1.3158e-04 | norm 0.2886 | dt 337.88ms | 1551701.49 tokens/sec
Step 14719 | loss: 3.128417 | lr:1.3154e-04 | norm 0.3233 | dt 337.80ms | 1552084.82 tokens/sec
Step 14720 | loss: 3.107455 | lr:1.3151e-04 | norm 0.2844 | dt 338.57ms | 1548533.77 tokens/sec
Step 14721 | loss: 3.114436 | lr:1.3148e-04 | norm 0.3034 | dt 337.88ms | 1551706.97 tokens/sec
Step 14722 | loss: 3.096848 | lr:1.3145e-04 | norm 0.3119 | dt 338.03ms | 1551004.34 tokens/sec
Step 14723 | loss: 3.057845 | lr:1.3142e-04 | norm 0.2903 | dt 338.13ms | 1550551.58 tokens/sec
Step 14724 | loss: 3.090662 | lr:1.3139e-04 | norm 0.3101 | dt 337.93ms | 1551477.07 tokens/sec
Step 14725 | loss: 3.062169 | lr:1.3136e-04 | norm 0.2837 | dt 337.48ms | 1553547.55 tokens/sec
Step 14726 | loss: 3.027821 | lr:1.3132e-04 | norm 0.2886 | dt 338.37ms | 1549429.56 tokens/sec
Step 14727 | loss: 3.063585 | lr:1.3129e-04 | norm 0.2915 | dt 337.76ms | 1552257.92 tokens/sec
Step 14728 | loss: 3.013750 | lr:1.3126e-04 | norm 0.2773 | dt 338.01ms | 1551106.08 tokens/sec
Step 14729 | loss: 3.109871 | lr:1.3123e-04 | norm 0.3057 | dt 337.56ms | 1553171.19 tokens/sec
Step 14730 | loss: 3.064807 | lr:1.3120e-04 | norm 0.3008 | dt 338.13ms | 1550557.04 tokens/sec
Step 14731 | loss: 3.091003 | lr:1.3117e-04 | norm 0.3386 | dt 337.92ms | 1551532.89 tokens/sec
Step 14732 | loss: 2.991158 | lr:1.3114e-04 | norm 0.2663 | dt 337.72ms | 1552434.35 tokens/sec
Step 14733 | loss: 3.073695 | lr:1.3111e-04 | norm 0.3163 | dt 338.85ms | 1547260.07 tokens/sec
Step 14734 | loss: 3.041305 | lr:1.3107e-04 | norm 0.2953 | dt 337.77ms | 1552187.80 tokens/sec
Step 14735 | loss: 3.064822 | lr:1.3104e-04 | norm 0.3230 | dt 337.47ms | 1553565.11 tokens/sec
Step 14736 | loss: 3.100634 | lr:1.3101e-04 | norm 0.3220 | dt 338.29ms | 1549835.79 tokens/sec
Step 14737 | loss: 3.099060 | lr:1.3098e-04 | norm 0.2879 | dt 338.04ms | 1550961.67 tokens/sec
Step 14738 | loss: 3.134998 | lr:1.3095e-04 | norm 0.3223 | dt 337.82ms | 1551978.56 tokens/sec
Step 14739 | loss: 3.067774 | lr:1.3092e-04 | norm 0.2814 | dt 337.48ms | 1553543.16 tokens/sec
Step 14740 | loss: 3.058606 | lr:1.3089e-04 | norm 0.2966 | dt 337.79ms | 1552097.96 tokens/sec
Step 14741 | loss: 3.057602 | lr:1.3086e-04 | norm 0.2898 | dt 905.23ms | 579177.60 tokens/sec
Step 14742 | loss: 3.086254 | lr:1.3083e-04 | norm 0.2911 | dt 335.33ms | 1563500.82 tokens/sec
Step 14743 | loss: 3.099253 | lr:1.3079e-04 | norm 0.2857 | dt 337.50ms | 1553434.51 tokens/sec
Step 14744 | loss: 3.150936 | lr:1.3076e-04 | norm 0.2997 | dt 339.00ms | 1546567.98 tokens/sec
Step 14745 | loss: 3.091327 | lr:1.3073e-04 | norm 0.2867 | dt 337.98ms | 1551251.61 tokens/sec
Step 14746 | loss: 3.101670 | lr:1.3070e-04 | norm 0.3124 | dt 337.40ms | 1553886.76 tokens/sec
Step 14747 | loss: 3.088022 | lr:1.3067e-04 | norm 0.2902 | dt 337.79ms | 1552090.29 tokens/sec
Step 14748 | loss: 3.089759 | lr:1.3064e-04 | norm 0.3080 | dt 338.15ms | 1550471.77 tokens/sec
Step 14749 | loss: 3.063990 | lr:1.3061e-04 | norm 0.2820 | dt 338.40ms | 1549300.75 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 14750: 3.1157
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3032/10042=0.3019


ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 2: ####### Printing generated samples ####### 



ddp_rank 5: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I thought I could get a copy done. Now, I'm thinking about...
Well, this isn't very
rank 2 sample 0 >Hello, I'm a language model, let's take a look at some of my favourite books:
- Kinojoke: The Language is a Fun
rank 7 sample 1 >Hello, I'm a language model, anyway, it's my idea! As we learned in Spanish, most of the data we need to analyze comes from English
rank 2 sample 1 >Hello, I'm a language model, and I believe you're right. Here's a quiz from a post we wrote:
"The way you look at
rank 5 sample 0 >Hello, I'm a language model, but a computer is so much more than the sum of the parts you need to understand.
You can use language models
rank 7 sample 2 >Hello, I'm a language model, so I'll leave it to the expert for a word search. The one that I'll be using is a text search
rank 2 sample 2 >Hello, I'm a language model, and I'll give you a quick and simple introduction to the core syntax of Java, how it interacts with the compiler and
rank 5 sample 1 >Hello, I'm a language model, right? Can anyone explain this with an example?
The main component of this example is a command named p -l
rank 7 sample 3 >Hello, I'm a language model, and you need to understand me. To say how I'm going, my language model is not one that I could type


rank 2 sample 3 >Hello, I'm a language model, but not having a grasp of the concepts to explain some of their concepts. There are many definitions.
For instance,


rank 5 sample 2 >Hello, I'm a language model, and you know what would be helpful for me if I could be part of the project at the beginning of the year to
rank 5 sample 3 >Hello, I'm a language model, an advocate, and a scientist, and my aim is to bring the language metaphor as far as possible. I want to




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm gonna make the first model on the second layer, the user who created it on the second layer, to
rank 3 sample 1 >Hello, I'm a language model, so you have to be mindful, with respect to the structure of your code.
That's not the intent and we
rank 3 sample 2 >Hello, I'm a language model, so this question is relevant to another one. Please describe in what way this model affects the syntax-based behavior used in
rank 3 sample 3 >Hello, I'm a language model, so to speak. I guess what's going on in a sentence. Let's do one by one without using a word




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I am a machine. I am really a linguist, and you say that because languages are languages of humans.


ddp_rank 1: ####### Printing generated samples ####### 

rank 4 sample 1 >Hello, I'm a language model, is this a non-fiction essay, can I ever add a "language" (that is, no) which would
rank 4 sample 2 >Hello, I'm a language model, I started having two questions when I heard a speech therapist talk about my son's school language. The kids are from the
rank 1 sample 0 >Hello, I'm a language model, so how can I create a language diagram for that? So as I'm modeling a database, the first thing I do
rank 4 sample 3 >Hello, I'm a language model, so what good did you learn? Where can I ask you help me figure this out?<|endoftext|>Cultural and Philosoph


rank 1 sample 1 >Hello, I'm a language model, a person, and a person that we speak. But they're people, people, it's fun to be a person
rank 1 sample 2 >Hello, I'm a language model, but is this something that I'm not going to describe to readers of my blog?
The main reason for my mistake
rank 1 sample 3 >Hello, I'm a language model, so I'm wondering how well you understood you.
One of the key lessons in developing deep learning is that your learning




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, i can't be bothered yet, because I'm so good. Now I am feeling very unhappy.
So, we
rank 6 sample 1 >Hello, I'm a language model, so I want to help my readers understand each one so that they can see how they're using the same word.

rank 6 sample 2 >Hello, I'm a language model, but how does the syntax differ for you? Let's take the function f2 as an example.
import Function(
rank 6 sample 3 >Hello, I'm a language model, so here you're trying to create a virtual object.
I created a text object in Tkls by entering Unicode




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know what you're trying to work with. I have tried. You can find great articles and information on it
rank 0 sample 1 >Hello, I'm a language model, and am planning to do a few workshops with you around the country. Maybe, I'm the only one to have any
rank 0 sample 2 >Hello, I'm a language model, and I wrote the last line of my code (like this code in Python):
import numpy as np.util
rank 0 sample 3 >Hello, I'm a language model, and am pretty sure that's how it's formed.
Cars were the favorite game of the first and most renowned


Step 14750 | loss: 3.149868 | lr:1.3058e-04 | norm 0.3256 | dt 12388.01ms | 42322.22 tokens/sec
Step 14751 | loss: 3.119774 | lr:1.3054e-04 | norm 0.2912 | dt 336.62ms | 1557525.31 tokens/sec
Step 14752 | loss: 3.083227 | lr:1.3051e-04 | norm 0.2739 | dt 336.65ms | 1557363.16 tokens/sec
Step 14753 | loss: 3.096823 | lr:1.3048e-04 | norm 0.2994 | dt 336.08ms | 1560022.46 tokens/sec
Step 14754 | loss: 3.025523 | lr:1.3045e-04 | norm 0.2736 | dt 337.40ms | 1553908.72 tokens/sec
Step 14755 | loss: 3.066575 | lr:1.3042e-04 | norm 0.2787 | dt 337.05ms | 1555515.73 tokens/sec
Step 14756 | loss: 3.050452 | lr:1.3039e-04 | norm 0.2798 | dt 337.49ms | 1553510.24 tokens/sec
Step 14757 | loss: 3.070443 | lr:1.3036e-04 | norm 0.2747 | dt 336.95ms | 1555989.01 tokens/sec
Step 14758 | loss: 3.171044 | lr:1.3033e-04 | norm 0.3673 | dt 337.77ms | 1552210.80 tokens/sec
Step 14759 | loss: 3.003821 | lr:1.3030e-04 | norm 0.2898 | dt 336.46ms | 1558239.38 tokens/sec
Step 14760 | loss: 3.065004 | lr:1.3026e-04 | norm 0.3000 | dt 337.01ms | 1555710.51 tokens/sec
Step 14761 | loss: 3.031175 | lr:1.3023e-04 | norm 0.2847 | dt 337.25ms | 1554615.09 tokens/sec
Step 14762 | loss: 3.039665 | lr:1.3020e-04 | norm 0.2796 | dt 337.23ms | 1554674.44 tokens/sec
Step 14763 | loss: 3.034878 | lr:1.3017e-04 | norm 0.2817 | dt 337.34ms | 1554172.30 tokens/sec
Step 14764 | loss: 3.080811 | lr:1.3014e-04 | norm 0.2738 | dt 336.71ms | 1557094.09 tokens/sec
Step 14765 | loss: 3.094388 | lr:1.3011e-04 | norm 0.2809 | dt 337.71ms | 1552497.92 tokens/sec
Step 14766 | loss: 3.055254 | lr:1.3008e-04 | norm 0.2704 | dt 337.41ms | 1553872.49 tokens/sec
Step 14767 | loss: 3.029341 | lr:1.3005e-04 | norm 0.2894 | dt 337.33ms | 1554237.11 tokens/sec
Step 14768 | loss: 3.069984 | lr:1.3002e-04 | norm 0.2909 | dt 338.10ms | 1550668.57 tokens/sec
Step 14769 | loss: 3.086837 | lr:1.2998e-04 | norm 0.2980 | dt 338.93ms | 1546895.45 tokens/sec
Step 14770 | loss: 3.059172 | lr:1.2995e-04 | norm 0.3311 | dt 337.27ms | 1554518.38 tokens/sec
Step 14771 | loss: 3.116577 | lr:1.2992e-04 | norm 0.2967 | dt 337.91ms | 1551572.30 tokens/sec
Step 14772 | loss: 3.157161 | lr:1.2989e-04 | norm 0.3234 | dt 338.23ms | 1550102.36 tokens/sec
Step 14773 | loss: 3.192451 | lr:1.2986e-04 | norm 0.3158 | dt 338.41ms | 1549285.47 tokens/sec
Step 14774 | loss: 3.054384 | lr:1.2983e-04 | norm 0.2982 | dt 337.83ms | 1551918.32 tokens/sec
Step 14775 | loss: 3.091048 | lr:1.2980e-04 | norm 0.2906 | dt 337.73ms | 1552371.88 tokens/sec
Step 14776 | loss: 3.120866 | lr:1.2977e-04 | norm 0.3055 | dt 337.68ms | 1552625.07 tokens/sec
Step 14777 | loss: 3.103060 | lr:1.2974e-04 | norm 0.3289 | dt 337.72ms | 1552442.02 tokens/sec
Step 14778 | loss: 3.096321 | lr:1.2971e-04 | norm 0.2857 | dt 338.53ms | 1548698.45 tokens/sec
Step 14779 | loss: 3.091184 | lr:1.2967e-04 | norm 0.3025 | dt 338.22ms | 1550140.60 tokens/sec
Step 14780 | loss: 3.058336 | lr:1.2964e-04 | norm 0.3046 | dt 338.10ms | 1550705.75 tokens/sec
Step 14781 | loss: 3.096057 | lr:1.2961e-04 | norm 0.2840 | dt 338.56ms | 1548570.85 tokens/sec
Step 14782 | loss: 3.051415 | lr:1.2958e-04 | norm 0.3104 | dt 338.78ms | 1547562.78 tokens/sec
Step 14783 | loss: 3.067706 | lr:1.2955e-04 | norm 0.3207 | dt 338.51ms | 1548794.44 tokens/sec
Step 14784 | loss: 3.071012 | lr:1.2952e-04 | norm 0.2778 | dt 338.44ms | 1549128.30 tokens/sec
Step 14785 | loss: 3.144532 | lr:1.2949e-04 | norm 0.3036 | dt 338.29ms | 1549803.02 tokens/sec
Step 14786 | loss: 3.095828 | lr:1.2946e-04 | norm 0.3110 | dt 338.91ms | 1546987.95 tokens/sec
Step 14787 | loss: 3.018899 | lr:1.2943e-04 | norm 0.3055 | dt 337.40ms | 1553905.43 tokens/sec
Step 14788 | loss: 3.065139 | lr:1.2940e-04 | norm 0.3179 | dt 337.88ms | 1551680.69 tokens/sec
Step 14789 | loss: 3.088604 | lr:1.2937e-04 | norm 0.2778 | dt 339.31ms | 1545137.89 tokens/sec
Step 14790 | loss: 3.100261 | lr:1.2933e-04 | norm 0.3037 | dt 338.81ms | 1547419.03 tokens/sec
Step 14791 | loss: 3.059204 | lr:1.2930e-04 | norm 0.2809 | dt 338.27ms | 1549928.64 tokens/sec
Step 14792 | loss: 3.020007 | lr:1.2927e-04 | norm 0.2867 | dt 337.72ms | 1552445.31 tokens/sec
Step 14793 | loss: 3.116323 | lr:1.2924e-04 | norm 0.3029 | dt 338.13ms | 1550560.32 tokens/sec
Step 14794 | loss: 3.080547 | lr:1.2921e-04 | norm 0.2734 | dt 337.94ms | 1551402.64 tokens/sec
Step 14795 | loss: 3.095650 | lr:1.2918e-04 | norm 0.2755 | dt 337.76ms | 1552251.34 tokens/sec
Step 14796 | loss: 3.050876 | lr:1.2915e-04 | norm 0.3000 | dt 338.29ms | 1549834.70 tokens/sec
Step 14797 | loss: 3.046737 | lr:1.2912e-04 | norm 0.2732 | dt 337.72ms | 1552416.81 tokens/sec
Step 14798 | loss: 3.011711 | lr:1.2909e-04 | norm 0.2926 | dt 338.46ms | 1549017.00 tokens/sec
Step 14799 | loss: 3.116793 | lr:1.2906e-04 | norm 0.2779 | dt 337.73ms | 1552385.03 tokens/sec
Step 14800 | loss: 3.020378 | lr:1.2903e-04 | norm 0.2679 | dt 338.51ms | 1548799.89 tokens/sec
Step 14801 | loss: 3.045501 | lr:1.2899e-04 | norm 0.2714 | dt 338.41ms | 1549248.36 tokens/sec
Step 14802 | loss: 3.113459 | lr:1.2896e-04 | norm 0.3010 | dt 338.66ms | 1548144.57 tokens/sec
Step 14803 | loss: 3.133045 | lr:1.2893e-04 | norm 0.3231 | dt 338.16ms | 1550398.53 tokens/sec
Step 14804 | loss: 3.056359 | lr:1.2890e-04 | norm 0.3430 | dt 338.23ms | 1550084.87 tokens/sec
Step 14805 | loss: 3.111519 | lr:1.2887e-04 | norm 0.3068 | dt 339.09ms | 1546161.29 tokens/sec
Step 14806 | loss: 3.023897 | lr:1.2884e-04 | norm 0.3101 | dt 338.20ms | 1550237.86 tokens/sec
Step 14807 | loss: 3.027314 | lr:1.2881e-04 | norm 0.3317 | dt 337.97ms | 1551272.40 tokens/sec
Step 14808 | loss: 3.057509 | lr:1.2878e-04 | norm 0.2899 | dt 337.69ms | 1552559.30 tokens/sec
Step 14809 | loss: 3.065898 | lr:1.2875e-04 | norm 0.2945 | dt 338.48ms | 1548930.80 tokens/sec
Step 14810 | loss: 3.049160 | lr:1.2872e-04 | norm 0.2954 | dt 339.30ms | 1545209.55 tokens/sec
Step 14811 | loss: 3.083503 | lr:1.2869e-04 | norm 0.2981 | dt 338.26ms | 1549961.41 tokens/sec
Step 14812 | loss: 3.130342 | lr:1.2866e-04 | norm 0.2964 | dt 338.76ms | 1547665.16 tokens/sec
Step 14813 | loss: 3.088486 | lr:1.2862e-04 | norm 0.2894 | dt 338.98ms | 1546655.00 tokens/sec
Step 14814 | loss: 3.107768 | lr:1.2859e-04 | norm 0.2890 | dt 338.21ms | 1550207.26 tokens/sec
Step 14815 | loss: 3.072163 | lr:1.2856e-04 | norm 0.2721 | dt 339.11ms | 1546056.93 tokens/sec
Step 14816 | loss: 3.080755 | lr:1.2853e-04 | norm 0.2810 | dt 339.17ms | 1545787.41 tokens/sec
Step 14817 | loss: 3.028324 | lr:1.2850e-04 | norm 0.2750 | dt 338.72ms | 1547847.08 tokens/sec
Step 14818 | loss: 3.151727 | lr:1.2847e-04 | norm 0.3266 | dt 338.81ms | 1547451.70 tokens/sec
Step 14819 | loss: 3.061197 | lr:1.2844e-04 | norm 0.3212 | dt 1042.01ms | 503150.26 tokens/sec
Step 14820 | loss: 2.770212 | lr:1.2841e-04 | norm 0.2820 | dt 338.58ms | 1548510.87 tokens/sec
Step 14821 | loss: 3.087944 | lr:1.2838e-04 | norm 0.3018 | dt 338.56ms | 1548602.47 tokens/sec
Step 14822 | loss: 3.138390 | lr:1.2835e-04 | norm 0.3011 | dt 338.37ms | 1549441.57 tokens/sec
Step 14823 | loss: 3.085393 | lr:1.2832e-04 | norm 0.3073 | dt 338.27ms | 1549888.22 tokens/sec
Step 14824 | loss: 3.079998 | lr:1.2829e-04 | norm 0.2873 | dt 339.13ms | 1545956.94 tokens/sec
Step 14825 | loss: 3.107824 | lr:1.2826e-04 | norm 0.2900 | dt 337.78ms | 1552143.97 tokens/sec
Step 14826 | loss: 3.073181 | lr:1.2823e-04 | norm 0.2909 | dt 338.05ms | 1550903.70 tokens/sec
Step 14827 | loss: 3.117919 | lr:1.2819e-04 | norm 0.2808 | dt 339.53ms | 1544146.21 tokens/sec
Step 14828 | loss: 3.077649 | lr:1.2816e-04 | norm 0.3190 | dt 339.12ms | 1546034.11 tokens/sec
Step 14829 | loss: 3.063775 | lr:1.2813e-04 | norm 0.2707 | dt 337.78ms | 1552137.40 tokens/sec
Step 14830 | loss: 2.998050 | lr:1.2810e-04 | norm 0.3207 | dt 338.42ms | 1549243.99 tokens/sec
Step 14831 | loss: 3.060038 | lr:1.2807e-04 | norm 0.2717 | dt 337.61ms | 1552927.69 tokens/sec
Step 14832 | loss: 3.081736 | lr:1.2804e-04 | norm 0.2834 | dt 338.26ms | 1549935.19 tokens/sec
Step 14833 | loss: 2.982657 | lr:1.2801e-04 | norm 0.2681 | dt 337.92ms | 1551494.58 tokens/sec
Step 14834 | loss: 3.054154 | lr:1.2798e-04 | norm 0.2712 | dt 337.86ms | 1551812.09 tokens/sec
Step 14835 | loss: 3.039310 | lr:1.2795e-04 | norm 0.3344 | dt 339.14ms | 1545925.42 tokens/sec
Step 14836 | loss: 3.010921 | lr:1.2792e-04 | norm 0.2960 | dt 339.15ms | 1545890.64 tokens/sec
Step 14837 | loss: 3.058610 | lr:1.2789e-04 | norm 0.2946 | dt 338.82ms | 1547404.87 tokens/sec
Step 14838 | loss: 3.021533 | lr:1.2786e-04 | norm 0.2774 | dt 338.38ms | 1549400.08 tokens/sec
Step 14839 | loss: 3.083430 | lr:1.2783e-04 | norm 0.2751 | dt 338.77ms | 1547639.02 tokens/sec
Step 14840 | loss: 3.065095 | lr:1.2780e-04 | norm 0.2902 | dt 338.68ms | 1548024.69 tokens/sec
Step 14841 | loss: 3.040474 | lr:1.2777e-04 | norm 0.2842 | dt 338.92ms | 1546953.12 tokens/sec
Step 14842 | loss: 3.007224 | lr:1.2773e-04 | norm 0.2724 | dt 338.64ms | 1548197.98 tokens/sec
Step 14843 | loss: 3.164434 | lr:1.2770e-04 | norm 0.3234 | dt 338.32ms | 1549686.16 tokens/sec
Step 14844 | loss: 3.069087 | lr:1.2767e-04 | norm 0.3048 | dt 337.78ms | 1552152.74 tokens/sec
Step 14845 | loss: 3.093165 | lr:1.2764e-04 | norm 0.2848 | dt 339.38ms | 1544831.79 tokens/sec
Step 14846 | loss: 3.069030 | lr:1.2761e-04 | norm 0.2983 | dt 339.00ms | 1546560.37 tokens/sec
Step 14847 | loss: 3.122640 | lr:1.2758e-04 | norm 0.3142 | dt 338.22ms | 1550149.34 tokens/sec
Step 14848 | loss: 3.117874 | lr:1.2755e-04 | norm 0.2946 | dt 338.17ms | 1550348.25 tokens/sec
Step 14849 | loss: 3.102763 | lr:1.2752e-04 | norm 0.2939 | dt 338.69ms | 1547987.64 tokens/sec
Step 14850 | loss: 3.150390 | lr:1.2749e-04 | norm 0.2930 | dt 338.87ms | 1547144.68 tokens/sec
Step 14851 | loss: 3.080605 | lr:1.2746e-04 | norm 0.3288 | dt 337.98ms | 1551233.01 tokens/sec
Step 14852 | loss: 3.123025 | lr:1.2743e-04 | norm 0.3025 | dt 337.88ms | 1551677.41 tokens/sec
Step 14853 | loss: 3.152926 | lr:1.2740e-04 | norm 0.3434 | dt 337.85ms | 1551826.33 tokens/sec
Step 14854 | loss: 3.098900 | lr:1.2737e-04 | norm 0.2906 | dt 337.85ms | 1551852.61 tokens/sec
Step 14855 | loss: 3.072152 | lr:1.2734e-04 | norm 0.3011 | dt 337.28ms | 1554446.95 tokens/sec
Step 14856 | loss: 3.094271 | lr:1.2731e-04 | norm 0.2988 | dt 337.75ms | 1552295.17 tokens/sec
Step 14857 | loss: 3.086690 | lr:1.2728e-04 | norm 0.2782 | dt 337.95ms | 1551363.23 tokens/sec
Step 14858 | loss: 3.057064 | lr:1.2725e-04 | norm 0.2894 | dt 337.55ms | 1553194.22 tokens/sec
Step 14859 | loss: 3.060432 | lr:1.2722e-04 | norm 0.2838 | dt 338.45ms | 1549074.83 tokens/sec
Step 14860 | loss: 3.084464 | lr:1.2718e-04 | norm 0.2786 | dt 338.19ms | 1550291.41 tokens/sec
Step 14861 | loss: 3.128733 | lr:1.2715e-04 | norm 0.3113 | dt 337.35ms | 1554149.23 tokens/sec
Step 14862 | loss: 3.041123 | lr:1.2712e-04 | norm 0.2851 | dt 338.91ms | 1546964.00 tokens/sec
Step 14863 | loss: 3.057986 | lr:1.2709e-04 | norm 0.3097 | dt 338.18ms | 1550329.67 tokens/sec
Step 14864 | loss: 3.065336 | lr:1.2706e-04 | norm 0.2972 | dt 338.14ms | 1550487.07 tokens/sec
Step 14865 | loss: 3.123484 | lr:1.2703e-04 | norm 0.2982 | dt 337.98ms | 1551239.57 tokens/sec
Step 14866 | loss: 3.030752 | lr:1.2700e-04 | norm 0.2798 | dt 338.94ms | 1546843.22 tokens/sec
Step 14867 | loss: 3.056211 | lr:1.2697e-04 | norm 0.2714 | dt 338.29ms | 1549813.94 tokens/sec
Step 14868 | loss: 2.994957 | lr:1.2694e-04 | norm 0.3033 | dt 337.99ms | 1551192.52 tokens/sec
Step 14869 | loss: 3.040073 | lr:1.2691e-04 | norm 0.2861 | dt 337.66ms | 1552724.83 tokens/sec
Step 14870 | loss: 3.018440 | lr:1.2688e-04 | norm 0.2720 | dt 338.24ms | 1550059.74 tokens/sec
Step 14871 | loss: 3.080538 | lr:1.2685e-04 | norm 0.2867 | dt 338.32ms | 1549679.61 tokens/sec
Step 14872 | loss: 3.046782 | lr:1.2682e-04 | norm 0.2760 | dt 337.39ms | 1553952.65 tokens/sec
Step 14873 | loss: 3.106613 | lr:1.2679e-04 | norm 0.2934 | dt 338.44ms | 1549116.30 tokens/sec
Step 14874 | loss: 3.029081 | lr:1.2676e-04 | norm 0.2697 | dt 339.38ms | 1544842.64 tokens/sec
Step 14875 | loss: 3.133634 | lr:1.2673e-04 | norm 0.2998 | dt 337.66ms | 1552723.74 tokens/sec
Step 14876 | loss: 3.109004 | lr:1.2670e-04 | norm 0.2961 | dt 338.83ms | 1547337.37 tokens/sec
Step 14877 | loss: 3.095636 | lr:1.2667e-04 | norm 0.2829 | dt 337.59ms | 1553041.75 tokens/sec
Step 14878 | loss: 3.069518 | lr:1.2664e-04 | norm 0.3038 | dt 338.61ms | 1548361.50 tokens/sec
Step 14879 | loss: 3.066237 | lr:1.2661e-04 | norm 0.2999 | dt 339.23ms | 1545501.69 tokens/sec
Step 14880 | loss: 3.056520 | lr:1.2658e-04 | norm 0.2742 | dt 338.05ms | 1550915.73 tokens/sec
Step 14881 | loss: 3.002557 | lr:1.2655e-04 | norm 0.2975 | dt 337.77ms | 1552187.80 tokens/sec
Step 14882 | loss: 3.028525 | lr:1.2652e-04 | norm 0.2624 | dt 337.72ms | 1552420.10 tokens/sec
Step 14883 | loss: 3.092174 | lr:1.2648e-04 | norm 0.2724 | dt 338.33ms | 1549650.12 tokens/sec
Step 14884 | loss: 3.155948 | lr:1.2645e-04 | norm 0.3302 | dt 338.48ms | 1548952.62 tokens/sec
Step 14885 | loss: 3.092192 | lr:1.2642e-04 | norm 0.2834 | dt 337.61ms | 1552917.82 tokens/sec
Step 14886 | loss: 3.054170 | lr:1.2639e-04 | norm 0.2945 | dt 337.65ms | 1552736.89 tokens/sec
Step 14887 | loss: 3.056251 | lr:1.2636e-04 | norm 0.2735 | dt 338.25ms | 1549998.56 tokens/sec
Step 14888 | loss: 3.088857 | lr:1.2633e-04 | norm 0.2861 | dt 337.46ms | 1553615.60 tokens/sec
Step 14889 | loss: 3.110980 | lr:1.2630e-04 | norm 0.3058 | dt 339.19ms | 1545688.53 tokens/sec
Step 14890 | loss: 3.070686 | lr:1.2627e-04 | norm 0.2900 | dt 338.15ms | 1550463.02 tokens/sec
Step 14891 | loss: 3.035203 | lr:1.2624e-04 | norm 0.2944 | dt 337.80ms | 1552088.10 tokens/sec
Step 14892 | loss: 3.048396 | lr:1.2621e-04 | norm 0.3097 | dt 338.08ms | 1550786.67 tokens/sec
Step 14893 | loss: 3.016080 | lr:1.2618e-04 | norm 0.2700 | dt 338.48ms | 1548946.08 tokens/sec
Step 14894 | loss: 3.122285 | lr:1.2615e-04 | norm 0.3141 | dt 337.96ms | 1551313.99 tokens/sec
Step 14895 | loss: 3.089353 | lr:1.2612e-04 | norm 0.2784 | dt 337.82ms | 1551955.56 tokens/sec
Step 14896 | loss: 3.066220 | lr:1.2609e-04 | norm 0.2987 | dt 337.76ms | 1552243.67 tokens/sec
Step 14897 | loss: 3.060224 | lr:1.2606e-04 | norm 0.2796 | dt 337.39ms | 1553964.73 tokens/sec
Step 14898 | loss: 3.054060 | lr:1.2603e-04 | norm 0.2923 | dt 338.44ms | 1549117.39 tokens/sec
Step 14899 | loss: 3.130401 | lr:1.2600e-04 | norm 0.3018 | dt 337.77ms | 1552218.47 tokens/sec
Step 14900 | loss: 3.024589 | lr:1.2597e-04 | norm 0.3004 | dt 337.50ms | 1553423.54 tokens/sec
Step 14901 | loss: 3.058927 | lr:1.2594e-04 | norm 0.2883 | dt 338.07ms | 1550814.01 tokens/sec
Step 14902 | loss: 3.043010 | lr:1.2591e-04 | norm 0.2954 | dt 338.39ms | 1549352.05 tokens/sec
Step 14903 | loss: 3.069427 | lr:1.2588e-04 | norm 0.3242 | dt 337.85ms | 1551859.18 tokens/sec
Step 14904 | loss: 3.136469 | lr:1.2585e-04 | norm 0.3108 | dt 339.84ms | 1542741.16 tokens/sec
Step 14905 | loss: 3.053046 | lr:1.2582e-04 | norm 0.2919 | dt 337.95ms | 1551371.99 tokens/sec
Step 14906 | loss: 3.029859 | lr:1.2579e-04 | norm 0.2894 | dt 338.45ms | 1549082.47 tokens/sec
Step 14907 | loss: 3.093395 | lr:1.2576e-04 | norm 0.2740 | dt 339.80ms | 1542923.02 tokens/sec
Step 14908 | loss: 3.075465 | lr:1.2573e-04 | norm 0.2881 | dt 337.99ms | 1551216.59 tokens/sec
Step 14909 | loss: 3.057322 | lr:1.2570e-04 | norm 0.2752 | dt 338.05ms | 1550903.70 tokens/sec
Step 14910 | loss: 3.036508 | lr:1.2567e-04 | norm 0.2792 | dt 338.12ms | 1550577.82 tokens/sec
Step 14911 | loss: 3.090425 | lr:1.2564e-04 | norm 0.3370 | dt 338.08ms | 1550792.14 tokens/sec
Step 14912 | loss: 3.069651 | lr:1.2561e-04 | norm 0.3119 | dt 337.77ms | 1552222.86 tokens/sec
Step 14913 | loss: 3.084321 | lr:1.2558e-04 | norm 0.3095 | dt 337.57ms | 1553110.85 tokens/sec
Step 14914 | loss: 3.055338 | lr:1.2555e-04 | norm 0.3042 | dt 337.87ms | 1551756.24 tokens/sec
Step 14915 | loss: 3.074924 | lr:1.2552e-04 | norm 0.3160 | dt 338.00ms | 1551160.79 tokens/sec
Step 14916 | loss: 3.084381 | lr:1.2549e-04 | norm 0.3030 | dt 337.43ms | 1553755.01 tokens/sec
Step 14917 | loss: 3.070515 | lr:1.2546e-04 | norm 0.2898 | dt 337.68ms | 1552629.46 tokens/sec
Step 14918 | loss: 3.134553 | lr:1.2543e-04 | norm 0.3224 | dt 337.36ms | 1554078.94 tokens/sec
Step 14919 | loss: 3.087526 | lr:1.2540e-04 | norm 0.2875 | dt 339.43ms | 1544607.18 tokens/sec
Step 14920 | loss: 3.081905 | lr:1.2537e-04 | norm 0.3158 | dt 339.03ms | 1546414.63 tokens/sec
Step 14921 | loss: 3.136151 | lr:1.2534e-04 | norm 0.3076 | dt 338.86ms | 1547214.34 tokens/sec
Step 14922 | loss: 3.112672 | lr:1.2531e-04 | norm 0.3065 | dt 338.56ms | 1548583.93 tokens/sec
Step 14923 | loss: 3.095273 | lr:1.2528e-04 | norm 0.2953 | dt 338.17ms | 1550352.62 tokens/sec
Step 14924 | loss: 3.106257 | lr:1.2525e-04 | norm 0.3018 | dt 338.48ms | 1548928.62 tokens/sec
Step 14925 | loss: 3.083431 | lr:1.2521e-04 | norm 0.2937 | dt 339.24ms | 1545475.62 tokens/sec
Step 14926 | loss: 3.062178 | lr:1.2518e-04 | norm 0.2902 | dt 339.44ms | 1544573.54 tokens/sec
Step 14927 | loss: 3.077414 | lr:1.2515e-04 | norm 0.2881 | dt 338.11ms | 1550653.26 tokens/sec
Step 14928 | loss: 3.091842 | lr:1.2512e-04 | norm 0.2895 | dt 338.14ms | 1550511.12 tokens/sec
Step 14929 | loss: 3.118911 | lr:1.2509e-04 | norm 0.2834 | dt 338.13ms | 1550547.20 tokens/sec
Step 14930 | loss: 3.049765 | lr:1.2506e-04 | norm 0.3145 | dt 910.02ms | 576127.17 tokens/sec
Step 14931 | loss: 3.095637 | lr:1.2503e-04 | norm 0.2658 | dt 335.60ms | 1562226.80 tokens/sec
Step 14932 | loss: 3.111722 | lr:1.2500e-04 | norm 0.2883 | dt 337.56ms | 1553165.70 tokens/sec
Step 14933 | loss: 3.088209 | lr:1.2497e-04 | norm 0.3008 | dt 340.06ms | 1541767.69 tokens/sec
Step 14934 | loss: 3.057846 | lr:1.2494e-04 | norm 0.2889 | dt 336.92ms | 1556121.14 tokens/sec
Step 14935 | loss: 3.112826 | lr:1.2491e-04 | norm 0.3206 | dt 337.20ms | 1554837.13 tokens/sec
Step 14936 | loss: 3.070435 | lr:1.2488e-04 | norm 0.2935 | dt 337.57ms | 1553114.14 tokens/sec
Step 14937 | loss: 2.975951 | lr:1.2485e-04 | norm 0.2954 | dt 338.16ms | 1550433.51 tokens/sec
Step 14938 | loss: 3.017399 | lr:1.2482e-04 | norm 0.2963 | dt 337.77ms | 1552205.33 tokens/sec
Step 14939 | loss: 3.011703 | lr:1.2479e-04 | norm 0.2720 | dt 337.31ms | 1554303.02 tokens/sec
Step 14940 | loss: 3.041216 | lr:1.2476e-04 | norm 0.2865 | dt 338.35ms | 1549522.36 tokens/sec
Step 14941 | loss: 3.096811 | lr:1.2473e-04 | norm 0.2840 | dt 337.52ms | 1553344.53 tokens/sec
Step 14942 | loss: 3.062912 | lr:1.2470e-04 | norm 0.2974 | dt 337.99ms | 1551193.61 tokens/sec
Step 14943 | loss: 3.051981 | lr:1.2467e-04 | norm 0.3038 | dt 337.78ms | 1552136.30 tokens/sec
Step 14944 | loss: 3.050899 | lr:1.2464e-04 | norm 0.2864 | dt 338.13ms | 1550538.46 tokens/sec
Step 14945 | loss: 3.040207 | lr:1.2461e-04 | norm 0.2863 | dt 338.09ms | 1550715.59 tokens/sec
Step 14946 | loss: 3.100948 | lr:1.2458e-04 | norm 0.3142 | dt 338.68ms | 1548035.59 tokens/sec
Step 14947 | loss: 3.067757 | lr:1.2455e-04 | norm 0.2811 | dt 338.81ms | 1547416.85 tokens/sec
Step 14948 | loss: 3.091030 | lr:1.2452e-04 | norm 0.3056 | dt 337.75ms | 1552274.35 tokens/sec
Step 14949 | loss: 3.120629 | lr:1.2449e-04 | norm 0.3058 | dt 337.41ms | 1553867.00 tokens/sec
Step 14950 | loss: 3.110562 | lr:1.2446e-04 | norm 0.3143 | dt 338.75ms | 1547713.09 tokens/sec
Step 14951 | loss: 3.093371 | lr:1.2443e-04 | norm 0.3062 | dt 337.70ms | 1552512.17 tokens/sec
Step 14952 | loss: 3.142728 | lr:1.2440e-04 | norm 0.2930 | dt 337.56ms | 1553160.22 tokens/sec
Step 14953 | loss: 3.060434 | lr:1.2437e-04 | norm 0.2824 | dt 338.56ms | 1548598.11 tokens/sec
Step 14954 | loss: 3.124278 | lr:1.2434e-04 | norm 0.3014 | dt 338.27ms | 1549895.87 tokens/sec
Step 14955 | loss: 3.183359 | lr:1.2431e-04 | norm 0.3691 | dt 338.07ms | 1550806.36 tokens/sec
Step 14956 | loss: 3.032969 | lr:1.2428e-04 | norm 0.3359 | dt 337.85ms | 1551858.09 tokens/sec
Step 14957 | loss: 3.050480 | lr:1.2425e-04 | norm 0.3174 | dt 338.44ms | 1549147.95 tokens/sec
Step 14958 | loss: 3.079802 | lr:1.2422e-04 | norm 0.3316 | dt 338.60ms | 1548409.47 tokens/sec
Step 14959 | loss: 3.093057 | lr:1.2419e-04 | norm 0.3421 | dt 337.55ms | 1553217.26 tokens/sec
Step 14960 | loss: 3.056699 | lr:1.2416e-04 | norm 0.2978 | dt 337.99ms | 1551179.39 tokens/sec
Step 14961 | loss: 3.118277 | lr:1.2413e-04 | norm 0.2953 | dt 338.60ms | 1548387.66 tokens/sec
Step 14962 | loss: 3.065844 | lr:1.2410e-04 | norm 0.3101 | dt 338.09ms | 1550738.55 tokens/sec
Step 14963 | loss: 3.080129 | lr:1.2408e-04 | norm 0.2827 | dt 337.81ms | 1552008.14 tokens/sec
Step 14964 | loss: 3.106115 | lr:1.2405e-04 | norm 0.3070 | dt 337.82ms | 1551986.23 tokens/sec
Step 14965 | loss: 3.048079 | lr:1.2402e-04 | norm 0.2689 | dt 338.14ms | 1550487.07 tokens/sec
Step 14966 | loss: 3.068829 | lr:1.2399e-04 | norm 0.3038 | dt 338.16ms | 1550422.58 tokens/sec
Step 14967 | loss: 3.099952 | lr:1.2396e-04 | norm 0.2852 | dt 338.66ms | 1548108.61 tokens/sec
Step 14968 | loss: 3.077599 | lr:1.2393e-04 | norm 0.2845 | dt 337.85ms | 1551814.28 tokens/sec
Step 14969 | loss: 3.015441 | lr:1.2390e-04 | norm 0.2837 | dt 338.07ms | 1550826.04 tokens/sec
Step 14970 | loss: 3.050494 | lr:1.2387e-04 | norm 0.2802 | dt 337.69ms | 1552588.90 tokens/sec
Step 14971 | loss: 3.039293 | lr:1.2384e-04 | norm 0.2677 | dt 338.11ms | 1550659.82 tokens/sec
Step 14972 | loss: 3.046089 | lr:1.2381e-04 | norm 0.2681 | dt 337.31ms | 1554322.80 tokens/sec
Step 14973 | loss: 3.114199 | lr:1.2378e-04 | norm 0.2798 | dt 338.00ms | 1551127.96 tokens/sec
Step 14974 | loss: 3.073819 | lr:1.2375e-04 | norm 0.2840 | dt 337.90ms | 1551619.38 tokens/sec
Step 14975 | loss: 3.038465 | lr:1.2372e-04 | norm 0.2929 | dt 337.97ms | 1551301.95 tokens/sec
Step 14976 | loss: 3.032975 | lr:1.2369e-04 | norm 0.3005 | dt 337.87ms | 1551740.91 tokens/sec
Step 14977 | loss: 3.065040 | lr:1.2366e-04 | norm 0.2771 | dt 337.84ms | 1551873.42 tokens/sec
Step 14978 | loss: 3.033188 | lr:1.2363e-04 | norm 0.2908 | dt 337.88ms | 1551681.79 tokens/sec
Step 14979 | loss: 3.087904 | lr:1.2360e-04 | norm 0.2831 | dt 338.20ms | 1550247.70 tokens/sec
Step 14980 | loss: 3.030249 | lr:1.2357e-04 | norm 0.2883 | dt 338.57ms | 1548549.04 tokens/sec
Step 14981 | loss: 3.102171 | lr:1.2354e-04 | norm 0.2887 | dt 337.90ms | 1551602.96 tokens/sec
Step 14982 | loss: 3.085846 | lr:1.2351e-04 | norm 0.2833 | dt 338.89ms | 1547070.66 tokens/sec
Step 14983 | loss: 3.030516 | lr:1.2348e-04 | norm 0.3030 | dt 337.90ms | 1551596.39 tokens/sec
Step 14984 | loss: 3.040389 | lr:1.2345e-04 | norm 0.2987 | dt 337.85ms | 1551842.75 tokens/sec
Step 14985 | loss: 3.053918 | lr:1.2342e-04 | norm 0.2655 | dt 338.60ms | 1548416.01 tokens/sec
Step 14986 | loss: 3.082646 | lr:1.2339e-04 | norm 0.2937 | dt 338.20ms | 1550234.58 tokens/sec
Step 14987 | loss: 3.078285 | lr:1.2336e-04 | norm 0.2857 | dt 338.22ms | 1550118.75 tokens/sec
Step 14988 | loss: 3.125487 | lr:1.2333e-04 | norm 0.2983 | dt 338.15ms | 1550437.88 tokens/sec
Step 14989 | loss: 3.149350 | lr:1.2330e-04 | norm 0.2888 | dt 338.43ms | 1549176.32 tokens/sec
Step 14990 | loss: 3.097612 | lr:1.2327e-04 | norm 0.3039 | dt 337.98ms | 1551252.70 tokens/sec
Step 14991 | loss: 3.034414 | lr:1.2324e-04 | norm 0.2830 | dt 339.09ms | 1546169.99 tokens/sec
Step 14992 | loss: 3.022858 | lr:1.2321e-04 | norm 0.2891 | dt 337.77ms | 1552219.57 tokens/sec
Step 14993 | loss: 3.114694 | lr:1.2318e-04 | norm 0.7480 | dt 338.05ms | 1550898.23 tokens/sec
Step 14994 | loss: 3.120009 | lr:1.2315e-04 | norm 0.2665 | dt 338.05ms | 1550926.67 tokens/sec
Step 14995 | loss: 3.101264 | lr:1.2312e-04 | norm 0.2853 | dt 338.10ms | 1550690.44 tokens/sec
Step 14996 | loss: 3.077056 | lr:1.2309e-04 | norm 0.2995 | dt 337.67ms | 1552650.28 tokens/sec
Step 14997 | loss: 3.060259 | lr:1.2306e-04 | norm 0.2760 | dt 337.55ms | 1553221.65 tokens/sec
Step 14998 | loss: 3.185464 | lr:1.2303e-04 | norm 0.3152 | dt 339.52ms | 1544211.27 tokens/sec
Step 14999 | loss: 3.056622 | lr:1.2300e-04 | norm 0.3221 | dt 338.24ms | 1550053.19 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 15000: 3.1135
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3022/10042=0.3009


ddp_rank 2: ####### Printing generated samples ####### 



ddp_rank 5: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, to do a demo, what's my take?
The answer is that it requires a lot of practice (and,
rank 5 sample 0 >Hello, I'm a language model, but my job is to write a simple code to represent a bunch of data. So, I can easily create a model
rank 2 sample 1 >Hello, I'm a language model, so I couldn't figure out how to type in sentences. So, that's a little bit more up-to-
rank 5 sample 1 >Hello, I'm a language model, who uses grammar in real, not only in math, but also in my world class classes, with the world's most
rank 2 sample 2 >Hello, I'm a language model, so I need to learn it in real life. I'm an ESL teacher who is an expert here. I find teaching
rank 5 sample 2 >Hello, I'm a language model, so I was thinking when I told your story.
I just finished teaching myself, after I had taught myself to learn
rank 2 sample 3 >Hello, I'm a language model, but that brings me a lot of anxiety into my hands. You know, every one of us know how much time we




ddp_rank 4: ####### Printing generated samples ####### 

rank 5 sample 3 >Hello, I'm a language model, here's the test:
The test has to include two tasks.
(It doesn't mean you have a native


rank 4 sample 0 >Hello, I'm a language model, so I don't have to be a beginner to help you learn a language. I need to improve my pronunciation.

rank 4 sample 1 >Hello, I'm a language model, well...
Not to be so funny
You might believe that to ask me for something, I will tell you what
rank 4 sample 2 >Hello, I'm a language model, I got a chance to teach you this one, and I thought I would do so for you. There are only a
rank 4 sample 3 >Hello, I'm a language model, and the subject was English. I tried English. Well, for my book I gave it to another student, and I




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm going to go to the first thing on this page. As a basic example, after a few hours,
rank 7 sample 1 >Hello, I'm a language model, since at one time I've thought programming at the programming block was more like a coding activity than a learning tool. In
rank 7 sample 2 >Hello, I'm a language model, and I've created a tool called it using the standard HTML. You would have to use the HTML tags, like <
rank 7 sample 3 >Hello, I'm a language model, so this is a lot of fun because I was interested in exploring it all. It's a wonderful hands-on tutorial




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and my mom is a language model....I was in Kindergarten at the age of 14 years old....I'm
rank 1 sample 1 >Hello, I'm a language model, a programmer who learns to code with language modeling. I hope you don't miss out from my career, but I'm
rank 1 sample 2 >Hello, I'm a language model, but then it's not me. I'm a great person for a language model, but it's me, but I
rank 1 sample 3 >Hello, I'm a language model, and I'm really interested in how things may be affected by AI. How may we predict in advance what's going to




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where everything is really in square: "I'm learning a lot! " (...)
Let's say we're having
rank 6 sample 1 >Hello, I'm a language model, and I love the things that make my head spin when I'm learning this language, so let me know in the comments
rank 6 sample 2 >Hello, I'm a language model, but if you want to get the feel of that, you need to get used to it.
First, start with
rank 6 sample 3 >Hello, I'm a language model, and so it's time to move on. It's a bit different than a lot in Australia, where most of the




ddp_rank 3: ####### Printing generated samples ####### 


rank 3 sample 0 >Hello, I'm a language model, and I'm a linguist, but I don't teach children to write I can. This is great for children that

ddp_rank 0: ####### Printing generated samples ####### 

rank 3 sample 1 >Hello, I'm a language model, and this post is about I-NLP, a kind of programming language.
I'm in a research department that
rank 0 sample 0 >Hello, I'm a language model, and I need to learn this to use proper dictionaries. I also need to learn some good pronunciation and some good spelling
rank 3 sample 2 >Hello, I'm a language model, and you know, just don't have to say it all over again, you know, that doesn't sound true,
rank 3 sample 3 >Hello, I'm a language model, and every time I need to change my language, I want to use it. Is it easy to run a script,


rank 0 sample 1 >Hello, I'm a language model, so we're going to spend some time learning the meaning of different words, which are words, which may not be very
rank 0 sample 2 >Hello, I'm a language model, so I guess that's because I do a couple of things here in class.
- I'm going to have a
rank 0 sample 3 >Hello, I'm a language model, so a lot of people are trying to understand us. I'm looking at the bottom of the screen here. It would


Step 15000 | loss: 3.091224 | lr:1.2297e-04 | norm 0.2982 | dt 18597.06ms | 28191.99 tokens/sec
Step 15001 | loss: 3.077604 | lr:1.2294e-04 | norm 0.3061 | dt 335.99ms | 1560442.01 tokens/sec
Step 15002 | loss: 3.048619 | lr:1.2291e-04 | norm 0.2867 | dt 337.66ms | 1552696.33 tokens/sec
Step 15003 | loss: 3.004670 | lr:1.2288e-04 | norm 0.2972 | dt 336.37ms | 1558666.82 tokens/sec
Step 15004 | loss: 3.062564 | lr:1.2285e-04 | norm 0.2912 | dt 335.72ms | 1561700.92 tokens/sec
Step 15005 | loss: 3.092406 | lr:1.2283e-04 | norm 0.3086 | dt 336.74ms | 1556930.93 tokens/sec
Step 15006 | loss: 3.062757 | lr:1.2280e-04 | norm 0.2870 | dt 336.63ms | 1557483.39 tokens/sec
Step 15007 | loss: 3.071532 | lr:1.2277e-04 | norm 0.3143 | dt 336.68ms | 1557237.43 tokens/sec
Step 15008 | loss: 3.020322 | lr:1.2274e-04 | norm 0.3028 | dt 337.46ms | 1553612.31 tokens/sec
Step 15009 | loss: 3.121670 | lr:1.2271e-04 | norm 0.2851 | dt 1033.32ms | 507383.58 tokens/sec
Step 15010 | loss: 2.982251 | lr:1.2268e-04 | norm 0.2970 | dt 337.00ms | 1555765.54 tokens/sec
Step 15011 | loss: 3.006946 | lr:1.2265e-04 | norm 0.3071 | dt 337.23ms | 1554679.94 tokens/sec
Step 15012 | loss: 3.058252 | lr:1.2262e-04 | norm 0.3009 | dt 337.07ms | 1555407.91 tokens/sec
Step 15013 | loss: 3.131280 | lr:1.2259e-04 | norm 0.3368 | dt 337.63ms | 1552827.90 tokens/sec
Step 15014 | loss: 3.025575 | lr:1.2256e-04 | norm 0.3057 | dt 338.00ms | 1551135.62 tokens/sec
Step 15015 | loss: 3.047038 | lr:1.2253e-04 | norm 0.3009 | dt 337.05ms | 1555521.23 tokens/sec
Step 15016 | loss: 3.061114 | lr:1.2250e-04 | norm 0.3002 | dt 338.56ms | 1548592.66 tokens/sec
Step 15017 | loss: 3.021770 | lr:1.2247e-04 | norm 0.2859 | dt 338.03ms | 1551026.22 tokens/sec
Step 15018 | loss: 3.094424 | lr:1.2244e-04 | norm 0.2959 | dt 337.65ms | 1552763.21 tokens/sec
Step 15019 | loss: 3.061356 | lr:1.2241e-04 | norm 0.2833 | dt 337.39ms | 1553936.18 tokens/sec
Step 15020 | loss: 3.052061 | lr:1.2238e-04 | norm 0.2884 | dt 337.10ms | 1555285.80 tokens/sec
Step 15021 | loss: 2.990077 | lr:1.2235e-04 | norm 0.2968 | dt 337.66ms | 1552723.74 tokens/sec
Step 15022 | loss: 3.043666 | lr:1.2232e-04 | norm 0.2791 | dt 336.32ms | 1558894.43 tokens/sec
Step 15023 | loss: 3.112327 | lr:1.2229e-04 | norm 0.3101 | dt 336.25ms | 1559213.87 tokens/sec
Step 15024 | loss: 3.039808 | lr:1.2226e-04 | norm 0.2934 | dt 336.81ms | 1556644.38 tokens/sec
Step 15025 | loss: 3.080344 | lr:1.2223e-04 | norm 0.2908 | dt 335.42ms | 1563101.84 tokens/sec
Step 15026 | loss: 3.111582 | lr:1.2220e-04 | norm 0.3039 | dt 337.55ms | 1553215.07 tokens/sec
Step 15027 | loss: 3.096217 | lr:1.2217e-04 | norm 0.2858 | dt 336.36ms | 1558697.75 tokens/sec
Step 15028 | loss: 3.068846 | lr:1.2215e-04 | norm 0.3001 | dt 336.34ms | 1558813.77 tokens/sec
Step 15029 | loss: 3.127096 | lr:1.2212e-04 | norm 0.2838 | dt 336.88ms | 1556305.06 tokens/sec
Step 15030 | loss: 3.100769 | lr:1.2209e-04 | norm 0.2762 | dt 337.21ms | 1554771.17 tokens/sec
Step 15031 | loss: 3.073411 | lr:1.2206e-04 | norm 0.2942 | dt 336.87ms | 1556338.10 tokens/sec
Step 15032 | loss: 3.055365 | lr:1.2203e-04 | norm 0.2990 | dt 337.82ms | 1551962.13 tokens/sec
Step 15033 | loss: 3.039577 | lr:1.2200e-04 | norm 0.3042 | dt 337.90ms | 1551585.44 tokens/sec
Step 15034 | loss: 3.071433 | lr:1.2197e-04 | norm 0.2785 | dt 336.99ms | 1555798.56 tokens/sec
Step 15035 | loss: 3.082480 | lr:1.2194e-04 | norm 0.2724 | dt 337.46ms | 1553614.50 tokens/sec
Step 15036 | loss: 3.097723 | lr:1.2191e-04 | norm 0.2901 | dt 337.69ms | 1552562.59 tokens/sec
Step 15037 | loss: 3.033172 | lr:1.2188e-04 | norm 0.2851 | dt 337.58ms | 1553076.85 tokens/sec
Step 15038 | loss: 3.037286 | lr:1.2185e-04 | norm 0.2852 | dt 337.74ms | 1552321.47 tokens/sec
Step 15039 | loss: 3.057241 | lr:1.2182e-04 | norm 0.2805 | dt 337.63ms | 1552859.70 tokens/sec
Step 15040 | loss: 3.038218 | lr:1.2179e-04 | norm 0.2813 | dt 338.30ms | 1549793.19 tokens/sec
Step 15041 | loss: 3.112885 | lr:1.2176e-04 | norm 0.2952 | dt 337.10ms | 1555267.10 tokens/sec
Step 15042 | loss: 2.996626 | lr:1.2173e-04 | norm 0.2828 | dt 338.11ms | 1550628.11 tokens/sec
Step 15043 | loss: 3.104671 | lr:1.2170e-04 | norm 0.2856 | dt 338.22ms | 1550137.32 tokens/sec
Step 15044 | loss: 2.976537 | lr:1.2167e-04 | norm 0.2839 | dt 337.39ms | 1553932.88 tokens/sec
Step 15045 | loss: 3.092019 | lr:1.2164e-04 | norm 0.2741 | dt 338.66ms | 1548132.58 tokens/sec
Step 15046 | loss: 3.094565 | lr:1.2162e-04 | norm 0.2884 | dt 337.38ms | 1554015.24 tokens/sec
Step 15047 | loss: 3.081925 | lr:1.2159e-04 | norm 0.2924 | dt 338.28ms | 1549856.54 tokens/sec
Step 15048 | loss: 3.133708 | lr:1.2156e-04 | norm 0.3217 | dt 338.28ms | 1549877.30 tokens/sec
Step 15049 | loss: 3.052104 | lr:1.2153e-04 | norm 0.3008 | dt 338.42ms | 1549218.89 tokens/sec
Step 15050 | loss: 3.160096 | lr:1.2150e-04 | norm 0.3166 | dt 337.64ms | 1552784.04 tokens/sec
Step 15051 | loss: 3.101165 | lr:1.2147e-04 | norm 0.2913 | dt 337.45ms | 1553691.34 tokens/sec
Step 15052 | loss: 3.077793 | lr:1.2144e-04 | norm 0.3249 | dt 338.20ms | 1550244.42 tokens/sec
Step 15053 | loss: 2.999329 | lr:1.2141e-04 | norm 0.3042 | dt 338.29ms | 1549821.59 tokens/sec
Step 15054 | loss: 3.117383 | lr:1.2138e-04 | norm 0.3291 | dt 339.77ms | 1543048.60 tokens/sec
Step 15055 | loss: 3.115584 | lr:1.2135e-04 | norm 0.3059 | dt 337.53ms | 1553326.98 tokens/sec
Step 15056 | loss: 3.027705 | lr:1.2132e-04 | norm 0.3256 | dt 338.32ms | 1549700.36 tokens/sec
Step 15057 | loss: 3.129735 | lr:1.2129e-04 | norm 0.3476 | dt 338.20ms | 1550210.54 tokens/sec
Step 15058 | loss: 3.054640 | lr:1.2126e-04 | norm 0.3022 | dt 337.46ms | 1553626.58 tokens/sec
Step 15059 | loss: 3.052377 | lr:1.2123e-04 | norm 0.3274 | dt 337.95ms | 1551384.03 tokens/sec
Step 15060 | loss: 3.076982 | lr:1.2120e-04 | norm 0.3009 | dt 338.78ms | 1547594.36 tokens/sec
Step 15061 | loss: 3.086494 | lr:1.2118e-04 | norm 0.3073 | dt 338.35ms | 1549557.30 tokens/sec
Step 15062 | loss: 3.061644 | lr:1.2115e-04 | norm 0.3079 | dt 337.72ms | 1552417.91 tokens/sec
Step 15063 | loss: 3.116448 | lr:1.2112e-04 | norm 0.3118 | dt 337.83ms | 1551943.51 tokens/sec
Step 15064 | loss: 3.080468 | lr:1.2109e-04 | norm 0.2942 | dt 337.59ms | 1553046.14 tokens/sec
Step 15065 | loss: 3.046796 | lr:1.2106e-04 | norm 0.2911 | dt 338.17ms | 1550366.83 tokens/sec
Step 15066 | loss: 3.104994 | lr:1.2103e-04 | norm 0.2870 | dt 338.37ms | 1549474.32 tokens/sec
Step 15067 | loss: 3.112156 | lr:1.2100e-04 | norm 0.3039 | dt 337.83ms | 1551944.61 tokens/sec
Step 15068 | loss: 3.092729 | lr:1.2097e-04 | norm 0.2696 | dt 338.83ms | 1547364.59 tokens/sec
Step 15069 | loss: 3.056968 | lr:1.2094e-04 | norm 0.2823 | dt 339.43ms | 1544619.11 tokens/sec
Step 15070 | loss: 3.061840 | lr:1.2091e-04 | norm 0.2990 | dt 339.93ms | 1542324.58 tokens/sec
Step 15071 | loss: 3.050649 | lr:1.2088e-04 | norm 0.3034 | dt 338.15ms | 1550459.74 tokens/sec
Step 15072 | loss: 3.101462 | lr:1.2085e-04 | norm 0.2844 | dt 338.77ms | 1547628.13 tokens/sec
Step 15073 | loss: 3.020324 | lr:1.2082e-04 | norm 0.2732 | dt 338.42ms | 1549226.53 tokens/sec
Step 15074 | loss: 3.068549 | lr:1.2080e-04 | norm 0.2899 | dt 339.30ms | 1545183.50 tokens/sec
Step 15075 | loss: 3.049511 | lr:1.2077e-04 | norm 0.2720 | dt 339.07ms | 1546265.66 tokens/sec
Step 15076 | loss: 3.093074 | lr:1.2074e-04 | norm 0.2853 | dt 338.27ms | 1549930.83 tokens/sec
Step 15077 | loss: 3.057193 | lr:1.2071e-04 | norm 0.2665 | dt 338.60ms | 1548400.75 tokens/sec
Step 15078 | loss: 3.037356 | lr:1.2068e-04 | norm 0.2735 | dt 338.45ms | 1549104.29 tokens/sec
Step 15079 | loss: 3.066616 | lr:1.2065e-04 | norm 0.3003 | dt 339.22ms | 1545571.21 tokens/sec
Step 15080 | loss: 3.084737 | lr:1.2062e-04 | norm 0.2814 | dt 338.48ms | 1548950.44 tokens/sec
Step 15081 | loss: 3.083745 | lr:1.2059e-04 | norm 0.2795 | dt 338.39ms | 1549362.97 tokens/sec
Step 15082 | loss: 3.084961 | lr:1.2056e-04 | norm 0.2804 | dt 339.43ms | 1544606.09 tokens/sec
Step 15083 | loss: 3.065140 | lr:1.2053e-04 | norm 0.2875 | dt 338.44ms | 1549153.40 tokens/sec
Step 15084 | loss: 3.095906 | lr:1.2050e-04 | norm 0.2994 | dt 338.66ms | 1548108.61 tokens/sec
Step 15085 | loss: 3.136239 | lr:1.2047e-04 | norm 0.2831 | dt 338.47ms | 1548985.36 tokens/sec
Step 15086 | loss: 3.107222 | lr:1.2045e-04 | norm 0.3031 | dt 338.61ms | 1548346.23 tokens/sec
Step 15087 | loss: 3.070552 | lr:1.2042e-04 | norm 0.3195 | dt 338.68ms | 1548042.13 tokens/sec
Step 15088 | loss: 3.090924 | lr:1.2039e-04 | norm 0.2879 | dt 337.58ms | 1553069.17 tokens/sec
Step 15089 | loss: 3.001874 | lr:1.2036e-04 | norm 0.2923 | dt 337.97ms | 1551276.78 tokens/sec
Step 15090 | loss: 3.106554 | lr:1.2033e-04 | norm 0.3172 | dt 338.73ms | 1547794.79 tokens/sec
Step 15091 | loss: 3.013710 | lr:1.2030e-04 | norm 0.3018 | dt 339.69ms | 1543441.74 tokens/sec
Step 15092 | loss: 3.075048 | lr:1.2027e-04 | norm 0.3039 | dt 337.84ms | 1551901.89 tokens/sec
Step 15093 | loss: 3.115840 | lr:1.2024e-04 | norm 0.2905 | dt 338.38ms | 1549416.46 tokens/sec
Step 15094 | loss: 3.068886 | lr:1.2021e-04 | norm 0.2957 | dt 338.15ms | 1550452.09 tokens/sec
Step 15095 | loss: 3.090713 | lr:1.2018e-04 | norm 0.3231 | dt 337.38ms | 1553979.00 tokens/sec
Step 15096 | loss: 3.071122 | lr:1.2015e-04 | norm 0.2907 | dt 338.53ms | 1548740.99 tokens/sec
Step 15097 | loss: 3.085238 | lr:1.2013e-04 | norm 0.2987 | dt 337.86ms | 1551813.19 tokens/sec
Step 15098 | loss: 3.133292 | lr:1.2010e-04 | norm 0.2817 | dt 338.02ms | 1551078.73 tokens/sec
Step 15099 | loss: 3.055555 | lr:1.2007e-04 | norm 0.2850 | dt 338.01ms | 1551117.02 tokens/sec
Step 15100 | loss: 3.015616 | lr:1.2004e-04 | norm 0.2863 | dt 337.65ms | 1552748.95 tokens/sec
Step 15101 | loss: 3.045625 | lr:1.2001e-04 | norm 0.2773 | dt 338.42ms | 1549243.99 tokens/sec
Step 15102 | loss: 3.098430 | lr:1.1998e-04 | norm 0.2670 | dt 338.38ms | 1549384.80 tokens/sec
Step 15103 | loss: 3.120279 | lr:1.1995e-04 | norm 0.3342 | dt 339.15ms | 1545886.30 tokens/sec
Step 15104 | loss: 3.057663 | lr:1.1992e-04 | norm 0.2682 | dt 338.79ms | 1547526.84 tokens/sec
Step 15105 | loss: 3.016904 | lr:1.1989e-04 | norm 0.2991 | dt 338.49ms | 1548886.07 tokens/sec
Step 15106 | loss: 3.058856 | lr:1.1986e-04 | norm 0.2888 | dt 338.29ms | 1549819.40 tokens/sec
Step 15107 | loss: 3.024033 | lr:1.1983e-04 | norm 0.2960 | dt 338.30ms | 1549769.16 tokens/sec
Step 15108 | loss: 3.058854 | lr:1.1981e-04 | norm 0.3168 | dt 338.33ms | 1549615.18 tokens/sec
Step 15109 | loss: 3.055436 | lr:1.1978e-04 | norm 0.2887 | dt 338.53ms | 1548704.99 tokens/sec
Step 15110 | loss: 3.011227 | lr:1.1975e-04 | norm 0.3145 | dt 338.66ms | 1548129.31 tokens/sec
Step 15111 | loss: 3.023324 | lr:1.1972e-04 | norm 0.2960 | dt 338.86ms | 1547231.76 tokens/sec
Step 15112 | loss: 3.069455 | lr:1.1969e-04 | norm 0.2941 | dt 338.18ms | 1550304.53 tokens/sec
Step 15113 | loss: 3.000018 | lr:1.1966e-04 | norm 0.2794 | dt 337.39ms | 1553950.45 tokens/sec
Step 15114 | loss: 3.047000 | lr:1.1963e-04 | norm 0.2808 | dt 337.62ms | 1552872.86 tokens/sec
Step 15115 | loss: 3.069286 | lr:1.1960e-04 | norm 0.2803 | dt 337.90ms | 1551606.24 tokens/sec
Step 15116 | loss: 3.047702 | lr:1.1957e-04 | norm 0.2887 | dt 337.29ms | 1554390.92 tokens/sec
Step 15117 | loss: 3.020489 | lr:1.1954e-04 | norm 0.2910 | dt 337.50ms | 1553450.97 tokens/sec
Step 15118 | loss: 3.095675 | lr:1.1952e-04 | norm 0.3125 | dt 338.45ms | 1549099.93 tokens/sec
Step 15119 | loss: 3.035165 | lr:1.1949e-04 | norm 0.2915 | dt 905.36ms | 579090.21 tokens/sec
Step 15120 | loss: 3.057390 | lr:1.1946e-04 | norm 0.2989 | dt 335.69ms | 1561825.15 tokens/sec
Step 15121 | loss: 3.039522 | lr:1.1943e-04 | norm 0.2917 | dt 337.50ms | 1553443.29 tokens/sec
Step 15122 | loss: 3.091263 | lr:1.1940e-04 | norm 0.2948 | dt 338.83ms | 1547355.88 tokens/sec
Step 15123 | loss: 3.081770 | lr:1.1937e-04 | norm 0.2997 | dt 338.17ms | 1550365.74 tokens/sec
Step 15124 | loss: 3.100656 | lr:1.1934e-04 | norm 0.3228 | dt 338.19ms | 1550269.55 tokens/sec
Step 15125 | loss: 2.983118 | lr:1.1931e-04 | norm 0.2965 | dt 338.33ms | 1549646.84 tokens/sec
Step 15126 | loss: 3.096663 | lr:1.1928e-04 | norm 0.3316 | dt 338.26ms | 1549975.62 tokens/sec
Step 15127 | loss: 3.055582 | lr:1.1926e-04 | norm 0.2933 | dt 338.60ms | 1548397.48 tokens/sec
Step 15128 | loss: 3.029779 | lr:1.1923e-04 | norm 0.2917 | dt 339.45ms | 1544541.00 tokens/sec
Step 15129 | loss: 3.033564 | lr:1.1920e-04 | norm 0.2932 | dt 338.83ms | 1547352.61 tokens/sec
Step 15130 | loss: 3.074333 | lr:1.1917e-04 | norm 0.3257 | dt 338.97ms | 1546725.71 tokens/sec
Step 15131 | loss: 3.092087 | lr:1.1914e-04 | norm 0.2957 | dt 339.19ms | 1545716.78 tokens/sec
Step 15132 | loss: 3.054297 | lr:1.1911e-04 | norm 0.2948 | dt 339.65ms | 1543620.51 tokens/sec
Step 15133 | loss: 3.065501 | lr:1.1908e-04 | norm 0.2858 | dt 338.93ms | 1546905.24 tokens/sec
Step 15134 | loss: 3.062282 | lr:1.1905e-04 | norm 0.2798 | dt 337.67ms | 1552673.31 tokens/sec
Step 15135 | loss: 3.044941 | lr:1.1902e-04 | norm 0.2880 | dt 338.26ms | 1549969.06 tokens/sec
Step 15136 | loss: 3.095102 | lr:1.1900e-04 | norm 0.2869 | dt 338.85ms | 1547256.80 tokens/sec
Step 15137 | loss: 3.061296 | lr:1.1897e-04 | norm 0.3044 | dt 337.50ms | 1553441.10 tokens/sec
Step 15138 | loss: 3.070976 | lr:1.1894e-04 | norm 0.2946 | dt 339.53ms | 1544173.32 tokens/sec
Step 15139 | loss: 3.045118 | lr:1.1891e-04 | norm 0.2773 | dt 338.30ms | 1549780.08 tokens/sec
Step 15140 | loss: 3.097877 | lr:1.1888e-04 | norm 0.3110 | dt 337.89ms | 1551662.08 tokens/sec
Step 15141 | loss: 3.021336 | lr:1.1885e-04 | norm 0.2868 | dt 338.29ms | 1549813.94 tokens/sec
Step 15142 | loss: 3.051917 | lr:1.1882e-04 | norm 0.2880 | dt 338.51ms | 1548807.53 tokens/sec
Step 15143 | loss: 3.054381 | lr:1.1879e-04 | norm 0.2946 | dt 337.95ms | 1551357.76 tokens/sec
Step 15144 | loss: 3.024413 | lr:1.1877e-04 | norm 0.2913 | dt 338.73ms | 1547825.29 tokens/sec
Step 15145 | loss: 3.103324 | lr:1.1874e-04 | norm 0.2997 | dt 339.29ms | 1545272.53 tokens/sec
Step 15146 | loss: 3.048944 | lr:1.1871e-04 | norm 0.2754 | dt 338.25ms | 1549992.00 tokens/sec
Step 15147 | loss: 3.059093 | lr:1.1868e-04 | norm 0.2792 | dt 338.62ms | 1548292.82 tokens/sec
Step 15148 | loss: 3.089513 | lr:1.1865e-04 | norm 0.2725 | dt 339.31ms | 1545173.72 tokens/sec
Step 15149 | loss: 3.074604 | lr:1.1862e-04 | norm 0.2804 | dt 337.85ms | 1551844.94 tokens/sec
Step 15150 | loss: 3.056300 | lr:1.1859e-04 | norm 0.2905 | dt 339.36ms | 1544909.93 tokens/sec
Step 15151 | loss: 3.069802 | lr:1.1856e-04 | norm 0.3114 | dt 338.58ms | 1548489.06 tokens/sec
Step 15152 | loss: 3.005660 | lr:1.1854e-04 | norm 0.2646 | dt 338.31ms | 1549740.77 tokens/sec
Step 15153 | loss: 3.076709 | lr:1.1851e-04 | norm 0.2890 | dt 338.91ms | 1546995.56 tokens/sec
Step 15154 | loss: 3.122702 | lr:1.1848e-04 | norm 0.2817 | dt 339.08ms | 1546206.95 tokens/sec
Step 15155 | loss: 3.078710 | lr:1.1845e-04 | norm 0.2782 | dt 339.15ms | 1545899.34 tokens/sec
Step 15156 | loss: 3.053561 | lr:1.1842e-04 | norm 0.2959 | dt 338.06ms | 1550880.73 tokens/sec
Step 15157 | loss: 3.101655 | lr:1.1839e-04 | norm 0.2748 | dt 338.52ms | 1548743.17 tokens/sec
Step 15158 | loss: 3.059120 | lr:1.1836e-04 | norm 0.2854 | dt 338.72ms | 1547871.05 tokens/sec
Step 15159 | loss: 3.029546 | lr:1.1833e-04 | norm 0.3096 | dt 338.36ms | 1549477.60 tokens/sec
Step 15160 | loss: 3.092988 | lr:1.1831e-04 | norm 0.3012 | dt 339.11ms | 1546062.37 tokens/sec
Step 15161 | loss: 3.047565 | lr:1.1828e-04 | norm 0.2887 | dt 338.61ms | 1548340.78 tokens/sec
Step 15162 | loss: 3.084295 | lr:1.1825e-04 | norm 0.3235 | dt 338.38ms | 1549425.19 tokens/sec
Step 15163 | loss: 3.063159 | lr:1.1822e-04 | norm 0.3027 | dt 338.23ms | 1550092.52 tokens/sec
Step 15164 | loss: 3.082385 | lr:1.1819e-04 | norm 0.3118 | dt 339.56ms | 1544041.04 tokens/sec
Step 15165 | loss: 3.088225 | lr:1.1816e-04 | norm 0.2816 | dt 338.85ms | 1547243.74 tokens/sec
Step 15166 | loss: 3.019273 | lr:1.1813e-04 | norm 0.3117 | dt 337.76ms | 1552267.78 tokens/sec
Step 15167 | loss: 3.101542 | lr:1.1811e-04 | norm 0.2839 | dt 338.35ms | 1549556.21 tokens/sec
Step 15168 | loss: 3.103328 | lr:1.1808e-04 | norm 0.2779 | dt 338.83ms | 1547335.19 tokens/sec
Step 15169 | loss: 3.090269 | lr:1.1805e-04 | norm 0.3480 | dt 338.01ms | 1551121.40 tokens/sec
Step 15170 | loss: 3.049959 | lr:1.1802e-04 | norm 0.2864 | dt 338.05ms | 1550903.70 tokens/sec
Step 15171 | loss: 3.126018 | lr:1.1799e-04 | norm 0.3130 | dt 337.48ms | 1553528.89 tokens/sec
Step 15172 | loss: 3.052701 | lr:1.1796e-04 | norm 0.2718 | dt 337.90ms | 1551609.53 tokens/sec
Step 15173 | loss: 3.063342 | lr:1.1793e-04 | norm 0.2969 | dt 339.21ms | 1545623.35 tokens/sec
Step 15174 | loss: 3.082590 | lr:1.1791e-04 | norm 0.2767 | dt 338.40ms | 1549325.85 tokens/sec
Step 15175 | loss: 3.096102 | lr:1.1788e-04 | norm 0.2804 | dt 339.01ms | 1546505.99 tokens/sec
Step 15176 | loss: 2.970526 | lr:1.1785e-04 | norm 0.3261 | dt 338.19ms | 1550278.30 tokens/sec
Step 15177 | loss: 3.018356 | lr:1.1782e-04 | norm 0.2892 | dt 339.36ms | 1544929.47 tokens/sec
Step 15178 | loss: 3.024462 | lr:1.1779e-04 | norm 0.3260 | dt 338.31ms | 1549708.00 tokens/sec
Step 15179 | loss: 3.058015 | lr:1.1776e-04 | norm 0.2920 | dt 338.43ms | 1549186.14 tokens/sec
Step 15180 | loss: 3.174120 | lr:1.1773e-04 | norm 0.3462 | dt 337.82ms | 1551997.18 tokens/sec
Step 15181 | loss: 3.047259 | lr:1.1771e-04 | norm 0.2853 | dt 338.36ms | 1549502.71 tokens/sec
Step 15182 | loss: 3.079262 | lr:1.1768e-04 | norm 0.3268 | dt 338.36ms | 1549512.54 tokens/sec
Step 15183 | loss: 3.065513 | lr:1.1765e-04 | norm 0.2956 | dt 338.10ms | 1550688.25 tokens/sec
Step 15184 | loss: 3.043397 | lr:1.1762e-04 | norm 0.2961 | dt 337.69ms | 1552591.09 tokens/sec
Step 15185 | loss: 3.025314 | lr:1.1759e-04 | norm 0.3182 | dt 339.33ms | 1545068.41 tokens/sec
Step 15186 | loss: 3.092081 | lr:1.1756e-04 | norm 0.2818 | dt 338.57ms | 1548529.41 tokens/sec
Step 15187 | loss: 3.063616 | lr:1.1753e-04 | norm 0.3084 | dt 338.96ms | 1546764.88 tokens/sec
Step 15188 | loss: 3.075860 | lr:1.1751e-04 | norm 0.3119 | dt 338.32ms | 1549686.16 tokens/sec
Step 15189 | loss: 3.039320 | lr:1.1748e-04 | norm 0.2900 | dt 338.32ms | 1549675.24 tokens/sec
Step 15190 | loss: 3.064213 | lr:1.1745e-04 | norm 0.3093 | dt 338.26ms | 1549972.34 tokens/sec
Step 15191 | loss: 3.066529 | lr:1.1742e-04 | norm 0.2982 | dt 338.58ms | 1548471.62 tokens/sec
Step 15192 | loss: 3.074348 | lr:1.1739e-04 | norm 0.2891 | dt 338.28ms | 1549883.85 tokens/sec
Step 15193 | loss: 3.110137 | lr:1.1736e-04 | norm 0.2938 | dt 337.21ms | 1554772.27 tokens/sec
Step 15194 | loss: 3.111330 | lr:1.1733e-04 | norm 0.3032 | dt 338.05ms | 1550934.33 tokens/sec
Step 15195 | loss: 3.067728 | lr:1.1731e-04 | norm 0.2788 | dt 338.21ms | 1550178.85 tokens/sec
Step 15196 | loss: 3.080691 | lr:1.1728e-04 | norm 0.3103 | dt 337.36ms | 1554110.79 tokens/sec
Step 15197 | loss: 3.129066 | lr:1.1725e-04 | norm 0.3075 | dt 337.64ms | 1552793.91 tokens/sec
Step 15198 | loss: 3.082644 | lr:1.1722e-04 | norm 0.2868 | dt 338.24ms | 1550040.08 tokens/sec
Step 15199 | loss: 3.031319 | lr:1.1719e-04 | norm 0.2995 | dt 916.79ms | 571875.38 tokens/sec
Step 15200 | loss: 3.058760 | lr:1.1716e-04 | norm 0.2774 | dt 336.83ms | 1556525.38 tokens/sec
Step 15201 | loss: 3.069487 | lr:1.1714e-04 | norm 0.2808 | dt 338.71ms | 1547893.93 tokens/sec
Step 15202 | loss: 3.054765 | lr:1.1711e-04 | norm 0.2771 | dt 337.48ms | 1553527.80 tokens/sec
Step 15203 | loss: 3.064620 | lr:1.1708e-04 | norm 0.3056 | dt 337.93ms | 1551452.99 tokens/sec
Step 15204 | loss: 3.055427 | lr:1.1705e-04 | norm 0.2800 | dt 337.84ms | 1551865.75 tokens/sec
Step 15205 | loss: 3.054166 | lr:1.1702e-04 | norm 0.3036 | dt 337.23ms | 1554709.61 tokens/sec
Step 15206 | loss: 3.167574 | lr:1.1699e-04 | norm 0.2874 | dt 338.50ms | 1548865.34 tokens/sec
Step 15207 | loss: 3.085778 | lr:1.1696e-04 | norm 0.2907 | dt 338.21ms | 1550162.46 tokens/sec
Step 15208 | loss: 3.100924 | lr:1.1694e-04 | norm 0.2903 | dt 338.21ms | 1550181.03 tokens/sec
Step 15209 | loss: 3.076642 | lr:1.1691e-04 | norm 0.2828 | dt 337.57ms | 1553117.44 tokens/sec
Step 15210 | loss: 3.050746 | lr:1.1688e-04 | norm 0.3052 | dt 338.85ms | 1547264.42 tokens/sec
Step 15211 | loss: 3.038425 | lr:1.1685e-04 | norm 0.2830 | dt 338.03ms | 1550995.59 tokens/sec
Step 15212 | loss: 3.101792 | lr:1.1682e-04 | norm 0.3118 | dt 337.87ms | 1551759.53 tokens/sec
Step 15213 | loss: 3.045458 | lr:1.1679e-04 | norm 0.2874 | dt 338.77ms | 1547640.11 tokens/sec
Step 15214 | loss: 3.075715 | lr:1.1677e-04 | norm 0.3201 | dt 337.98ms | 1551259.27 tokens/sec
Step 15215 | loss: 3.034445 | lr:1.1674e-04 | norm 0.2747 | dt 338.94ms | 1546855.19 tokens/sec
Step 15216 | loss: 3.022915 | lr:1.1671e-04 | norm 0.3250 | dt 340.40ms | 1540222.39 tokens/sec
Step 15217 | loss: 3.037254 | lr:1.1668e-04 | norm 0.3207 | dt 338.26ms | 1549961.41 tokens/sec
Step 15218 | loss: 3.058801 | lr:1.1665e-04 | norm 0.3143 | dt 338.23ms | 1550075.04 tokens/sec
Step 15219 | loss: 3.057680 | lr:1.1662e-04 | norm 0.2869 | dt 339.13ms | 1545960.20 tokens/sec
Step 15220 | loss: 3.111206 | lr:1.1660e-04 | norm 0.3265 | dt 338.59ms | 1548432.36 tokens/sec
Step 15221 | loss: 3.024508 | lr:1.1657e-04 | norm 0.2790 | dt 338.95ms | 1546779.02 tokens/sec
Step 15222 | loss: 3.064588 | lr:1.1654e-04 | norm 0.3282 | dt 338.38ms | 1549421.92 tokens/sec
Step 15223 | loss: 3.034467 | lr:1.1651e-04 | norm 0.2822 | dt 338.98ms | 1546668.06 tokens/sec
Step 15224 | loss: 3.072314 | lr:1.1648e-04 | norm 0.3142 | dt 338.59ms | 1548423.64 tokens/sec
Step 15225 | loss: 3.071304 | lr:1.1645e-04 | norm 0.2768 | dt 338.56ms | 1548561.03 tokens/sec
Step 15226 | loss: 3.083559 | lr:1.1643e-04 | norm 0.3200 | dt 339.51ms | 1544255.74 tokens/sec
Step 15227 | loss: 3.059234 | lr:1.1640e-04 | norm 0.2934 | dt 339.18ms | 1545764.59 tokens/sec
Step 15228 | loss: 3.046610 | lr:1.1637e-04 | norm 0.2969 | dt 338.00ms | 1551156.41 tokens/sec
Step 15229 | loss: 3.036041 | lr:1.1634e-04 | norm 0.2950 | dt 337.79ms | 1552095.77 tokens/sec
Step 15230 | loss: 3.113950 | lr:1.1631e-04 | norm 0.3008 | dt 339.40ms | 1544750.40 tokens/sec
Step 15231 | loss: 3.021145 | lr:1.1629e-04 | norm 0.2943 | dt 338.70ms | 1547929.89 tokens/sec
Step 15232 | loss: 3.065998 | lr:1.1626e-04 | norm 0.3272 | dt 338.24ms | 1550028.06 tokens/sec
Step 15233 | loss: 3.103520 | lr:1.1623e-04 | norm 0.3152 | dt 338.31ms | 1549748.41 tokens/sec
Step 15234 | loss: 3.057143 | lr:1.1620e-04 | norm 0.3111 | dt 338.44ms | 1549125.03 tokens/sec
Step 15235 | loss: 3.074256 | lr:1.1617e-04 | norm 0.2850 | dt 338.31ms | 1549723.29 tokens/sec
Step 15236 | loss: 3.047408 | lr:1.1614e-04 | norm 0.3264 | dt 338.13ms | 1550547.20 tokens/sec
Step 15237 | loss: 3.069051 | lr:1.1612e-04 | norm 0.2879 | dt 338.67ms | 1548058.47 tokens/sec
Step 15238 | loss: 3.066454 | lr:1.1609e-04 | norm 0.2943 | dt 338.10ms | 1550675.13 tokens/sec
Step 15239 | loss: 3.073991 | lr:1.1606e-04 | norm 0.3082 | dt 337.98ms | 1551260.36 tokens/sec
Step 15240 | loss: 3.045452 | lr:1.1603e-04 | norm 0.2875 | dt 338.37ms | 1549450.30 tokens/sec
Step 15241 | loss: 3.048018 | lr:1.1600e-04 | norm 0.2921 | dt 338.45ms | 1549079.20 tokens/sec
Step 15242 | loss: 3.101338 | lr:1.1598e-04 | norm 0.2910 | dt 338.37ms | 1549462.31 tokens/sec
Step 15243 | loss: 3.082272 | lr:1.1595e-04 | norm 0.2902 | dt 338.58ms | 1548492.33 tokens/sec
Step 15244 | loss: 3.087037 | lr:1.1592e-04 | norm 0.2979 | dt 338.46ms | 1549017.00 tokens/sec
Step 15245 | loss: 3.045552 | lr:1.1589e-04 | norm 0.3187 | dt 338.52ms | 1548782.44 tokens/sec
Step 15246 | loss: 3.074672 | lr:1.1586e-04 | norm 0.2864 | dt 337.88ms | 1551679.60 tokens/sec
Step 15247 | loss: 3.074219 | lr:1.1583e-04 | norm 0.3052 | dt 338.51ms | 1548803.16 tokens/sec
Step 15248 | loss: 3.040517 | lr:1.1581e-04 | norm 0.2716 | dt 337.87ms | 1551765.00 tokens/sec
Step 15249 | loss: 3.038257 | lr:1.1578e-04 | norm 0.2957 | dt 337.61ms | 1552941.95 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 15250: 3.1086
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3051/10042=0.3038


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm a linguist, and I love the field of language. Anyway, you'll find plenty of posts of
rank 3 sample 1 >Hello, I'm a language model, and this week I'm adding in the following:
Here's the code for the following code:
Let t-
rank 3 sample 2 >Hello, I'm a language model, and you know, just like how I mentioned how I used it, you see, I use T. So here is
rank 3 sample 3 >Hello, I'm a language model, and a language theorist. I was just trying to make an analogy, so I don't like to assume I know what




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, and so, we're going to create an application that can do this without having to explicitly know how to get it done
rank 2 sample 1 >Hello, I'm a language model, so I read some information on the
Web site and I want to add a link to this. So, I go
rank 2 sample 2 >Hello, I'm a language model, so I want to create a model based on what I'm thinking,
"A good way to go is to look
rank 2 sample 3 >Hello, I'm a language model, and if I'm a teacher, I should be like this (I'm actually doing the right thing). So I know




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I'm going to show you how there's an element of the argument value a = 3 at the beginning of this
rank 7 sample 1 >Hello, I'm a language model, is good enough for this job because as long as I continue to keep my language the way I want it to be taught
rank 7 sample 2 >Hello, I'm a language model, and I want to demonstrate how I know that I am in a situation where I can't speak English.
This example
rank 7 sample 3 >Hello, I'm a language model, so I'm going to start by drawing a table. I want the columns to be the same but with different colors so




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but what I'm looking for is how to do that in a web development environment.
I know that it's always
rank 5 sample 1 >Hello, I'm a language model, here's a few other cool examples for you from my blog:
I know some languages like Finnish and Hebrew. I
rank 5 sample 2 >Hello, I'm a language model, so I love the different ways of communicating with you. I'm the kind of guy called "O-L-A
rank 5 sample 3 >Hello, I'm a language model, or even you might be reading about it. And the first thing that comes to your notice, why do I have to




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I can't seem to figure out in seconds what to do and expect that I'm going to get some answers on
rank 0 sample 1 >Hello, I'm a language model, so a very good example of what has been said out there. And at first I'm still not as good as someone
rank 0 sample 2 >Hello, I'm a language model, so I guess that's right. And I do find that language. But I know, I'm not a native English
rank 0 sample 3 >Hello, I'm a language model, so a lot of the people who want to think about it are using language model questions like 'what's the best form




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and a visual learner, and every single point is as unique as the next. And there, I have to say
rank 1 sample 1 >Hello, I'm a language model, a computer language software developer. I'm curious about the code. You can ask questions but you'll never know.

rank 1 sample 2 >Hello, I'm a language model, but could be called a language model, but I'm still doing my research. I'm trying to find out whether or
rank 1 sample 3 >Hello, I'm a language model, and I'm writing this tutorial for someone - I'm new to Lulu
I didn't get into the concept of




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I wanted to help you out with writing your own language.
- Write a brief tutorial on how to create your


ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 1 >Hello, I'm a language model, thanks for the last part. What's important to you
- Do you like what you read? Are you tired?
rank 4 sample 2 >Hello, I'm a language model, I live in Tokyo. In the course of my research, I was able to say the following in detail: 'The
rank 6 sample 0 >Hello, I'm a language model, in other words, what my friend said about the world of programming languages is true.
Here's a simple example -
rank 4 sample 3 >Hello, I'm a language model, and my program works pretty well. Thank you in advance! So I really enjoyed this article, if I can remember,


rank 6 sample 1 >Hello, I'm a language model, and I'll be speaking with you. When you get to the end of the chapter, I'm gonna go on to
rank 6 sample 2 >Hello, I'm a language model, and a software engineer. And you guys are in the middle, working in Python. And you're all pretty confident,
rank 6 sample 3 >Hello, I'm a language model, and it's only been a day. If you're like me, you can learn to have a quick time through those


Step 15250 | loss: 3.014110 | lr:1.1575e-04 | norm 0.2779 | dt 18751.06ms | 27960.45 tokens/sec
Step 15251 | loss: 3.101385 | lr:1.1572e-04 | norm 0.3045 | dt 333.52ms | 1571987.36 tokens/sec
Step 15252 | loss: 3.068582 | lr:1.1569e-04 | norm 0.2719 | dt 333.86ms | 1570395.51 tokens/sec
Step 15253 | loss: 3.093320 | lr:1.1567e-04 | norm 0.3102 | dt 336.44ms | 1558326.62 tokens/sec
Step 15254 | loss: 3.069257 | lr:1.1564e-04 | norm 0.2585 | dt 335.74ms | 1561600.00 tokens/sec
Step 15255 | loss: 3.069251 | lr:1.1561e-04 | norm 0.2862 | dt 335.75ms | 1561550.10 tokens/sec
Step 15256 | loss: 3.059189 | lr:1.1558e-04 | norm 0.2851 | dt 335.41ms | 1563118.51 tokens/sec
Step 15257 | loss: 3.036638 | lr:1.1555e-04 | norm 0.3088 | dt 341.04ms | 1537324.83 tokens/sec
Step 15258 | loss: 3.054302 | lr:1.1553e-04 | norm 0.2754 | dt 335.94ms | 1560645.78 tokens/sec
Step 15259 | loss: 3.072011 | lr:1.1550e-04 | norm 0.3113 | dt 336.27ms | 1559135.38 tokens/sec
Step 15260 | loss: 3.086652 | lr:1.1547e-04 | norm 0.3378 | dt 335.87ms | 1560970.37 tokens/sec
Step 15261 | loss: 3.122219 | lr:1.1544e-04 | norm 0.2771 | dt 335.67ms | 1561892.81 tokens/sec
Step 15262 | loss: 3.095577 | lr:1.1541e-04 | norm 0.2835 | dt 336.11ms | 1559853.15 tokens/sec
Step 15263 | loss: 3.029546 | lr:1.1539e-04 | norm 0.2754 | dt 335.76ms | 1561502.42 tokens/sec
Step 15264 | loss: 3.091459 | lr:1.1536e-04 | norm 0.3059 | dt 335.90ms | 1560848.50 tokens/sec
Step 15265 | loss: 3.029320 | lr:1.1533e-04 | norm 0.2968 | dt 335.90ms | 1560862.90 tokens/sec
Step 15266 | loss: 3.028589 | lr:1.1530e-04 | norm 0.2819 | dt 336.72ms | 1557033.45 tokens/sec
Step 15267 | loss: 3.068357 | lr:1.1527e-04 | norm 0.3254 | dt 336.62ms | 1557485.59 tokens/sec
Step 15268 | loss: 3.052312 | lr:1.1524e-04 | norm 0.2988 | dt 336.61ms | 1557574.95 tokens/sec
Step 15269 | loss: 3.092662 | lr:1.1522e-04 | norm 0.2993 | dt 338.24ms | 1550037.89 tokens/sec
Step 15270 | loss: 3.071749 | lr:1.1519e-04 | norm 0.3107 | dt 336.68ms | 1557214.28 tokens/sec
Step 15271 | loss: 3.054174 | lr:1.1516e-04 | norm 0.2960 | dt 337.04ms | 1555571.85 tokens/sec
Step 15272 | loss: 3.094286 | lr:1.1513e-04 | norm 0.2813 | dt 337.37ms | 1554035.01 tokens/sec
Step 15273 | loss: 3.027372 | lr:1.1511e-04 | norm 0.3000 | dt 336.13ms | 1559788.98 tokens/sec
Step 15274 | loss: 3.048683 | lr:1.1508e-04 | norm 0.2824 | dt 336.72ms | 1557024.63 tokens/sec
Step 15275 | loss: 3.067542 | lr:1.1505e-04 | norm 0.2941 | dt 337.16ms | 1555028.44 tokens/sec
Step 15276 | loss: 3.040421 | lr:1.1502e-04 | norm 0.2739 | dt 336.70ms | 1557158.04 tokens/sec
Step 15277 | loss: 3.070387 | lr:1.1499e-04 | norm 0.2734 | dt 337.23ms | 1554708.52 tokens/sec
Step 15278 | loss: 3.106007 | lr:1.1497e-04 | norm 0.2859 | dt 336.40ms | 1558516.58 tokens/sec
Step 15279 | loss: 3.099511 | lr:1.1494e-04 | norm 0.2810 | dt 336.91ms | 1556151.98 tokens/sec
Step 15280 | loss: 3.044945 | lr:1.1491e-04 | norm 0.2874 | dt 337.19ms | 1554859.12 tokens/sec
Step 15281 | loss: 3.047015 | lr:1.1488e-04 | norm 0.2822 | dt 336.55ms | 1557829.84 tokens/sec
Step 15282 | loss: 3.024286 | lr:1.1485e-04 | norm 0.3172 | dt 336.93ms | 1556093.61 tokens/sec
Step 15283 | loss: 3.056888 | lr:1.1483e-04 | norm 0.3012 | dt 337.02ms | 1555676.39 tokens/sec
Step 15284 | loss: 3.067367 | lr:1.1480e-04 | norm 0.2924 | dt 337.54ms | 1553247.98 tokens/sec
Step 15285 | loss: 3.184158 | lr:1.1477e-04 | norm 0.3665 | dt 337.31ms | 1554337.08 tokens/sec
Step 15286 | loss: 3.078442 | lr:1.1474e-04 | norm 0.3112 | dt 338.67ms | 1548060.65 tokens/sec
Step 15287 | loss: 3.033131 | lr:1.1471e-04 | norm 0.3088 | dt 338.38ms | 1549405.54 tokens/sec
Step 15288 | loss: 3.055205 | lr:1.1469e-04 | norm 0.2755 | dt 337.44ms | 1553736.35 tokens/sec
Step 15289 | loss: 3.093868 | lr:1.1466e-04 | norm 0.2992 | dt 339.35ms | 1544957.69 tokens/sec
Step 15290 | loss: 3.051279 | lr:1.1463e-04 | norm 0.3200 | dt 338.66ms | 1548136.94 tokens/sec
Step 15291 | loss: 3.034986 | lr:1.1460e-04 | norm 0.2913 | dt 337.22ms | 1554742.59 tokens/sec
Step 15292 | loss: 3.139718 | lr:1.1457e-04 | norm 0.3195 | dt 338.14ms | 1550514.40 tokens/sec
Step 15293 | loss: 3.084742 | lr:1.1455e-04 | norm 0.3007 | dt 337.53ms | 1553309.42 tokens/sec
Step 15294 | loss: 3.055196 | lr:1.1452e-04 | norm 0.3170 | dt 337.28ms | 1554443.66 tokens/sec
Step 15295 | loss: 3.153508 | lr:1.1449e-04 | norm 0.3399 | dt 338.15ms | 1550476.14 tokens/sec
Step 15296 | loss: 2.959870 | lr:1.1446e-04 | norm 0.3222 | dt 337.43ms | 1553748.43 tokens/sec
Step 15297 | loss: 3.089474 | lr:1.1444e-04 | norm 0.3215 | dt 337.60ms | 1552967.17 tokens/sec
Step 15298 | loss: 3.054345 | lr:1.1441e-04 | norm 0.3066 | dt 337.66ms | 1552719.35 tokens/sec
Step 15299 | loss: 3.099507 | lr:1.1438e-04 | norm 0.3045 | dt 337.70ms | 1552547.24 tokens/sec
Step 15300 | loss: 3.046358 | lr:1.1435e-04 | norm 0.2905 | dt 338.28ms | 1549863.10 tokens/sec
Step 15301 | loss: 3.073359 | lr:1.1432e-04 | norm 0.2886 | dt 337.31ms | 1554309.62 tokens/sec
Step 15302 | loss: 3.055211 | lr:1.1430e-04 | norm 0.3033 | dt 337.50ms | 1553430.12 tokens/sec
Step 15303 | loss: 3.088619 | lr:1.1427e-04 | norm 0.2980 | dt 338.64ms | 1548237.22 tokens/sec
Step 15304 | loss: 3.080714 | lr:1.1424e-04 | norm 0.2943 | dt 337.78ms | 1552143.97 tokens/sec
Step 15305 | loss: 3.094193 | lr:1.1421e-04 | norm 0.2945 | dt 337.63ms | 1552869.57 tokens/sec
Step 15306 | loss: 3.129082 | lr:1.1419e-04 | norm 0.2593 | dt 337.84ms | 1551890.94 tokens/sec
Step 15307 | loss: 3.103862 | lr:1.1416e-04 | norm 0.2848 | dt 338.21ms | 1550164.64 tokens/sec
Step 15308 | loss: 3.040443 | lr:1.1413e-04 | norm 0.2786 | dt 896.74ms | 584657.06 tokens/sec
Step 15309 | loss: 3.048629 | lr:1.1410e-04 | norm 0.2866 | dt 335.67ms | 1561922.77 tokens/sec
Step 15310 | loss: 3.064365 | lr:1.1407e-04 | norm 0.2820 | dt 337.48ms | 1553554.14 tokens/sec
Step 15311 | loss: 3.079217 | lr:1.1405e-04 | norm 0.2767 | dt 338.91ms | 1546962.92 tokens/sec
Step 15312 | loss: 3.045694 | lr:1.1402e-04 | norm 0.2661 | dt 337.53ms | 1553311.62 tokens/sec
Step 15313 | loss: 3.053697 | lr:1.1399e-04 | norm 0.2759 | dt 337.98ms | 1551241.76 tokens/sec
Step 15314 | loss: 3.077019 | lr:1.1396e-04 | norm 0.2851 | dt 338.07ms | 1550809.64 tokens/sec
Step 15315 | loss: 3.067586 | lr:1.1394e-04 | norm 0.2907 | dt 337.81ms | 1552013.61 tokens/sec
Step 15316 | loss: 3.053843 | lr:1.1391e-04 | norm 0.2785 | dt 337.38ms | 1554004.26 tokens/sec
Step 15317 | loss: 3.086496 | lr:1.1388e-04 | norm 0.3200 | dt 338.31ms | 1549738.58 tokens/sec
Step 15318 | loss: 3.045793 | lr:1.1385e-04 | norm 0.2899 | dt 338.69ms | 1547990.91 tokens/sec
Step 15319 | loss: 2.999668 | lr:1.1382e-04 | norm 0.3056 | dt 338.05ms | 1550916.83 tokens/sec
Step 15320 | loss: 3.096581 | lr:1.1380e-04 | norm 0.2916 | dt 338.94ms | 1546862.80 tokens/sec
Step 15321 | loss: 3.069051 | lr:1.1377e-04 | norm 0.3062 | dt 338.28ms | 1549886.04 tokens/sec
Step 15322 | loss: 3.014198 | lr:1.1374e-04 | norm 0.3135 | dt 338.31ms | 1549712.37 tokens/sec
Step 15323 | loss: 3.023351 | lr:1.1371e-04 | norm 0.3101 | dt 338.59ms | 1548443.27 tokens/sec
Step 15324 | loss: 3.058612 | lr:1.1369e-04 | norm 0.3061 | dt 338.09ms | 1550723.24 tokens/sec
Step 15325 | loss: 3.071623 | lr:1.1366e-04 | norm 0.3022 | dt 338.65ms | 1548170.73 tokens/sec
Step 15326 | loss: 3.077404 | lr:1.1363e-04 | norm 0.3080 | dt 338.21ms | 1550202.89 tokens/sec
Step 15327 | loss: 3.127094 | lr:1.1360e-04 | norm 0.3074 | dt 339.21ms | 1545630.95 tokens/sec
Step 15328 | loss: 3.061391 | lr:1.1358e-04 | norm 0.3418 | dt 338.67ms | 1548081.36 tokens/sec
Step 15329 | loss: 3.030272 | lr:1.1355e-04 | norm 0.3345 | dt 338.94ms | 1546846.48 tokens/sec
Step 15330 | loss: 3.008607 | lr:1.1352e-04 | norm 0.3355 | dt 339.45ms | 1544509.54 tokens/sec
Step 15331 | loss: 3.044105 | lr:1.1349e-04 | norm 0.3396 | dt 339.36ms | 1544913.19 tokens/sec
Step 15332 | loss: 3.191061 | lr:1.1347e-04 | norm 0.3321 | dt 338.63ms | 1548247.03 tokens/sec
Step 15333 | loss: 3.136227 | lr:1.1344e-04 | norm 0.3651 | dt 338.86ms | 1547189.31 tokens/sec
Step 15334 | loss: 3.049973 | lr:1.1341e-04 | norm 0.2913 | dt 338.85ms | 1547273.13 tokens/sec
Step 15335 | loss: 3.022192 | lr:1.1338e-04 | norm 0.3486 | dt 338.85ms | 1547277.49 tokens/sec
Step 15336 | loss: 3.070833 | lr:1.1336e-04 | norm 0.2945 | dt 338.35ms | 1549562.76 tokens/sec
Step 15337 | loss: 3.096791 | lr:1.1333e-04 | norm 0.3198 | dt 338.01ms | 1551086.39 tokens/sec
Step 15338 | loss: 3.066136 | lr:1.1330e-04 | norm 0.3147 | dt 338.39ms | 1549369.52 tokens/sec
Step 15339 | loss: 3.102293 | lr:1.1327e-04 | norm 0.3063 | dt 337.81ms | 1552032.23 tokens/sec
Step 15340 | loss: 3.098181 | lr:1.1324e-04 | norm 0.3108 | dt 338.21ms | 1550203.98 tokens/sec
Step 15341 | loss: 3.016629 | lr:1.1322e-04 | norm 0.2931 | dt 338.59ms | 1548433.46 tokens/sec
Step 15342 | loss: 3.096552 | lr:1.1319e-04 | norm 0.3505 | dt 337.11ms | 1555244.00 tokens/sec
Step 15343 | loss: 3.076115 | lr:1.1316e-04 | norm 0.2890 | dt 338.44ms | 1549118.48 tokens/sec
Step 15344 | loss: 3.072869 | lr:1.1313e-04 | norm 0.3049 | dt 337.99ms | 1551215.50 tokens/sec
Step 15345 | loss: 3.050144 | lr:1.1311e-04 | norm 0.3035 | dt 337.11ms | 1555227.50 tokens/sec
Step 15346 | loss: 3.102486 | lr:1.1308e-04 | norm 0.3005 | dt 337.78ms | 1552158.22 tokens/sec
Step 15347 | loss: 3.068390 | lr:1.1305e-04 | norm 0.3103 | dt 338.63ms | 1548260.11 tokens/sec
Step 15348 | loss: 3.091735 | lr:1.1302e-04 | norm 0.2919 | dt 337.93ms | 1551491.30 tokens/sec
Step 15349 | loss: 3.105324 | lr:1.1300e-04 | norm 0.2961 | dt 338.74ms | 1547741.41 tokens/sec
Step 15350 | loss: 3.038560 | lr:1.1297e-04 | norm 0.2791 | dt 337.82ms | 1551959.94 tokens/sec
Step 15351 | loss: 3.076385 | lr:1.1294e-04 | norm 0.2932 | dt 337.44ms | 1553722.08 tokens/sec
Step 15352 | loss: 3.022983 | lr:1.1291e-04 | norm 0.2929 | dt 337.55ms | 1553205.20 tokens/sec
Step 15353 | loss: 3.074395 | lr:1.1289e-04 | norm 0.2800 | dt 338.21ms | 1550172.29 tokens/sec
Step 15354 | loss: 3.077063 | lr:1.1286e-04 | norm 0.2947 | dt 337.17ms | 1554975.66 tokens/sec
Step 15355 | loss: 3.099041 | lr:1.1283e-04 | norm 0.2634 | dt 338.09ms | 1550731.99 tokens/sec
Step 15356 | loss: 3.059382 | lr:1.1280e-04 | norm 0.2652 | dt 338.60ms | 1548395.29 tokens/sec
Step 15357 | loss: 3.095798 | lr:1.1278e-04 | norm 0.4145 | dt 337.81ms | 1552037.71 tokens/sec
Step 15358 | loss: 3.012781 | lr:1.1275e-04 | norm 0.2855 | dt 337.78ms | 1552176.84 tokens/sec
Step 15359 | loss: 3.039417 | lr:1.1272e-04 | norm 0.3174 | dt 338.32ms | 1549665.41 tokens/sec
Step 15360 | loss: 3.073465 | lr:1.1270e-04 | norm 0.2964 | dt 337.83ms | 1551917.23 tokens/sec
Step 15361 | loss: 3.102937 | lr:1.1267e-04 | norm 0.2828 | dt 337.36ms | 1554089.92 tokens/sec
Step 15362 | loss: 3.080210 | lr:1.1264e-04 | norm 0.2880 | dt 337.79ms | 1552097.96 tokens/sec
Step 15363 | loss: 3.140021 | lr:1.1261e-04 | norm 0.2858 | dt 338.26ms | 1549955.95 tokens/sec
Step 15364 | loss: 3.043082 | lr:1.1259e-04 | norm 0.3010 | dt 337.45ms | 1553685.85 tokens/sec
Step 15365 | loss: 3.017331 | lr:1.1256e-04 | norm 0.2794 | dt 337.62ms | 1552916.72 tokens/sec
Step 15366 | loss: 3.156032 | lr:1.1253e-04 | norm 0.3278 | dt 338.13ms | 1550535.18 tokens/sec
Step 15367 | loss: 3.066445 | lr:1.1250e-04 | norm 0.3060 | dt 337.79ms | 1552093.58 tokens/sec
Step 15368 | loss: 3.089911 | lr:1.1248e-04 | norm 0.2861 | dt 337.49ms | 1553502.55 tokens/sec
Step 15369 | loss: 3.097690 | lr:1.1245e-04 | norm 0.2936 | dt 337.77ms | 1552192.18 tokens/sec
Step 15370 | loss: 3.074704 | lr:1.1242e-04 | norm 0.3047 | dt 337.22ms | 1554728.30 tokens/sec
Step 15371 | loss: 3.028923 | lr:1.1239e-04 | norm 0.3072 | dt 338.04ms | 1550950.74 tokens/sec
Step 15372 | loss: 3.078080 | lr:1.1237e-04 | norm 0.3089 | dt 337.98ms | 1551234.10 tokens/sec
Step 15373 | loss: 3.113184 | lr:1.1234e-04 | norm 0.2829 | dt 338.71ms | 1547907.01 tokens/sec
Step 15374 | loss: 3.125084 | lr:1.1231e-04 | norm 0.2813 | dt 338.31ms | 1549733.12 tokens/sec
Step 15375 | loss: 3.093114 | lr:1.1228e-04 | norm 0.3198 | dt 337.77ms | 1552196.56 tokens/sec
Step 15376 | loss: 3.077605 | lr:1.1226e-04 | norm 0.2737 | dt 337.85ms | 1551833.99 tokens/sec
Step 15377 | loss: 3.119799 | lr:1.1223e-04 | norm 0.3051 | dt 338.70ms | 1547952.77 tokens/sec
Step 15378 | loss: 3.047067 | lr:1.1220e-04 | norm 0.2868 | dt 338.09ms | 1550733.09 tokens/sec
Step 15379 | loss: 3.115183 | lr:1.1218e-04 | norm 0.2887 | dt 337.48ms | 1553527.80 tokens/sec
Step 15380 | loss: 3.092352 | lr:1.1215e-04 | norm 0.2918 | dt 338.29ms | 1549795.38 tokens/sec
Step 15381 | loss: 3.106754 | lr:1.1212e-04 | norm 0.2849 | dt 338.70ms | 1547950.59 tokens/sec
Step 15382 | loss: 3.071635 | lr:1.1209e-04 | norm 0.2634 | dt 338.83ms | 1547327.57 tokens/sec
Step 15383 | loss: 3.109473 | lr:1.1207e-04 | norm 0.2811 | dt 339.19ms | 1545691.79 tokens/sec
Step 15384 | loss: 3.110744 | lr:1.1204e-04 | norm 0.2967 | dt 338.84ms | 1547290.55 tokens/sec
Step 15385 | loss: 3.118289 | lr:1.1201e-04 | norm 0.2796 | dt 339.06ms | 1546316.76 tokens/sec
Step 15386 | loss: 3.074236 | lr:1.1198e-04 | norm 0.3003 | dt 339.21ms | 1545597.28 tokens/sec
Step 15387 | loss: 3.074112 | lr:1.1196e-04 | norm 0.2924 | dt 339.65ms | 1543627.01 tokens/sec
Step 15388 | loss: 3.034789 | lr:1.1193e-04 | norm 0.2766 | dt 338.97ms | 1546730.07 tokens/sec
Step 15389 | loss: 3.057848 | lr:1.1190e-04 | norm 0.2850 | dt 1011.19ms | 518484.46 tokens/sec
Step 15390 | loss: 3.060545 | lr:1.1188e-04 | norm 0.2745 | dt 337.82ms | 1551952.27 tokens/sec
Step 15391 | loss: 3.132601 | lr:1.1185e-04 | norm 0.2997 | dt 341.00ms | 1537495.73 tokens/sec
Step 15392 | loss: 3.033501 | lr:1.1182e-04 | norm 0.2949 | dt 338.53ms | 1548710.45 tokens/sec
Step 15393 | loss: 3.064306 | lr:1.1179e-04 | norm 0.2897 | dt 338.80ms | 1547499.61 tokens/sec
Step 15394 | loss: 3.048351 | lr:1.1177e-04 | norm 0.3068 | dt 337.74ms | 1552323.66 tokens/sec
Step 15395 | loss: 3.033052 | lr:1.1174e-04 | norm 0.2729 | dt 338.50ms | 1548864.25 tokens/sec
Step 15396 | loss: 3.071177 | lr:1.1171e-04 | norm 0.3118 | dt 339.24ms | 1545460.41 tokens/sec
Step 15397 | loss: 3.089998 | lr:1.1168e-04 | norm 0.3410 | dt 337.92ms | 1551531.80 tokens/sec
Step 15398 | loss: 3.140038 | lr:1.1166e-04 | norm 0.3860 | dt 339.68ms | 1543459.08 tokens/sec
Step 15399 | loss: 3.117692 | lr:1.1163e-04 | norm 0.3290 | dt 339.52ms | 1544199.35 tokens/sec
Step 15400 | loss: 3.065783 | lr:1.1160e-04 | norm 0.3669 | dt 339.19ms | 1545700.49 tokens/sec
Step 15401 | loss: 3.068042 | lr:1.1158e-04 | norm 0.3431 | dt 338.61ms | 1548371.31 tokens/sec
Step 15402 | loss: 3.079131 | lr:1.1155e-04 | norm 0.3259 | dt 338.67ms | 1548088.99 tokens/sec
Step 15403 | loss: 3.085934 | lr:1.1152e-04 | norm 0.3499 | dt 339.41ms | 1544699.40 tokens/sec
Step 15404 | loss: 3.077969 | lr:1.1149e-04 | norm 0.3296 | dt 339.33ms | 1545072.76 tokens/sec
Step 15405 | loss: 3.095495 | lr:1.1147e-04 | norm 0.3505 | dt 337.65ms | 1552751.15 tokens/sec
Step 15406 | loss: 3.099471 | lr:1.1144e-04 | norm 0.3263 | dt 337.74ms | 1552319.28 tokens/sec
Step 15407 | loss: 3.074911 | lr:1.1141e-04 | norm 0.3026 | dt 337.60ms | 1552975.94 tokens/sec
Step 15408 | loss: 3.107288 | lr:1.1139e-04 | norm 0.3373 | dt 338.47ms | 1549001.72 tokens/sec
Step 15409 | loss: 3.069488 | lr:1.1136e-04 | norm 0.3121 | dt 338.84ms | 1547291.64 tokens/sec
Step 15410 | loss: 3.078107 | lr:1.1133e-04 | norm 0.3551 | dt 338.36ms | 1549516.90 tokens/sec
Step 15411 | loss: 3.067423 | lr:1.1130e-04 | norm 0.2878 | dt 338.44ms | 1549125.03 tokens/sec
Step 15412 | loss: 3.163496 | lr:1.1128e-04 | norm 0.4287 | dt 338.77ms | 1547608.52 tokens/sec
Step 15413 | loss: 3.045369 | lr:1.1125e-04 | norm 0.3290 | dt 338.24ms | 1550048.82 tokens/sec
Step 15414 | loss: 3.043237 | lr:1.1122e-04 | norm 0.3267 | dt 337.79ms | 1552134.11 tokens/sec
Step 15415 | loss: 3.038671 | lr:1.1120e-04 | norm 0.3250 | dt 339.05ms | 1546362.43 tokens/sec
Step 15416 | loss: 3.083789 | lr:1.1117e-04 | norm 0.3072 | dt 345.68ms | 1516667.13 tokens/sec
Step 15417 | loss: 3.106999 | lr:1.1114e-04 | norm 0.3038 | dt 338.68ms | 1548035.59 tokens/sec
Step 15418 | loss: 3.053998 | lr:1.1112e-04 | norm 0.2976 | dt 339.48ms | 1544379.37 tokens/sec
Step 15419 | loss: 3.132426 | lr:1.1109e-04 | norm 0.3289 | dt 339.00ms | 1546554.93 tokens/sec
Step 15420 | loss: 3.088732 | lr:1.1106e-04 | norm 0.3005 | dt 338.99ms | 1546624.54 tokens/sec
Step 15421 | loss: 3.039348 | lr:1.1103e-04 | norm 0.2902 | dt 339.00ms | 1546578.86 tokens/sec
Step 15422 | loss: 3.032794 | lr:1.1101e-04 | norm 0.2954 | dt 338.25ms | 1549983.26 tokens/sec
Step 15423 | loss: 3.069905 | lr:1.1098e-04 | norm 0.2937 | dt 338.47ms | 1549008.27 tokens/sec
Step 15424 | loss: 3.063556 | lr:1.1095e-04 | norm 0.2649 | dt 338.22ms | 1550116.56 tokens/sec
Step 15425 | loss: 3.102921 | lr:1.1093e-04 | norm 0.2856 | dt 338.58ms | 1548513.05 tokens/sec
Step 15426 | loss: 3.106197 | lr:1.1090e-04 | norm 0.2800 | dt 338.17ms | 1550374.48 tokens/sec
Step 15427 | loss: 3.101691 | lr:1.1087e-04 | norm 0.3042 | dt 338.47ms | 1548995.18 tokens/sec
Step 15428 | loss: 3.065572 | lr:1.1085e-04 | norm 0.2840 | dt 339.40ms | 1544752.57 tokens/sec
Step 15429 | loss: 3.047892 | lr:1.1082e-04 | norm 0.2982 | dt 339.78ms | 1543012.87 tokens/sec
Step 15430 | loss: 3.065128 | lr:1.1079e-04 | norm 0.2934 | dt 338.84ms | 1547322.12 tokens/sec
Step 15431 | loss: 3.019201 | lr:1.1076e-04 | norm 0.2899 | dt 339.00ms | 1546576.68 tokens/sec
Step 15432 | loss: 3.006233 | lr:1.1074e-04 | norm 0.2822 | dt 338.83ms | 1547340.63 tokens/sec
Step 15433 | loss: 3.067683 | lr:1.1071e-04 | norm 0.2930 | dt 338.32ms | 1549671.96 tokens/sec
Step 15434 | loss: 3.073956 | lr:1.1068e-04 | norm 0.2741 | dt 339.34ms | 1545044.53 tokens/sec
Step 15435 | loss: 3.099010 | lr:1.1066e-04 | norm 0.2934 | dt 338.59ms | 1548466.17 tokens/sec
Step 15436 | loss: 3.085571 | lr:1.1063e-04 | norm 0.3102 | dt 338.55ms | 1548612.29 tokens/sec
Step 15437 | loss: 3.109540 | lr:1.1060e-04 | norm 0.2882 | dt 339.09ms | 1546143.90 tokens/sec
Step 15438 | loss: 3.035988 | lr:1.1058e-04 | norm 0.2761 | dt 338.92ms | 1546954.21 tokens/sec
Step 15439 | loss: 3.124467 | lr:1.1055e-04 | norm 0.2842 | dt 338.89ms | 1547081.55 tokens/sec
Step 15440 | loss: 3.097665 | lr:1.1052e-04 | norm 0.2736 | dt 338.40ms | 1549305.11 tokens/sec
Step 15441 | loss: 3.090740 | lr:1.1049e-04 | norm 0.2748 | dt 338.20ms | 1550211.63 tokens/sec
Step 15442 | loss: 3.073708 | lr:1.1047e-04 | norm 0.2777 | dt 337.61ms | 1552937.56 tokens/sec
Step 15443 | loss: 3.106649 | lr:1.1044e-04 | norm 0.2650 | dt 337.74ms | 1552321.47 tokens/sec
Step 15444 | loss: 3.229585 | lr:1.1041e-04 | norm 0.3499 | dt 338.33ms | 1549653.40 tokens/sec
Step 15445 | loss: 3.076371 | lr:1.1039e-04 | norm 0.2866 | dt 337.99ms | 1551203.46 tokens/sec
Step 15446 | loss: 3.091903 | lr:1.1036e-04 | norm 0.2718 | dt 336.99ms | 1555788.66 tokens/sec
Step 15447 | loss: 3.076977 | lr:1.1033e-04 | norm 0.2771 | dt 337.61ms | 1552948.53 tokens/sec
Step 15448 | loss: 3.087565 | lr:1.1031e-04 | norm 0.2721 | dt 337.88ms | 1551694.93 tokens/sec
Step 15449 | loss: 3.074486 | lr:1.1028e-04 | norm 0.2800 | dt 338.54ms | 1548667.91 tokens/sec
Step 15450 | loss: 3.078847 | lr:1.1025e-04 | norm 0.2838 | dt 337.86ms | 1551811.00 tokens/sec
Step 15451 | loss: 3.146853 | lr:1.1023e-04 | norm 0.2808 | dt 338.22ms | 1550132.95 tokens/sec
Step 15452 | loss: 3.082240 | lr:1.1020e-04 | norm 0.2805 | dt 337.61ms | 1552928.79 tokens/sec
Step 15453 | loss: 3.046266 | lr:1.1017e-04 | norm 0.2794 | dt 337.53ms | 1553312.71 tokens/sec
Step 15454 | loss: 3.091297 | lr:1.1015e-04 | norm 0.2868 | dt 337.90ms | 1551596.39 tokens/sec
Step 15455 | loss: 3.076301 | lr:1.1012e-04 | norm 0.2850 | dt 338.15ms | 1550475.05 tokens/sec
Step 15456 | loss: 3.037911 | lr:1.1009e-04 | norm 0.2780 | dt 337.87ms | 1551761.72 tokens/sec
Step 15457 | loss: 3.047692 | lr:1.1007e-04 | norm 0.2867 | dt 338.17ms | 1550349.34 tokens/sec
Step 15458 | loss: 3.036330 | lr:1.1004e-04 | norm 0.2839 | dt 337.49ms | 1553498.16 tokens/sec
Step 15459 | loss: 3.061768 | lr:1.1001e-04 | norm 0.2849 | dt 338.02ms | 1551055.75 tokens/sec
Step 15460 | loss: 3.094460 | lr:1.0998e-04 | norm 0.2831 | dt 337.58ms | 1553070.27 tokens/sec
Step 15461 | loss: 3.086403 | lr:1.0996e-04 | norm 0.3197 | dt 337.99ms | 1551208.93 tokens/sec
Step 15462 | loss: 3.109241 | lr:1.0993e-04 | norm 0.3028 | dt 338.14ms | 1550518.78 tokens/sec
Step 15463 | loss: 3.060519 | lr:1.0990e-04 | norm 0.3169 | dt 338.08ms | 1550774.64 tokens/sec
Step 15464 | loss: 3.054997 | lr:1.0988e-04 | norm 0.2972 | dt 338.40ms | 1549322.58 tokens/sec
Step 15465 | loss: 3.093571 | lr:1.0985e-04 | norm 0.3070 | dt 337.56ms | 1553178.87 tokens/sec
Step 15466 | loss: 3.063743 | lr:1.0982e-04 | norm 0.3107 | dt 337.74ms | 1552333.53 tokens/sec
Step 15467 | loss: 3.056269 | lr:1.0980e-04 | norm 0.3062 | dt 338.22ms | 1550125.30 tokens/sec
Step 15468 | loss: 2.997385 | lr:1.0977e-04 | norm 0.2998 | dt 337.91ms | 1551538.37 tokens/sec
Step 15469 | loss: 3.045056 | lr:1.0974e-04 | norm 0.3515 | dt 337.95ms | 1551384.03 tokens/sec
Step 15470 | loss: 3.073047 | lr:1.0972e-04 | norm 0.2992 | dt 337.91ms | 1551574.49 tokens/sec
Step 15471 | loss: 3.047535 | lr:1.0969e-04 | norm 0.3018 | dt 338.70ms | 1547930.98 tokens/sec
Step 15472 | loss: 3.065696 | lr:1.0966e-04 | norm 0.3077 | dt 337.43ms | 1553782.46 tokens/sec
Step 15473 | loss: 3.123916 | lr:1.0964e-04 | norm 0.3175 | dt 337.79ms | 1552125.35 tokens/sec
Step 15474 | loss: 3.055685 | lr:1.0961e-04 | norm 0.3038 | dt 338.52ms | 1548784.62 tokens/sec
Step 15475 | loss: 3.063074 | lr:1.0958e-04 | norm 0.2904 | dt 338.51ms | 1548797.71 tokens/sec
Step 15476 | loss: 3.087081 | lr:1.0956e-04 | norm 0.2837 | dt 337.13ms | 1555170.31 tokens/sec
Step 15477 | loss: 3.071656 | lr:1.0953e-04 | norm 0.3020 | dt 337.90ms | 1551611.72 tokens/sec
Step 15478 | loss: 3.092710 | lr:1.0950e-04 | norm 0.2981 | dt 339.36ms | 1544930.56 tokens/sec
Step 15479 | loss: 3.083420 | lr:1.0948e-04 | norm 0.3102 | dt 338.39ms | 1549365.15 tokens/sec
Step 15480 | loss: 3.068459 | lr:1.0945e-04 | norm 0.2926 | dt 338.64ms | 1548201.25 tokens/sec
Step 15481 | loss: 3.039092 | lr:1.0942e-04 | norm 0.2972 | dt 339.30ms | 1545201.95 tokens/sec
Step 15482 | loss: 3.037858 | lr:1.0940e-04 | norm 0.2722 | dt 338.18ms | 1550335.13 tokens/sec
Step 15483 | loss: 3.121758 | lr:1.0937e-04 | norm 0.3517 | dt 339.61ms | 1543807.99 tokens/sec
Step 15484 | loss: 3.056110 | lr:1.0934e-04 | norm 0.2988 | dt 338.07ms | 1550811.83 tokens/sec
Step 15485 | loss: 3.053299 | lr:1.0932e-04 | norm 0.2794 | dt 337.72ms | 1552427.77 tokens/sec
Step 15486 | loss: 3.055362 | lr:1.0929e-04 | norm 0.2836 | dt 338.20ms | 1550228.02 tokens/sec
Step 15487 | loss: 3.092677 | lr:1.0926e-04 | norm 0.2924 | dt 337.04ms | 1555585.05 tokens/sec
Step 15488 | loss: 3.068292 | lr:1.0924e-04 | norm 0.2746 | dt 337.97ms | 1551308.51 tokens/sec
Step 15489 | loss: 3.105113 | lr:1.0921e-04 | norm 0.2808 | dt 338.50ms | 1548851.16 tokens/sec
Step 15490 | loss: 3.069539 | lr:1.0918e-04 | norm 0.2714 | dt 338.31ms | 1549708.00 tokens/sec
Step 15491 | loss: 3.036363 | lr:1.0916e-04 | norm 0.2915 | dt 337.30ms | 1554355.76 tokens/sec
Step 15492 | loss: 3.032450 | lr:1.0913e-04 | norm 0.2789 | dt 338.40ms | 1549331.31 tokens/sec
Step 15493 | loss: 3.061133 | lr:1.0910e-04 | norm 0.2959 | dt 338.10ms | 1550691.53 tokens/sec
Step 15494 | loss: 3.073178 | lr:1.0908e-04 | norm 0.2784 | dt 338.08ms | 1550768.08 tokens/sec
Step 15495 | loss: 3.058115 | lr:1.0905e-04 | norm 0.2949 | dt 338.30ms | 1549777.90 tokens/sec
Step 15496 | loss: 3.046985 | lr:1.0902e-04 | norm 0.2916 | dt 338.05ms | 1550897.14 tokens/sec
Step 15497 | loss: 3.163163 | lr:1.0900e-04 | norm 0.3248 | dt 901.65ms | 581476.83 tokens/sec
Step 15498 | loss: 3.052288 | lr:1.0897e-04 | norm 0.3180 | dt 335.26ms | 1563817.71 tokens/sec
Step 15499 | loss: 3.091339 | lr:1.0895e-04 | norm 0.3050 | dt 338.06ms | 1550874.17 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 15500: 3.1061
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3037/10042=0.3024


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not an artist, but I've written a number of games or books I would like to write on other
rank 3 sample 1 >Hello, I'm a language model, so my question is: And can we do that with the language?
I'm not sure what I'm seeing on
rank 3 sample 2 >Hello, I'm a language model, so this one is easier to figure out. Thanks to your patience, and my mind is full of fun, please,
rank 3 sample 3 >Hello, I'm a language model, so a lot of people talk about the difference between an XML and an XML-based language, but do I understand why




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so here are a few basic things:
1. I can't actually write to start with.
- I've


ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 1 >Hello, I'm a language model, but I work as a teacher. One issue is one of being very knowledgeable about the language of my native tongue. If
rank 2 sample 2 >Hello, I'm a language model, and I've been using it in high school. I've worked for 25 years with language modeling on language and a range
rank 7 sample 0 >Hello, I'm a language model, so I think I need some help in defining it, and I'm pretty sure people would get off their feet if I
rank 2 sample 3 >Hello, I'm a language model, but not only. It's a good-looking website that provides a lot of resources, videos, and more to help


rank 7 sample 1 >Hello, I'm a language model, having grown up in a real family like most people. No, I'm not crazy, I'm still an immigrant who


ddp_rank 0: ####### Printing generated samples ####### 

rank 7 sample 2 >Hello, I'm a language model, so I'll post stuff on this to my friends, but if you read this, you'll see that there will be
rank 0 sample 0 >Hello, I'm a language model, so I know that you know, "We only have one person!" Is he a teacher, is he an administrator or
rank 7 sample 3 >Hello, I'm a language model, and I think I should take a cue from being a language model, right? Well, I suppose there's a kind


rank 0 sample 1 >Hello, I'm a language model, and in the middle of the article, I want to look into this process in a different way, one of which is
rank 0 sample 2 >Hello, I'm a language model, and I wanna see how far I can go...
I think the next step is to think about how to write Python
rank 0 sample 3 >Hello, I'm a language model, so in this lesson, I'll cover how to write a program that works in another language. I'll also talk about




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, with two languages: French and German, but I'd love for any other language to learn!
I've been wondering
rank 6 sample 1 >Hello, I'm a language model, so I can't even start with a grammar. It's a pretty big deal. But, that's all I'll
rank 6 sample 2 >Hello, I'm a language model, but what's a language modeling software?
In a language modeling software, one of the things that needs to learn is
rank 6 sample 3 >Hello, I'm a language model, so I can describe a language that I am using. I'm trying to convey the meaning correctly, from language to language




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I think I'll use it. You're making a movie, so we're trying to help you make that film
rank 4 sample 1 >Hello, I'm a language model, anyway!
Can anybody help me?
Well, yes, I would. My husband and I went to another language
rank 4 sample 2 >Hello, I'm a language model, I thought I were doing some work and I guess I'm getting into a lot of different languages. (I was thinking
rank 4 sample 3 >Hello, I'm a language model, so you start using your native language before using another one, maybe even trying out a new one... I'm hoping you




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but a computer scientist!
I'm currently a member of the National Association for Computer Science (NACS), and an
rank 5 sample 1 >Hello, I'm a language model, so when I see these videos that they're confusing, I don't want my model to look like any older language.
rank 5 sample 2 >Hello, I'm a language model, and I was thinking around this with a lot of my friends. In my family's high school, I'm a student
rank 5 sample 3 >Hello, I'm a language model, if you see it, we'll help. If the user clicks the "Language". The user also has to click the




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so I understand how to do it." He continued to do these exercises with one of our staff at the end of the
rank 1 sample 1 >Hello, I'm a language model, a computer programmer and a computer scientist. I'm a programming language writer and a program interpreter, a software engineer and a
rank 1 sample 2 >Hello, I'm a language model, but thanks, everyone.
I'm a language model, thank you for joining me.<|endoftext|>|Part of||Music
rank 1 sample 3 >Hello, I'm a language model, so I'm looking at all the parts
of the course (with links!).
Step 1: Start with the course


Step 15500 | loss: 3.052776 | lr:1.0892e-04 | norm 0.2919 | dt 18694.03ms | 28045.74 tokens/sec
Step 15501 | loss: 3.153838 | lr:1.0889e-04 | norm 0.2927 | dt 332.96ms | 1574617.95 tokens/sec
Step 15502 | loss: 3.067110 | lr:1.0887e-04 | norm 0.3064 | dt 334.22ms | 1568706.15 tokens/sec
Step 15503 | loss: 3.036213 | lr:1.0884e-04 | norm 0.2876 | dt 338.46ms | 1549060.64 tokens/sec
Step 15504 | loss: 3.072899 | lr:1.0881e-04 | norm 0.2930 | dt 335.71ms | 1561736.41 tokens/sec
Step 15505 | loss: 3.013573 | lr:1.0879e-04 | norm 0.3188 | dt 335.91ms | 1560777.60 tokens/sec
Step 15506 | loss: 3.083984 | lr:1.0876e-04 | norm 0.3103 | dt 336.45ms | 1558307.84 tokens/sec
Step 15507 | loss: 3.103859 | lr:1.0873e-04 | norm 0.3593 | dt 337.16ms | 1555031.74 tokens/sec
Step 15508 | loss: 3.057348 | lr:1.0871e-04 | norm 0.3383 | dt 336.64ms | 1557396.25 tokens/sec
Step 15509 | loss: 3.055990 | lr:1.0868e-04 | norm 0.3297 | dt 337.83ms | 1551942.42 tokens/sec
Step 15510 | loss: 3.068760 | lr:1.0865e-04 | norm 0.2917 | dt 338.17ms | 1550383.22 tokens/sec
Step 15511 | loss: 3.045311 | lr:1.0863e-04 | norm 0.3115 | dt 336.30ms | 1559011.58 tokens/sec
Step 15512 | loss: 3.124825 | lr:1.0860e-04 | norm 0.3096 | dt 337.01ms | 1555703.91 tokens/sec
Step 15513 | loss: 3.048251 | lr:1.0857e-04 | norm 0.2948 | dt 336.29ms | 1559049.16 tokens/sec
Step 15514 | loss: 3.065959 | lr:1.0855e-04 | norm 0.2765 | dt 337.50ms | 1553447.68 tokens/sec
Step 15515 | loss: 3.061675 | lr:1.0852e-04 | norm 0.2802 | dt 336.74ms | 1556970.61 tokens/sec
Step 15516 | loss: 3.063718 | lr:1.0850e-04 | norm 0.2815 | dt 337.55ms | 1553207.39 tokens/sec
Step 15517 | loss: 3.057839 | lr:1.0847e-04 | norm 0.2943 | dt 336.31ms | 1558936.43 tokens/sec
Step 15518 | loss: 3.077554 | lr:1.0844e-04 | norm 0.2723 | dt 337.13ms | 1555152.71 tokens/sec
Step 15519 | loss: 3.102747 | lr:1.0842e-04 | norm 0.2849 | dt 337.49ms | 1553474.02 tokens/sec
Step 15520 | loss: 3.041749 | lr:1.0839e-04 | norm 0.2981 | dt 337.35ms | 1554122.87 tokens/sec
Step 15521 | loss: 3.094052 | lr:1.0836e-04 | norm 0.2809 | dt 336.98ms | 1555829.39 tokens/sec
Step 15522 | loss: 3.205407 | lr:1.0834e-04 | norm 0.3671 | dt 336.71ms | 1557098.50 tokens/sec
Step 15523 | loss: 3.080725 | lr:1.0831e-04 | norm 0.3367 | dt 337.87ms | 1551751.86 tokens/sec
Step 15524 | loss: 3.050393 | lr:1.0828e-04 | norm 0.3314 | dt 337.46ms | 1553616.70 tokens/sec
Step 15525 | loss: 3.091598 | lr:1.0826e-04 | norm 0.3035 | dt 336.06ms | 1560118.75 tokens/sec
Step 15526 | loss: 3.152812 | lr:1.0823e-04 | norm 0.3306 | dt 337.35ms | 1554145.94 tokens/sec
Step 15527 | loss: 3.012236 | lr:1.0821e-04 | norm 0.3177 | dt 337.49ms | 1553482.80 tokens/sec
Step 15528 | loss: 3.036276 | lr:1.0818e-04 | norm 0.3083 | dt 337.10ms | 1555295.70 tokens/sec
Step 15529 | loss: 3.055714 | lr:1.0815e-04 | norm 0.3076 | dt 337.50ms | 1553438.90 tokens/sec
Step 15530 | loss: 3.129347 | lr:1.0813e-04 | norm 0.3512 | dt 337.28ms | 1554439.26 tokens/sec
Step 15531 | loss: 3.057771 | lr:1.0810e-04 | norm 0.2808 | dt 338.48ms | 1548947.17 tokens/sec
Step 15532 | loss: 3.069356 | lr:1.0807e-04 | norm 0.3109 | dt 337.72ms | 1552438.73 tokens/sec
Step 15533 | loss: 3.064545 | lr:1.0805e-04 | norm 0.2941 | dt 337.38ms | 1554000.97 tokens/sec
Step 15534 | loss: 3.056943 | lr:1.0802e-04 | norm 0.2934 | dt 337.19ms | 1554887.70 tokens/sec
Step 15535 | loss: 3.013820 | lr:1.0799e-04 | norm 0.3036 | dt 337.26ms | 1554566.73 tokens/sec
Step 15536 | loss: 3.096861 | lr:1.0797e-04 | norm 0.3231 | dt 337.59ms | 1553042.85 tokens/sec
Step 15537 | loss: 3.067876 | lr:1.0794e-04 | norm 0.3138 | dt 338.31ms | 1549722.20 tokens/sec
Step 15538 | loss: 3.083048 | lr:1.0792e-04 | norm 0.3198 | dt 338.05ms | 1550914.64 tokens/sec
Step 15539 | loss: 3.064687 | lr:1.0789e-04 | norm 0.3040 | dt 337.66ms | 1552690.85 tokens/sec
Step 15540 | loss: 3.125719 | lr:1.0786e-04 | norm 0.3032 | dt 338.12ms | 1550608.43 tokens/sec
Step 15541 | loss: 3.086757 | lr:1.0784e-04 | norm 0.3273 | dt 336.91ms | 1556187.22 tokens/sec
Step 15542 | loss: 3.155561 | lr:1.0781e-04 | norm 0.3153 | dt 338.38ms | 1549406.63 tokens/sec
Step 15543 | loss: 3.108099 | lr:1.0778e-04 | norm 0.3165 | dt 337.84ms | 1551886.56 tokens/sec
Step 15544 | loss: 3.119275 | lr:1.0776e-04 | norm 0.2769 | dt 337.17ms | 1554955.87 tokens/sec
Step 15545 | loss: 3.107623 | lr:1.0773e-04 | norm 0.2988 | dt 338.03ms | 1550995.59 tokens/sec
Step 15546 | loss: 3.120161 | lr:1.0771e-04 | norm 0.3119 | dt 338.39ms | 1549367.33 tokens/sec
Step 15547 | loss: 3.089359 | lr:1.0768e-04 | norm 0.2804 | dt 336.84ms | 1556469.19 tokens/sec
Step 15548 | loss: 3.101119 | lr:1.0765e-04 | norm 0.3095 | dt 337.82ms | 1551975.28 tokens/sec
Step 15549 | loss: 3.116843 | lr:1.0763e-04 | norm 0.2881 | dt 338.67ms | 1548087.90 tokens/sec
Step 15550 | loss: 3.052349 | lr:1.0760e-04 | norm 0.3152 | dt 337.70ms | 1552509.97 tokens/sec
Step 15551 | loss: 3.063506 | lr:1.0757e-04 | norm 0.2990 | dt 338.21ms | 1550201.80 tokens/sec
Step 15552 | loss: 3.075491 | lr:1.0755e-04 | norm 0.2583 | dt 338.15ms | 1550476.14 tokens/sec
Step 15553 | loss: 3.078576 | lr:1.0752e-04 | norm 0.3265 | dt 338.13ms | 1550550.48 tokens/sec
Step 15554 | loss: 3.090907 | lr:1.0750e-04 | norm 0.2846 | dt 337.74ms | 1552362.02 tokens/sec
Step 15555 | loss: 3.063308 | lr:1.0747e-04 | norm 0.2889 | dt 337.97ms | 1551272.40 tokens/sec
Step 15556 | loss: 3.085291 | lr:1.0744e-04 | norm 0.2939 | dt 337.78ms | 1552173.55 tokens/sec
Step 15557 | loss: 3.067868 | lr:1.0742e-04 | norm 0.2888 | dt 337.90ms | 1551609.53 tokens/sec
Step 15558 | loss: 3.071392 | lr:1.0739e-04 | norm 0.2736 | dt 338.54ms | 1548663.55 tokens/sec
Step 15559 | loss: 3.080492 | lr:1.0737e-04 | norm 0.2776 | dt 338.84ms | 1547280.75 tokens/sec
Step 15560 | loss: 3.087874 | lr:1.0734e-04 | norm 0.2863 | dt 339.10ms | 1546130.85 tokens/sec
Step 15561 | loss: 3.041266 | lr:1.0731e-04 | norm 0.2826 | dt 337.96ms | 1551335.87 tokens/sec
Step 15562 | loss: 3.045462 | lr:1.0729e-04 | norm 0.2841 | dt 338.27ms | 1549891.50 tokens/sec
Step 15563 | loss: 3.034994 | lr:1.0726e-04 | norm 0.2743 | dt 338.25ms | 1549978.89 tokens/sec
Step 15564 | loss: 3.156926 | lr:1.0723e-04 | norm 0.3081 | dt 338.58ms | 1548504.33 tokens/sec
Step 15565 | loss: 3.086263 | lr:1.0721e-04 | norm 0.2953 | dt 338.38ms | 1549416.46 tokens/sec
Step 15566 | loss: 3.129596 | lr:1.0718e-04 | norm 0.2668 | dt 338.09ms | 1550746.21 tokens/sec
Step 15567 | loss: 3.081073 | lr:1.0716e-04 | norm 0.3125 | dt 337.61ms | 1552944.14 tokens/sec
Step 15568 | loss: 3.042748 | lr:1.0713e-04 | norm 0.2972 | dt 338.39ms | 1549364.06 tokens/sec
Step 15569 | loss: 3.111079 | lr:1.0710e-04 | norm 0.2773 | dt 337.70ms | 1552520.94 tokens/sec
Step 15570 | loss: 3.005823 | lr:1.0708e-04 | norm 0.3005 | dt 338.00ms | 1551160.79 tokens/sec
Step 15571 | loss: 3.063143 | lr:1.0705e-04 | norm 0.2900 | dt 338.34ms | 1549594.43 tokens/sec
Step 15572 | loss: 3.051919 | lr:1.0703e-04 | norm 0.2602 | dt 337.51ms | 1553391.72 tokens/sec
Step 15573 | loss: 3.034480 | lr:1.0700e-04 | norm 0.2896 | dt 338.13ms | 1550558.14 tokens/sec
Step 15574 | loss: 3.054733 | lr:1.0697e-04 | norm 0.2773 | dt 337.67ms | 1552649.19 tokens/sec
Step 15575 | loss: 3.071365 | lr:1.0695e-04 | norm 0.2890 | dt 337.80ms | 1552061.81 tokens/sec
Step 15576 | loss: 3.092779 | lr:1.0692e-04 | norm 0.2816 | dt 337.81ms | 1552030.04 tokens/sec
Step 15577 | loss: 3.114989 | lr:1.0690e-04 | norm 0.3012 | dt 337.64ms | 1552809.26 tokens/sec
Step 15578 | loss: 3.121079 | lr:1.0687e-04 | norm 0.2829 | dt 338.14ms | 1550483.79 tokens/sec
Step 15579 | loss: 3.124875 | lr:1.0684e-04 | norm 0.3047 | dt 933.72ms | 561505.31 tokens/sec
Step 15580 | loss: 3.092996 | lr:1.0682e-04 | norm 0.2897 | dt 337.12ms | 1555203.30 tokens/sec
Step 15581 | loss: 3.094584 | lr:1.0679e-04 | norm 0.2732 | dt 338.03ms | 1551026.22 tokens/sec
Step 15582 | loss: 3.105875 | lr:1.0677e-04 | norm 0.3180 | dt 339.61ms | 1543806.90 tokens/sec
Step 15583 | loss: 3.057922 | lr:1.0674e-04 | norm 0.2932 | dt 338.07ms | 1550812.92 tokens/sec
Step 15584 | loss: 3.103955 | lr:1.0671e-04 | norm 0.3094 | dt 336.98ms | 1555849.20 tokens/sec
Step 15585 | loss: 3.089410 | lr:1.0669e-04 | norm 0.3011 | dt 337.36ms | 1554087.73 tokens/sec
Step 15586 | loss: 3.045195 | lr:1.0666e-04 | norm 0.3016 | dt 337.78ms | 1552173.55 tokens/sec
Step 15587 | loss: 3.092218 | lr:1.0664e-04 | norm 0.3106 | dt 337.99ms | 1551196.90 tokens/sec
Step 15588 | loss: 3.078721 | lr:1.0661e-04 | norm 0.3144 | dt 338.94ms | 1546854.10 tokens/sec
Step 15589 | loss: 3.104213 | lr:1.0658e-04 | norm 0.3002 | dt 338.45ms | 1549106.48 tokens/sec
Step 15590 | loss: 3.097468 | lr:1.0656e-04 | norm 0.2915 | dt 338.76ms | 1547681.50 tokens/sec
Step 15591 | loss: 3.021606 | lr:1.0653e-04 | norm 0.3076 | dt 338.60ms | 1548377.85 tokens/sec
Step 15592 | loss: 3.079473 | lr:1.0651e-04 | norm 0.2878 | dt 338.20ms | 1550219.28 tokens/sec
Step 15593 | loss: 3.098328 | lr:1.0648e-04 | norm 0.3167 | dt 338.39ms | 1549340.04 tokens/sec
Step 15594 | loss: 3.145972 | lr:1.0645e-04 | norm 0.2774 | dt 337.85ms | 1551854.80 tokens/sec
Step 15595 | loss: 3.097106 | lr:1.0643e-04 | norm 0.2947 | dt 338.13ms | 1550540.64 tokens/sec
Step 15596 | loss: 3.066091 | lr:1.0640e-04 | norm 0.2740 | dt 338.72ms | 1547862.34 tokens/sec
Step 15597 | loss: 3.079624 | lr:1.0638e-04 | norm 0.2902 | dt 338.99ms | 1546613.67 tokens/sec
Step 15598 | loss: 3.069756 | lr:1.0635e-04 | norm 0.2971 | dt 337.84ms | 1551900.80 tokens/sec
Step 15599 | loss: 3.055348 | lr:1.0632e-04 | norm 0.2725 | dt 337.50ms | 1553467.44 tokens/sec
Step 15600 | loss: 3.036955 | lr:1.0630e-04 | norm 0.2682 | dt 339.12ms | 1546010.19 tokens/sec
Step 15601 | loss: 3.069268 | lr:1.0627e-04 | norm 0.2890 | dt 339.73ms | 1543232.69 tokens/sec
Step 15602 | loss: 3.049109 | lr:1.0625e-04 | norm 0.2790 | dt 339.13ms | 1545996.06 tokens/sec
Step 15603 | loss: 3.076223 | lr:1.0622e-04 | norm 0.2917 | dt 338.88ms | 1547108.76 tokens/sec
Step 15604 | loss: 3.091990 | lr:1.0620e-04 | norm 0.2885 | dt 338.89ms | 1547064.13 tokens/sec
Step 15605 | loss: 3.096065 | lr:1.0617e-04 | norm 0.2978 | dt 338.89ms | 1547063.04 tokens/sec
Step 15606 | loss: 3.059068 | lr:1.0614e-04 | norm 0.2962 | dt 339.65ms | 1543608.59 tokens/sec
Step 15607 | loss: 3.042008 | lr:1.0612e-04 | norm 0.2633 | dt 338.71ms | 1547883.04 tokens/sec
Step 15608 | loss: 3.113284 | lr:1.0609e-04 | norm 0.3105 | dt 338.65ms | 1548177.27 tokens/sec
Step 15609 | loss: 3.051284 | lr:1.0607e-04 | norm 0.2767 | dt 337.51ms | 1553382.94 tokens/sec
Step 15610 | loss: 3.121568 | lr:1.0604e-04 | norm 0.3272 | dt 338.00ms | 1551147.66 tokens/sec
Step 15611 | loss: 3.084262 | lr:1.0601e-04 | norm 0.3156 | dt 338.61ms | 1548368.04 tokens/sec
Step 15612 | loss: 3.172788 | lr:1.0599e-04 | norm 0.2968 | dt 337.49ms | 1553471.82 tokens/sec
Step 15613 | loss: 3.146053 | lr:1.0596e-04 | norm 0.3212 | dt 338.37ms | 1549451.40 tokens/sec
Step 15614 | loss: 3.131572 | lr:1.0594e-04 | norm 0.3034 | dt 338.01ms | 1551102.80 tokens/sec
Step 15615 | loss: 3.080126 | lr:1.0591e-04 | norm 0.3140 | dt 337.97ms | 1551292.10 tokens/sec
Step 15616 | loss: 3.082026 | lr:1.0589e-04 | norm 0.3076 | dt 338.69ms | 1547977.83 tokens/sec
Step 15617 | loss: 3.070583 | lr:1.0586e-04 | norm 0.2762 | dt 339.04ms | 1546409.19 tokens/sec
Step 15618 | loss: 3.121343 | lr:1.0583e-04 | norm 0.3216 | dt 339.97ms | 1542173.15 tokens/sec
Step 15619 | loss: 3.044599 | lr:1.0581e-04 | norm 0.2903 | dt 339.50ms | 1544294.78 tokens/sec
Step 15620 | loss: 3.067740 | lr:1.0578e-04 | norm 0.2837 | dt 338.63ms | 1548271.01 tokens/sec
Step 15621 | loss: 3.083202 | lr:1.0576e-04 | norm 0.2865 | dt 338.03ms | 1551024.03 tokens/sec
Step 15622 | loss: 3.079019 | lr:1.0573e-04 | norm 0.2984 | dt 338.80ms | 1547483.28 tokens/sec
Step 15623 | loss: 3.062953 | lr:1.0571e-04 | norm 0.2746 | dt 338.17ms | 1550389.78 tokens/sec
Step 15624 | loss: 3.090550 | lr:1.0568e-04 | norm 0.3003 | dt 337.41ms | 1553867.00 tokens/sec
Step 15625 | loss: 3.047520 | lr:1.0565e-04 | norm 0.2764 | dt 338.49ms | 1548904.62 tokens/sec
Step 15626 | loss: 3.079973 | lr:1.0563e-04 | norm 0.2942 | dt 338.25ms | 1550013.85 tokens/sec
Step 15627 | loss: 3.035795 | lr:1.0560e-04 | norm 0.2898 | dt 338.50ms | 1548855.53 tokens/sec
Step 15628 | loss: 3.046892 | lr:1.0558e-04 | norm 0.2940 | dt 337.69ms | 1552580.13 tokens/sec
Step 15629 | loss: 3.088821 | lr:1.0555e-04 | norm 0.2820 | dt 338.05ms | 1550938.70 tokens/sec
Step 15630 | loss: 3.100484 | lr:1.0553e-04 | norm 0.2963 | dt 337.81ms | 1552041.00 tokens/sec
Step 15631 | loss: 3.037915 | lr:1.0550e-04 | norm 0.2907 | dt 338.07ms | 1550834.79 tokens/sec
Step 15632 | loss: 3.089678 | lr:1.0547e-04 | norm 0.2912 | dt 337.30ms | 1554364.55 tokens/sec
Step 15633 | loss: 3.037814 | lr:1.0545e-04 | norm 0.2945 | dt 338.15ms | 1550438.97 tokens/sec
Step 15634 | loss: 3.046861 | lr:1.0542e-04 | norm 0.3040 | dt 338.73ms | 1547800.24 tokens/sec
Step 15635 | loss: 3.033872 | lr:1.0540e-04 | norm 0.3141 | dt 338.16ms | 1550397.43 tokens/sec
Step 15636 | loss: 3.174440 | lr:1.0537e-04 | norm 0.3057 | dt 336.92ms | 1556106.83 tokens/sec
Step 15637 | loss: 3.118526 | lr:1.0535e-04 | norm 0.3401 | dt 338.25ms | 1549981.08 tokens/sec
Step 15638 | loss: 3.022353 | lr:1.0532e-04 | norm 0.3182 | dt 338.51ms | 1548829.34 tokens/sec
Step 15639 | loss: 3.076439 | lr:1.0529e-04 | norm 0.3209 | dt 337.67ms | 1552656.86 tokens/sec
Step 15640 | loss: 3.048673 | lr:1.0527e-04 | norm 0.3169 | dt 337.95ms | 1551358.86 tokens/sec
Step 15641 | loss: 3.069833 | lr:1.0524e-04 | norm 0.2873 | dt 338.38ms | 1549389.17 tokens/sec
Step 15642 | loss: 3.063019 | lr:1.0522e-04 | norm 0.3475 | dt 337.91ms | 1551538.37 tokens/sec
Step 15643 | loss: 3.067907 | lr:1.0519e-04 | norm 0.3063 | dt 337.92ms | 1551517.57 tokens/sec
Step 15644 | loss: 3.056064 | lr:1.0517e-04 | norm 0.3105 | dt 337.33ms | 1554239.31 tokens/sec
Step 15645 | loss: 3.081129 | lr:1.0514e-04 | norm 0.3410 | dt 339.12ms | 1546044.98 tokens/sec
Step 15646 | loss: 3.086466 | lr:1.0512e-04 | norm 0.3313 | dt 337.59ms | 1553037.36 tokens/sec
Step 15647 | loss: 3.122656 | lr:1.0509e-04 | norm 0.3099 | dt 338.44ms | 1549142.49 tokens/sec
Step 15648 | loss: 3.079184 | lr:1.0506e-04 | norm 0.3385 | dt 338.83ms | 1547360.23 tokens/sec
Step 15649 | loss: 3.075304 | lr:1.0504e-04 | norm 0.3287 | dt 339.30ms | 1545186.75 tokens/sec
Step 15650 | loss: 3.095043 | lr:1.0501e-04 | norm 0.3037 | dt 338.18ms | 1550337.32 tokens/sec
Step 15651 | loss: 3.099300 | lr:1.0499e-04 | norm 0.3124 | dt 338.02ms | 1551049.19 tokens/sec
Step 15652 | loss: 3.048779 | lr:1.0496e-04 | norm 0.3130 | dt 338.98ms | 1546676.76 tokens/sec
Step 15653 | loss: 3.030385 | lr:1.0494e-04 | norm 0.3011 | dt 338.36ms | 1549484.15 tokens/sec
Step 15654 | loss: 3.066020 | lr:1.0491e-04 | norm 0.2777 | dt 337.98ms | 1551260.36 tokens/sec
Step 15655 | loss: 3.083241 | lr:1.0489e-04 | norm 0.2974 | dt 339.47ms | 1544421.67 tokens/sec
Step 15656 | loss: 3.033799 | lr:1.0486e-04 | norm 0.2670 | dt 339.04ms | 1546409.19 tokens/sec
Step 15657 | loss: 3.043349 | lr:1.0483e-04 | norm 0.2812 | dt 337.88ms | 1551701.49 tokens/sec
Step 15658 | loss: 3.047457 | lr:1.0481e-04 | norm 0.3085 | dt 339.01ms | 1546507.07 tokens/sec
Step 15659 | loss: 3.157124 | lr:1.0478e-04 | norm 0.2861 | dt 338.71ms | 1547907.01 tokens/sec
Step 15660 | loss: 3.114489 | lr:1.0476e-04 | norm 0.3015 | dt 338.98ms | 1546648.48 tokens/sec
Step 15661 | loss: 3.109138 | lr:1.0473e-04 | norm 0.2962 | dt 337.45ms | 1553657.31 tokens/sec
Step 15662 | loss: 3.057906 | lr:1.0471e-04 | norm 0.3091 | dt 337.91ms | 1551542.75 tokens/sec
Step 15663 | loss: 3.110804 | lr:1.0468e-04 | norm 0.2762 | dt 338.05ms | 1550928.86 tokens/sec
Step 15664 | loss: 3.040457 | lr:1.0466e-04 | norm 0.3069 | dt 338.58ms | 1548468.35 tokens/sec
Step 15665 | loss: 3.043730 | lr:1.0463e-04 | norm 0.3077 | dt 338.29ms | 1549796.47 tokens/sec
Step 15666 | loss: 3.031999 | lr:1.0461e-04 | norm 0.2820 | dt 337.84ms | 1551882.18 tokens/sec
Step 15667 | loss: 3.042082 | lr:1.0458e-04 | norm 0.3317 | dt 338.07ms | 1550807.45 tokens/sec
Step 15668 | loss: 3.081448 | lr:1.0455e-04 | norm 0.2793 | dt 337.91ms | 1551560.26 tokens/sec
Step 15669 | loss: 3.064457 | lr:1.0453e-04 | norm 0.2958 | dt 337.51ms | 1553382.94 tokens/sec
Step 15670 | loss: 3.028534 | lr:1.0450e-04 | norm 0.2953 | dt 338.17ms | 1550350.43 tokens/sec
Step 15671 | loss: 3.026047 | lr:1.0448e-04 | norm 0.2688 | dt 337.78ms | 1552154.93 tokens/sec
Step 15672 | loss: 3.022233 | lr:1.0445e-04 | norm 0.2811 | dt 337.79ms | 1552100.15 tokens/sec
Step 15673 | loss: 3.045341 | lr:1.0443e-04 | norm 0.2748 | dt 337.96ms | 1551352.29 tokens/sec
Step 15674 | loss: 3.069210 | lr:1.0440e-04 | norm 0.2757 | dt 339.19ms | 1545705.92 tokens/sec
Step 15675 | loss: 3.104289 | lr:1.0438e-04 | norm 0.2945 | dt 337.77ms | 1552192.18 tokens/sec
Step 15676 | loss: 3.027309 | lr:1.0435e-04 | norm 0.2793 | dt 338.11ms | 1550641.23 tokens/sec
Step 15677 | loss: 3.032508 | lr:1.0433e-04 | norm 0.3044 | dt 337.97ms | 1551273.49 tokens/sec
Step 15678 | loss: 3.142951 | lr:1.0430e-04 | norm 0.3129 | dt 338.38ms | 1549385.89 tokens/sec
Step 15679 | loss: 3.087398 | lr:1.0428e-04 | norm 0.2804 | dt 338.04ms | 1550957.30 tokens/sec
Step 15680 | loss: 3.058722 | lr:1.0425e-04 | norm 0.2848 | dt 338.24ms | 1550025.87 tokens/sec
Step 15681 | loss: 3.021477 | lr:1.0422e-04 | norm 0.3002 | dt 337.54ms | 1553264.44 tokens/sec
Step 15682 | loss: 3.151529 | lr:1.0420e-04 | norm 0.3029 | dt 338.04ms | 1550980.27 tokens/sec
Step 15683 | loss: 3.090700 | lr:1.0417e-04 | norm 0.3092 | dt 337.38ms | 1553992.18 tokens/sec
Step 15684 | loss: 3.035171 | lr:1.0415e-04 | norm 0.2847 | dt 337.96ms | 1551343.54 tokens/sec
Step 15685 | loss: 3.203231 | lr:1.0412e-04 | norm 0.3919 | dt 338.31ms | 1549747.32 tokens/sec
Step 15686 | loss: 3.029190 | lr:1.0410e-04 | norm 0.3113 | dt 911.21ms | 575378.72 tokens/sec
Step 15687 | loss: 3.145888 | lr:1.0407e-04 | norm 0.3364 | dt 335.10ms | 1564577.64 tokens/sec
Step 15688 | loss: 3.084954 | lr:1.0405e-04 | norm 0.3145 | dt 337.37ms | 1554060.27 tokens/sec
Step 15689 | loss: 3.063165 | lr:1.0402e-04 | norm 0.3136 | dt 338.07ms | 1550839.17 tokens/sec
Step 15690 | loss: 3.065132 | lr:1.0400e-04 | norm 0.3282 | dt 337.62ms | 1552900.27 tokens/sec
Step 15691 | loss: 3.020907 | lr:1.0397e-04 | norm 0.3176 | dt 336.95ms | 1555991.21 tokens/sec
Step 15692 | loss: 3.057785 | lr:1.0395e-04 | norm 0.3218 | dt 337.13ms | 1555129.61 tokens/sec
Step 15693 | loss: 3.086794 | lr:1.0392e-04 | norm 0.3122 | dt 337.77ms | 1552202.04 tokens/sec
Step 15694 | loss: 3.054662 | lr:1.0390e-04 | norm 0.3132 | dt 337.43ms | 1553761.60 tokens/sec
Step 15695 | loss: 3.054065 | lr:1.0387e-04 | norm 0.2912 | dt 338.29ms | 1549810.67 tokens/sec
Step 15696 | loss: 3.088958 | lr:1.0385e-04 | norm 0.3170 | dt 337.69ms | 1552562.59 tokens/sec
Step 15697 | loss: 3.094548 | lr:1.0382e-04 | norm 0.2933 | dt 337.25ms | 1554597.51 tokens/sec
Step 15698 | loss: 3.044786 | lr:1.0379e-04 | norm 0.3337 | dt 338.75ms | 1547735.96 tokens/sec
Step 15699 | loss: 3.024348 | lr:1.0377e-04 | norm 0.2779 | dt 338.24ms | 1550060.84 tokens/sec
Step 15700 | loss: 3.069238 | lr:1.0374e-04 | norm 0.2813 | dt 337.41ms | 1553843.94 tokens/sec
Step 15701 | loss: 3.034039 | lr:1.0372e-04 | norm 0.3084 | dt 337.77ms | 1552183.41 tokens/sec
Step 15702 | loss: 3.026890 | lr:1.0369e-04 | norm 0.3150 | dt 338.76ms | 1547647.73 tokens/sec
Step 15703 | loss: 3.075042 | lr:1.0367e-04 | norm 0.3104 | dt 337.68ms | 1552602.05 tokens/sec
Step 15704 | loss: 3.078803 | lr:1.0364e-04 | norm 0.2849 | dt 337.67ms | 1552678.79 tokens/sec
Step 15705 | loss: 3.023260 | lr:1.0362e-04 | norm 0.2876 | dt 338.63ms | 1548253.57 tokens/sec
Step 15706 | loss: 3.063127 | lr:1.0359e-04 | norm 0.2915 | dt 338.64ms | 1548238.31 tokens/sec
Step 15707 | loss: 3.021645 | lr:1.0357e-04 | norm 0.2936 | dt 337.82ms | 1551957.75 tokens/sec
Step 15708 | loss: 3.148433 | lr:1.0354e-04 | norm 0.3268 | dt 337.30ms | 1554376.63 tokens/sec
Step 15709 | loss: 3.066683 | lr:1.0352e-04 | norm 0.2922 | dt 339.65ms | 1543603.17 tokens/sec
Step 15710 | loss: 3.050316 | lr:1.0349e-04 | norm 0.3119 | dt 339.45ms | 1544520.39 tokens/sec
Step 15711 | loss: 3.046328 | lr:1.0347e-04 | norm 0.2830 | dt 338.83ms | 1547366.76 tokens/sec
Step 15712 | loss: 3.106895 | lr:1.0344e-04 | norm 0.3104 | dt 339.18ms | 1545761.33 tokens/sec
Step 15713 | loss: 3.103772 | lr:1.0342e-04 | norm 0.2997 | dt 339.17ms | 1545809.14 tokens/sec
Step 15714 | loss: 2.995933 | lr:1.0339e-04 | norm 0.3171 | dt 337.64ms | 1552788.43 tokens/sec
Step 15715 | loss: 3.124084 | lr:1.0337e-04 | norm 0.3062 | dt 338.79ms | 1547531.19 tokens/sec
Step 15716 | loss: 3.017751 | lr:1.0334e-04 | norm 0.3766 | dt 338.09ms | 1550712.31 tokens/sec
Step 15717 | loss: 3.046514 | lr:1.0332e-04 | norm 0.3195 | dt 338.25ms | 1549978.89 tokens/sec
Step 15718 | loss: 3.167267 | lr:1.0329e-04 | norm 0.3529 | dt 339.08ms | 1546216.73 tokens/sec
Step 15719 | loss: 3.104844 | lr:1.0327e-04 | norm 0.3305 | dt 339.14ms | 1545915.64 tokens/sec
Step 15720 | loss: 3.041938 | lr:1.0324e-04 | norm 0.2966 | dt 338.11ms | 1550635.76 tokens/sec
Step 15721 | loss: 3.037611 | lr:1.0322e-04 | norm 0.3066 | dt 338.60ms | 1548392.02 tokens/sec
Step 15722 | loss: 3.060530 | lr:1.0319e-04 | norm 0.3099 | dt 338.71ms | 1547885.22 tokens/sec
Step 15723 | loss: 3.090317 | lr:1.0317e-04 | norm 0.3134 | dt 339.05ms | 1546362.43 tokens/sec
Step 15724 | loss: 3.092678 | lr:1.0314e-04 | norm 0.3227 | dt 338.59ms | 1548436.73 tokens/sec
Step 15725 | loss: 3.118438 | lr:1.0312e-04 | norm 0.3073 | dt 338.32ms | 1549682.88 tokens/sec
Step 15726 | loss: 3.092704 | lr:1.0309e-04 | norm 0.3059 | dt 338.97ms | 1546689.81 tokens/sec
Step 15727 | loss: 3.076506 | lr:1.0307e-04 | norm 0.2828 | dt 338.32ms | 1549698.17 tokens/sec
Step 15728 | loss: 3.121440 | lr:1.0304e-04 | norm 0.2861 | dt 337.97ms | 1551293.19 tokens/sec
Step 15729 | loss: 3.080383 | lr:1.0302e-04 | norm 0.2726 | dt 338.42ms | 1549216.70 tokens/sec
Step 15730 | loss: 3.050227 | lr:1.0299e-04 | norm 0.2854 | dt 337.76ms | 1552246.96 tokens/sec
Step 15731 | loss: 3.070100 | lr:1.0297e-04 | norm 0.2637 | dt 339.26ms | 1545377.87 tokens/sec
Step 15732 | loss: 3.112477 | lr:1.0294e-04 | norm 0.2824 | dt 338.74ms | 1547762.11 tokens/sec
Step 15733 | loss: 3.072633 | lr:1.0292e-04 | norm 0.2911 | dt 338.81ms | 1547438.63 tokens/sec
Step 15734 | loss: 3.083544 | lr:1.0289e-04 | norm 0.2576 | dt 339.04ms | 1546376.57 tokens/sec
Step 15735 | loss: 3.015360 | lr:1.0287e-04 | norm 0.2900 | dt 338.48ms | 1548937.35 tokens/sec
Step 15736 | loss: 3.060414 | lr:1.0284e-04 | norm 0.2673 | dt 338.17ms | 1550389.78 tokens/sec
Step 15737 | loss: 3.039968 | lr:1.0282e-04 | norm 0.2775 | dt 338.87ms | 1547155.56 tokens/sec
Step 15738 | loss: 3.072884 | lr:1.0279e-04 | norm 0.2665 | dt 338.79ms | 1547552.97 tokens/sec
Step 15739 | loss: 3.022285 | lr:1.0277e-04 | norm 0.2750 | dt 338.17ms | 1550387.60 tokens/sec
Step 15740 | loss: 3.062832 | lr:1.0274e-04 | norm 0.2885 | dt 338.90ms | 1547021.68 tokens/sec
Step 15741 | loss: 3.038277 | lr:1.0272e-04 | norm 0.2975 | dt 338.37ms | 1549447.03 tokens/sec
Step 15742 | loss: 3.028778 | lr:1.0269e-04 | norm 0.2696 | dt 339.19ms | 1545696.14 tokens/sec
Step 15743 | loss: 3.080859 | lr:1.0267e-04 | norm 0.3242 | dt 338.72ms | 1547852.53 tokens/sec
Step 15744 | loss: 3.041952 | lr:1.0264e-04 | norm 0.2845 | dt 338.16ms | 1550429.13 tokens/sec
Step 15745 | loss: 3.060746 | lr:1.0262e-04 | norm 0.3106 | dt 339.15ms | 1545885.21 tokens/sec
Step 15746 | loss: 3.055584 | lr:1.0259e-04 | norm 0.3241 | dt 338.73ms | 1547815.49 tokens/sec
Step 15747 | loss: 3.105951 | lr:1.0257e-04 | norm 0.3127 | dt 339.15ms | 1545865.65 tokens/sec
Step 15748 | loss: 3.094038 | lr:1.0254e-04 | norm 0.3021 | dt 339.29ms | 1545269.27 tokens/sec
Step 15749 | loss: 3.154028 | lr:1.0252e-04 | norm 0.2996 | dt 338.70ms | 1547959.31 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 15750: 3.1030
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3022/10042=0.3009


ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 5: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I was wondering about why I'm the only person I know who did not love this type of music, and why
rank 7 sample 1 >Hello, I'm a language model, too, that's the key in every software development method--everything needs to be understood in terms of context--so if
rank 5 sample 0 >Hello, I'm a language model, but what does that mean?
- We can call a language what we call a language. The thing about a language
rank 7 sample 2 >Hello, I'm a language model, and I want to put together a quick demonstration of a new language and try to make a language that works for a group
rank 5 sample 1 >Hello, I'm a language model, one you'd have to use a lot of software to write programs.
What about a program that takes some text and
rank 7 sample 3 >Hello, I'm a language model, so I'd like to show you three more uses of the machine language.
First, let's show you how you


rank 5 sample 2 >Hello, I'm a language model, so I thought it went something like this:
I'm a real computer programmer and one of the best programmers I have
rank 5 sample 3 >Hello, I'm a language model, my brother's great-great-great grandmother was so good at that, my teacher is a bad person, so I




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, using C and XML. I've done some things and you know, like I just mean, "I like to learn
rank 2 sample 1 >Hello, I'm a language model, so I tried and tested this myself so first I couldn't understand my friends. I thought, 'Yeah, I'll
rank 2 sample 2 >Hello, I'm a language model, so I need to understand the way he goes and what he does to say and I'm going to look at all of
rank 2 sample 3 >Hello, I'm a language model, but that looks like the part I'm referring to (a).
I say: 'you've been looking for,




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and so with this language model, by definition this model represents only the most basic of concepts in this language.
The


ddp_rank 6: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, a person, a person, a system, a process/process. I want to go into detail on how to do
rank 6 sample 0 >Hello, I'm a language model, if you're not an intermediate learner. I want to be a teacher who knows how to teach a foreign language thatrank 1 sample 2 >Hello, I'm a language model, but after the Second World War, I was a researcher and language researcher in the field of computer systems engineering and artificial intelligence

rank 6 sample 1 >Hello, I'm a language model, and I love to be able to use those tools for a while. When I was younger I worked on programming, and
rank 1 sample 3 >Hello, I'm a language model, and I'm interested to discover the different phonetic symbols they produce over time or over successive attempts to translate them.



rank 6 sample 2 >Hello, I'm a language model, but this is a question I get asked. Is it a language. Is it a sentence? Is it a function or
rank 6 sample 3 >Hello, I'm a language model, and so I hope this is true for me.
I'm assuming that this is true -- I found my code --




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I'd like to see how you think your future language would look based on the structure given by the different languages you
rank 4 sample 1 >Hello, I'm a language model, to understand the things in this site, the grammar rules for the English verb "are" (the part of speech which
rank 4 sample 2 >Hello, I'm a language model, I feel like learning new words. The language model is pretty cool. I'm good at translating things; I have to
rank 4 sample 3 >Hello, I'm a language model, and you got all kinds of problems solving that would've to do in life, and I think in a lot of ways




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not the kind to look at a human in order to analyze code. Instead, I're interested in language
rank 3 sample 1 >Hello, I'm a language model, and a teacher. I will show you a little bit more about how to do this.
First, let's talk
rank 3 sample 2 >Hello, I'm a language model, and it has a "yes" and "no" for my language, "f" for which I can change them
rank 3 sample 3 >Hello, I'm a language model, and so I can't write programs, so I can do that with the help of another human. Well, it's




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I need to learn English and write better phrases.
The goal of learning English is to help me improve my grammar
rank 0 sample 1 >Hello, I'm a language model, so to speak. But I can work with any piece of software or in any other domain I'm writing for. And
rank 0 sample 2 >Hello, I'm a language model, so I really need to find a way to find a way, but I'm not a linguist. I just need
rank 0 sample 3 >Hello, I'm a language model, so for the first time I can teach my people how to code. The only major challenge in my classroom is to let


Step 15750 | loss: 3.080919 | lr:1.0249e-04 | norm 0.3363 | dt 18506.39ms | 28330.11 tokens/sec
Step 15751 | loss: 3.111284 | lr:1.0247e-04 | norm 0.2986 | dt 333.25ms | 1573242.47 tokens/sec
Step 15752 | loss: 3.137029 | lr:1.0244e-04 | norm 0.3257 | dt 335.36ms | 1563337.43 tokens/sec
Step 15753 | loss: 3.101863 | lr:1.0242e-04 | norm 0.3064 | dt 337.45ms | 1553678.17 tokens/sec
Step 15754 | loss: 3.117458 | lr:1.0239e-04 | norm 0.3324 | dt 336.75ms | 1556925.42 tokens/sec
Step 15755 | loss: 3.111508 | lr:1.0237e-04 | norm 0.3252 | dt 336.17ms | 1559595.39 tokens/sec
Step 15756 | loss: 3.113522 | lr:1.0234e-04 | norm 0.3004 | dt 336.01ms | 1560318.00 tokens/sec
Step 15757 | loss: 3.077588 | lr:1.0232e-04 | norm 0.3154 | dt 336.29ms | 1559025.95 tokens/sec
Step 15758 | loss: 3.041851 | lr:1.0229e-04 | norm 0.3206 | dt 336.56ms | 1557786.80 tokens/sec
Step 15759 | loss: 3.083568 | lr:1.0227e-04 | norm 0.3013 | dt 336.11ms | 1559853.15 tokens/sec
Step 15760 | loss: 3.059355 | lr:1.0224e-04 | norm 0.2813 | dt 336.65ms | 1557362.06 tokens/sec
Step 15761 | loss: 3.091601 | lr:1.0222e-04 | norm 0.2866 | dt 336.91ms | 1556148.67 tokens/sec
Step 15762 | loss: 3.063018 | lr:1.0219e-04 | norm 0.2963 | dt 336.91ms | 1556166.29 tokens/sec
Step 15763 | loss: 3.079614 | lr:1.0217e-04 | norm 0.2718 | dt 337.26ms | 1554559.04 tokens/sec
Step 15764 | loss: 3.094913 | lr:1.0214e-04 | norm 0.2865 | dt 338.44ms | 1549116.30 tokens/sec
Step 15765 | loss: 3.063181 | lr:1.0212e-04 | norm 0.2879 | dt 338.01ms | 1551122.49 tokens/sec
Step 15766 | loss: 3.093356 | lr:1.0209e-04 | norm 0.2734 | dt 337.30ms | 1554374.44 tokens/sec
Step 15767 | loss: 3.082705 | lr:1.0207e-04 | norm 0.2833 | dt 337.78ms | 1552148.35 tokens/sec
Step 15768 | loss: 3.087897 | lr:1.0205e-04 | norm 0.2815 | dt 338.06ms | 1550879.64 tokens/sec
Step 15769 | loss: 3.104932 | lr:1.0202e-04 | norm 0.2974 | dt 999.22ms | 524694.94 tokens/sec
Step 15770 | loss: 2.983266 | lr:1.0200e-04 | norm 0.2877 | dt 336.21ms | 1559428.38 tokens/sec
Step 15771 | loss: 3.017980 | lr:1.0197e-04 | norm 0.2781 | dt 337.51ms | 1553389.52 tokens/sec
Step 15772 | loss: 3.076115 | lr:1.0195e-04 | norm 0.2753 | dt 335.96ms | 1560562.72 tokens/sec
Step 15773 | loss: 3.174885 | lr:1.0192e-04 | norm 0.2839 | dt 337.14ms | 1555125.22 tokens/sec
Step 15774 | loss: 3.071733 | lr:1.0190e-04 | norm 0.2755 | dt 337.99ms | 1551210.03 tokens/sec
Step 15775 | loss: 3.031505 | lr:1.0187e-04 | norm 0.2887 | dt 336.84ms | 1556511.06 tokens/sec
Step 15776 | loss: 3.068927 | lr:1.0185e-04 | norm 0.3100 | dt 337.56ms | 1553161.31 tokens/sec
Step 15777 | loss: 3.049189 | lr:1.0182e-04 | norm 0.2845 | dt 337.28ms | 1554443.66 tokens/sec
Step 15778 | loss: 3.117940 | lr:1.0180e-04 | norm 0.3187 | dt 337.59ms | 1553023.11 tokens/sec
Step 15779 | loss: 3.088668 | lr:1.0177e-04 | norm 0.3071 | dt 338.09ms | 1550753.86 tokens/sec
Step 15780 | loss: 3.063406 | lr:1.0175e-04 | norm 0.2865 | dt 337.75ms | 1552279.83 tokens/sec
Step 15781 | loss: 3.049056 | lr:1.0172e-04 | norm 0.2914 | dt 336.53ms | 1557911.51 tokens/sec
Step 15782 | loss: 3.056220 | lr:1.0170e-04 | norm 0.3200 | dt 338.22ms | 1550154.81 tokens/sec
Step 15783 | loss: 3.038399 | lr:1.0167e-04 | norm 0.2744 | dt 338.25ms | 1550009.48 tokens/sec
Step 15784 | loss: 3.053906 | lr:1.0165e-04 | norm 0.3195 | dt 337.66ms | 1552712.77 tokens/sec
Step 15785 | loss: 3.086380 | lr:1.0163e-04 | norm 0.3028 | dt 337.38ms | 1554016.34 tokens/sec
Step 15786 | loss: 3.097249 | lr:1.0160e-04 | norm 0.3007 | dt 339.17ms | 1545777.63 tokens/sec
Step 15787 | loss: 3.042655 | lr:1.0158e-04 | norm 0.3116 | dt 338.22ms | 1550136.23 tokens/sec
Step 15788 | loss: 2.957220 | lr:1.0155e-04 | norm 0.3174 | dt 338.67ms | 1548080.27 tokens/sec
Step 15789 | loss: 3.085146 | lr:1.0153e-04 | norm 0.3047 | dt 338.47ms | 1548988.63 tokens/sec
Step 15790 | loss: 3.096737 | lr:1.0150e-04 | norm 0.3044 | dt 338.13ms | 1550542.83 tokens/sec
Step 15791 | loss: 3.042757 | lr:1.0148e-04 | norm 0.3020 | dt 337.60ms | 1552986.91 tokens/sec
Step 15792 | loss: 3.100717 | lr:1.0145e-04 | norm 0.2908 | dt 338.74ms | 1547775.18 tokens/sec
Step 15793 | loss: 3.054252 | lr:1.0143e-04 | norm 0.2941 | dt 337.90ms | 1551622.66 tokens/sec
Step 15794 | loss: 3.052031 | lr:1.0140e-04 | norm 0.2760 | dt 338.30ms | 1549794.28 tokens/sec
Step 15795 | loss: 3.072795 | lr:1.0138e-04 | norm 0.2789 | dt 337.41ms | 1553842.84 tokens/sec
Step 15796 | loss: 3.074623 | lr:1.0135e-04 | norm 0.2827 | dt 337.40ms | 1553890.06 tokens/sec
Step 15797 | loss: 3.016576 | lr:1.0133e-04 | norm 0.3789 | dt 338.07ms | 1550804.17 tokens/sec
Step 15798 | loss: 3.098292 | lr:1.0131e-04 | norm 0.3000 | dt 337.33ms | 1554243.70 tokens/sec
Step 15799 | loss: 3.051585 | lr:1.0128e-04 | norm 0.3097 | dt 337.28ms | 1554457.94 tokens/sec
Step 15800 | loss: 3.067826 | lr:1.0126e-04 | norm 0.2716 | dt 338.02ms | 1551039.34 tokens/sec
Step 15801 | loss: 3.128211 | lr:1.0123e-04 | norm 0.3055 | dt 337.63ms | 1552825.71 tokens/sec
Step 15802 | loss: 3.145320 | lr:1.0121e-04 | norm 0.3137 | dt 337.84ms | 1551885.46 tokens/sec
Step 15803 | loss: 3.046635 | lr:1.0118e-04 | norm 0.2962 | dt 338.11ms | 1550660.91 tokens/sec
Step 15804 | loss: 3.080013 | lr:1.0116e-04 | norm 0.3078 | dt 337.94ms | 1551412.49 tokens/sec
Step 15805 | loss: 3.029013 | lr:1.0113e-04 | norm 0.3004 | dt 337.27ms | 1554520.58 tokens/sec
Step 15806 | loss: 3.038508 | lr:1.0111e-04 | norm 0.2894 | dt 337.79ms | 1552131.92 tokens/sec
Step 15807 | loss: 3.037105 | lr:1.0108e-04 | norm 0.3186 | dt 338.17ms | 1550383.22 tokens/sec
Step 15808 | loss: 3.062131 | lr:1.0106e-04 | norm 0.2783 | dt 337.73ms | 1552369.69 tokens/sec
Step 15809 | loss: 3.032693 | lr:1.0104e-04 | norm 0.2923 | dt 337.80ms | 1552084.82 tokens/sec
Step 15810 | loss: 3.081141 | lr:1.0101e-04 | norm 0.2827 | dt 337.60ms | 1552973.75 tokens/sec
Step 15811 | loss: 3.060709 | lr:1.0099e-04 | norm 0.2844 | dt 338.01ms | 1551091.86 tokens/sec
Step 15812 | loss: 3.116835 | lr:1.0096e-04 | norm 0.2972 | dt 338.00ms | 1551147.66 tokens/sec
Step 15813 | loss: 3.040266 | lr:1.0094e-04 | norm 0.2908 | dt 337.76ms | 1552230.53 tokens/sec
Step 15814 | loss: 3.129238 | lr:1.0091e-04 | norm 0.2848 | dt 338.18ms | 1550311.09 tokens/sec
Step 15815 | loss: 3.082953 | lr:1.0089e-04 | norm 0.2812 | dt 338.14ms | 1550528.62 tokens/sec
Step 15816 | loss: 3.082612 | lr:1.0086e-04 | norm 0.2986 | dt 338.00ms | 1551154.22 tokens/sec
Step 15817 | loss: 3.049667 | lr:1.0084e-04 | norm 0.2846 | dt 337.61ms | 1552933.17 tokens/sec
Step 15818 | loss: 3.104247 | lr:1.0082e-04 | norm 0.3121 | dt 338.41ms | 1549250.54 tokens/sec
Step 15819 | loss: 3.117427 | lr:1.0079e-04 | norm 0.3091 | dt 337.52ms | 1553353.31 tokens/sec
Step 15820 | loss: 3.036452 | lr:1.0077e-04 | norm 0.3238 | dt 337.82ms | 1551956.66 tokens/sec
Step 15821 | loss: 3.085185 | lr:1.0074e-04 | norm 0.2891 | dt 338.07ms | 1550805.26 tokens/sec
Step 15822 | loss: 3.061480 | lr:1.0072e-04 | norm 0.3016 | dt 337.98ms | 1551231.91 tokens/sec
Step 15823 | loss: 3.037956 | lr:1.0069e-04 | norm 0.2932 | dt 337.17ms | 1554982.26 tokens/sec
Step 15824 | loss: 3.058304 | lr:1.0067e-04 | norm 0.2961 | dt 338.07ms | 1550810.73 tokens/sec
Step 15825 | loss: 3.037429 | lr:1.0064e-04 | norm 0.2982 | dt 339.70ms | 1543368.08 tokens/sec
Step 15826 | loss: 3.070461 | lr:1.0062e-04 | norm 0.2931 | dt 337.93ms | 1551489.11 tokens/sec
Step 15827 | loss: 3.026259 | lr:1.0060e-04 | norm 0.2955 | dt 336.96ms | 1555958.19 tokens/sec
Step 15828 | loss: 3.059528 | lr:1.0057e-04 | norm 0.2954 | dt 338.27ms | 1549926.46 tokens/sec
Step 15829 | loss: 3.042583 | lr:1.0055e-04 | norm 0.3104 | dt 338.58ms | 1548482.52 tokens/sec
Step 15830 | loss: 3.082323 | lr:1.0052e-04 | norm 0.2855 | dt 338.12ms | 1550616.08 tokens/sec
Step 15831 | loss: 3.073871 | lr:1.0050e-04 | norm 0.2972 | dt 338.06ms | 1550889.48 tokens/sec
Step 15832 | loss: 3.087460 | lr:1.0047e-04 | norm 0.2919 | dt 338.30ms | 1549775.72 tokens/sec
Step 15833 | loss: 3.049965 | lr:1.0045e-04 | norm 0.2630 | dt 337.80ms | 1552078.24 tokens/sec
Step 15834 | loss: 3.079217 | lr:1.0043e-04 | norm 0.2826 | dt 337.55ms | 1553212.87 tokens/sec
Step 15835 | loss: 3.052405 | lr:1.0040e-04 | norm 0.3027 | dt 338.52ms | 1548743.17 tokens/sec
Step 15836 | loss: 3.029973 | lr:1.0038e-04 | norm 0.2583 | dt 338.12ms | 1550610.62 tokens/sec
Step 15837 | loss: 3.093156 | lr:1.0035e-04 | norm 0.2764 | dt 337.48ms | 1553520.11 tokens/sec
Step 15838 | loss: 3.056334 | lr:1.0033e-04 | norm 0.2832 | dt 338.32ms | 1549689.44 tokens/sec
Step 15839 | loss: 3.125702 | lr:1.0030e-04 | norm 0.3183 | dt 338.45ms | 1549093.38 tokens/sec
Step 15840 | loss: 3.072632 | lr:1.0028e-04 | norm 0.3019 | dt 337.12ms | 1555217.60 tokens/sec
Step 15841 | loss: 3.063982 | lr:1.0026e-04 | norm 0.2935 | dt 338.33ms | 1549650.12 tokens/sec
Step 15842 | loss: 3.076946 | lr:1.0023e-04 | norm 0.2862 | dt 337.77ms | 1552193.27 tokens/sec
Step 15843 | loss: 3.084034 | lr:1.0021e-04 | norm 0.3012 | dt 337.56ms | 1553156.93 tokens/sec
Step 15844 | loss: 3.064247 | lr:1.0018e-04 | norm 0.3176 | dt 338.38ms | 1549413.18 tokens/sec
Step 15845 | loss: 3.015706 | lr:1.0016e-04 | norm 0.2793 | dt 337.43ms | 1553764.89 tokens/sec
Step 15846 | loss: 3.083247 | lr:1.0013e-04 | norm 0.3011 | dt 337.57ms | 1553103.18 tokens/sec
Step 15847 | loss: 3.001135 | lr:1.0011e-04 | norm 0.2905 | dt 338.89ms | 1547061.95 tokens/sec
Step 15848 | loss: 3.080210 | lr:1.0009e-04 | norm 0.2761 | dt 338.73ms | 1547811.13 tokens/sec
Step 15849 | loss: 3.062551 | lr:1.0006e-04 | norm 0.3040 | dt 337.78ms | 1552152.74 tokens/sec
Step 15850 | loss: 3.036483 | lr:1.0004e-04 | norm 0.2965 | dt 338.23ms | 1550104.54 tokens/sec
Step 15851 | loss: 3.058426 | lr:1.0001e-04 | norm 0.2944 | dt 337.87ms | 1551740.91 tokens/sec
Step 15852 | loss: 3.061536 | lr:9.9989e-05 | norm 0.2965 | dt 337.67ms | 1552647.00 tokens/sec
Step 15853 | loss: 3.027320 | lr:9.9965e-05 | norm 0.2838 | dt 337.76ms | 1552248.06 tokens/sec
Step 15854 | loss: 3.084617 | lr:9.9941e-05 | norm 0.3054 | dt 338.07ms | 1550812.92 tokens/sec
Step 15855 | loss: 3.121375 | lr:9.9916e-05 | norm 0.2606 | dt 338.12ms | 1550609.52 tokens/sec
Step 15856 | loss: 3.032417 | lr:9.9892e-05 | norm 0.2932 | dt 337.67ms | 1552649.19 tokens/sec
Step 15857 | loss: 3.053026 | lr:9.9868e-05 | norm 0.2913 | dt 338.35ms | 1549548.57 tokens/sec
Step 15858 | loss: 3.030575 | lr:9.9844e-05 | norm 0.2781 | dt 338.45ms | 1549103.20 tokens/sec
Step 15859 | loss: 3.068769 | lr:9.9820e-05 | norm 0.2857 | dt 337.66ms | 1552706.20 tokens/sec
Step 15860 | loss: 3.103529 | lr:9.9796e-05 | norm 0.2950 | dt 338.03ms | 1551021.84 tokens/sec
Step 15861 | loss: 3.060801 | lr:9.9771e-05 | norm 0.3039 | dt 338.05ms | 1550913.55 tokens/sec
Step 15862 | loss: 3.020704 | lr:9.9747e-05 | norm 0.2759 | dt 337.24ms | 1554631.58 tokens/sec
Step 15863 | loss: 3.055221 | lr:9.9723e-05 | norm 0.2869 | dt 338.59ms | 1548441.09 tokens/sec
Step 15864 | loss: 3.045329 | lr:9.9699e-05 | norm 0.2718 | dt 338.04ms | 1550952.92 tokens/sec
Step 15865 | loss: 3.142656 | lr:9.9675e-05 | norm 0.2965 | dt 337.88ms | 1551678.50 tokens/sec
Step 15866 | loss: 3.079268 | lr:9.9651e-05 | norm 0.2792 | dt 337.53ms | 1553307.23 tokens/sec
Step 15867 | loss: 3.063141 | lr:9.9627e-05 | norm 0.2837 | dt 337.52ms | 1553376.35 tokens/sec
Step 15868 | loss: 3.082402 | lr:9.9603e-05 | norm 0.2706 | dt 339.19ms | 1545718.96 tokens/sec
Step 15869 | loss: 3.064811 | lr:9.9579e-05 | norm 0.2896 | dt 338.67ms | 1548073.73 tokens/sec
Step 15870 | loss: 3.100222 | lr:9.9554e-05 | norm 0.2663 | dt 338.46ms | 1549024.64 tokens/sec
Step 15871 | loss: 3.074711 | lr:9.9530e-05 | norm 0.2854 | dt 338.40ms | 1549336.77 tokens/sec
Step 15872 | loss: 3.060177 | lr:9.9506e-05 | norm 0.2839 | dt 338.19ms | 1550285.95 tokens/sec
Step 15873 | loss: 3.053416 | lr:9.9482e-05 | norm 0.2787 | dt 338.47ms | 1548995.18 tokens/sec
Step 15874 | loss: 3.018428 | lr:9.9458e-05 | norm 0.2800 | dt 339.07ms | 1546266.75 tokens/sec
Step 15875 | loss: 3.031301 | lr:9.9434e-05 | norm 0.2778 | dt 1027.89ms | 510060.37 tokens/sec
Step 15876 | loss: 3.007343 | lr:9.9410e-05 | norm 0.2818 | dt 335.07ms | 1564727.93 tokens/sec
Step 15877 | loss: 3.031299 | lr:9.9386e-05 | norm 0.3035 | dt 337.36ms | 1554104.20 tokens/sec
Step 15878 | loss: 3.041718 | lr:9.9362e-05 | norm 0.2692 | dt 339.06ms | 1546300.45 tokens/sec
Step 15879 | loss: 3.090926 | lr:9.9338e-05 | norm 0.2642 | dt 337.13ms | 1555142.81 tokens/sec
Step 15880 | loss: 3.060957 | lr:9.9314e-05 | norm 0.2884 | dt 338.70ms | 1547961.49 tokens/sec
Step 15881 | loss: 3.054021 | lr:9.9290e-05 | norm 0.2831 | dt 340.38ms | 1540318.41 tokens/sec
Step 15882 | loss: 3.048485 | lr:9.9266e-05 | norm 0.3091 | dt 337.95ms | 1551387.31 tokens/sec
Step 15883 | loss: 3.035527 | lr:9.9242e-05 | norm 0.2677 | dt 338.96ms | 1546742.03 tokens/sec
Step 15884 | loss: 3.051633 | lr:9.9218e-05 | norm 0.2986 | dt 338.94ms | 1546845.39 tokens/sec
Step 15885 | loss: 3.094347 | lr:9.9194e-05 | norm 0.3052 | dt 338.08ms | 1550783.39 tokens/sec
Step 15886 | loss: 3.019824 | lr:9.9170e-05 | norm 0.2961 | dt 339.09ms | 1546176.51 tokens/sec
Step 15887 | loss: 3.064889 | lr:9.9146e-05 | norm 0.3174 | dt 338.68ms | 1548029.05 tokens/sec
Step 15888 | loss: 3.037547 | lr:9.9122e-05 | norm 0.3391 | dt 339.22ms | 1545569.03 tokens/sec
Step 15889 | loss: 3.066417 | lr:9.9098e-05 | norm 0.2879 | dt 337.68ms | 1552632.74 tokens/sec
Step 15890 | loss: 2.996602 | lr:9.9074e-05 | norm 0.3093 | dt 338.52ms | 1548773.71 tokens/sec
Step 15891 | loss: 3.066418 | lr:9.9050e-05 | norm 0.3171 | dt 339.74ms | 1543202.37 tokens/sec
Step 15892 | loss: 3.023143 | lr:9.9026e-05 | norm 0.2806 | dt 338.94ms | 1546830.16 tokens/sec
Step 15893 | loss: 3.064301 | lr:9.9003e-05 | norm 0.3281 | dt 338.95ms | 1546797.52 tokens/sec
Step 15894 | loss: 3.015010 | lr:9.8979e-05 | norm 0.2833 | dt 339.04ms | 1546408.11 tokens/sec
Step 15895 | loss: 3.023036 | lr:9.8955e-05 | norm 0.2951 | dt 338.75ms | 1547708.73 tokens/sec
Step 15896 | loss: 3.024798 | lr:9.8931e-05 | norm 0.2940 | dt 338.74ms | 1547776.27 tokens/sec
Step 15897 | loss: 3.095405 | lr:9.8907e-05 | norm 0.2818 | dt 339.72ms | 1543316.09 tokens/sec
Step 15898 | loss: 3.106766 | lr:9.8883e-05 | norm 0.3024 | dt 339.32ms | 1545135.72 tokens/sec
Step 15899 | loss: 3.031751 | lr:9.8859e-05 | norm 0.2968 | dt 338.50ms | 1548869.71 tokens/sec
Step 15900 | loss: 3.077596 | lr:9.8835e-05 | norm 0.2882 | dt 339.49ms | 1544347.92 tokens/sec
Step 15901 | loss: 3.036258 | lr:9.8811e-05 | norm 0.2729 | dt 338.67ms | 1548084.63 tokens/sec
Step 15902 | loss: 3.093778 | lr:9.8787e-05 | norm 0.2825 | dt 338.61ms | 1548352.78 tokens/sec
Step 15903 | loss: 3.055390 | lr:9.8764e-05 | norm 0.2913 | dt 338.67ms | 1548095.53 tokens/sec
Step 15904 | loss: 3.046228 | lr:9.8740e-05 | norm 0.2752 | dt 339.04ms | 1546386.36 tokens/sec
Step 15905 | loss: 3.079089 | lr:9.8716e-05 | norm 0.2821 | dt 338.90ms | 1547043.45 tokens/sec
Step 15906 | loss: 3.050374 | lr:9.8692e-05 | norm 0.2880 | dt 338.09ms | 1550748.40 tokens/sec
Step 15907 | loss: 3.079843 | lr:9.8668e-05 | norm 0.2802 | dt 338.51ms | 1548818.43 tokens/sec
Step 15908 | loss: 3.145202 | lr:9.8644e-05 | norm 0.2778 | dt 338.60ms | 1548378.94 tokens/sec
Step 15909 | loss: 3.017714 | lr:9.8621e-05 | norm 0.2829 | dt 338.19ms | 1550294.69 tokens/sec
Step 15910 | loss: 3.079967 | lr:9.8597e-05 | norm 0.2632 | dt 339.40ms | 1544742.81 tokens/sec
Step 15911 | loss: 3.049765 | lr:9.8573e-05 | norm 0.2716 | dt 337.78ms | 1552136.30 tokens/sec
Step 15912 | loss: 3.037654 | lr:9.8549e-05 | norm 0.2887 | dt 337.70ms | 1552517.65 tokens/sec
Step 15913 | loss: 3.054630 | lr:9.8525e-05 | norm 0.2754 | dt 338.84ms | 1547316.68 tokens/sec
Step 15914 | loss: 3.046762 | lr:9.8502e-05 | norm 0.3098 | dt 338.42ms | 1549227.62 tokens/sec
Step 15915 | loss: 3.049360 | lr:9.8478e-05 | norm 0.3056 | dt 338.73ms | 1547806.77 tokens/sec
Step 15916 | loss: 3.066761 | lr:9.8454e-05 | norm 0.2816 | dt 338.24ms | 1550060.84 tokens/sec
Step 15917 | loss: 3.025416 | lr:9.8430e-05 | norm 0.2770 | dt 338.64ms | 1548200.16 tokens/sec
Step 15918 | loss: 3.072946 | lr:9.8407e-05 | norm 0.2752 | dt 337.83ms | 1551928.18 tokens/sec
Step 15919 | loss: 3.031523 | lr:9.8383e-05 | norm 0.2793 | dt 338.21ms | 1550174.48 tokens/sec
Step 15920 | loss: 3.104812 | lr:9.8359e-05 | norm 0.2891 | dt 338.32ms | 1549682.88 tokens/sec
Step 15921 | loss: 3.042516 | lr:9.8335e-05 | norm 0.3366 | dt 337.83ms | 1551920.51 tokens/sec
Step 15922 | loss: 3.068962 | lr:9.8312e-05 | norm 0.3001 | dt 338.70ms | 1547948.41 tokens/sec
Step 15923 | loss: 3.112318 | lr:9.8288e-05 | norm 0.2914 | dt 338.01ms | 1551088.58 tokens/sec
Step 15924 | loss: 3.113328 | lr:9.8264e-05 | norm 0.3056 | dt 338.23ms | 1550087.06 tokens/sec
Step 15925 | loss: 3.032438 | lr:9.8240e-05 | norm 0.3140 | dt 338.28ms | 1549884.94 tokens/sec
Step 15926 | loss: 3.064120 | lr:9.8217e-05 | norm 0.3091 | dt 338.68ms | 1548045.40 tokens/sec
Step 15927 | loss: 3.111723 | lr:9.8193e-05 | norm 0.3012 | dt 337.39ms | 1553946.06 tokens/sec
Step 15928 | loss: 3.071108 | lr:9.8169e-05 | norm 0.2974 | dt 338.04ms | 1550971.52 tokens/sec
Step 15929 | loss: 3.039635 | lr:9.8146e-05 | norm 0.3100 | dt 338.19ms | 1550258.62 tokens/sec
Step 15930 | loss: 3.069418 | lr:9.8122e-05 | norm 0.3007 | dt 337.68ms | 1552609.72 tokens/sec
Step 15931 | loss: 3.019820 | lr:9.8098e-05 | norm 0.3299 | dt 338.11ms | 1550637.95 tokens/sec
Step 15932 | loss: 3.119011 | lr:9.8075e-05 | norm 0.3071 | dt 337.36ms | 1554083.33 tokens/sec
Step 15933 | loss: 3.037691 | lr:9.8051e-05 | norm 0.2918 | dt 338.74ms | 1547769.73 tokens/sec
Step 15934 | loss: 3.087337 | lr:9.8027e-05 | norm 0.2966 | dt 337.60ms | 1552970.46 tokens/sec
Step 15935 | loss: 3.059892 | lr:9.8004e-05 | norm 0.2964 | dt 337.44ms | 1553712.20 tokens/sec
Step 15936 | loss: 3.025111 | lr:9.7980e-05 | norm 0.2838 | dt 338.20ms | 1550214.91 tokens/sec
Step 15937 | loss: 3.074240 | lr:9.7957e-05 | norm 0.2769 | dt 338.50ms | 1548867.53 tokens/sec
Step 15938 | loss: 3.065633 | lr:9.7933e-05 | norm 0.3105 | dt 337.42ms | 1553805.52 tokens/sec
Step 15939 | loss: 3.034007 | lr:9.7909e-05 | norm 0.2778 | dt 337.83ms | 1551930.37 tokens/sec
Step 15940 | loss: 3.076000 | lr:9.7886e-05 | norm 0.2841 | dt 337.83ms | 1551942.42 tokens/sec
Step 15941 | loss: 3.031905 | lr:9.7862e-05 | norm 0.3064 | dt 338.60ms | 1548397.48 tokens/sec
Step 15942 | loss: 3.086236 | lr:9.7838e-05 | norm 0.2916 | dt 337.94ms | 1551402.64 tokens/sec
Step 15943 | loss: 3.096055 | lr:9.7815e-05 | norm 0.3019 | dt 337.57ms | 1553140.47 tokens/sec
Step 15944 | loss: 3.024913 | lr:9.7791e-05 | norm 0.2894 | dt 338.12ms | 1550619.36 tokens/sec
Step 15945 | loss: 3.060705 | lr:9.7768e-05 | norm 0.2990 | dt 338.62ms | 1548291.73 tokens/sec
Step 15946 | loss: 3.046157 | lr:9.7744e-05 | norm 0.2969 | dt 338.43ms | 1549186.14 tokens/sec
Step 15947 | loss: 3.097419 | lr:9.7721e-05 | norm 0.2813 | dt 337.94ms | 1551411.39 tokens/sec
Step 15948 | loss: 3.026062 | lr:9.7697e-05 | norm 0.2947 | dt 337.97ms | 1551272.40 tokens/sec
Step 15949 | loss: 3.107055 | lr:9.7674e-05 | norm 0.2756 | dt 337.99ms | 1551171.73 tokens/sec
Step 15950 | loss: 3.038690 | lr:9.7650e-05 | norm 0.2713 | dt 338.20ms | 1550246.60 tokens/sec
Step 15951 | loss: 3.066478 | lr:9.7626e-05 | norm 0.2928 | dt 338.47ms | 1549001.72 tokens/sec
Step 15952 | loss: 3.099802 | lr:9.7603e-05 | norm 0.2754 | dt 338.49ms | 1548886.07 tokens/sec
Step 15953 | loss: 3.040677 | lr:9.7579e-05 | norm 0.2793 | dt 339.90ms | 1542458.73 tokens/sec
Step 15954 | loss: 3.086160 | lr:9.7556e-05 | norm 0.2831 | dt 338.30ms | 1549760.42 tokens/sec
Step 15955 | loss: 3.053253 | lr:9.7532e-05 | norm 0.2689 | dt 338.17ms | 1550359.18 tokens/sec
Step 15956 | loss: 3.073077 | lr:9.7509e-05 | norm 0.2924 | dt 338.24ms | 1550044.45 tokens/sec
Step 15957 | loss: 3.074813 | lr:9.7485e-05 | norm 0.2826 | dt 338.18ms | 1550327.48 tokens/sec
Step 15958 | loss: 3.047394 | lr:9.7462e-05 | norm 0.3136 | dt 339.80ms | 1542910.02 tokens/sec
Step 15959 | loss: 3.066390 | lr:9.7438e-05 | norm 0.2753 | dt 1044.53ms | 501935.65 tokens/sec
Step 15960 | loss: 3.126511 | lr:9.7415e-05 | norm 0.3012 | dt 338.14ms | 1550511.12 tokens/sec
Step 15961 | loss: 3.092295 | lr:9.7392e-05 | norm 0.3134 | dt 338.34ms | 1549580.23 tokens/sec
Step 15962 | loss: 3.037923 | lr:9.7368e-05 | norm 0.2859 | dt 337.46ms | 1553651.82 tokens/sec
Step 15963 | loss: 3.022449 | lr:9.7345e-05 | norm 0.3186 | dt 338.51ms | 1548814.07 tokens/sec
Step 15964 | loss: 3.062872 | lr:9.7321e-05 | norm 0.2930 | dt 338.18ms | 1550309.99 tokens/sec
Step 15965 | loss: 3.146649 | lr:9.7298e-05 | norm 0.3345 | dt 338.20ms | 1550234.58 tokens/sec
Step 15966 | loss: 3.028387 | lr:9.7274e-05 | norm 0.3124 | dt 338.98ms | 1546651.74 tokens/sec
Step 15967 | loss: 3.082696 | lr:9.7251e-05 | norm 0.3256 | dt 339.28ms | 1545289.91 tokens/sec
Step 15968 | loss: 3.072083 | lr:9.7227e-05 | norm 0.2944 | dt 337.33ms | 1554238.21 tokens/sec
Step 15969 | loss: 3.095570 | lr:9.7204e-05 | norm 0.3106 | dt 338.34ms | 1549590.06 tokens/sec
Step 15970 | loss: 3.059310 | lr:9.7181e-05 | norm 0.2997 | dt 338.03ms | 1551024.03 tokens/sec
Step 15971 | loss: 3.090307 | lr:9.7157e-05 | norm 0.3007 | dt 339.15ms | 1545889.56 tokens/sec
Step 15972 | loss: 3.039628 | lr:9.7134e-05 | norm 0.2964 | dt 338.32ms | 1549694.90 tokens/sec
Step 15973 | loss: 3.063920 | lr:9.7111e-05 | norm 0.2724 | dt 338.46ms | 1549055.19 tokens/sec
Step 15974 | loss: 3.010353 | lr:9.7087e-05 | norm 0.2854 | dt 338.19ms | 1550257.53 tokens/sec
Step 15975 | loss: 3.053127 | lr:9.7064e-05 | norm 0.2879 | dt 338.65ms | 1548185.99 tokens/sec
Step 15976 | loss: 3.068619 | lr:9.7040e-05 | norm 0.2937 | dt 339.41ms | 1544685.30 tokens/sec
Step 15977 | loss: 3.070549 | lr:9.7017e-05 | norm 0.2954 | dt 338.74ms | 1547771.91 tokens/sec
Step 15978 | loss: 3.060547 | lr:9.6994e-05 | norm 0.3148 | dt 338.79ms | 1547534.46 tokens/sec
Step 15979 | loss: 3.052670 | lr:9.6970e-05 | norm 0.2986 | dt 338.73ms | 1547795.88 tokens/sec
Step 15980 | loss: 3.042027 | lr:9.6947e-05 | norm 0.3050 | dt 338.26ms | 1549966.88 tokens/sec
Step 15981 | loss: 3.038176 | lr:9.6924e-05 | norm 0.3163 | dt 339.05ms | 1546354.82 tokens/sec
Step 15982 | loss: 3.023831 | lr:9.6900e-05 | norm 0.2986 | dt 338.84ms | 1547305.79 tokens/sec
Step 15983 | loss: 3.114133 | lr:9.6877e-05 | norm 0.3013 | dt 337.65ms | 1552734.70 tokens/sec
Step 15984 | loss: 3.092617 | lr:9.6854e-05 | norm 0.3069 | dt 339.12ms | 1546012.37 tokens/sec
Step 15985 | loss: 3.060932 | lr:9.6830e-05 | norm 0.3009 | dt 338.67ms | 1548095.53 tokens/sec
Step 15986 | loss: 3.140846 | lr:9.6807e-05 | norm 0.2881 | dt 338.33ms | 1549629.37 tokens/sec
Step 15987 | loss: 3.040246 | lr:9.6784e-05 | norm 0.2952 | dt 338.57ms | 1548547.95 tokens/sec
Step 15988 | loss: 3.120667 | lr:9.6761e-05 | norm 0.2974 | dt 338.09ms | 1550731.99 tokens/sec
Step 15989 | loss: 3.070210 | lr:9.6737e-05 | norm 0.2797 | dt 339.05ms | 1546346.12 tokens/sec
Step 15990 | loss: 3.062879 | lr:9.6714e-05 | norm 0.2876 | dt 338.60ms | 1548411.65 tokens/sec
Step 15991 | loss: 3.051737 | lr:9.6691e-05 | norm 0.2891 | dt 338.58ms | 1548505.42 tokens/sec
Step 15992 | loss: 3.073471 | lr:9.6668e-05 | norm 0.3655 | dt 344.89ms | 1520175.30 tokens/sec
Step 15993 | loss: 3.069479 | lr:9.6644e-05 | norm 0.3271 | dt 338.66ms | 1548116.24 tokens/sec
Step 15994 | loss: 3.171649 | lr:9.6621e-05 | norm 0.3161 | dt 338.91ms | 1546994.48 tokens/sec
Step 15995 | loss: 3.083699 | lr:9.6598e-05 | norm 0.3063 | dt 338.73ms | 1547826.38 tokens/sec
Step 15996 | loss: 3.051580 | lr:9.6575e-05 | norm 0.3238 | dt 338.62ms | 1548306.99 tokens/sec
Step 15997 | loss: 3.074390 | lr:9.6551e-05 | norm 0.3135 | dt 338.54ms | 1548678.82 tokens/sec
Step 15998 | loss: 3.081797 | lr:9.6528e-05 | norm 0.2950 | dt 338.61ms | 1548335.33 tokens/sec
Step 15999 | loss: 3.104600 | lr:9.6505e-05 | norm 0.2874 | dt 338.34ms | 1549611.90 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 16000: 3.0996
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3026/10042=0.3013


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm a native-speaker.
But if I were as cool as this person, if I wanted you
rank 3 sample 1 >Hello, I'm a language model, and the only way to define and edit the type of an object in Java is to use the java function. To make
rank 3 sample 2 >Hello, I'm a language model, and it has a native speaker language. To translate this, you need a machine learning model for translating. I will write
rank 3 sample 3 >Hello, I'm a language model, and i'm doing a tutorial to teach me about programming with a Raspberry Pi. so i'll make u go and build




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I like to make models that are simpler and easier to understand. To do that, I wanted to create a model


ddp_rank 2: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, there are two ways to understand that we don't teach (and not teach) computers to us. It's the "


ddp_rank 5: ####### Printing generated samples ####### 

rank 7 sample 2 >Hello, I'm a language model, and I want to put this knowledge in context. As I've mentioned there, the most important thing is that if a
rank 2 sample 0 >Hello, I'm a language model, and this one does a great job of showing me how the different languages behave in practice.
The first line of input
rank 7 sample 3 >Hello, I'm a language model, so I'd like to take a long way off the internet.
This is what I'm about to do. To


rank 5 sample 0 >Hello, I'm a language model, but this isn't going to be an important part of the lecture I was doing in class.
One of the main
rank 2 sample 1 >Hello, I'm a language model, so I still can't tell you when our language. I've written something like this before, so you might like to
rank 5 sample 1 >Hello, I'm a language model, here's what I use with it.
How do we create a class, with all of the things listed in the
rank 2 sample 2 >Hello, I'm a language model, so I want to write a post called - a post about programming language called 'I'm a teacher - I'm just
rank 5 sample 2 >Hello, I'm a language model, so I want you to know your understanding of it.
And to know why, take a look at the following questions
rank 2 sample 3 >Hello, I'm a language model, but how can I use the language model?
So, to answer this question, you basically build your application on the


rank 5 sample 3 >Hello, I'm a language model, an intro-programmer, a language designer, a computer scientist and a translator to make it appear as though I'm




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and I'm going to do a web application project on Google or Facebook. Because I know it'll be interesting to see
rank 1 sample 1 >Hello, I'm a language model, a programmer who makes me think and learn language. I believe that programming is just like this: it is a skill that
rank 1 sample 2 >Hello, I'm a language model, but is this my first language?
I'm a linguist. And I'm a computer scientist.
The term
rank 1 sample 3 >Hello, I'm a language model, and I'm interested to know how well anybody can know someone in real life of a fluent speakers who can speak fluently




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, who needs to be the code! I've been teaching my classes with the code of the course, and it's one
rank 6 sample 1 >Hello, I'm a language model, and I'll be the one to get on with some of the projects at the end, so we'll talk about the
rank 6 sample 2 >Hello, I'm a language model, but I've been teaching English in general for over a year, and I just don't know how to start off,
rank 6 sample 3 >Hello, I'm a language model, and I'll keep reading about a few of the things I've seen in other people, at least right now. First




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I think I've heard some stuff (it's fun) all the time. It's so much fun to write
rank 0 sample 1 >Hello, I'm a language model, so we're going to build the algorithm. I like going to have everything, so let's just move on. Let
rank 0 sample 2 >Hello, I'm a language model, so I could't give some context about the differences between the words and the context of a sentence. I'm a machine
rank 0 sample 3 >Hello, I'm a language model, so for me, I'm going to make certain that I'm gonna be using JL (JL stands for Hebrew




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I'd like to start with the program that will take a look at the problem statement
[#3] =
rank 4 sample 1 >Hello, I'm a language model, writing programs that interact with your hardware and how it makes my code do different things when I use it. So like,
rank 4 sample 2 >Hello, I'm a language model, I started it learning at elementary school and have grown to be one of the most valuable learners in my program. As I
rank 4 sample 3 >Hello, I'm a language model, and my math classes are very similar. If anyone told you so I found out that I have three languages, this means


Step 16000 | loss: 3.015647 | lr:9.6482e-05 | norm 0.2998 | dt 18792.72ms | 27898.46 tokens/sec
Step 16001 | loss: 3.064632 | lr:9.6459e-05 | norm 0.2967 | dt 333.36ms | 1572714.76 tokens/sec
Step 16002 | loss: 3.071056 | lr:9.6435e-05 | norm 0.2888 | dt 335.03ms | 1564904.98 tokens/sec
Step 16003 | loss: 3.096283 | lr:9.6412e-05 | norm 0.3070 | dt 336.38ms | 1558607.16 tokens/sec
Step 16004 | loss: 3.095428 | lr:9.6389e-05 | norm 0.2917 | dt 335.55ms | 1562485.44 tokens/sec
Step 16005 | loss: 3.016495 | lr:9.6366e-05 | norm 0.2848 | dt 336.05ms | 1560166.34 tokens/sec
Step 16006 | loss: 3.118907 | lr:9.6343e-05 | norm 0.3048 | dt 336.68ms | 1557208.76 tokens/sec
Step 16007 | loss: 3.103446 | lr:9.6320e-05 | norm 0.2775 | dt 336.00ms | 1560396.61 tokens/sec
Step 16008 | loss: 3.053440 | lr:9.6296e-05 | norm 0.2686 | dt 335.88ms | 1560919.40 tokens/sec
Step 16009 | loss: 3.034274 | lr:9.6273e-05 | norm 0.2845 | dt 337.05ms | 1555508.03 tokens/sec
Step 16010 | loss: 3.083003 | lr:9.6250e-05 | norm 0.2917 | dt 336.09ms | 1559947.20 tokens/sec
Step 16011 | loss: 3.089653 | lr:9.6227e-05 | norm 0.2695 | dt 336.56ms | 1557792.32 tokens/sec
Step 16012 | loss: 3.056474 | lr:9.6204e-05 | norm 0.2870 | dt 337.60ms | 1552988.01 tokens/sec
Step 16013 | loss: 3.045333 | lr:9.6181e-05 | norm 0.2696 | dt 336.87ms | 1556368.95 tokens/sec
Step 16014 | loss: 2.998483 | lr:9.6158e-05 | norm 0.3076 | dt 336.78ms | 1556774.41 tokens/sec
Step 16015 | loss: 3.083062 | lr:9.6135e-05 | norm 0.2810 | dt 336.73ms | 1556999.28 tokens/sec
Step 16016 | loss: 3.023700 | lr:9.6112e-05 | norm 0.2753 | dt 337.50ms | 1553435.61 tokens/sec
Step 16017 | loss: 3.030114 | lr:9.6088e-05 | norm 0.3073 | dt 337.30ms | 1554381.03 tokens/sec
Step 16018 | loss: 2.994266 | lr:9.6065e-05 | norm 0.2893 | dt 337.42ms | 1553796.73 tokens/sec
Step 16019 | loss: 3.012724 | lr:9.6042e-05 | norm 0.2864 | dt 336.81ms | 1556608.01 tokens/sec
Step 16020 | loss: 3.016312 | lr:9.6019e-05 | norm 0.2942 | dt 337.28ms | 1554476.62 tokens/sec
Step 16021 | loss: 3.083646 | lr:9.5996e-05 | norm 0.2842 | dt 336.80ms | 1556696.17 tokens/sec
Step 16022 | loss: 2.975292 | lr:9.5973e-05 | norm 0.3043 | dt 337.23ms | 1554667.85 tokens/sec
Step 16023 | loss: 3.066547 | lr:9.5950e-05 | norm 0.3085 | dt 336.41ms | 1558471.29 tokens/sec
Step 16024 | loss: 3.050958 | lr:9.5927e-05 | norm 0.2715 | dt 336.09ms | 1559982.62 tokens/sec
Step 16025 | loss: 3.022892 | lr:9.5904e-05 | norm 0.2949 | dt 336.57ms | 1557754.80 tokens/sec
Step 16026 | loss: 3.103955 | lr:9.5881e-05 | norm 0.3349 | dt 336.79ms | 1556741.35 tokens/sec
Step 16027 | loss: 3.020556 | lr:9.5858e-05 | norm 0.3620 | dt 336.35ms | 1558756.31 tokens/sec
Step 16028 | loss: 3.084000 | lr:9.5835e-05 | norm 0.3304 | dt 336.27ms | 1559137.60 tokens/sec
Step 16029 | loss: 3.087269 | lr:9.5812e-05 | norm 0.3525 | dt 336.63ms | 1557467.94 tokens/sec
Step 16030 | loss: 3.100569 | lr:9.5789e-05 | norm 0.3111 | dt 336.31ms | 1558943.06 tokens/sec
Step 16031 | loss: 3.068420 | lr:9.5766e-05 | norm 0.2835 | dt 336.80ms | 1556685.15 tokens/sec
Step 16032 | loss: 3.039729 | lr:9.5743e-05 | norm 0.2875 | dt 336.70ms | 1557117.24 tokens/sec
Step 16033 | loss: 3.079245 | lr:9.5720e-05 | norm 0.3075 | dt 336.89ms | 1556247.79 tokens/sec
Step 16034 | loss: 3.076572 | lr:9.5697e-05 | norm 0.3001 | dt 337.04ms | 1555544.34 tokens/sec
Step 16035 | loss: 3.080328 | lr:9.5674e-05 | norm 0.2965 | dt 336.65ms | 1557374.19 tokens/sec
Step 16036 | loss: 3.041101 | lr:9.5651e-05 | norm 0.2818 | dt 336.80ms | 1556669.72 tokens/sec
Step 16037 | loss: 3.055036 | lr:9.5628e-05 | norm 0.2949 | dt 336.63ms | 1557471.25 tokens/sec
Step 16038 | loss: 3.057418 | lr:9.5605e-05 | norm 0.2603 | dt 337.15ms | 1555072.43 tokens/sec
Step 16039 | loss: 3.112724 | lr:9.5582e-05 | norm 0.2963 | dt 336.73ms | 1556998.17 tokens/sec
Step 16040 | loss: 3.080049 | lr:9.5560e-05 | norm 0.3033 | dt 337.02ms | 1555676.39 tokens/sec
Step 16041 | loss: 3.082172 | lr:9.5537e-05 | norm 0.2688 | dt 337.85ms | 1551858.09 tokens/sec
Step 16042 | loss: 3.022637 | lr:9.5514e-05 | norm 0.2870 | dt 337.15ms | 1555066.93 tokens/sec
Step 16043 | loss: 3.031804 | lr:9.5491e-05 | norm 0.2863 | dt 337.07ms | 1555449.71 tokens/sec
Step 16044 | loss: 3.041993 | lr:9.5468e-05 | norm 0.2736 | dt 338.11ms | 1550632.48 tokens/sec
Step 16045 | loss: 3.029850 | lr:9.5445e-05 | norm 0.2791 | dt 338.16ms | 1550429.13 tokens/sec
Step 16046 | loss: 3.078599 | lr:9.5422e-05 | norm 0.2750 | dt 337.47ms | 1553595.84 tokens/sec
Step 16047 | loss: 3.031564 | lr:9.5399e-05 | norm 0.2805 | dt 337.38ms | 1554011.95 tokens/sec
Step 16048 | loss: 3.065042 | lr:9.5376e-05 | norm 0.2864 | dt 337.58ms | 1553070.27 tokens/sec
Step 16049 | loss: 2.944699 | lr:9.5354e-05 | norm 0.2846 | dt 338.02ms | 1551065.60 tokens/sec
Step 16050 | loss: 3.048678 | lr:9.5331e-05 | norm 0.2850 | dt 337.19ms | 1554896.50 tokens/sec
Step 16051 | loss: 3.038339 | lr:9.5308e-05 | norm 0.2761 | dt 337.43ms | 1553758.31 tokens/sec
Step 16052 | loss: 3.037902 | lr:9.5285e-05 | norm 0.2819 | dt 338.30ms | 1549794.28 tokens/sec
Step 16053 | loss: 3.055935 | lr:9.5262e-05 | norm 0.2872 | dt 338.04ms | 1550978.08 tokens/sec
Step 16054 | loss: 3.089369 | lr:9.5239e-05 | norm 0.2744 | dt 338.57ms | 1548525.05 tokens/sec
Step 16055 | loss: 3.090059 | lr:9.5216e-05 | norm 0.5260 | dt 337.65ms | 1552761.02 tokens/sec
Step 16056 | loss: 3.014287 | lr:9.5194e-05 | norm 0.3178 | dt 337.41ms | 1553874.69 tokens/sec
Step 16057 | loss: 3.028332 | lr:9.5171e-05 | norm 0.2982 | dt 338.16ms | 1550406.18 tokens/sec
Step 16058 | loss: 3.005928 | lr:9.5148e-05 | norm 0.2921 | dt 337.27ms | 1554486.51 tokens/sec
Step 16059 | loss: 3.065126 | lr:9.5125e-05 | norm 0.3012 | dt 338.00ms | 1551150.94 tokens/sec
Step 16060 | loss: 3.038177 | lr:9.5102e-05 | norm 0.2758 | dt 337.45ms | 1553681.46 tokens/sec
Step 16061 | loss: 3.002635 | lr:9.5080e-05 | norm 0.3204 | dt 338.00ms | 1551137.81 tokens/sec
Step 16062 | loss: 3.129318 | lr:9.5057e-05 | norm 0.3003 | dt 337.86ms | 1551778.14 tokens/sec
Step 16063 | loss: 3.014557 | lr:9.5034e-05 | norm 0.3067 | dt 337.78ms | 1552174.65 tokens/sec
Step 16064 | loss: 3.073267 | lr:9.5011e-05 | norm 0.3150 | dt 1027.05ms | 510479.40 tokens/sec
Step 16065 | loss: 3.069100 | lr:9.4989e-05 | norm 0.2743 | dt 335.16ms | 1564301.62 tokens/sec
Step 16066 | loss: 3.085414 | lr:9.4966e-05 | norm 0.2983 | dt 337.44ms | 1553715.49 tokens/sec
Step 16067 | loss: 2.993946 | lr:9.4943e-05 | norm 0.3024 | dt 338.33ms | 1549615.18 tokens/sec
Step 16068 | loss: 3.037081 | lr:9.4920e-05 | norm 0.2691 | dt 337.00ms | 1555755.64 tokens/sec
Step 16069 | loss: 3.009149 | lr:9.4898e-05 | norm 0.2841 | dt 338.16ms | 1550402.90 tokens/sec
Step 16070 | loss: 3.004687 | lr:9.4875e-05 | norm 0.3194 | dt 337.54ms | 1553276.51 tokens/sec
Step 16071 | loss: 3.043106 | lr:9.4852e-05 | norm 0.3025 | dt 337.77ms | 1552200.94 tokens/sec
Step 16072 | loss: 3.059098 | lr:9.4830e-05 | norm 0.2999 | dt 338.02ms | 1551037.16 tokens/sec
Step 16073 | loss: 3.060176 | lr:9.4807e-05 | norm 0.3010 | dt 337.46ms | 1553613.41 tokens/sec
Step 16074 | loss: 3.111850 | lr:9.4784e-05 | norm 0.3082 | dt 337.85ms | 1551852.61 tokens/sec
Step 16075 | loss: 3.039111 | lr:9.4762e-05 | norm 0.2862 | dt 337.93ms | 1551462.84 tokens/sec
Step 16076 | loss: 3.104165 | lr:9.4739e-05 | norm 0.2802 | dt 338.10ms | 1550701.37 tokens/sec
Step 16077 | loss: 3.118394 | lr:9.4716e-05 | norm 0.2842 | dt 337.83ms | 1551921.61 tokens/sec
Step 16078 | loss: 3.015999 | lr:9.4694e-05 | norm 0.3174 | dt 338.16ms | 1550436.79 tokens/sec
Step 16079 | loss: 3.069760 | lr:9.4671e-05 | norm 0.2786 | dt 338.07ms | 1550832.61 tokens/sec
Step 16080 | loss: 3.096504 | lr:9.4648e-05 | norm 0.3326 | dt 337.57ms | 1553119.63 tokens/sec
Step 16081 | loss: 3.119229 | lr:9.4626e-05 | norm 0.3014 | dt 338.84ms | 1547314.50 tokens/sec
Step 16082 | loss: 3.033633 | lr:9.4603e-05 | norm 0.3108 | dt 338.36ms | 1549475.41 tokens/sec
Step 16083 | loss: 3.097287 | lr:9.4580e-05 | norm 0.2969 | dt 338.01ms | 1551100.61 tokens/sec
Step 16084 | loss: 3.026705 | lr:9.4558e-05 | norm 0.2869 | dt 337.55ms | 1553215.07 tokens/sec
Step 16085 | loss: 3.014520 | lr:9.4535e-05 | norm 0.2970 | dt 338.32ms | 1549699.26 tokens/sec
Step 16086 | loss: 2.996130 | lr:9.4512e-05 | norm 0.2889 | dt 338.19ms | 1550263.00 tokens/sec
Step 16087 | loss: 3.009722 | lr:9.4490e-05 | norm 0.3213 | dt 338.41ms | 1549290.92 tokens/sec
Step 16088 | loss: 3.022367 | lr:9.4467e-05 | norm 0.2957 | dt 337.76ms | 1552232.72 tokens/sec
Step 16089 | loss: 2.936316 | lr:9.4445e-05 | norm 0.2899 | dt 338.53ms | 1548735.53 tokens/sec
Step 16090 | loss: 2.987654 | lr:9.4422e-05 | norm 0.2958 | dt 338.82ms | 1547402.70 tokens/sec
Step 16091 | loss: 3.029619 | lr:9.4400e-05 | norm 0.3157 | dt 338.75ms | 1547732.69 tokens/sec
Step 16092 | loss: 3.062389 | lr:9.4377e-05 | norm 0.2731 | dt 338.71ms | 1547877.59 tokens/sec
Step 16093 | loss: 3.014924 | lr:9.4354e-05 | norm 0.3010 | dt 338.63ms | 1548240.49 tokens/sec
Step 16094 | loss: 3.037203 | lr:9.4332e-05 | norm 0.2852 | dt 337.89ms | 1551667.55 tokens/sec
Step 16095 | loss: 3.035096 | lr:9.4309e-05 | norm 0.2905 | dt 338.18ms | 1550335.13 tokens/sec
Step 16096 | loss: 3.086586 | lr:9.4287e-05 | norm 0.2973 | dt 338.12ms | 1550594.22 tokens/sec
Step 16097 | loss: 3.079848 | lr:9.4264e-05 | norm 0.3020 | dt 338.24ms | 1550042.26 tokens/sec
Step 16098 | loss: 3.036630 | lr:9.4242e-05 | norm 0.2867 | dt 337.79ms | 1552129.73 tokens/sec
Step 16099 | loss: 3.107941 | lr:9.4219e-05 | norm 0.2957 | dt 337.99ms | 1551178.29 tokens/sec
Step 16100 | loss: 3.042076 | lr:9.4197e-05 | norm 0.2879 | dt 338.18ms | 1550330.76 tokens/sec
Step 16101 | loss: 3.054976 | lr:9.4174e-05 | norm 0.2988 | dt 338.13ms | 1550555.95 tokens/sec
Step 16102 | loss: 3.002823 | lr:9.4152e-05 | norm 0.3025 | dt 338.12ms | 1550597.50 tokens/sec
Step 16103 | loss: 3.003403 | lr:9.4129e-05 | norm 0.3001 | dt 337.84ms | 1551902.99 tokens/sec
Step 16104 | loss: 3.034992 | lr:9.4107e-05 | norm 0.2979 | dt 338.48ms | 1548965.72 tokens/sec
Step 16105 | loss: 3.049048 | lr:9.4084e-05 | norm 0.2872 | dt 338.30ms | 1549750.59 tokens/sec
Step 16106 | loss: 3.094541 | lr:9.4062e-05 | norm 0.2848 | dt 338.35ms | 1549547.48 tokens/sec
Step 16107 | loss: 3.061433 | lr:9.4039e-05 | norm 0.3164 | dt 338.23ms | 1550108.91 tokens/sec
Step 16108 | loss: 3.074800 | lr:9.4017e-05 | norm 0.2677 | dt 338.04ms | 1550972.61 tokens/sec
Step 16109 | loss: 3.012596 | lr:9.3994e-05 | norm 0.2981 | dt 338.19ms | 1550294.69 tokens/sec
Step 16110 | loss: 3.119686 | lr:9.3972e-05 | norm 0.3732 | dt 337.94ms | 1551414.68 tokens/sec
Step 16111 | loss: 3.098069 | lr:9.3950e-05 | norm 0.3138 | dt 338.17ms | 1550347.15 tokens/sec
Step 16112 | loss: 3.074587 | lr:9.3927e-05 | norm 0.3148 | dt 337.87ms | 1551739.82 tokens/sec
Step 16113 | loss: 3.029501 | lr:9.3905e-05 | norm 0.3006 | dt 338.04ms | 1550959.49 tokens/sec
Step 16114 | loss: 3.083883 | lr:9.3882e-05 | norm 0.3037 | dt 337.74ms | 1552325.86 tokens/sec
Step 16115 | loss: 3.095341 | lr:9.3860e-05 | norm 0.3015 | dt 338.31ms | 1549746.23 tokens/sec
Step 16116 | loss: 3.084299 | lr:9.3837e-05 | norm 0.2913 | dt 337.63ms | 1552869.57 tokens/sec
Step 16117 | loss: 3.058417 | lr:9.3815e-05 | norm 0.3336 | dt 337.91ms | 1551547.12 tokens/sec
Step 16118 | loss: 3.053166 | lr:9.3793e-05 | norm 0.2989 | dt 338.16ms | 1550432.41 tokens/sec
Step 16119 | loss: 3.058432 | lr:9.3770e-05 | norm 0.3074 | dt 338.49ms | 1548884.98 tokens/sec
Step 16120 | loss: 3.020483 | lr:9.3748e-05 | norm 0.2871 | dt 337.87ms | 1551746.39 tokens/sec
Step 16121 | loss: 3.086579 | lr:9.3726e-05 | norm 0.2989 | dt 338.23ms | 1550099.08 tokens/sec
Step 16122 | loss: 3.025231 | lr:9.3703e-05 | norm 0.2978 | dt 337.65ms | 1552752.24 tokens/sec
Step 16123 | loss: 3.066676 | lr:9.3681e-05 | norm 0.2814 | dt 337.97ms | 1551304.14 tokens/sec
Step 16124 | loss: 3.060693 | lr:9.3659e-05 | norm 0.3032 | dt 338.21ms | 1550203.98 tokens/sec
Step 16125 | loss: 3.028629 | lr:9.3636e-05 | norm 0.2712 | dt 338.17ms | 1550389.78 tokens/sec
Step 16126 | loss: 3.017388 | lr:9.3614e-05 | norm 0.2709 | dt 337.64ms | 1552804.87 tokens/sec
Step 16127 | loss: 3.043336 | lr:9.3592e-05 | norm 0.3015 | dt 338.25ms | 1549996.37 tokens/sec
Step 16128 | loss: 3.023843 | lr:9.3569e-05 | norm 0.2793 | dt 338.05ms | 1550932.14 tokens/sec
Step 16129 | loss: 3.013838 | lr:9.3547e-05 | norm 0.2732 | dt 338.34ms | 1549606.44 tokens/sec
Step 16130 | loss: 3.069618 | lr:9.3525e-05 | norm 0.2825 | dt 338.21ms | 1550172.29 tokens/sec
Step 16131 | loss: 3.096408 | lr:9.3502e-05 | norm 0.2908 | dt 336.86ms | 1556392.08 tokens/sec
Step 16132 | loss: 3.098316 | lr:9.3480e-05 | norm 0.2930 | dt 338.86ms | 1547213.26 tokens/sec
Step 16133 | loss: 3.078932 | lr:9.3458e-05 | norm 0.2878 | dt 339.33ms | 1545085.78 tokens/sec
Step 16134 | loss: 3.024898 | lr:9.3435e-05 | norm 0.2736 | dt 338.70ms | 1547961.49 tokens/sec
Step 16135 | loss: 3.059096 | lr:9.3413e-05 | norm 0.2726 | dt 338.70ms | 1547940.79 tokens/sec
Step 16136 | loss: 3.061601 | lr:9.3391e-05 | norm 0.2827 | dt 338.78ms | 1547584.56 tokens/sec
Step 16137 | loss: 3.021484 | lr:9.3369e-05 | norm 0.2619 | dt 339.11ms | 1546066.72 tokens/sec
Step 16138 | loss: 3.106187 | lr:9.3346e-05 | norm 0.3028 | dt 338.33ms | 1549649.03 tokens/sec
Step 16139 | loss: 3.011471 | lr:9.3324e-05 | norm 0.3073 | dt 339.03ms | 1546457.04 tokens/sec
Step 16140 | loss: 2.983824 | lr:9.3302e-05 | norm 0.3205 | dt 338.49ms | 1548912.25 tokens/sec
Step 16141 | loss: 3.053104 | lr:9.3280e-05 | norm 0.2883 | dt 338.84ms | 1547304.70 tokens/sec
Step 16142 | loss: 3.083740 | lr:9.3258e-05 | norm 0.3000 | dt 338.79ms | 1547544.26 tokens/sec
Step 16143 | loss: 3.080088 | lr:9.3235e-05 | norm 0.2679 | dt 338.81ms | 1547416.85 tokens/sec
Step 16144 | loss: 3.112966 | lr:9.3213e-05 | norm 0.2868 | dt 338.29ms | 1549813.94 tokens/sec
Step 16145 | loss: 3.029592 | lr:9.3191e-05 | norm 0.2797 | dt 338.72ms | 1547842.73 tokens/sec
Step 16146 | loss: 3.111714 | lr:9.3169e-05 | norm 0.2921 | dt 339.36ms | 1544953.35 tokens/sec
Step 16147 | loss: 3.081431 | lr:9.3147e-05 | norm 0.3079 | dt 338.59ms | 1548433.46 tokens/sec
Step 16148 | loss: 3.088253 | lr:9.3124e-05 | norm 0.3072 | dt 338.14ms | 1550510.03 tokens/sec
Step 16149 | loss: 3.108834 | lr:9.3102e-05 | norm 0.3099 | dt 990.42ms | 529357.93 tokens/sec
Step 16150 | loss: 3.039484 | lr:9.3080e-05 | norm 0.2942 | dt 337.67ms | 1552648.09 tokens/sec
Step 16151 | loss: 3.081473 | lr:9.3058e-05 | norm 0.3063 | dt 340.11ms | 1541539.64 tokens/sec
Step 16152 | loss: 3.005719 | lr:9.3036e-05 | norm 0.2721 | dt 337.53ms | 1553306.13 tokens/sec
Step 16153 | loss: 3.086707 | lr:9.3014e-05 | norm 0.3106 | dt 338.15ms | 1550461.93 tokens/sec
Step 16154 | loss: 3.085258 | lr:9.2991e-05 | norm 0.2838 | dt 338.76ms | 1547678.23 tokens/sec
Step 16155 | loss: 3.024325 | lr:9.2969e-05 | norm 0.2784 | dt 338.77ms | 1547627.04 tokens/sec
Step 16156 | loss: 3.099832 | lr:9.2947e-05 | norm 0.2786 | dt 337.58ms | 1553087.82 tokens/sec
Step 16157 | loss: 3.038379 | lr:9.2925e-05 | norm 0.2960 | dt 337.19ms | 1554859.12 tokens/sec
Step 16158 | loss: 3.108305 | lr:9.2903e-05 | norm 0.2980 | dt 337.88ms | 1551700.40 tokens/sec
Step 16159 | loss: 3.007521 | lr:9.2881e-05 | norm 0.2724 | dt 337.75ms | 1552283.12 tokens/sec
Step 16160 | loss: 3.071149 | lr:9.2859e-05 | norm 0.2973 | dt 337.09ms | 1555321.00 tokens/sec
Step 16161 | loss: 3.044078 | lr:9.2837e-05 | norm 0.2911 | dt 338.36ms | 1549493.98 tokens/sec
Step 16162 | loss: 3.042733 | lr:9.2815e-05 | norm 0.2934 | dt 337.54ms | 1553241.40 tokens/sec
Step 16163 | loss: 3.043380 | lr:9.2793e-05 | norm 0.2773 | dt 338.08ms | 1550768.08 tokens/sec
Step 16164 | loss: 3.021374 | lr:9.2770e-05 | norm 0.3907 | dt 338.18ms | 1550335.13 tokens/sec
Step 16165 | loss: 3.042185 | lr:9.2748e-05 | norm 0.3484 | dt 337.99ms | 1551171.73 tokens/sec
Step 16166 | loss: 3.057315 | lr:9.2726e-05 | norm 0.3085 | dt 337.91ms | 1551538.37 tokens/sec
Step 16167 | loss: 3.139123 | lr:9.2704e-05 | norm 0.3192 | dt 338.94ms | 1546859.54 tokens/sec
Step 16168 | loss: 3.065649 | lr:9.2682e-05 | norm 0.3067 | dt 337.27ms | 1554517.28 tokens/sec
Step 16169 | loss: 3.051067 | lr:9.2660e-05 | norm 0.3006 | dt 338.96ms | 1546739.86 tokens/sec
Step 16170 | loss: 3.155012 | lr:9.2638e-05 | norm 0.3159 | dt 337.82ms | 1551988.42 tokens/sec
Step 16171 | loss: 3.058868 | lr:9.2616e-05 | norm 0.3113 | dt 337.25ms | 1554600.80 tokens/sec
Step 16172 | loss: 3.103890 | lr:9.2594e-05 | norm 0.3016 | dt 339.30ms | 1545221.50 tokens/sec
Step 16173 | loss: 3.075021 | lr:9.2572e-05 | norm 0.3273 | dt 339.85ms | 1542709.78 tokens/sec
Step 16174 | loss: 3.029422 | lr:9.2550e-05 | norm 0.2906 | dt 338.81ms | 1547439.72 tokens/sec
Step 16175 | loss: 3.056187 | lr:9.2528e-05 | norm 0.2783 | dt 337.77ms | 1552211.90 tokens/sec
Step 16176 | loss: 3.052145 | lr:9.2506e-05 | norm 0.2856 | dt 339.53ms | 1544158.14 tokens/sec
Step 16177 | loss: 3.065947 | lr:9.2484e-05 | norm 0.2843 | dt 338.67ms | 1548088.99 tokens/sec
Step 16178 | loss: 3.069881 | lr:9.2462e-05 | norm 0.2831 | dt 338.17ms | 1550362.46 tokens/sec
Step 16179 | loss: 3.042809 | lr:9.2440e-05 | norm 0.2723 | dt 338.77ms | 1547639.02 tokens/sec
Step 16180 | loss: 3.072574 | lr:9.2418e-05 | norm 0.2792 | dt 338.41ms | 1549253.81 tokens/sec
Step 16181 | loss: 3.011361 | lr:9.2396e-05 | norm 0.2750 | dt 339.28ms | 1545275.79 tokens/sec
Step 16182 | loss: 3.065576 | lr:9.2374e-05 | norm 0.2725 | dt 339.17ms | 1545793.93 tokens/sec
Step 16183 | loss: 3.043437 | lr:9.2353e-05 | norm 0.2817 | dt 338.76ms | 1547683.68 tokens/sec
Step 16184 | loss: 3.069004 | lr:9.2331e-05 | norm 0.2772 | dt 339.61ms | 1543798.23 tokens/sec
Step 16185 | loss: 3.016528 | lr:9.2309e-05 | norm 0.2758 | dt 338.04ms | 1550954.02 tokens/sec
Step 16186 | loss: 3.087029 | lr:9.2287e-05 | norm 0.2830 | dt 338.58ms | 1548491.24 tokens/sec
Step 16187 | loss: 2.998615 | lr:9.2265e-05 | norm 0.2842 | dt 342.18ms | 1532213.24 tokens/sec
Step 16188 | loss: 3.013340 | lr:9.2243e-05 | norm 0.2704 | dt 338.55ms | 1548607.93 tokens/sec
Step 16189 | loss: 3.022207 | lr:9.2221e-05 | norm 0.2828 | dt 339.13ms | 1545966.72 tokens/sec
Step 16190 | loss: 3.017234 | lr:9.2199e-05 | norm 0.2930 | dt 338.60ms | 1548398.57 tokens/sec
Step 16191 | loss: 3.003504 | lr:9.2177e-05 | norm 0.2722 | dt 339.49ms | 1544327.31 tokens/sec
Step 16192 | loss: 3.017635 | lr:9.2155e-05 | norm 0.2862 | dt 339.30ms | 1545185.67 tokens/sec
Step 16193 | loss: 2.990367 | lr:9.2134e-05 | norm 0.2820 | dt 339.15ms | 1545884.12 tokens/sec
Step 16194 | loss: 3.019953 | lr:9.2112e-05 | norm 0.2778 | dt 338.82ms | 1547392.90 tokens/sec
Step 16195 | loss: 3.029833 | lr:9.2090e-05 | norm 0.2754 | dt 338.57ms | 1548529.41 tokens/sec
Step 16196 | loss: 3.052433 | lr:9.2068e-05 | norm 0.2768 | dt 338.90ms | 1547031.48 tokens/sec
Step 16197 | loss: 3.086509 | lr:9.2046e-05 | norm 0.2646 | dt 339.51ms | 1544251.40 tokens/sec
Step 16198 | loss: 3.014499 | lr:9.2024e-05 | norm 0.2821 | dt 338.51ms | 1548830.43 tokens/sec
Step 16199 | loss: 3.058294 | lr:9.2003e-05 | norm 0.2810 | dt 338.31ms | 1549735.30 tokens/sec
Step 16200 | loss: 3.101969 | lr:9.1981e-05 | norm 0.3154 | dt 338.69ms | 1547983.28 tokens/sec
Step 16201 | loss: 3.122401 | lr:9.1959e-05 | norm 0.2934 | dt 337.86ms | 1551811.00 tokens/sec
Step 16202 | loss: 3.059748 | lr:9.1937e-05 | norm 0.2892 | dt 338.87ms | 1547145.76 tokens/sec
Step 16203 | loss: 3.037057 | lr:9.1915e-05 | norm 0.3023 | dt 339.01ms | 1546536.44 tokens/sec
Step 16204 | loss: 3.077775 | lr:9.1894e-05 | norm 0.2928 | dt 338.46ms | 1549038.82 tokens/sec
Step 16205 | loss: 3.067457 | lr:9.1872e-05 | norm 0.2828 | dt 337.87ms | 1551748.58 tokens/sec
Step 16206 | loss: 2.994500 | lr:9.1850e-05 | norm 0.3109 | dt 340.27ms | 1540811.64 tokens/sec
Step 16207 | loss: 2.980484 | lr:9.1828e-05 | norm 0.2826 | dt 338.10ms | 1550676.22 tokens/sec
Step 16208 | loss: 3.078168 | lr:9.1806e-05 | norm 0.2869 | dt 338.89ms | 1547068.48 tokens/sec
Step 16209 | loss: 3.059358 | lr:9.1785e-05 | norm 0.3273 | dt 339.14ms | 1545927.59 tokens/sec
Step 16210 | loss: 2.996305 | lr:9.1763e-05 | norm 0.2709 | dt 337.97ms | 1551272.40 tokens/sec
Step 16211 | loss: 3.126589 | lr:9.1741e-05 | norm 0.2857 | dt 338.76ms | 1547649.91 tokens/sec
Step 16212 | loss: 3.053955 | lr:9.1719e-05 | norm 0.2601 | dt 338.80ms | 1547487.63 tokens/sec
Step 16213 | loss: 3.046579 | lr:9.1698e-05 | norm 0.2883 | dt 337.94ms | 1551417.96 tokens/sec
Step 16214 | loss: 3.041967 | lr:9.1676e-05 | norm 0.2791 | dt 338.56ms | 1548582.84 tokens/sec
Step 16215 | loss: 3.109539 | lr:9.1654e-05 | norm 0.2618 | dt 337.96ms | 1551321.65 tokens/sec
Step 16216 | loss: 3.061205 | lr:9.1633e-05 | norm 0.2974 | dt 339.55ms | 1544052.97 tokens/sec
Step 16217 | loss: 3.055596 | lr:9.1611e-05 | norm 0.2811 | dt 338.05ms | 1550914.64 tokens/sec
Step 16218 | loss: 3.068235 | lr:9.1589e-05 | norm 0.2788 | dt 337.64ms | 1552779.65 tokens/sec
Step 16219 | loss: 3.091331 | lr:9.1568e-05 | norm 0.2674 | dt 338.51ms | 1548790.07 tokens/sec
Step 16220 | loss: 3.049867 | lr:9.1546e-05 | norm 0.2783 | dt 338.98ms | 1546671.32 tokens/sec
Step 16221 | loss: 3.057185 | lr:9.1524e-05 | norm 0.3010 | dt 337.31ms | 1554338.18 tokens/sec
Step 16222 | loss: 2.977406 | lr:9.1503e-05 | norm 0.3043 | dt 337.67ms | 1552662.34 tokens/sec
Step 16223 | loss: 3.044999 | lr:9.1481e-05 | norm 0.2921 | dt 337.66ms | 1552693.04 tokens/sec
Step 16224 | loss: 3.053951 | lr:9.1459e-05 | norm 0.2933 | dt 337.95ms | 1551384.03 tokens/sec
Step 16225 | loss: 3.029608 | lr:9.1438e-05 | norm 0.2657 | dt 337.70ms | 1552539.57 tokens/sec
Step 16226 | loss: 3.029935 | lr:9.1416e-05 | norm 0.2923 | dt 337.64ms | 1552779.65 tokens/sec
Step 16227 | loss: 3.099096 | lr:9.1394e-05 | norm 0.2911 | dt 337.45ms | 1553693.54 tokens/sec
Step 16228 | loss: 3.103810 | lr:9.1373e-05 | norm 0.3068 | dt 338.12ms | 1550613.90 tokens/sec
Step 16229 | loss: 3.027070 | lr:9.1351e-05 | norm 0.2984 | dt 337.54ms | 1553254.56 tokens/sec
Step 16230 | loss: 3.069091 | lr:9.1329e-05 | norm 0.2816 | dt 337.87ms | 1551745.29 tokens/sec
Step 16231 | loss: 3.049934 | lr:9.1308e-05 | norm 0.3108 | dt 338.46ms | 1549039.91 tokens/sec
Step 16232 | loss: 3.092382 | lr:9.1286e-05 | norm 0.2783 | dt 338.34ms | 1549605.35 tokens/sec
Step 16233 | loss: 3.014106 | lr:9.1265e-05 | norm 0.2911 | dt 338.10ms | 1550670.76 tokens/sec
Step 16234 | loss: 3.068616 | lr:9.1243e-05 | norm 0.3000 | dt 338.20ms | 1550208.35 tokens/sec
Step 16235 | loss: 3.128063 | lr:9.1222e-05 | norm 0.2865 | dt 338.37ms | 1549450.30 tokens/sec
Step 16236 | loss: 3.106707 | lr:9.1200e-05 | norm 0.3436 | dt 338.78ms | 1547594.36 tokens/sec
Step 16237 | loss: 3.070246 | lr:9.1178e-05 | norm 0.3086 | dt 338.60ms | 1548408.38 tokens/sec
Step 16238 | loss: 3.110965 | lr:9.1157e-05 | norm 0.3352 | dt 338.59ms | 1548442.18 tokens/sec
Step 16239 | loss: 3.027383 | lr:9.1135e-05 | norm 0.3268 | dt 338.89ms | 1547053.25 tokens/sec
Step 16240 | loss: 3.106943 | lr:9.1114e-05 | norm 0.3136 | dt 338.06ms | 1550878.54 tokens/sec
Step 16241 | loss: 3.069751 | lr:9.1092e-05 | norm 0.2900 | dt 338.53ms | 1548723.54 tokens/sec
Step 16242 | loss: 3.040170 | lr:9.1071e-05 | norm 0.2907 | dt 339.00ms | 1546574.51 tokens/sec
Step 16243 | loss: 3.101169 | lr:9.1049e-05 | norm 0.2964 | dt 338.69ms | 1547998.54 tokens/sec
Step 16244 | loss: 3.040643 | lr:9.1028e-05 | norm 0.2913 | dt 337.88ms | 1551685.07 tokens/sec
Step 16245 | loss: 3.032022 | lr:9.1006e-05 | norm 0.3018 | dt 338.10ms | 1550682.78 tokens/sec
Step 16246 | loss: 3.084302 | lr:9.0985e-05 | norm 0.2866 | dt 339.12ms | 1546026.50 tokens/sec
Step 16247 | loss: 3.055019 | lr:9.0963e-05 | norm 0.2788 | dt 338.49ms | 1548923.17 tokens/sec
Step 16248 | loss: 3.086835 | lr:9.0942e-05 | norm 0.3053 | dt 338.00ms | 1551129.06 tokens/sec
Step 16249 | loss: 3.060786 | lr:9.0920e-05 | norm 0.2889 | dt 339.32ms | 1545091.21 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 16250: 3.0979
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3047/10042=0.3034


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this isn't really a blog, but it's an outline on my own blog.
In the beginning, it
rank 5 sample 1 >Hello, I'm a language model, or any language model where people talk, they read, or they ask for clarification (like a text to describe something).
rank 5 sample 2 >Hello, I'm a language model, and I just wanted to know some words that make me feel as if I was speaking about the first time. I don
rank 5 sample 3 >Hello, I'm a language model, here is my page on "The Language Model", so if you're interested in understanding more, make sure you read me




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so my question has to do with writing a language. And that's a good start.
So, to answer the
rank 2 sample 1 >Hello, I'm a language model, and I get it right the first time you're building a website. How do I do that in an app?

rank 2 sample 2 >Hello, I'm a language model, and I've just finished writing an experiment about what it would take to go from one language model to the next. If
rank 2 sample 3 >Hello, I'm a language model, but a general purpose programming language. I had a big idea about how to set up my workspace on the day of the




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I thought I should make a sentence called "I can't do it" about how to stop myself.
A
rank 7 sample 1 >Hello, I'm a language model, since I like to learn to express emotions and have friends whom I consider to be in the same class of emotions, that


ddp_rank 3: ####### Printing generated samples ####### 

rank 7 sample 2 >Hello, I'm a language model, so I'll keep hearing about new approaches, but even if you aren't a language model, you may be inspired by
rank 7 sample 3 >Hello, I'm a language model, and it's a language where I love teaching because I love learning new things. It's a huge comfort to me.


rank 3 sample 0 >Hello, I'm a language model, so I'm not trying to make anything for the computer, but I wanna pretend it has nothing to do with it being
rank 3 sample 1 >Hello, I'm a language model, so what I'm going to ask you is if you want to know how to write a program? I'm assuming the
rank 3 sample 2 >Hello, I'm a language model, so this question is hard to answer. (What are some reasons why I need this question and an example?) A great
rank 3 sample 3 >Hello, I'm a language model, so in this example, the way we use 'Hello" to indicate a 'Hello' should be equal to 'Hello




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I can't see anything wrong.<|endoftext|>This week, we are featuring a couple of amazing nonfiction book reviews from
rank 4 sample 1 >Hello, I'm a language model, am not a professor of languages -
It should work even better if i don't write it in C, perl or
rank 4 sample 2 >Hello, I'm a language model, I read one every year and I think the other one is going to be the number one. And with that we're
rank 4 sample 3 >Hello, I'm a language model, so it didn't take any time to make this tutorial look awesome, what should I do now for the sake of the




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so I do a lot of things which make writing and interpreting possible. I make sure my programming code is written in a
rank 1 sample 1 >Hello, I'm a language model, a programmer who creates a language to do complex tasks. As a programmer, I build out my most basic language, the
rank 1 sample 2 >Hello, I'm a language model, but people still say that I'm a language model. But why is this?
Well, there's no real relationship
rank 1 sample 3 >Hello, I'm a language model, so I'm thinking of making a new world.
Thank you for any guidance. Also, you can visit http://




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know what you're trying to work with. I want to look in the world, to find the best solution


ddp_rank 6: ####### Printing generated samples ####### 

rank 0 sample 1 >Hello, I'm a language model, and here's what I did. By the time some data was uploaded, and the program was ready to use, it
rank 0 sample 2 >Hello, I'm a language model, and I used it once (since it's only really a few hours ago). If I didn't use it in my
rank 6 sample 0 >Hello, I'm a language model, as if I was speaking here.
The first word I hear comes in the dictionary. It's a noun. This
rank 0 sample 3 >Hello, I'm a language model, and for the first time I've worked out myself. I'm using C, an older and much more powerful C for


rank 6 sample 1 >Hello, I'm a language model, so I can't get a newbie started. And I'm not very good at writing programs, but the software is
rank 6 sample 2 >Hello, I'm a language model, but that's a very big topic. In particular, I think, it needs to be a language based one."<|endoftext|>
rank 6 sample 3 >Hello, I'm a language model, so I can describe it as being like this:
- A block of words
The rest of the text must be


Step 16250 | loss: 3.047051 | lr:9.0899e-05 | norm 0.2807 | dt 13700.43ms | 38268.00 tokens/sec
Step 16251 | loss: 3.068492 | lr:9.0877e-05 | norm 0.3057 | dt 336.10ms | 1559923.97 tokens/sec
Step 16252 | loss: 3.152267 | lr:9.0856e-05 | norm 0.3258 | dt 337.03ms | 1555626.87 tokens/sec
Step 16253 | loss: 3.082739 | lr:9.0834e-05 | norm 0.3057 | dt 1022.41ms | 512795.44 tokens/sec
Step 16254 | loss: 3.096250 | lr:9.0813e-05 | norm 0.3092 | dt 334.44ms | 1567676.17 tokens/sec
Step 16255 | loss: 3.082400 | lr:9.0792e-05 | norm 0.2897 | dt 337.48ms | 1553553.04 tokens/sec
Step 16256 | loss: 3.076246 | lr:9.0770e-05 | norm 0.3251 | dt 339.16ms | 1545831.96 tokens/sec
Step 16257 | loss: 3.048395 | lr:9.0749e-05 | norm 0.2975 | dt 337.98ms | 1551229.72 tokens/sec
Step 16258 | loss: 3.092734 | lr:9.0727e-05 | norm 0.2833 | dt 336.97ms | 1555885.53 tokens/sec
Step 16259 | loss: 3.074959 | lr:9.0706e-05 | norm 0.2758 | dt 337.46ms | 1553640.85 tokens/sec
Step 16260 | loss: 3.074496 | lr:9.0685e-05 | norm 0.2945 | dt 336.83ms | 1556541.91 tokens/sec
Step 16261 | loss: 3.061623 | lr:9.0663e-05 | norm 0.2701 | dt 336.44ms | 1558360.85 tokens/sec
Step 16262 | loss: 3.061777 | lr:9.0642e-05 | norm 0.2688 | dt 337.97ms | 1551273.49 tokens/sec
Step 16263 | loss: 3.029914 | lr:9.0620e-05 | norm 0.2897 | dt 337.33ms | 1554215.14 tokens/sec
Step 16264 | loss: 3.055287 | lr:9.0599e-05 | norm 0.2845 | dt 336.68ms | 1557249.56 tokens/sec
Step 16265 | loss: 3.000400 | lr:9.0578e-05 | norm 0.2937 | dt 337.16ms | 1555024.04 tokens/sec
Step 16266 | loss: 3.033330 | lr:9.0556e-05 | norm 0.2796 | dt 337.35ms | 1554151.43 tokens/sec
Step 16267 | loss: 3.044192 | lr:9.0535e-05 | norm 0.2960 | dt 336.63ms | 1557463.53 tokens/sec
Step 16268 | loss: 3.089226 | lr:9.0514e-05 | norm 0.3107 | dt 337.99ms | 1551215.50 tokens/sec
Step 16269 | loss: 3.018649 | lr:9.0492e-05 | norm 0.2919 | dt 337.28ms | 1554478.82 tokens/sec
Step 16270 | loss: 3.140658 | lr:9.0471e-05 | norm 0.3101 | dt 337.47ms | 1553600.23 tokens/sec
Step 16271 | loss: 3.075837 | lr:9.0450e-05 | norm 0.3184 | dt 337.48ms | 1553548.65 tokens/sec
Step 16272 | loss: 3.125558 | lr:9.0428e-05 | norm 0.2825 | dt 337.63ms | 1552860.80 tokens/sec
Step 16273 | loss: 3.093388 | lr:9.0407e-05 | norm 0.3209 | dt 336.66ms | 1557331.17 tokens/sec
Step 16274 | loss: 3.083956 | lr:9.0386e-05 | norm 0.2980 | dt 337.14ms | 1555082.33 tokens/sec
Step 16275 | loss: 3.045979 | lr:9.0364e-05 | norm 0.2995 | dt 337.78ms | 1552173.55 tokens/sec
Step 16276 | loss: 3.074723 | lr:9.0343e-05 | norm 0.2996 | dt 337.26ms | 1554557.94 tokens/sec
Step 16277 | loss: 3.131615 | lr:9.0322e-05 | norm 0.3045 | dt 337.35ms | 1554153.63 tokens/sec
Step 16278 | loss: 3.054753 | lr:9.0301e-05 | norm 0.2979 | dt 337.08ms | 1555390.30 tokens/sec
Step 16279 | loss: 3.072657 | lr:9.0279e-05 | norm 0.2974 | dt 337.75ms | 1552289.70 tokens/sec
Step 16280 | loss: 3.056159 | lr:9.0258e-05 | norm 0.2761 | dt 337.53ms | 1553295.16 tokens/sec
Step 16281 | loss: 3.127168 | lr:9.0237e-05 | norm 0.2940 | dt 337.16ms | 1554989.96 tokens/sec
Step 16282 | loss: 3.096369 | lr:9.0216e-05 | norm 0.2807 | dt 337.95ms | 1551391.69 tokens/sec
Step 16283 | loss: 3.064138 | lr:9.0194e-05 | norm 0.2956 | dt 337.81ms | 1552026.76 tokens/sec
Step 16284 | loss: 3.169086 | lr:9.0173e-05 | norm 0.2969 | dt 337.75ms | 1552309.42 tokens/sec
Step 16285 | loss: 3.053928 | lr:9.0152e-05 | norm 0.2976 | dt 337.78ms | 1552151.64 tokens/sec
Step 16286 | loss: 3.081702 | lr:9.0131e-05 | norm 0.2928 | dt 339.13ms | 1545976.50 tokens/sec
Step 16287 | loss: 3.077202 | lr:9.0109e-05 | norm 0.2714 | dt 337.86ms | 1551796.76 tokens/sec
Step 16288 | loss: 3.044454 | lr:9.0088e-05 | norm 0.2775 | dt 338.01ms | 1551109.36 tokens/sec
Step 16289 | loss: 3.056150 | lr:9.0067e-05 | norm 0.2728 | dt 338.19ms | 1550299.06 tokens/sec
Step 16290 | loss: 3.092640 | lr:9.0046e-05 | norm 0.3094 | dt 338.36ms | 1549485.24 tokens/sec
Step 16291 | loss: 3.073812 | lr:9.0025e-05 | norm 0.2744 | dt 337.49ms | 1553493.77 tokens/sec
Step 16292 | loss: 3.095428 | lr:9.0004e-05 | norm 0.2790 | dt 337.50ms | 1553438.90 tokens/sec
Step 16293 | loss: 3.029818 | lr:8.9982e-05 | norm 0.2923 | dt 338.81ms | 1547428.83 tokens/sec
Step 16294 | loss: 2.997496 | lr:8.9961e-05 | norm 0.2890 | dt 338.25ms | 1549986.54 tokens/sec
Step 16295 | loss: 3.020399 | lr:8.9940e-05 | norm 0.2925 | dt 338.96ms | 1546762.70 tokens/sec
Step 16296 | loss: 3.021722 | lr:8.9919e-05 | norm 0.2867 | dt 337.65ms | 1552747.86 tokens/sec
Step 16297 | loss: 3.029785 | lr:8.9898e-05 | norm 0.2855 | dt 338.78ms | 1547568.22 tokens/sec
Step 16298 | loss: 3.059715 | lr:8.9877e-05 | norm 0.2780 | dt 338.51ms | 1548800.98 tokens/sec
Step 16299 | loss: 3.049520 | lr:8.9856e-05 | norm 0.3005 | dt 339.41ms | 1544697.23 tokens/sec
Step 16300 | loss: 3.069821 | lr:8.9834e-05 | norm 0.2728 | dt 338.51ms | 1548826.07 tokens/sec
Step 16301 | loss: 3.051270 | lr:8.9813e-05 | norm 0.2936 | dt 338.74ms | 1547775.18 tokens/sec
Step 16302 | loss: 3.020285 | lr:8.9792e-05 | norm 0.2782 | dt 338.61ms | 1548330.97 tokens/sec
Step 16303 | loss: 3.124121 | lr:8.9771e-05 | norm 0.3068 | dt 337.58ms | 1553064.78 tokens/sec
Step 16304 | loss: 3.064444 | lr:8.9750e-05 | norm 0.3066 | dt 338.54ms | 1548695.18 tokens/sec
Step 16305 | loss: 3.035969 | lr:8.9729e-05 | norm 0.2888 | dt 337.67ms | 1552649.19 tokens/sec
Step 16306 | loss: 3.048549 | lr:8.9708e-05 | norm 0.2987 | dt 337.74ms | 1552360.92 tokens/sec
Step 16307 | loss: 3.062606 | lr:8.9687e-05 | norm 0.2883 | dt 338.34ms | 1549593.34 tokens/sec
Step 16308 | loss: 3.018584 | lr:8.9666e-05 | norm 0.3016 | dt 337.87ms | 1551747.48 tokens/sec
Step 16309 | loss: 3.045888 | lr:8.9645e-05 | norm 0.2623 | dt 338.00ms | 1551132.34 tokens/sec
Step 16310 | loss: 3.019668 | lr:8.9624e-05 | norm 0.3014 | dt 339.07ms | 1546253.70 tokens/sec
Step 16311 | loss: 3.057153 | lr:8.9603e-05 | norm 0.2763 | dt 338.28ms | 1549841.25 tokens/sec
Step 16312 | loss: 3.080946 | lr:8.9582e-05 | norm 0.2799 | dt 338.66ms | 1548134.76 tokens/sec
Step 16313 | loss: 3.044577 | lr:8.9561e-05 | norm 0.2845 | dt 338.61ms | 1548351.69 tokens/sec
Step 16314 | loss: 3.091064 | lr:8.9540e-05 | norm 0.2749 | dt 337.39ms | 1553955.94 tokens/sec
Step 16315 | loss: 3.073588 | lr:8.9519e-05 | norm 0.2766 | dt 337.70ms | 1552517.65 tokens/sec
Step 16316 | loss: 3.061067 | lr:8.9497e-05 | norm 0.3008 | dt 338.48ms | 1548939.53 tokens/sec
Step 16317 | loss: 3.101942 | lr:8.9477e-05 | norm 0.2700 | dt 337.50ms | 1553429.03 tokens/sec
Step 16318 | loss: 3.097590 | lr:8.9456e-05 | norm 0.2869 | dt 338.30ms | 1549787.73 tokens/sec
Step 16319 | loss: 3.100438 | lr:8.9435e-05 | norm 0.2903 | dt 338.77ms | 1547625.95 tokens/sec
Step 16320 | loss: 3.098949 | lr:8.9414e-05 | norm 0.2733 | dt 337.62ms | 1552902.47 tokens/sec
Step 16321 | loss: 3.071612 | lr:8.9393e-05 | norm 0.2853 | dt 338.15ms | 1550451.00 tokens/sec
Step 16322 | loss: 3.064687 | lr:8.9372e-05 | norm 0.2760 | dt 338.43ms | 1549197.06 tokens/sec
Step 16323 | loss: 3.047761 | lr:8.9351e-05 | norm 0.2757 | dt 337.33ms | 1554206.35 tokens/sec
Step 16324 | loss: 3.075359 | lr:8.9330e-05 | norm 0.2791 | dt 338.71ms | 1547874.32 tokens/sec
Step 16325 | loss: 3.072186 | lr:8.9309e-05 | norm 0.2827 | dt 337.89ms | 1551631.42 tokens/sec
Step 16326 | loss: 3.052307 | lr:8.9288e-05 | norm 0.2802 | dt 337.79ms | 1552124.25 tokens/sec
Step 16327 | loss: 3.050387 | lr:8.9267e-05 | norm 0.2838 | dt 338.54ms | 1548667.91 tokens/sec
Step 16328 | loss: 3.042270 | lr:8.9246e-05 | norm 0.2640 | dt 338.26ms | 1549962.51 tokens/sec
Step 16329 | loss: 3.042457 | lr:8.9225e-05 | norm 0.2709 | dt 338.09ms | 1550740.74 tokens/sec
Step 16330 | loss: 3.064495 | lr:8.9204e-05 | norm 0.2745 | dt 337.82ms | 1551962.13 tokens/sec
Step 16331 | loss: 3.065073 | lr:8.9183e-05 | norm 0.2779 | dt 337.50ms | 1553435.61 tokens/sec
Step 16332 | loss: 3.079220 | lr:8.9162e-05 | norm 0.2882 | dt 337.69ms | 1552580.13 tokens/sec
Step 16333 | loss: 3.014104 | lr:8.9142e-05 | norm 0.2559 | dt 337.79ms | 1552096.87 tokens/sec
Step 16334 | loss: 3.062791 | lr:8.9121e-05 | norm 0.2841 | dt 337.87ms | 1551726.68 tokens/sec
Step 16335 | loss: 3.095402 | lr:8.9100e-05 | norm 0.2545 | dt 337.26ms | 1554529.37 tokens/sec
Step 16336 | loss: 3.055861 | lr:8.9079e-05 | norm 0.3020 | dt 338.05ms | 1550934.33 tokens/sec
Step 16337 | loss: 3.064442 | lr:8.9058e-05 | norm 0.3209 | dt 338.06ms | 1550867.61 tokens/sec
Step 16338 | loss: 3.098109 | lr:8.9037e-05 | norm 0.3087 | dt 337.24ms | 1554642.57 tokens/sec
Step 16339 | loss: 3.159461 | lr:8.9016e-05 | norm 0.2987 | dt 934.41ms | 561092.83 tokens/sec
Step 16340 | loss: 3.069985 | lr:8.8996e-05 | norm 0.2999 | dt 336.77ms | 1556825.11 tokens/sec
Step 16341 | loss: 3.089138 | lr:8.8975e-05 | norm 0.2964 | dt 338.62ms | 1548287.37 tokens/sec
Step 16342 | loss: 3.079761 | lr:8.8954e-05 | norm 0.2742 | dt 338.22ms | 1550117.65 tokens/sec
Step 16343 | loss: 3.195579 | lr:8.8933e-05 | norm 0.3585 | dt 336.03ms | 1560257.11 tokens/sec
Step 16344 | loss: 3.065443 | lr:8.8912e-05 | norm 0.2833 | dt 337.89ms | 1551662.08 tokens/sec
Step 16345 | loss: 3.042442 | lr:8.8891e-05 | norm 0.3178 | dt 337.74ms | 1552332.43 tokens/sec
Step 16346 | loss: 2.995268 | lr:8.8871e-05 | norm 0.2758 | dt 337.22ms | 1554748.09 tokens/sec
Step 16347 | loss: 3.046334 | lr:8.8850e-05 | norm 0.3147 | dt 337.43ms | 1553757.21 tokens/sec
Step 16348 | loss: 3.108569 | lr:8.8829e-05 | norm 0.3109 | dt 337.53ms | 1553309.42 tokens/sec
Step 16349 | loss: 3.049237 | lr:8.8808e-05 | norm 0.2636 | dt 337.31ms | 1554329.39 tokens/sec
Step 16350 | loss: 3.047178 | lr:8.8788e-05 | norm 0.3093 | dt 337.21ms | 1554782.16 tokens/sec
Step 16351 | loss: 3.073917 | lr:8.8767e-05 | norm 0.2764 | dt 338.33ms | 1549622.82 tokens/sec
Step 16352 | loss: 3.066086 | lr:8.8746e-05 | norm 0.2900 | dt 338.16ms | 1550412.74 tokens/sec
Step 16353 | loss: 3.010771 | lr:8.8725e-05 | norm 0.2889 | dt 340.61ms | 1539243.47 tokens/sec
Step 16354 | loss: 3.121398 | lr:8.8705e-05 | norm 0.2596 | dt 337.56ms | 1553154.73 tokens/sec
Step 16355 | loss: 3.076909 | lr:8.8684e-05 | norm 0.2956 | dt 337.60ms | 1552979.23 tokens/sec
Step 16356 | loss: 3.089337 | lr:8.8663e-05 | norm 0.2812 | dt 338.66ms | 1548117.33 tokens/sec
Step 16357 | loss: 3.043082 | lr:8.8642e-05 | norm 0.2914 | dt 337.96ms | 1551322.74 tokens/sec
Step 16358 | loss: 3.053795 | lr:8.8622e-05 | norm 0.3110 | dt 337.46ms | 1553650.73 tokens/sec
Step 16359 | loss: 3.066310 | lr:8.8601e-05 | norm 0.2887 | dt 338.67ms | 1548078.09 tokens/sec
Step 16360 | loss: 3.074625 | lr:8.8580e-05 | norm 0.2946 | dt 337.32ms | 1554262.38 tokens/sec
Step 16361 | loss: 3.076078 | lr:8.8560e-05 | norm 0.2979 | dt 338.06ms | 1550893.86 tokens/sec
Step 16362 | loss: 3.033993 | lr:8.8539e-05 | norm 0.2819 | dt 338.67ms | 1548072.64 tokens/sec
Step 16363 | loss: 3.053892 | lr:8.8518e-05 | norm 0.2961 | dt 337.69ms | 1552587.80 tokens/sec
Step 16364 | loss: 3.080989 | lr:8.8498e-05 | norm 0.2887 | dt 338.23ms | 1550078.32 tokens/sec
Step 16365 | loss: 3.087236 | lr:8.8477e-05 | norm 0.2935 | dt 337.76ms | 1552237.10 tokens/sec
Step 16366 | loss: 2.987237 | lr:8.8456e-05 | norm 0.2759 | dt 337.54ms | 1553263.34 tokens/sec
Step 16367 | loss: 3.062944 | lr:8.8436e-05 | norm 0.2769 | dt 338.08ms | 1550768.08 tokens/sec
Step 16368 | loss: 3.042750 | lr:8.8415e-05 | norm 0.2706 | dt 338.20ms | 1550242.23 tokens/sec
Step 16369 | loss: 3.143328 | lr:8.8394e-05 | norm 0.3577 | dt 337.54ms | 1553266.63 tokens/sec
Step 16370 | loss: 3.067838 | lr:8.8374e-05 | norm 0.3180 | dt 338.17ms | 1550389.78 tokens/sec
Step 16371 | loss: 3.000579 | lr:8.8353e-05 | norm 0.3126 | dt 338.23ms | 1550072.85 tokens/sec
Step 16372 | loss: 3.116661 | lr:8.8333e-05 | norm 0.3145 | dt 337.79ms | 1552106.72 tokens/sec
Step 16373 | loss: 3.054317 | lr:8.8312e-05 | norm 0.5594 | dt 337.87ms | 1551757.34 tokens/sec
Step 16374 | loss: 3.038614 | lr:8.8291e-05 | norm 0.3310 | dt 338.40ms | 1549301.84 tokens/sec
Step 16375 | loss: 3.068444 | lr:8.8271e-05 | norm 0.3246 | dt 337.06ms | 1555451.91 tokens/sec
Step 16376 | loss: 3.041092 | lr:8.8250e-05 | norm 0.3087 | dt 338.62ms | 1548317.89 tokens/sec
Step 16377 | loss: 3.078772 | lr:8.8230e-05 | norm 0.2997 | dt 338.34ms | 1549573.68 tokens/sec
Step 16378 | loss: 3.061423 | lr:8.8209e-05 | norm 0.3177 | dt 338.11ms | 1550643.42 tokens/sec
Step 16379 | loss: 3.001030 | lr:8.8189e-05 | norm 0.3073 | dt 337.27ms | 1554484.32 tokens/sec
Step 16380 | loss: 3.024757 | lr:8.8168e-05 | norm 0.2897 | dt 338.18ms | 1550302.34 tokens/sec
Step 16381 | loss: 3.080661 | lr:8.8147e-05 | norm 0.3066 | dt 338.50ms | 1548852.25 tokens/sec
Step 16382 | loss: 3.042501 | lr:8.8127e-05 | norm 0.2912 | dt 337.66ms | 1552724.83 tokens/sec
Step 16383 | loss: 3.014152 | lr:8.8106e-05 | norm 0.2722 | dt 337.65ms | 1552768.69 tokens/sec
Step 16384 | loss: 3.021099 | lr:8.8086e-05 | norm 0.2779 | dt 338.26ms | 1549957.04 tokens/sec
Step 16385 | loss: 3.061664 | lr:8.8065e-05 | norm 0.2872 | dt 338.34ms | 1549597.70 tokens/sec
Step 16386 | loss: 3.046218 | lr:8.8045e-05 | norm 0.2987 | dt 337.65ms | 1552770.88 tokens/sec
Step 16387 | loss: 3.103418 | lr:8.8024e-05 | norm 0.2886 | dt 338.35ms | 1549524.55 tokens/sec
Step 16388 | loss: 3.095203 | lr:8.8004e-05 | norm 0.2786 | dt 338.08ms | 1550800.89 tokens/sec
Step 16389 | loss: 3.062934 | lr:8.7983e-05 | norm 0.2881 | dt 338.31ms | 1549733.12 tokens/sec
Step 16390 | loss: 3.096986 | lr:8.7963e-05 | norm 0.3128 | dt 337.19ms | 1554871.21 tokens/sec
Step 16391 | loss: 3.100585 | lr:8.7942e-05 | norm 0.2768 | dt 337.75ms | 1552298.46 tokens/sec
Step 16392 | loss: 2.997374 | lr:8.7922e-05 | norm 0.2808 | dt 338.17ms | 1550363.55 tokens/sec
Step 16393 | loss: 3.038383 | lr:8.7901e-05 | norm 0.3030 | dt 338.40ms | 1549312.75 tokens/sec
Step 16394 | loss: 3.073736 | lr:8.7881e-05 | norm 0.2906 | dt 338.05ms | 1550924.48 tokens/sec
Step 16395 | loss: 3.039761 | lr:8.7861e-05 | norm 0.2891 | dt 337.23ms | 1554703.02 tokens/sec
Step 16396 | loss: 3.059845 | lr:8.7840e-05 | norm 0.2948 | dt 337.93ms | 1551485.82 tokens/sec
Step 16397 | loss: 3.071823 | lr:8.7820e-05 | norm 0.2866 | dt 338.65ms | 1548184.90 tokens/sec
Step 16398 | loss: 3.039759 | lr:8.7799e-05 | norm 0.2835 | dt 336.97ms | 1555866.81 tokens/sec
Step 16399 | loss: 3.085113 | lr:8.7779e-05 | norm 0.2749 | dt 337.96ms | 1551343.54 tokens/sec
Step 16400 | loss: 3.035615 | lr:8.7758e-05 | norm 0.2839 | dt 337.86ms | 1551784.71 tokens/sec
Step 16401 | loss: 3.065022 | lr:8.7738e-05 | norm 0.2688 | dt 338.42ms | 1549203.61 tokens/sec
Step 16402 | loss: 3.048343 | lr:8.7718e-05 | norm 0.2707 | dt 337.92ms | 1551508.81 tokens/sec
Step 16403 | loss: 3.075733 | lr:8.7697e-05 | norm 0.2771 | dt 337.45ms | 1553669.39 tokens/sec
Step 16404 | loss: 3.069227 | lr:8.7677e-05 | norm 0.2845 | dt 337.54ms | 1553246.88 tokens/sec
Step 16405 | loss: 3.122176 | lr:8.7656e-05 | norm 0.3090 | dt 338.12ms | 1550597.50 tokens/sec
Step 16406 | loss: 3.054182 | lr:8.7636e-05 | norm 0.2697 | dt 338.06ms | 1550891.67 tokens/sec
Step 16407 | loss: 3.034717 | lr:8.7616e-05 | norm 0.3019 | dt 338.23ms | 1550115.47 tokens/sec
Step 16408 | loss: 3.082760 | lr:8.7595e-05 | norm 0.2893 | dt 337.51ms | 1553379.65 tokens/sec
Step 16409 | loss: 3.189100 | lr:8.7575e-05 | norm 0.3417 | dt 338.58ms | 1548502.15 tokens/sec
Step 16410 | loss: 3.015174 | lr:8.7555e-05 | norm 0.3272 | dt 337.60ms | 1552978.14 tokens/sec
Step 16411 | loss: 3.080883 | lr:8.7534e-05 | norm 0.2883 | dt 337.06ms | 1555494.83 tokens/sec
Step 16412 | loss: 3.059000 | lr:8.7514e-05 | norm 0.2921 | dt 338.19ms | 1550254.25 tokens/sec
Step 16413 | loss: 3.012764 | lr:8.7494e-05 | norm 0.2832 | dt 337.86ms | 1551781.43 tokens/sec
Step 16414 | loss: 3.030727 | lr:8.7473e-05 | norm 0.2719 | dt 338.71ms | 1547909.19 tokens/sec
Step 16415 | loss: 3.037950 | lr:8.7453e-05 | norm 0.2827 | dt 337.68ms | 1552608.63 tokens/sec
Step 16416 | loss: 3.059958 | lr:8.7433e-05 | norm 0.2856 | dt 338.39ms | 1549344.41 tokens/sec
Step 16417 | loss: 3.097834 | lr:8.7413e-05 | norm 0.2688 | dt 338.06ms | 1550875.26 tokens/sec
Step 16418 | loss: 3.008598 | lr:8.7392e-05 | norm 0.2713 | dt 337.81ms | 1552031.14 tokens/sec
Step 16419 | loss: 3.049234 | lr:8.7372e-05 | norm 0.2825 | dt 337.80ms | 1552082.62 tokens/sec
Step 16420 | loss: 3.072913 | lr:8.7352e-05 | norm 0.2797 | dt 337.82ms | 1551966.51 tokens/sec
Step 16421 | loss: 3.050558 | lr:8.7331e-05 | norm 0.2733 | dt 337.98ms | 1551236.29 tokens/sec
Step 16422 | loss: 3.083507 | lr:8.7311e-05 | norm 0.2880 | dt 339.89ms | 1542503.09 tokens/sec
Step 16423 | loss: 3.067059 | lr:8.7291e-05 | norm 0.3016 | dt 337.16ms | 1555011.95 tokens/sec
Step 16424 | loss: 3.098237 | lr:8.7271e-05 | norm 0.2844 | dt 338.20ms | 1550209.45 tokens/sec
Step 16425 | loss: 3.091492 | lr:8.7250e-05 | norm 0.2679 | dt 338.45ms | 1549066.10 tokens/sec
Step 16426 | loss: 3.056815 | lr:8.7230e-05 | norm 0.2942 | dt 337.94ms | 1551400.45 tokens/sec
Step 16427 | loss: 3.046029 | lr:8.7210e-05 | norm 0.2917 | dt 338.32ms | 1549683.97 tokens/sec
Step 16428 | loss: 3.113223 | lr:8.7190e-05 | norm 0.2897 | dt 338.06ms | 1550879.64 tokens/sec
Step 16429 | loss: 3.035997 | lr:8.7170e-05 | norm 0.2987 | dt 340.17ms | 1541263.05 tokens/sec
Step 16430 | loss: 3.011592 | lr:8.7149e-05 | norm 0.3299 | dt 338.45ms | 1549075.92 tokens/sec
Step 16431 | loss: 3.077179 | lr:8.7129e-05 | norm 0.3083 | dt 339.50ms | 1544304.54 tokens/sec
Step 16432 | loss: 3.062608 | lr:8.7109e-05 | norm 0.3047 | dt 339.30ms | 1545211.73 tokens/sec
Step 16433 | loss: 3.056007 | lr:8.7089e-05 | norm 0.3094 | dt 338.60ms | 1548392.02 tokens/sec
Step 16434 | loss: 3.055226 | lr:8.7069e-05 | norm 0.3032 | dt 338.87ms | 1547152.30 tokens/sec
Step 16435 | loss: 3.039558 | lr:8.7049e-05 | norm 0.2978 | dt 338.70ms | 1547963.67 tokens/sec
Step 16436 | loss: 3.038055 | lr:8.7028e-05 | norm 0.3043 | dt 338.89ms | 1547080.46 tokens/sec
Step 16437 | loss: 3.144892 | lr:8.7008e-05 | norm 0.2947 | dt 338.94ms | 1546824.72 tokens/sec
Step 16438 | loss: 3.057462 | lr:8.6988e-05 | norm 0.3624 | dt 338.36ms | 1549483.06 tokens/sec
Step 16439 | loss: 3.084060 | lr:8.6968e-05 | norm 0.2830 | dt 338.93ms | 1546873.68 tokens/sec
Step 16440 | loss: 3.116023 | lr:8.6948e-05 | norm 0.3487 | dt 338.28ms | 1549856.54 tokens/sec
Step 16441 | loss: 3.110447 | lr:8.6928e-05 | norm 0.3300 | dt 339.15ms | 1545872.17 tokens/sec
Step 16442 | loss: 3.082224 | lr:8.6908e-05 | norm 0.3304 | dt 899.09ms | 583133.19 tokens/sec
Step 16443 | loss: 3.102154 | lr:8.6888e-05 | norm 0.3243 | dt 336.60ms | 1557600.33 tokens/sec
Step 16444 | loss: 3.086322 | lr:8.6867e-05 | norm 0.2994 | dt 336.63ms | 1557483.39 tokens/sec
Step 16445 | loss: 3.042650 | lr:8.6847e-05 | norm 0.2968 | dt 340.24ms | 1540916.37 tokens/sec
Step 16446 | loss: 3.032542 | lr:8.6827e-05 | norm 0.3194 | dt 338.15ms | 1550459.74 tokens/sec
Step 16447 | loss: 3.121668 | lr:8.6807e-05 | norm 0.3382 | dt 337.11ms | 1555234.10 tokens/sec
Step 16448 | loss: 2.953714 | lr:8.6787e-05 | norm 0.3617 | dt 339.45ms | 1544505.20 tokens/sec
Step 16449 | loss: 3.092566 | lr:8.6767e-05 | norm 0.2767 | dt 337.78ms | 1552179.03 tokens/sec
Step 16450 | loss: 3.081309 | lr:8.6747e-05 | norm 0.3218 | dt 337.87ms | 1551762.81 tokens/sec
Step 16451 | loss: 3.036567 | lr:8.6727e-05 | norm 0.3127 | dt 338.75ms | 1547706.55 tokens/sec
Step 16452 | loss: 3.073904 | lr:8.6707e-05 | norm 0.3078 | dt 338.25ms | 1549989.82 tokens/sec
Step 16453 | loss: 3.036358 | lr:8.6687e-05 | norm 0.3166 | dt 337.76ms | 1552260.11 tokens/sec
Step 16454 | loss: 3.039469 | lr:8.6667e-05 | norm 0.2968 | dt 338.05ms | 1550916.83 tokens/sec
Step 16455 | loss: 3.034674 | lr:8.6647e-05 | norm 0.3113 | dt 338.07ms | 1550823.86 tokens/sec
Step 16456 | loss: 3.047656 | lr:8.6627e-05 | norm 0.2853 | dt 338.48ms | 1548928.62 tokens/sec
Step 16457 | loss: 3.067671 | lr:8.6607e-05 | norm 0.3125 | dt 338.05ms | 1550912.45 tokens/sec
Step 16458 | loss: 3.040540 | lr:8.6587e-05 | norm 0.2750 | dt 337.46ms | 1553640.85 tokens/sec
Step 16459 | loss: 3.042277 | lr:8.6567e-05 | norm 0.2899 | dt 338.59ms | 1548456.35 tokens/sec
Step 16460 | loss: 3.026983 | lr:8.6547e-05 | norm 0.2914 | dt 338.66ms | 1548142.39 tokens/sec
Step 16461 | loss: 3.100914 | lr:8.6527e-05 | norm 0.2842 | dt 338.93ms | 1546894.36 tokens/sec
Step 16462 | loss: 3.004103 | lr:8.6507e-05 | norm 0.2814 | dt 338.20ms | 1550224.75 tokens/sec
Step 16463 | loss: 2.997006 | lr:8.6487e-05 | norm 0.2933 | dt 338.41ms | 1549268.00 tokens/sec
Step 16464 | loss: 3.049181 | lr:8.6467e-05 | norm 0.2805 | dt 343.93ms | 1524388.40 tokens/sec
Step 16465 | loss: 3.019022 | lr:8.6447e-05 | norm 0.2871 | dt 337.67ms | 1552668.92 tokens/sec
Step 16466 | loss: 3.051934 | lr:8.6427e-05 | norm 0.2902 | dt 337.16ms | 1555021.84 tokens/sec
Step 16467 | loss: 3.013337 | lr:8.6407e-05 | norm 0.2886 | dt 339.96ms | 1542216.42 tokens/sec
Step 16468 | loss: 3.034254 | lr:8.6387e-05 | norm 0.2807 | dt 338.01ms | 1551108.27 tokens/sec
Step 16469 | loss: 3.027333 | lr:8.6367e-05 | norm 0.3104 | dt 338.12ms | 1550614.99 tokens/sec
Step 16470 | loss: 3.057786 | lr:8.6347e-05 | norm 0.2970 | dt 338.21ms | 1550176.66 tokens/sec
Step 16471 | loss: 3.029467 | lr:8.6328e-05 | norm 0.2946 | dt 337.72ms | 1552428.87 tokens/sec
Step 16472 | loss: 3.027023 | lr:8.6308e-05 | norm 0.3118 | dt 338.26ms | 1549933.01 tokens/sec
Step 16473 | loss: 3.047796 | lr:8.6288e-05 | norm 0.2818 | dt 339.15ms | 1545874.34 tokens/sec
Step 16474 | loss: 2.997708 | lr:8.6268e-05 | norm 0.3812 | dt 337.98ms | 1551234.10 tokens/sec
Step 16475 | loss: 3.128674 | lr:8.6248e-05 | norm 0.3470 | dt 338.26ms | 1549959.23 tokens/sec
Step 16476 | loss: 3.086343 | lr:8.6228e-05 | norm 0.3255 | dt 338.30ms | 1549786.64 tokens/sec
Step 16477 | loss: 3.123697 | lr:8.6208e-05 | norm 0.2834 | dt 338.37ms | 1549447.03 tokens/sec
Step 16478 | loss: 3.108781 | lr:8.6188e-05 | norm 0.3483 | dt 337.95ms | 1551397.16 tokens/sec
Step 16479 | loss: 3.089676 | lr:8.6169e-05 | norm 0.3143 | dt 337.77ms | 1552215.19 tokens/sec
Step 16480 | loss: 3.014256 | lr:8.6149e-05 | norm 0.3074 | dt 338.35ms | 1549556.21 tokens/sec
Step 16481 | loss: 3.079388 | lr:8.6129e-05 | norm 0.3281 | dt 338.19ms | 1550282.67 tokens/sec
Step 16482 | loss: 3.052079 | lr:8.6109e-05 | norm 0.3023 | dt 337.46ms | 1553641.95 tokens/sec
Step 16483 | loss: 3.123398 | lr:8.6089e-05 | norm 0.3356 | dt 338.22ms | 1550124.21 tokens/sec
Step 16484 | loss: 3.066807 | lr:8.6069e-05 | norm 0.3017 | dt 338.18ms | 1550302.34 tokens/sec
Step 16485 | loss: 3.035557 | lr:8.6050e-05 | norm 0.3056 | dt 337.97ms | 1551297.57 tokens/sec
Step 16486 | loss: 3.023840 | lr:8.6030e-05 | norm 0.2977 | dt 337.79ms | 1552097.96 tokens/sec
Step 16487 | loss: 3.058732 | lr:8.6010e-05 | norm 0.3166 | dt 338.28ms | 1549878.39 tokens/sec
Step 16488 | loss: 3.055391 | lr:8.5990e-05 | norm 0.3179 | dt 337.98ms | 1551239.57 tokens/sec
Step 16489 | loss: 3.056736 | lr:8.5970e-05 | norm 0.2968 | dt 338.11ms | 1550658.73 tokens/sec
Step 16490 | loss: 3.065522 | lr:8.5951e-05 | norm 0.3364 | dt 337.98ms | 1551255.99 tokens/sec
Step 16491 | loss: 3.093067 | lr:8.5931e-05 | norm 0.3332 | dt 338.40ms | 1549323.67 tokens/sec
Step 16492 | loss: 3.050670 | lr:8.5911e-05 | norm 0.2650 | dt 338.74ms | 1547781.72 tokens/sec
Step 16493 | loss: 3.104758 | lr:8.5891e-05 | norm 0.3480 | dt 338.06ms | 1550894.95 tokens/sec
Step 16494 | loss: 3.111086 | lr:8.5872e-05 | norm 0.2888 | dt 338.48ms | 1548943.89 tokens/sec
Step 16495 | loss: 3.116270 | lr:8.5852e-05 | norm 0.2992 | dt 337.99ms | 1551200.18 tokens/sec
Step 16496 | loss: 3.060039 | lr:8.5832e-05 | norm 0.2949 | dt 338.02ms | 1551060.13 tokens/sec
Step 16497 | loss: 3.127327 | lr:8.5812e-05 | norm 0.4481 | dt 338.21ms | 1550164.64 tokens/sec
Step 16498 | loss: 3.010151 | lr:8.5793e-05 | norm 0.3818 | dt 337.52ms | 1553370.87 tokens/sec
Step 16499 | loss: 3.021900 | lr:8.5773e-05 | norm 0.3286 | dt 338.52ms | 1548766.08 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 16500: 3.0979
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3040/10042=0.3027


ddp_rank 5: ####### Printing generated samples ####### 


rank 5 sample 0 >Hello, I'm a language model, but that is not how I thought. Now I just need to be aware of what I'm talking about and it really
rank 5 sample 1 >Hello, I'm a language model, here's the first tutorial, it's really quite simple and doesn't have a lot of jargon. Just to be honest

ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 2 >Hello, I'm a language model, and I was thinking up a programming language. So, I created a tool to make sense of the world. And what
rank 5 sample 3 >Hello, I'm a language model, my name is Elizabeth. Thanks for your time and feedback on this.
Thanks for sharing your interest in learning a new


rank 3 sample 0 >Hello, I'm a language model, so I'm gonna try my hand at reading a bunch of HTML files, instead of getting bogged down with what that


ddp_rank 7: ####### Printing generated samples ####### 

rank 3 sample 1 >Hello, I'm a language model, so you need to be knowledgeable, curious and, at times, very strict.
As a matter of fact (ifrank 7 sample 0 >Hello, I'm a language model, so I thought I should look at some pretty cool tools to help me better understand code. One piece of software I thought

rank 7 sample 1 >Hello, I'm a language model, don't let me forget, because to put it in simpler language, you need to get a lot of details, right
rank 3 sample 2 >Hello, I'm a language model, so this question is your first thought. Does programming really teach us anything about our natural world? To me, all programming
rank 7 sample 2 >Hello, I'm a language model, so I'll get into it
to help you think about how to understand it.
I'm a language trainer who
rank 3 sample 3 >Hello, I'm a language model, so i'm trying to define it. I am a teacher and my teachers are good in different ways all of them.


rank 7 sample 3 >Hello, I'm a language model, and you want to create a system whose implementation details are easy to find.
You can do this by using the package




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so my question has to do with languages, not languages themselves.
My first attempt to ask a question came from an
rank 2 sample 1 >Hello, I'm a language model, and I enjoy the fun of it -- writing, checking, reading, math, and so on -- so that I can
rank 2 sample 2 >Hello, I'm a language model, and I've just got to make your language model, and I want to write that into your project, so I just
rank 2 sample 3 >Hello, I'm a language model, but if I'm a programmer, I use one because I think it's kind of like making yourself an assistant for something




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, meaning to teach it as easy and useful as possible. So what's my approach?
When I teach, I don
rank 6 sample 1 >Hello, I'm a language model, so I want to explain what the different variables are:
- 1/2: 1
- 2nd: 1
rank 6 sample 2 >Hello, I'm a language model, but this one's got me wondering whether I'll be able to help out myself.
I'm actually interested in learning
rank 6 sample 3 >Hello, I'm a language model, so I want to use the C programming language, but I don't know the syntax of why I's calling it an




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so, after I teach my students Arabic to French in grade seven, I could have students learn more about French and Arabic
rank 1 sample 1 >Hello, I'm a language model, a programmer who creates the code, and tries to create models to simulate the environment of these planets.
I'm a
rank 1 sample 2 >Hello, I'm a language model, but still I need to learn how to write code. And for this to happen, I'll first create an HTML language
rank 1 sample 3 >Hello, I'm a language model, so I'm looking at that as a "language" based on context information--what might appear to be a simple sentence




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I'd like you to take a snapshot of that part. That's my first time ever in a course on Python
rank 0 sample 1 >Hello, I'm a language model, and when I'm done, I figured out that everything depends on your location: the internet, your device, your browser
rank 0 sample 2 >Hello, I'm a language model, and I wish you my first hand on the difference between a real machine and a simulated one.
I'm not a
rank 0 sample 3 >Hello, I'm a language model, and for the sake of this article, you first need to understand the meaning. "C" stands for "language",




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I'd like to introduce you to something called 'language grammar'. Let's start by considering the syntax, the structure
rank 4 sample 1 >Hello, I'm a language model, learning about the main principles of using an application or something useful. I use it, as well as many of the language
rank 4 sample 2 >Hello, I'm a language model, I see and then say it, you have to do it with the basic syntax just, well, a little stuff,
rank 4 sample 3 >Hello, I'm a language model, so it seemed an impossible task to model what that machine represents so well. But to me it really seemed like a great


Step 16500 | loss: 3.052473 | lr:8.5753e-05 | norm 0.3321 | dt 13179.28ms | 39781.24 tokens/sec
Step 16501 | loss: 3.013513 | lr:8.5734e-05 | norm 0.3334 | dt 335.78ms | 1561395.98 tokens/sec
Step 16502 | loss: 3.013659 | lr:8.5714e-05 | norm 0.3263 | dt 336.82ms | 1556597.00 tokens/sec
Step 16503 | loss: 3.057426 | lr:8.5694e-05 | norm 0.3273 | dt 337.04ms | 1555543.24 tokens/sec
Step 16504 | loss: 2.995116 | lr:8.5675e-05 | norm 0.3259 | dt 336.77ms | 1556814.09 tokens/sec
Step 16505 | loss: 3.035507 | lr:8.5655e-05 | norm 0.3078 | dt 336.41ms | 1558480.13 tokens/sec
Step 16506 | loss: 3.066191 | lr:8.5635e-05 | norm 0.3021 | dt 337.89ms | 1551673.03 tokens/sec
Step 16507 | loss: 3.039743 | lr:8.5616e-05 | norm 0.3212 | dt 337.16ms | 1555035.04 tokens/sec
Step 16508 | loss: 3.108911 | lr:8.5596e-05 | norm 0.3151 | dt 336.72ms | 1557026.84 tokens/sec
Step 16509 | loss: 3.051143 | lr:8.5576e-05 | norm 0.3275 | dt 337.06ms | 1555477.22 tokens/sec
Step 16510 | loss: 3.094378 | lr:8.5557e-05 | norm 0.3219 | dt 337.26ms | 1554566.73 tokens/sec
Step 16511 | loss: 3.096856 | lr:8.5537e-05 | norm 0.3330 | dt 336.52ms | 1557965.59 tokens/sec
Step 16512 | loss: 3.079212 | lr:8.5518e-05 | norm 0.3113 | dt 338.03ms | 1551002.15 tokens/sec
Step 16513 | loss: 3.018740 | lr:8.5498e-05 | norm 0.3212 | dt 336.78ms | 1556747.96 tokens/sec
Step 16514 | loss: 3.037660 | lr:8.5478e-05 | norm 0.2981 | dt 336.04ms | 1560192.91 tokens/sec
Step 16515 | loss: 3.087738 | lr:8.5459e-05 | norm 0.3148 | dt 337.52ms | 1553332.46 tokens/sec
Step 16516 | loss: 3.079316 | lr:8.5439e-05 | norm 0.2976 | dt 337.01ms | 1555722.62 tokens/sec
Step 16517 | loss: 3.077703 | lr:8.5420e-05 | norm 0.2937 | dt 337.40ms | 1553886.76 tokens/sec
Step 16518 | loss: 3.093316 | lr:8.5400e-05 | norm 0.3060 | dt 337.14ms | 1555094.42 tokens/sec
Step 16519 | loss: 3.106463 | lr:8.5380e-05 | norm 0.2736 | dt 337.89ms | 1551656.60 tokens/sec
Step 16520 | loss: 3.078855 | lr:8.5361e-05 | norm 0.3305 | dt 336.95ms | 1555978.00 tokens/sec
Step 16521 | loss: 3.081335 | lr:8.5341e-05 | norm 0.2995 | dt 336.53ms | 1557907.10 tokens/sec
Step 16522 | loss: 3.016861 | lr:8.5322e-05 | norm 0.2851 | dt 337.69ms | 1552583.41 tokens/sec
Step 16523 | loss: 3.085273 | lr:8.5302e-05 | norm 0.2834 | dt 336.97ms | 1555875.62 tokens/sec
Step 16524 | loss: 3.056311 | lr:8.5283e-05 | norm 0.2881 | dt 337.60ms | 1552962.78 tokens/sec
Step 16525 | loss: 3.073009 | lr:8.5263e-05 | norm 0.2811 | dt 337.69ms | 1552562.59 tokens/sec
Step 16526 | loss: 3.109205 | lr:8.5244e-05 | norm 0.2925 | dt 337.97ms | 1551263.65 tokens/sec
Step 16527 | loss: 3.070835 | lr:8.5224e-05 | norm 0.2769 | dt 337.33ms | 1554242.60 tokens/sec
Step 16528 | loss: 3.103765 | lr:8.5205e-05 | norm 0.2893 | dt 337.22ms | 1554742.59 tokens/sec
Step 16529 | loss: 3.060662 | lr:8.5185e-05 | norm 0.2884 | dt 931.55ms | 562814.95 tokens/sec
Step 16530 | loss: 3.077273 | lr:8.5166e-05 | norm 0.2874 | dt 336.75ms | 1556922.11 tokens/sec
Step 16531 | loss: 3.103410 | lr:8.5146e-05 | norm 0.2980 | dt 338.02ms | 1551045.91 tokens/sec
Step 16532 | loss: 3.028177 | lr:8.5127e-05 | norm 0.2931 | dt 337.62ms | 1552887.11 tokens/sec
Step 16533 | loss: 3.051513 | lr:8.5107e-05 | norm 0.2835 | dt 336.86ms | 1556376.66 tokens/sec
Step 16534 | loss: 3.054231 | lr:8.5088e-05 | norm 0.3045 | dt 337.92ms | 1551492.39 tokens/sec
Step 16535 | loss: 3.038809 | lr:8.5068e-05 | norm 0.2858 | dt 338.98ms | 1546653.91 tokens/sec
Step 16536 | loss: 3.048201 | lr:8.5049e-05 | norm 0.2984 | dt 339.39ms | 1544775.36 tokens/sec
Step 16537 | loss: 3.009101 | lr:8.5030e-05 | norm 0.2956 | dt 338.72ms | 1547842.73 tokens/sec
Step 16538 | loss: 3.039956 | lr:8.5010e-05 | norm 0.2994 | dt 337.89ms | 1551630.33 tokens/sec
Step 16539 | loss: 3.039780 | lr:8.4991e-05 | norm 0.2744 | dt 338.99ms | 1546634.33 tokens/sec
Step 16540 | loss: 3.007920 | lr:8.4971e-05 | norm 0.2886 | dt 338.98ms | 1546676.76 tokens/sec
Step 16541 | loss: 3.047575 | lr:8.4952e-05 | norm 0.2809 | dt 337.65ms | 1552737.99 tokens/sec
Step 16542 | loss: 2.996847 | lr:8.4933e-05 | norm 0.2873 | dt 337.99ms | 1551181.58 tokens/sec
Step 16543 | loss: 3.199881 | lr:8.4913e-05 | norm 0.3391 | dt 337.85ms | 1551826.33 tokens/sec
Step 16544 | loss: 3.053136 | lr:8.4894e-05 | norm 0.2998 | dt 338.64ms | 1548232.86 tokens/sec
Step 16545 | loss: 3.094300 | lr:8.4874e-05 | norm 0.2890 | dt 337.92ms | 1551517.57 tokens/sec
Step 16546 | loss: 3.033409 | lr:8.4855e-05 | norm 0.2891 | dt 338.75ms | 1547695.66 tokens/sec
Step 16547 | loss: 3.124876 | lr:8.4836e-05 | norm 0.3313 | dt 339.00ms | 1546581.03 tokens/sec
Step 16548 | loss: 3.108237 | lr:8.4816e-05 | norm 0.3157 | dt 338.75ms | 1547720.71 tokens/sec
Step 16549 | loss: 3.079127 | lr:8.4797e-05 | norm 0.2801 | dt 339.56ms | 1544036.71 tokens/sec
Step 16550 | loss: 3.060840 | lr:8.4778e-05 | norm 0.3208 | dt 338.80ms | 1547488.72 tokens/sec
Step 16551 | loss: 3.108816 | lr:8.4758e-05 | norm 0.2885 | dt 339.28ms | 1545317.05 tokens/sec
Step 16552 | loss: 3.052641 | lr:8.4739e-05 | norm 0.3295 | dt 339.39ms | 1544816.60 tokens/sec
Step 16553 | loss: 3.082287 | lr:8.4720e-05 | norm 0.3338 | dt 339.15ms | 1545894.99 tokens/sec
Step 16554 | loss: 3.133056 | lr:8.4700e-05 | norm 0.3333 | dt 339.10ms | 1546098.24 tokens/sec
Step 16555 | loss: 3.065104 | lr:8.4681e-05 | norm 0.2856 | dt 340.11ms | 1541508.30 tokens/sec
Step 16556 | loss: 3.025765 | lr:8.4662e-05 | norm 0.3241 | dt 339.24ms | 1545468.01 tokens/sec
Step 16557 | loss: 2.959676 | lr:8.4642e-05 | norm 0.3534 | dt 339.21ms | 1545619.00 tokens/sec
Step 16558 | loss: 3.059476 | lr:8.4623e-05 | norm 0.3230 | dt 339.37ms | 1544870.86 tokens/sec
Step 16559 | loss: 3.077080 | lr:8.4604e-05 | norm 0.3173 | dt 338.85ms | 1547279.66 tokens/sec
Step 16560 | loss: 2.987308 | lr:8.4585e-05 | norm 0.2900 | dt 339.36ms | 1544941.41 tokens/sec
Step 16561 | loss: 3.021383 | lr:8.4565e-05 | norm 0.3120 | dt 339.55ms | 1544061.64 tokens/sec
Step 16562 | loss: 3.128888 | lr:8.4546e-05 | norm 0.3053 | dt 339.39ms | 1544774.28 tokens/sec
Step 16563 | loss: 3.059650 | lr:8.4527e-05 | norm 0.3154 | dt 338.38ms | 1549396.81 tokens/sec
Step 16564 | loss: 3.030737 | lr:8.4508e-05 | norm 0.3091 | dt 338.84ms | 1547305.79 tokens/sec
Step 16565 | loss: 3.074940 | lr:8.4488e-05 | norm 0.3012 | dt 338.87ms | 1547155.56 tokens/sec
Step 16566 | loss: 3.091944 | lr:8.4469e-05 | norm 0.2925 | dt 339.84ms | 1542757.40 tokens/sec
Step 16567 | loss: 2.995947 | lr:8.4450e-05 | norm 0.3186 | dt 338.15ms | 1550480.51 tokens/sec
Step 16568 | loss: 3.075237 | lr:8.4431e-05 | norm 0.3004 | dt 338.32ms | 1549682.88 tokens/sec
Step 16569 | loss: 3.000696 | lr:8.4412e-05 | norm 0.2959 | dt 338.22ms | 1550129.67 tokens/sec
Step 16570 | loss: 3.043518 | lr:8.4392e-05 | norm 0.3092 | dt 338.76ms | 1547672.78 tokens/sec
Step 16571 | loss: 3.032456 | lr:8.4373e-05 | norm 0.2822 | dt 337.91ms | 1551556.98 tokens/sec
Step 16572 | loss: 2.997122 | lr:8.4354e-05 | norm 0.2912 | dt 339.04ms | 1546404.84 tokens/sec
Step 16573 | loss: 3.084196 | lr:8.4335e-05 | norm 0.2916 | dt 338.04ms | 1550944.17 tokens/sec
Step 16574 | loss: 3.040595 | lr:8.4316e-05 | norm 0.2731 | dt 337.63ms | 1552838.87 tokens/sec
Step 16575 | loss: 2.998237 | lr:8.4297e-05 | norm 0.2968 | dt 338.55ms | 1548628.65 tokens/sec
Step 16576 | loss: 3.010465 | lr:8.4277e-05 | norm 0.2945 | dt 338.15ms | 1550482.70 tokens/sec
Step 16577 | loss: 3.100567 | lr:8.4258e-05 | norm 0.2800 | dt 337.95ms | 1551356.67 tokens/sec
Step 16578 | loss: 3.027302 | lr:8.4239e-05 | norm 0.2959 | dt 338.28ms | 1549883.85 tokens/sec
Step 16579 | loss: 3.127924 | lr:8.4220e-05 | norm 0.3075 | dt 337.71ms | 1552463.94 tokens/sec
Step 16580 | loss: 3.090329 | lr:8.4201e-05 | norm 0.2891 | dt 338.15ms | 1550443.35 tokens/sec
Step 16581 | loss: 3.080582 | lr:8.4182e-05 | norm 0.2763 | dt 338.00ms | 1551154.22 tokens/sec
Step 16582 | loss: 3.120861 | lr:8.4163e-05 | norm 0.2836 | dt 337.27ms | 1554504.09 tokens/sec
Step 16583 | loss: 3.050761 | lr:8.4143e-05 | norm 0.2797 | dt 337.64ms | 1552797.20 tokens/sec
Step 16584 | loss: 3.055138 | lr:8.4124e-05 | norm 0.3156 | dt 337.43ms | 1553761.60 tokens/sec
Step 16585 | loss: 3.089701 | lr:8.4105e-05 | norm 0.2801 | dt 337.72ms | 1552451.88 tokens/sec
Step 16586 | loss: 3.086449 | lr:8.4086e-05 | norm 0.2856 | dt 337.67ms | 1552684.27 tokens/sec
Step 16587 | loss: 3.050933 | lr:8.4067e-05 | norm 0.2895 | dt 338.24ms | 1550025.87 tokens/sec
Step 16588 | loss: 3.095805 | lr:8.4048e-05 | norm 0.2851 | dt 337.10ms | 1555275.90 tokens/sec
Step 16589 | loss: 3.167444 | lr:8.4029e-05 | norm 0.2857 | dt 337.49ms | 1553503.65 tokens/sec
Step 16590 | loss: 3.026202 | lr:8.4010e-05 | norm 0.3166 | dt 338.63ms | 1548243.76 tokens/sec
Step 16591 | loss: 3.049945 | lr:8.3991e-05 | norm 0.2667 | dt 339.72ms | 1543306.34 tokens/sec
Step 16592 | loss: 3.092767 | lr:8.3972e-05 | norm 0.3046 | dt 337.79ms | 1552101.25 tokens/sec
Step 16593 | loss: 3.038994 | lr:8.3953e-05 | norm 0.2779 | dt 337.20ms | 1554848.12 tokens/sec
Step 16594 | loss: 3.060984 | lr:8.3934e-05 | norm 0.3171 | dt 338.02ms | 1551066.69 tokens/sec
Step 16595 | loss: 3.051947 | lr:8.3915e-05 | norm 0.2962 | dt 337.96ms | 1551328.21 tokens/sec
Step 16596 | loss: 3.043921 | lr:8.3896e-05 | norm 0.2884 | dt 337.54ms | 1553280.90 tokens/sec
Step 16597 | loss: 3.123793 | lr:8.3877e-05 | norm 0.2738 | dt 337.81ms | 1552002.66 tokens/sec
Step 16598 | loss: 3.036102 | lr:8.3858e-05 | norm 0.3096 | dt 337.58ms | 1553095.50 tokens/sec
Step 16599 | loss: 3.096738 | lr:8.3839e-05 | norm 0.3112 | dt 337.92ms | 1551509.91 tokens/sec
Step 16600 | loss: 3.012296 | lr:8.3820e-05 | norm 0.2704 | dt 337.24ms | 1554631.58 tokens/sec
Step 16601 | loss: 3.117396 | lr:8.3801e-05 | norm 0.3242 | dt 337.70ms | 1552506.69 tokens/sec
Step 16602 | loss: 3.082276 | lr:8.3782e-05 | norm 0.2904 | dt 339.20ms | 1545650.51 tokens/sec
Step 16603 | loss: 3.063914 | lr:8.3763e-05 | norm 0.2902 | dt 337.58ms | 1553079.04 tokens/sec
Step 16604 | loss: 2.953547 | lr:8.3744e-05 | norm 0.3096 | dt 337.86ms | 1551794.57 tokens/sec
Step 16605 | loss: 3.017922 | lr:8.3725e-05 | norm 0.2928 | dt 338.09ms | 1550749.49 tokens/sec
Step 16606 | loss: 3.003704 | lr:8.3706e-05 | norm 0.2919 | dt 338.03ms | 1550994.49 tokens/sec
Step 16607 | loss: 3.039253 | lr:8.3687e-05 | norm 0.2779 | dt 337.34ms | 1554203.06 tokens/sec
Step 16608 | loss: 3.057357 | lr:8.3668e-05 | norm 0.3306 | dt 338.30ms | 1549768.07 tokens/sec
Step 16609 | loss: 3.083019 | lr:8.3649e-05 | norm 0.2977 | dt 337.82ms | 1551991.71 tokens/sec
Step 16610 | loss: 3.015742 | lr:8.3630e-05 | norm 0.3109 | dt 337.12ms | 1555179.11 tokens/sec
Step 16611 | loss: 3.036846 | lr:8.3612e-05 | norm 0.3115 | dt 337.49ms | 1553492.68 tokens/sec
Step 16612 | loss: 2.991314 | lr:8.3593e-05 | norm 0.2985 | dt 337.72ms | 1552446.40 tokens/sec
Step 16613 | loss: 3.079068 | lr:8.3574e-05 | norm 0.3484 | dt 338.26ms | 1549970.15 tokens/sec
Step 16614 | loss: 3.104919 | lr:8.3555e-05 | norm 0.3051 | dt 338.41ms | 1549268.00 tokens/sec
Step 16615 | loss: 3.107989 | lr:8.3536e-05 | norm 0.3309 | dt 337.99ms | 1551189.24 tokens/sec
Step 16616 | loss: 3.132064 | lr:8.3517e-05 | norm 0.3170 | dt 337.82ms | 1551970.89 tokens/sec
Step 16617 | loss: 3.114613 | lr:8.3498e-05 | norm 0.3092 | dt 338.22ms | 1550159.18 tokens/sec
Step 16618 | loss: 3.091668 | lr:8.3479e-05 | norm 0.3184 | dt 337.72ms | 1552450.79 tokens/sec
Step 16619 | loss: 3.073606 | lr:8.3461e-05 | norm 0.2968 | dt 338.82ms | 1547396.16 tokens/sec
Step 16620 | loss: 3.103936 | lr:8.3442e-05 | norm 0.3170 | dt 337.55ms | 1553223.85 tokens/sec
Step 16621 | loss: 3.083686 | lr:8.3423e-05 | norm 0.3388 | dt 338.34ms | 1549592.24 tokens/sec
Step 16622 | loss: 3.031308 | lr:8.3404e-05 | norm 0.2940 | dt 338.38ms | 1549421.92 tokens/sec
Step 16623 | loss: 3.071694 | lr:8.3385e-05 | norm 0.2984 | dt 338.14ms | 1550492.54 tokens/sec
Step 16624 | loss: 2.992578 | lr:8.3367e-05 | norm 0.2992 | dt 338.50ms | 1548860.98 tokens/sec
Step 16625 | loss: 3.038493 | lr:8.3348e-05 | norm 0.2737 | dt 338.38ms | 1549403.36 tokens/sec
Step 16626 | loss: 3.061350 | lr:8.3329e-05 | norm 0.2760 | dt 338.52ms | 1548779.16 tokens/sec
Step 16627 | loss: 3.056185 | lr:8.3310e-05 | norm 0.2887 | dt 337.72ms | 1552423.39 tokens/sec
Step 16628 | loss: 3.042620 | lr:8.3291e-05 | norm 0.2619 | dt 338.14ms | 1550491.45 tokens/sec
Step 16629 | loss: 3.082583 | lr:8.3273e-05 | norm 0.2953 | dt 339.18ms | 1545768.94 tokens/sec
Step 16630 | loss: 3.123401 | lr:8.3254e-05 | norm 0.4533 | dt 338.60ms | 1548413.83 tokens/sec
Step 16631 | loss: 3.056171 | lr:8.3235e-05 | norm 0.3424 | dt 902.09ms | 581193.44 tokens/sec
Step 16632 | loss: 3.109753 | lr:8.3216e-05 | norm 0.3268 | dt 336.65ms | 1557358.75 tokens/sec
Step 16633 | loss: 3.055912 | lr:8.3198e-05 | norm 0.2857 | dt 337.95ms | 1551392.79 tokens/sec
Step 16634 | loss: 3.064197 | lr:8.3179e-05 | norm 0.3237 | dt 338.65ms | 1548152.20 tokens/sec
Step 16635 | loss: 3.055509 | lr:8.3160e-05 | norm 0.2942 | dt 337.35ms | 1554117.38 tokens/sec
Step 16636 | loss: 3.040779 | lr:8.3141e-05 | norm 0.3073 | dt 338.04ms | 1550941.98 tokens/sec
Step 16637 | loss: 3.009336 | lr:8.3123e-05 | norm 0.2838 | dt 337.89ms | 1551658.79 tokens/sec
Step 16638 | loss: 3.048482 | lr:8.3104e-05 | norm 0.2901 | dt 338.06ms | 1550871.98 tokens/sec
Step 16639 | loss: 2.985807 | lr:8.3085e-05 | norm 0.3014 | dt 337.59ms | 1553020.91 tokens/sec
Step 16640 | loss: 3.044061 | lr:8.3067e-05 | norm 0.2835 | dt 337.64ms | 1552808.16 tokens/sec
Step 16641 | loss: 3.007045 | lr:8.3048e-05 | norm 0.2735 | dt 337.57ms | 1553107.56 tokens/sec
Step 16642 | loss: 3.090279 | lr:8.3029e-05 | norm 0.2879 | dt 338.10ms | 1550680.60 tokens/sec
Step 16643 | loss: 3.056066 | lr:8.3011e-05 | norm 0.2763 | dt 338.14ms | 1550492.54 tokens/sec
Step 16644 | loss: 3.076253 | lr:8.2992e-05 | norm 0.2862 | dt 337.33ms | 1554214.04 tokens/sec
Step 16645 | loss: 2.989908 | lr:8.2973e-05 | norm 0.2769 | dt 337.12ms | 1555203.30 tokens/sec
Step 16646 | loss: 3.054652 | lr:8.2955e-05 | norm 0.2743 | dt 338.19ms | 1550266.27 tokens/sec
Step 16647 | loss: 2.983790 | lr:8.2936e-05 | norm 0.2634 | dt 337.26ms | 1554573.33 tokens/sec
Step 16648 | loss: 3.156693 | lr:8.2917e-05 | norm 0.3076 | dt 337.64ms | 1552814.74 tokens/sec
Step 16649 | loss: 3.097444 | lr:8.2899e-05 | norm 0.2956 | dt 338.38ms | 1549421.92 tokens/sec
Step 16650 | loss: 3.022927 | lr:8.2880e-05 | norm 0.2966 | dt 338.47ms | 1548997.36 tokens/sec
Step 16651 | loss: 3.079076 | lr:8.2862e-05 | norm 0.2971 | dt 338.43ms | 1549154.49 tokens/sec
Step 16652 | loss: 3.060228 | lr:8.2843e-05 | norm 0.3033 | dt 338.43ms | 1549183.96 tokens/sec
Step 16653 | loss: 3.070110 | lr:8.2824e-05 | norm 0.2942 | dt 338.47ms | 1548985.36 tokens/sec
Step 16654 | loss: 3.067764 | lr:8.2806e-05 | norm 0.2975 | dt 337.92ms | 1551521.95 tokens/sec
Step 16655 | loss: 3.143784 | lr:8.2787e-05 | norm 0.3001 | dt 338.63ms | 1548241.58 tokens/sec
Step 16656 | loss: 3.045035 | lr:8.2769e-05 | norm 0.3019 | dt 338.41ms | 1549265.82 tokens/sec
Step 16657 | loss: 3.131496 | lr:8.2750e-05 | norm 0.3032 | dt 337.98ms | 1551257.08 tokens/sec
Step 16658 | loss: 3.083673 | lr:8.2731e-05 | norm 0.3109 | dt 338.44ms | 1549134.85 tokens/sec
Step 16659 | loss: 3.072059 | lr:8.2713e-05 | norm 0.2968 | dt 342.93ms | 1528843.85 tokens/sec
Step 16660 | loss: 3.104655 | lr:8.2694e-05 | norm 0.3015 | dt 338.47ms | 1548978.81 tokens/sec
Step 16661 | loss: 3.066711 | lr:8.2676e-05 | norm 0.2774 | dt 338.11ms | 1550628.11 tokens/sec
Step 16662 | loss: 3.058100 | lr:8.2657e-05 | norm 0.2725 | dt 338.54ms | 1548676.64 tokens/sec
Step 16663 | loss: 3.084760 | lr:8.2639e-05 | norm 0.2972 | dt 338.37ms | 1549439.39 tokens/sec
Step 16664 | loss: 3.074491 | lr:8.2620e-05 | norm 0.2834 | dt 339.31ms | 1545143.32 tokens/sec
Step 16665 | loss: 3.110109 | lr:8.2602e-05 | norm 0.2781 | dt 338.80ms | 1547475.65 tokens/sec
Step 16666 | loss: 3.056712 | lr:8.2583e-05 | norm 0.2964 | dt 339.30ms | 1545195.44 tokens/sec
Step 16667 | loss: 3.041431 | lr:8.2565e-05 | norm 0.2898 | dt 338.16ms | 1550394.16 tokens/sec
Step 16668 | loss: 3.073406 | lr:8.2546e-05 | norm 0.2848 | dt 339.01ms | 1546510.34 tokens/sec
Step 16669 | loss: 3.042808 | lr:8.2528e-05 | norm 0.3306 | dt 338.25ms | 1550014.95 tokens/sec
Step 16670 | loss: 3.128698 | lr:8.2509e-05 | norm 0.2769 | dt 338.60ms | 1548381.12 tokens/sec
Step 16671 | loss: 3.185113 | lr:8.2491e-05 | norm 0.3658 | dt 338.35ms | 1549561.67 tokens/sec
Step 16672 | loss: 3.072299 | lr:8.2472e-05 | norm 0.3005 | dt 338.47ms | 1549010.45 tokens/sec
Step 16673 | loss: 3.033401 | lr:8.2454e-05 | norm 0.3101 | dt 339.37ms | 1544898.00 tokens/sec
Step 16674 | loss: 3.048231 | lr:8.2435e-05 | norm 0.2943 | dt 337.42ms | 1553809.91 tokens/sec
Step 16675 | loss: 3.052239 | lr:8.2417e-05 | norm 0.3100 | dt 337.54ms | 1553241.40 tokens/sec
Step 16676 | loss: 3.129457 | lr:8.2399e-05 | norm 0.2883 | dt 338.41ms | 1549286.56 tokens/sec
Step 16677 | loss: 3.041402 | lr:8.2380e-05 | norm 0.2892 | dt 337.70ms | 1552509.97 tokens/sec
Step 16678 | loss: 3.078989 | lr:8.2362e-05 | norm 0.2907 | dt 337.29ms | 1554424.98 tokens/sec
Step 16679 | loss: 3.030405 | lr:8.2343e-05 | norm 0.2863 | dt 338.64ms | 1548203.43 tokens/sec
Step 16680 | loss: 3.021665 | lr:8.2325e-05 | norm 0.3063 | dt 337.70ms | 1552531.90 tokens/sec
Step 16681 | loss: 3.086727 | lr:8.2307e-05 | norm 0.2892 | dt 338.22ms | 1550135.14 tokens/sec
Step 16682 | loss: 3.034417 | lr:8.2288e-05 | norm 0.2720 | dt 339.11ms | 1546077.59 tokens/sec
Step 16683 | loss: 3.009057 | lr:8.2270e-05 | norm 0.2922 | dt 338.95ms | 1546802.96 tokens/sec
Step 16684 | loss: 3.076346 | lr:8.2251e-05 | norm 0.3019 | dt 338.26ms | 1549941.75 tokens/sec
Step 16685 | loss: 3.080365 | lr:8.2233e-05 | norm 0.2948 | dt 338.97ms | 1546728.98 tokens/sec
Step 16686 | loss: 3.144300 | lr:8.2215e-05 | norm 0.3015 | dt 339.04ms | 1546368.96 tokens/sec
Step 16687 | loss: 3.105218 | lr:8.2196e-05 | norm 0.3151 | dt 337.86ms | 1551774.86 tokens/sec
Step 16688 | loss: 3.119262 | lr:8.2178e-05 | norm 0.2939 | dt 339.22ms | 1545547.31 tokens/sec
Step 16689 | loss: 3.046629 | lr:8.2160e-05 | norm 0.2906 | dt 338.28ms | 1549846.71 tokens/sec
Step 16690 | loss: 3.056724 | lr:8.2141e-05 | norm 0.3034 | dt 337.74ms | 1552329.14 tokens/sec
Step 16691 | loss: 3.095085 | lr:8.2123e-05 | norm 0.3398 | dt 339.41ms | 1544689.64 tokens/sec
Step 16692 | loss: 3.070487 | lr:8.2105e-05 | norm 0.3174 | dt 338.77ms | 1547632.48 tokens/sec
Step 16693 | loss: 3.094100 | lr:8.2086e-05 | norm 0.3147 | dt 337.53ms | 1553300.64 tokens/sec
Step 16694 | loss: 3.083426 | lr:8.2068e-05 | norm 0.3030 | dt 339.37ms | 1544904.51 tokens/sec
Step 16695 | loss: 3.047074 | lr:8.2050e-05 | norm 0.3173 | dt 338.84ms | 1547316.68 tokens/sec
Step 16696 | loss: 3.061106 | lr:8.2032e-05 | norm 0.2969 | dt 338.21ms | 1550201.80 tokens/sec
Step 16697 | loss: 3.088582 | lr:8.2013e-05 | norm 0.2950 | dt 337.95ms | 1551396.07 tokens/sec
Step 16698 | loss: 3.094695 | lr:8.1995e-05 | norm 0.3060 | dt 338.35ms | 1549554.03 tokens/sec
Step 16699 | loss: 3.054026 | lr:8.1977e-05 | norm 0.2844 | dt 338.31ms | 1549737.49 tokens/sec
Step 16700 | loss: 3.087208 | lr:8.1958e-05 | norm 0.2943 | dt 338.06ms | 1550878.54 tokens/sec
Step 16701 | loss: 3.063085 | lr:8.1940e-05 | norm 0.2784 | dt 338.29ms | 1549805.21 tokens/sec
Step 16702 | loss: 3.077142 | lr:8.1922e-05 | norm 0.2958 | dt 338.78ms | 1547598.72 tokens/sec
Step 16703 | loss: 3.026398 | lr:8.1904e-05 | norm 0.2700 | dt 337.88ms | 1551680.69 tokens/sec
Step 16704 | loss: 3.092579 | lr:8.1886e-05 | norm 0.2783 | dt 338.41ms | 1549249.45 tokens/sec
Step 16705 | loss: 3.095690 | lr:8.1867e-05 | norm 0.2948 | dt 338.62ms | 1548305.90 tokens/sec
Step 16706 | loss: 3.104990 | lr:8.1849e-05 | norm 0.2689 | dt 338.06ms | 1550867.61 tokens/sec
Step 16707 | loss: 3.033279 | lr:8.1831e-05 | norm 0.2753 | dt 338.07ms | 1550828.23 tokens/sec
Step 16708 | loss: 3.103253 | lr:8.1813e-05 | norm 0.2998 | dt 338.69ms | 1548001.81 tokens/sec
Step 16709 | loss: 3.011849 | lr:8.1794e-05 | norm 0.2831 | dt 338.76ms | 1547674.96 tokens/sec
Step 16710 | loss: 3.004426 | lr:8.1776e-05 | norm 0.2810 | dt 338.58ms | 1548505.42 tokens/sec
Step 16711 | loss: 3.029430 | lr:8.1758e-05 | norm 0.3155 | dt 337.99ms | 1551197.99 tokens/sec
Step 16712 | loss: 3.010821 | lr:8.1740e-05 | norm 0.2693 | dt 338.32ms | 1549661.04 tokens/sec
Step 16713 | loss: 3.013813 | lr:8.1722e-05 | norm 0.2854 | dt 338.83ms | 1547341.72 tokens/sec
Step 16714 | loss: 3.046075 | lr:8.1704e-05 | norm 0.2802 | dt 338.35ms | 1549534.37 tokens/sec
Step 16715 | loss: 3.047266 | lr:8.1685e-05 | norm 0.2754 | dt 338.90ms | 1547044.54 tokens/sec
Step 16716 | loss: 3.044303 | lr:8.1667e-05 | norm 0.2869 | dt 338.28ms | 1549871.84 tokens/sec
Step 16717 | loss: 3.038657 | lr:8.1649e-05 | norm 0.3219 | dt 338.89ms | 1547080.46 tokens/sec
Step 16718 | loss: 3.091730 | lr:8.1631e-05 | norm 0.2854 | dt 338.55ms | 1548640.64 tokens/sec
Step 16719 | loss: 3.116380 | lr:8.1613e-05 | norm 0.3113 | dt 1003.31ms | 522555.99 tokens/sec
Step 16720 | loss: 3.075740 | lr:8.1595e-05 | norm 0.2838 | dt 338.27ms | 1549903.51 tokens/sec
Step 16721 | loss: 3.079467 | lr:8.1577e-05 | norm 0.2934 | dt 338.26ms | 1549933.01 tokens/sec
Step 16722 | loss: 3.146938 | lr:8.1559e-05 | norm 0.2894 | dt 337.17ms | 1554981.16 tokens/sec
Step 16723 | loss: 3.049908 | lr:8.1541e-05 | norm 0.2869 | dt 338.23ms | 1550094.71 tokens/sec
Step 16724 | loss: 3.151668 | lr:8.1523e-05 | norm 0.3076 | dt 337.51ms | 1553379.65 tokens/sec
Step 16725 | loss: 3.063591 | lr:8.1504e-05 | norm 0.2957 | dt 338.21ms | 1550178.85 tokens/sec
Step 16726 | loss: 3.114046 | lr:8.1486e-05 | norm 0.2717 | dt 339.26ms | 1545368.09 tokens/sec
Step 16727 | loss: 3.087057 | lr:8.1468e-05 | norm 0.2788 | dt 337.97ms | 1551280.06 tokens/sec
Step 16728 | loss: 3.054680 | lr:8.1450e-05 | norm 0.3294 | dt 338.37ms | 1549433.93 tokens/sec
Step 16729 | loss: 3.094486 | lr:8.1432e-05 | norm 0.2923 | dt 338.62ms | 1548310.26 tokens/sec
Step 16730 | loss: 3.040552 | lr:8.1414e-05 | norm 0.2873 | dt 338.22ms | 1550161.36 tokens/sec
Step 16731 | loss: 3.041080 | lr:8.1396e-05 | norm 0.3061 | dt 338.92ms | 1546935.71 tokens/sec
Step 16732 | loss: 3.061084 | lr:8.1378e-05 | norm 0.2916 | dt 338.03ms | 1550992.30 tokens/sec
Step 16733 | loss: 3.013442 | lr:8.1360e-05 | norm 0.2689 | dt 338.79ms | 1547526.84 tokens/sec
Step 16734 | loss: 3.075470 | lr:8.1342e-05 | norm 0.2825 | dt 339.14ms | 1545955.85 tokens/sec
Step 16735 | loss: 3.127522 | lr:8.1324e-05 | norm 0.3704 | dt 338.41ms | 1549264.73 tokens/sec
Step 16736 | loss: 3.054478 | lr:8.1306e-05 | norm 0.2896 | dt 338.10ms | 1550700.28 tokens/sec
Step 16737 | loss: 3.111931 | lr:8.1288e-05 | norm 0.3116 | dt 338.38ms | 1549412.09 tokens/sec
Step 16738 | loss: 3.077223 | lr:8.1270e-05 | norm 0.3034 | dt 337.98ms | 1551251.61 tokens/sec
Step 16739 | loss: 3.085779 | lr:8.1252e-05 | norm 0.3212 | dt 337.83ms | 1551921.61 tokens/sec
Step 16740 | loss: 3.038946 | lr:8.1234e-05 | norm 0.2900 | dt 337.78ms | 1552148.35 tokens/sec
Step 16741 | loss: 2.990929 | lr:8.1216e-05 | norm 0.2948 | dt 338.05ms | 1550905.89 tokens/sec
Step 16742 | loss: 3.026814 | lr:8.1198e-05 | norm 0.3028 | dt 337.90ms | 1551619.38 tokens/sec
Step 16743 | loss: 2.980186 | lr:8.1180e-05 | norm 0.2868 | dt 338.31ms | 1549715.65 tokens/sec
Step 16744 | loss: 2.994103 | lr:8.1162e-05 | norm 0.3211 | dt 337.73ms | 1552382.84 tokens/sec
Step 16745 | loss: 3.027574 | lr:8.1144e-05 | norm 0.2931 | dt 337.94ms | 1551401.54 tokens/sec
Step 16746 | loss: 3.067807 | lr:8.1127e-05 | norm 0.2990 | dt 338.52ms | 1548764.98 tokens/sec
Step 16747 | loss: 3.043011 | lr:8.1109e-05 | norm 0.2950 | dt 337.59ms | 1553029.69 tokens/sec
Step 16748 | loss: 3.028374 | lr:8.1091e-05 | norm 0.3319 | dt 338.18ms | 1550314.36 tokens/sec
Step 16749 | loss: 3.007470 | lr:8.1073e-05 | norm 0.3072 | dt 338.10ms | 1550692.63 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 16750: 3.0949
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3019/10042=0.3006


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but how does it work?
In Python you start by working with classes, then you work with objects and functions,
rank 5 sample 1 >Hello, I'm a language model, because of the many words to say in the vocabulary. So we already have 'b-b-g-e-
rank 5 sample 2 >Hello, I'm a language model, and I would love to hear about something that could be done with these projects, so just leave a comment below.

rank 5 sample 3 >Hello, I'm a language model, here is the course outline from my GitHub repo:
I'm a language model based around a natural language, which is




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I think I need some help in Python. I also used the Java/iPad framework. And, I need
rank 7 sample 1 >Hello, I'm a language model, to you!
You know me all the time, even if I'm not right at all. When I say the
rank 7 sample 2 >Hello, I'm a language model, so I'll help you with grammar, vocabulary, spellings, and punctuation. I'm a big believer... but
rank 7 sample 3 >Hello, I'm a language model, and you might be interested in the subject of sentence structure. I've learned a lot in the grammar course, and grammar




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm gonna talk in the most powerful language, to keep it purely pure of meaning. There are two methods
rank 3 sample 1 >Hello, I'm a language model, so let’s translate my last name.
Hi, my name is Krakato. I'm so proud
rank 3 sample 2 >Hello, I'm a language model, so this one is based on learning the C language. The idea is that when you learn it one day, this is
rank 3 sample 3 >Hello, I'm a language model, so if you ask me what languages do you know that do not support Unicode, I'll provide a diagram of it.




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so what's one thing I'm still trying to accomplish, but I can do more.
- I was born into
rank 2 sample 1 >Hello, I'm a language model, and I get it because I'm writing articles. Also, the articles do not have to be the only ones. But
rank 2 sample 2 >Hello, I'm a language model, and I've always wanted to learn new languages. So I took this online course for you, this year, and now

rank 2 sample 3 >Hello, I'm a language model, but that brings me a lot of trouble - one person said they're not even aware that if there is a word it



ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't have to learn this thing because I've already mastered it before. And I have no problem with using
rank 0 sample 1 >Hello, I'm a language model, and when I'm learning a new language, the words get very complex and confusing, but this isn't an insurmount
rank 0 sample 2 >Hello, I'm a language model, and I thought you might try to figure out an English language, if that's how it works.
I think it
rank 0 sample 3 >Hello, I'm a language model, and have a great article on the language, one of the most basic features that help in language modeling.
My colleague




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so when you're building something, its like when you just try to get around the problem and don't want to do
rank 1 sample 1 >Hello, I'm a language model, a programmer who spends a lot of time explaining how to be a bit more careful about learning the best language for me.
rank 1 sample 2 >Hello, I'm a language model, but did you know that there are many different ways to create them, but most of them are not easy. The other
rank 1 sample 3 >Hello, I'm a language model, so I'm doing some tests to determine whether I'm looking at 'tilde' a useful term.
In the





ddp_rank 4: ####### Printing generated samples ####### 


ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, if you're in high school and it doesn't fit in a word processing textbook, you could be a little bit rustyrank 4 sample 0 >Hello, I'm a language model, and I can't believe how much I hated this process. It's one of the worst methods I've had to learn

rank 4 sample 1 >Hello, I'm a language model, or as I myself say, any code should be easily compiled into a new language as part of a code. With thisrank 6 sample 1 >Hello, I'm a language model, so I can't make it any easier - I need to make it even easier to make the parts. The two videos

rank 4 sample 2 >Hello, I'm a language model, I always recommend making the tools available on my site. I can use the tools as part of my site, if I
rank 6 sample 2 >Hello, I'm a language model, but not really. Here is an example to understand the syntax:
# example: the program that ran my python script
rank 4 sample 3 >Hello, I'm a language model, so it actually sounds more like a very long program called. When I take the value of a line, I start with


rank 6 sample 3 >Hello, I'm a language model, so here we're just going to be writing code that's going to be read and executed directly. (It's more


Step 16750 | loss: 3.188774 | lr:8.1055e-05 | norm 0.3912 | dt 18732.96ms | 27987.45 tokens/sec
Step 16751 | loss: 3.043683 | lr:8.1037e-05 | norm 0.3113 | dt 332.44ms | 1577076.38 tokens/sec
Step 16752 | loss: 3.055141 | lr:8.1019e-05 | norm 0.2999 | dt 334.87ms | 1565636.99 tokens/sec
Step 16753 | loss: 3.046199 | lr:8.1001e-05 | norm 0.3173 | dt 336.53ms | 1557912.61 tokens/sec
Step 16754 | loss: 3.122189 | lr:8.0983e-05 | norm 0.2980 | dt 335.01ms | 1564990.73 tokens/sec
Step 16755 | loss: 3.092796 | lr:8.0966e-05 | norm 0.3049 | dt 335.92ms | 1560757.66 tokens/sec
Step 16756 | loss: 3.116003 | lr:8.0948e-05 | norm 0.3085 | dt 336.28ms | 1559079.01 tokens/sec
Step 16757 | loss: 3.019870 | lr:8.0930e-05 | norm 0.3227 | dt 336.96ms | 1555950.48 tokens/sec
Step 16758 | loss: 3.077978 | lr:8.0912e-05 | norm 0.3055 | dt 335.28ms | 1563728.74 tokens/sec
Step 16759 | loss: 3.070333 | lr:8.0894e-05 | norm 0.3264 | dt 336.34ms | 1558782.83 tokens/sec
Step 16760 | loss: 3.100606 | lr:8.0876e-05 | norm 0.2951 | dt 335.50ms | 1562696.40 tokens/sec
Step 16761 | loss: 3.037178 | lr:8.0859e-05 | norm 0.3085 | dt 335.74ms | 1561568.95 tokens/sec
Step 16762 | loss: 3.042624 | lr:8.0841e-05 | norm 0.2992 | dt 336.68ms | 1557213.17 tokens/sec
Step 16763 | loss: 3.185685 | lr:8.0823e-05 | norm 0.3110 | dt 336.50ms | 1558049.49 tokens/sec
Step 16764 | loss: 3.013955 | lr:8.0805e-05 | norm 0.2914 | dt 336.37ms | 1558641.41 tokens/sec
Step 16765 | loss: 3.060423 | lr:8.0787e-05 | norm 0.2856 | dt 336.75ms | 1556919.91 tokens/sec
Step 16766 | loss: 3.027826 | lr:8.0770e-05 | norm 0.2859 | dt 336.66ms | 1557301.40 tokens/sec
Step 16767 | loss: 3.032238 | lr:8.0752e-05 | norm 0.2699 | dt 336.24ms | 1559250.36 tokens/sec
Step 16768 | loss: 3.051563 | lr:8.0734e-05 | norm 0.2998 | dt 337.12ms | 1555182.41 tokens/sec
Step 16769 | loss: 3.085076 | lr:8.0716e-05 | norm 0.3344 | dt 336.18ms | 1559546.72 tokens/sec
Step 16770 | loss: 3.084926 | lr:8.0699e-05 | norm 0.3048 | dt 336.08ms | 1559992.58 tokens/sec
Step 16771 | loss: 3.068505 | lr:8.0681e-05 | norm 0.3148 | dt 335.97ms | 1560516.20 tokens/sec
Step 16772 | loss: 3.011399 | lr:8.0663e-05 | norm 0.2775 | dt 337.09ms | 1555313.30 tokens/sec
Step 16773 | loss: 3.027845 | lr:8.0645e-05 | norm 0.3084 | dt 335.83ms | 1561189.80 tokens/sec
Step 16774 | loss: 3.033305 | lr:8.0628e-05 | norm 0.2811 | dt 336.60ms | 1557579.36 tokens/sec
Step 16775 | loss: 3.078681 | lr:8.0610e-05 | norm 0.2956 | dt 336.35ms | 1558750.78 tokens/sec
Step 16776 | loss: 3.012844 | lr:8.0592e-05 | norm 0.2688 | dt 336.79ms | 1556702.78 tokens/sec
Step 16777 | loss: 3.040507 | lr:8.0575e-05 | norm 0.2659 | dt 336.65ms | 1557348.82 tokens/sec
Step 16778 | loss: 3.046938 | lr:8.0557e-05 | norm 0.2901 | dt 337.05ms | 1555503.63 tokens/sec
Step 16779 | loss: 3.050714 | lr:8.0539e-05 | norm 0.2746 | dt 336.29ms | 1559046.95 tokens/sec
Step 16780 | loss: 2.996691 | lr:8.0522e-05 | norm 0.2817 | dt 337.63ms | 1552831.19 tokens/sec
Step 16781 | loss: 3.036629 | lr:8.0504e-05 | norm 0.2859 | dt 337.48ms | 1553553.04 tokens/sec
Step 16782 | loss: 3.040509 | lr:8.0486e-05 | norm 0.2749 | dt 337.12ms | 1555200.00 tokens/sec
Step 16783 | loss: 3.006511 | lr:8.0469e-05 | norm 0.2747 | dt 337.78ms | 1552142.88 tokens/sec
Step 16784 | loss: 3.011921 | lr:8.0451e-05 | norm 0.2816 | dt 337.06ms | 1555450.81 tokens/sec
Step 16785 | loss: 3.055931 | lr:8.0433e-05 | norm 0.2734 | dt 337.80ms | 1552059.62 tokens/sec
Step 16786 | loss: 2.987557 | lr:8.0416e-05 | norm 0.2826 | dt 337.81ms | 1552013.61 tokens/sec
Step 16787 | loss: 3.048466 | lr:8.0398e-05 | norm 0.2657 | dt 337.39ms | 1553966.92 tokens/sec
Step 16788 | loss: 3.094952 | lr:8.0380e-05 | norm 0.2738 | dt 337.50ms | 1553447.68 tokens/sec
Step 16789 | loss: 3.131574 | lr:8.0363e-05 | norm 0.2977 | dt 338.58ms | 1548478.16 tokens/sec
Step 16790 | loss: 3.051402 | lr:8.0345e-05 | norm 0.2857 | dt 338.16ms | 1550411.64 tokens/sec
Step 16791 | loss: 3.115020 | lr:8.0328e-05 | norm 0.2798 | dt 337.56ms | 1553155.83 tokens/sec
Step 16792 | loss: 3.145555 | lr:8.0310e-05 | norm 0.2857 | dt 337.83ms | 1551918.32 tokens/sec
Step 16793 | loss: 3.061126 | lr:8.0293e-05 | norm 0.2938 | dt 337.86ms | 1551773.76 tokens/sec
Step 16794 | loss: 3.063554 | lr:8.0275e-05 | norm 0.2967 | dt 337.85ms | 1551829.61 tokens/sec
Step 16795 | loss: 3.065487 | lr:8.0257e-05 | norm 0.2791 | dt 338.16ms | 1550421.48 tokens/sec
Step 16796 | loss: 3.053371 | lr:8.0240e-05 | norm 0.2902 | dt 338.68ms | 1548026.87 tokens/sec
Step 16797 | loss: 3.062811 | lr:8.0222e-05 | norm 0.2907 | dt 338.01ms | 1551117.02 tokens/sec
Step 16798 | loss: 3.068408 | lr:8.0205e-05 | norm 0.2685 | dt 337.54ms | 1553266.63 tokens/sec
Step 16799 | loss: 3.086522 | lr:8.0187e-05 | norm 0.3098 | dt 339.05ms | 1546365.70 tokens/sec
Step 16800 | loss: 3.043061 | lr:8.0170e-05 | norm 0.2729 | dt 338.23ms | 1550101.26 tokens/sec
Step 16801 | loss: 3.037510 | lr:8.0152e-05 | norm 0.2678 | dt 337.95ms | 1551385.12 tokens/sec
Step 16802 | loss: 3.078147 | lr:8.0135e-05 | norm 0.2779 | dt 338.73ms | 1547794.79 tokens/sec
Step 16803 | loss: 3.046824 | lr:8.0117e-05 | norm 0.2733 | dt 338.33ms | 1549615.18 tokens/sec
Step 16804 | loss: 3.054475 | lr:8.0100e-05 | norm 0.2772 | dt 338.32ms | 1549669.78 tokens/sec
Step 16805 | loss: 3.042490 | lr:8.0082e-05 | norm 0.2673 | dt 338.38ms | 1549423.01 tokens/sec
Step 16806 | loss: 3.083055 | lr:8.0065e-05 | norm 0.2840 | dt 338.65ms | 1548184.90 tokens/sec
Step 16807 | loss: 3.087952 | lr:8.0047e-05 | norm 0.2734 | dt 338.50ms | 1548869.71 tokens/sec
Step 16808 | loss: 3.060741 | lr:8.0030e-05 | norm 0.2839 | dt 337.59ms | 1553020.91 tokens/sec
Step 16809 | loss: 3.061010 | lr:8.0012e-05 | norm 0.2661 | dt 338.43ms | 1549182.87 tokens/sec
Step 16810 | loss: 3.043763 | lr:7.9995e-05 | norm 0.2777 | dt 340.38ms | 1540298.99 tokens/sec
Step 16811 | loss: 3.062862 | lr:7.9977e-05 | norm 0.2791 | dt 337.93ms | 1551448.61 tokens/sec
Step 16812 | loss: 2.982283 | lr:7.9960e-05 | norm 0.3340 | dt 337.65ms | 1552764.30 tokens/sec
Step 16813 | loss: 3.022551 | lr:7.9942e-05 | norm 0.2794 | dt 338.60ms | 1548418.19 tokens/sec
Step 16814 | loss: 2.988309 | lr:7.9925e-05 | norm 0.2820 | dt 337.46ms | 1553623.28 tokens/sec
Step 16815 | loss: 3.028288 | lr:7.9908e-05 | norm 0.2857 | dt 338.56ms | 1548574.12 tokens/sec
Step 16816 | loss: 3.023172 | lr:7.9890e-05 | norm 0.2979 | dt 338.70ms | 1547946.23 tokens/sec
Step 16817 | loss: 3.040289 | lr:7.9873e-05 | norm 0.2815 | dt 338.40ms | 1549322.58 tokens/sec
Step 16818 | loss: 3.047029 | lr:7.9855e-05 | norm 0.2990 | dt 338.29ms | 1549797.56 tokens/sec
Step 16819 | loss: 3.050135 | lr:7.9838e-05 | norm 0.3147 | dt 338.14ms | 1550493.63 tokens/sec
Step 16820 | loss: 3.006563 | lr:7.9821e-05 | norm 0.2751 | dt 905.41ms | 579061.69 tokens/sec
Step 16821 | loss: 2.998532 | lr:7.9803e-05 | norm 0.2933 | dt 336.79ms | 1556700.58 tokens/sec
Step 16822 | loss: 3.121602 | lr:7.9786e-05 | norm 0.2908 | dt 339.02ms | 1546461.39 tokens/sec
Step 16823 | loss: 3.046524 | lr:7.9769e-05 | norm 0.3112 | dt 338.67ms | 1548078.09 tokens/sec
Step 16824 | loss: 3.120568 | lr:7.9751e-05 | norm 0.3034 | dt 337.40ms | 1553925.20 tokens/sec
Step 16825 | loss: 3.079374 | lr:7.9734e-05 | norm 0.3092 | dt 338.06ms | 1550865.42 tokens/sec
Step 16826 | loss: 3.098496 | lr:7.9717e-05 | norm 0.2841 | dt 338.14ms | 1550498.01 tokens/sec
Step 16827 | loss: 3.092068 | lr:7.9699e-05 | norm 0.3128 | dt 337.67ms | 1552670.02 tokens/sec
Step 16828 | loss: 3.048308 | lr:7.9682e-05 | norm 0.2987 | dt 339.27ms | 1545362.66 tokens/sec
Step 16829 | loss: 3.080717 | lr:7.9665e-05 | norm 0.2927 | dt 338.85ms | 1547263.33 tokens/sec
Step 16830 | loss: 3.005917 | lr:7.9647e-05 | norm 0.3096 | dt 338.31ms | 1549725.47 tokens/sec
Step 16831 | loss: 3.086370 | lr:7.9630e-05 | norm 0.3167 | dt 338.22ms | 1550154.81 tokens/sec
Step 16832 | loss: 3.093747 | lr:7.9613e-05 | norm 0.3307 | dt 337.88ms | 1551716.82 tokens/sec
Step 16833 | loss: 3.097365 | lr:7.9595e-05 | norm 0.3193 | dt 337.61ms | 1552930.98 tokens/sec
Step 16834 | loss: 3.008839 | lr:7.9578e-05 | norm 0.3729 | dt 337.39ms | 1553938.37 tokens/sec
Step 16835 | loss: 3.034572 | lr:7.9561e-05 | norm 0.3001 | dt 338.68ms | 1548032.32 tokens/sec
Step 16836 | loss: 3.019439 | lr:7.9544e-05 | norm 0.3204 | dt 338.77ms | 1547636.84 tokens/sec
Step 16837 | loss: 3.055674 | lr:7.9526e-05 | norm 0.3216 | dt 339.05ms | 1546333.07 tokens/sec
Step 16838 | loss: 3.013358 | lr:7.9509e-05 | norm 0.3024 | dt 338.21ms | 1550198.52 tokens/sec
Step 16839 | loss: 3.109296 | lr:7.9492e-05 | norm 0.3012 | dt 338.96ms | 1546745.30 tokens/sec
Step 16840 | loss: 3.117546 | lr:7.9475e-05 | norm 0.3189 | dt 338.95ms | 1546788.82 tokens/sec
Step 16841 | loss: 3.080995 | lr:7.9457e-05 | norm 0.3182 | dt 338.81ms | 1547436.45 tokens/sec
Step 16842 | loss: 3.079670 | lr:7.9440e-05 | norm 0.2978 | dt 338.92ms | 1546934.62 tokens/sec
Step 16843 | loss: 3.066386 | lr:7.9423e-05 | norm 0.3108 | dt 338.89ms | 1547086.99 tokens/sec
Step 16844 | loss: 3.041572 | lr:7.9406e-05 | norm 0.3100 | dt 338.74ms | 1547753.39 tokens/sec
Step 16845 | loss: 2.992734 | lr:7.9389e-05 | norm 0.3015 | dt 339.37ms | 1544878.46 tokens/sec
Step 16846 | loss: 3.108522 | lr:7.9371e-05 | norm 0.3108 | dt 338.72ms | 1547855.80 tokens/sec
Step 16847 | loss: 2.987127 | lr:7.9354e-05 | norm 0.2937 | dt 337.80ms | 1552048.67 tokens/sec
Step 16848 | loss: 3.007212 | lr:7.9337e-05 | norm 0.3002 | dt 338.04ms | 1550958.39 tokens/sec
Step 16849 | loss: 3.053579 | lr:7.9320e-05 | norm 0.2996 | dt 339.72ms | 1543307.43 tokens/sec
Step 16850 | loss: 2.965881 | lr:7.9303e-05 | norm 0.2865 | dt 339.48ms | 1544379.37 tokens/sec
Step 16851 | loss: 2.961214 | lr:7.9286e-05 | norm 0.2963 | dt 338.55ms | 1548637.37 tokens/sec
Step 16852 | loss: 3.001443 | lr:7.9268e-05 | norm 0.2943 | dt 338.50ms | 1548844.62 tokens/sec
Step 16853 | loss: 3.077181 | lr:7.9251e-05 | norm 0.3254 | dt 338.84ms | 1547313.41 tokens/sec
Step 16854 | loss: 3.070917 | lr:7.9234e-05 | norm 0.2841 | dt 338.16ms | 1550424.76 tokens/sec
Step 16855 | loss: 3.049438 | lr:7.9217e-05 | norm 0.2917 | dt 338.67ms | 1548091.17 tokens/sec
Step 16856 | loss: 3.013323 | lr:7.9200e-05 | norm 0.2962 | dt 337.95ms | 1551388.41 tokens/sec
Step 16857 | loss: 2.996877 | lr:7.9183e-05 | norm 0.2827 | dt 338.43ms | 1549183.96 tokens/sec
Step 16858 | loss: 3.141181 | lr:7.9166e-05 | norm 0.3244 | dt 337.62ms | 1552878.34 tokens/sec
Step 16859 | loss: 3.153287 | lr:7.9149e-05 | norm 0.3196 | dt 338.33ms | 1549642.48 tokens/sec
Step 16860 | loss: 3.119254 | lr:7.9131e-05 | norm 0.3123 | dt 337.57ms | 1553107.56 tokens/sec
Step 16861 | loss: 3.039084 | lr:7.9114e-05 | norm 0.2818 | dt 337.60ms | 1552985.82 tokens/sec
Step 16862 | loss: 3.119548 | lr:7.9097e-05 | norm 0.3642 | dt 337.73ms | 1552374.07 tokens/sec
Step 16863 | loss: 3.089036 | lr:7.9080e-05 | norm 0.3558 | dt 338.47ms | 1549008.27 tokens/sec
Step 16864 | loss: 3.064648 | lr:7.9063e-05 | norm 0.3244 | dt 338.07ms | 1550844.64 tokens/sec
Step 16865 | loss: 3.090349 | lr:7.9046e-05 | norm 0.2919 | dt 337.52ms | 1553375.26 tokens/sec
Step 16866 | loss: 3.091402 | lr:7.9029e-05 | norm 0.3185 | dt 337.75ms | 1552285.31 tokens/sec
Step 16867 | loss: 3.126397 | lr:7.9012e-05 | norm 0.3843 | dt 338.03ms | 1551017.46 tokens/sec
Step 16868 | loss: 3.025478 | lr:7.8995e-05 | norm 0.3017 | dt 337.87ms | 1551754.05 tokens/sec
Step 16869 | loss: 3.088009 | lr:7.8978e-05 | norm 0.3432 | dt 338.02ms | 1551033.87 tokens/sec
Step 16870 | loss: 3.153479 | lr:7.8961e-05 | norm 0.3888 | dt 337.54ms | 1553276.51 tokens/sec
Step 16871 | loss: 3.060605 | lr:7.8944e-05 | norm 0.2983 | dt 337.92ms | 1551504.43 tokens/sec
Step 16872 | loss: 3.077418 | lr:7.8927e-05 | norm 0.3207 | dt 338.09ms | 1550745.12 tokens/sec
Step 16873 | loss: 3.076679 | lr:7.8910e-05 | norm 0.3498 | dt 337.45ms | 1553655.12 tokens/sec
Step 16874 | loss: 3.083665 | lr:7.8893e-05 | norm 0.3147 | dt 338.18ms | 1550316.55 tokens/sec
Step 16875 | loss: 3.090506 | lr:7.8876e-05 | norm 0.3059 | dt 337.96ms | 1551341.35 tokens/sec
Step 16876 | loss: 3.038988 | lr:7.8859e-05 | norm 0.3226 | dt 338.17ms | 1550373.39 tokens/sec
Step 16877 | loss: 3.085645 | lr:7.8842e-05 | norm 0.3033 | dt 337.51ms | 1553395.01 tokens/sec
Step 16878 | loss: 3.056808 | lr:7.8825e-05 | norm 0.2863 | dt 338.31ms | 1549737.49 tokens/sec
Step 16879 | loss: 3.082816 | lr:7.8808e-05 | norm 0.3236 | dt 337.75ms | 1552315.99 tokens/sec
Step 16880 | loss: 3.068307 | lr:7.8791e-05 | norm 0.3015 | dt 337.55ms | 1553233.72 tokens/sec
Step 16881 | loss: 3.060073 | lr:7.8774e-05 | norm 0.3094 | dt 337.63ms | 1552855.31 tokens/sec
Step 16882 | loss: 3.002860 | lr:7.8757e-05 | norm 0.3040 | dt 337.48ms | 1553556.33 tokens/sec
Step 16883 | loss: 3.105039 | lr:7.8741e-05 | norm 0.3943 | dt 338.10ms | 1550698.09 tokens/sec
Step 16884 | loss: 3.018876 | lr:7.8724e-05 | norm 0.3351 | dt 338.49ms | 1548899.16 tokens/sec
Step 16885 | loss: 3.029305 | lr:7.8707e-05 | norm 0.3104 | dt 337.91ms | 1551581.06 tokens/sec
Step 16886 | loss: 3.020637 | lr:7.8690e-05 | norm 0.3024 | dt 337.75ms | 1552276.55 tokens/sec
Step 16887 | loss: 3.024016 | lr:7.8673e-05 | norm 0.3201 | dt 338.62ms | 1548322.25 tokens/sec
Step 16888 | loss: 3.092093 | lr:7.8656e-05 | norm 0.3036 | dt 337.65ms | 1552777.46 tokens/sec
Step 16889 | loss: 2.997744 | lr:7.8639e-05 | norm 0.3088 | dt 337.67ms | 1552645.90 tokens/sec
Step 16890 | loss: 3.005714 | lr:7.8622e-05 | norm 0.3017 | dt 337.96ms | 1551342.44 tokens/sec
Step 16891 | loss: 3.056588 | lr:7.8605e-05 | norm 0.2884 | dt 338.74ms | 1547753.39 tokens/sec
Step 16892 | loss: 3.079195 | lr:7.8589e-05 | norm 0.3011 | dt 337.31ms | 1554342.57 tokens/sec
Step 16893 | loss: 3.025100 | lr:7.8572e-05 | norm 0.2813 | dt 337.78ms | 1552180.13 tokens/sec
Step 16894 | loss: 3.098587 | lr:7.8555e-05 | norm 0.2966 | dt 338.47ms | 1548984.26 tokens/sec
Step 16895 | loss: 3.108855 | lr:7.8538e-05 | norm 0.3074 | dt 338.40ms | 1549320.40 tokens/sec
Step 16896 | loss: 3.017433 | lr:7.8521e-05 | norm 0.3300 | dt 338.11ms | 1550649.98 tokens/sec
Step 16897 | loss: 3.038584 | lr:7.8504e-05 | norm 0.3073 | dt 337.92ms | 1551527.42 tokens/sec
Step 16898 | loss: 3.062865 | lr:7.8488e-05 | norm 0.3112 | dt 337.91ms | 1551553.69 tokens/sec
Step 16899 | loss: 3.126090 | lr:7.8471e-05 | norm 0.3228 | dt 337.62ms | 1552903.56 tokens/sec
Step 16900 | loss: 3.059398 | lr:7.8454e-05 | norm 0.3095 | dt 338.45ms | 1549085.74 tokens/sec
Step 16901 | loss: 3.109359 | lr:7.8437e-05 | norm 0.3167 | dt 338.10ms | 1550697.00 tokens/sec
Step 16902 | loss: 3.179527 | lr:7.8420e-05 | norm 0.3015 | dt 337.98ms | 1551230.82 tokens/sec
Step 16903 | loss: 3.110919 | lr:7.8404e-05 | norm 0.2985 | dt 338.39ms | 1549354.24 tokens/sec
Step 16904 | loss: 3.062907 | lr:7.8387e-05 | norm 0.2881 | dt 337.60ms | 1552985.82 tokens/sec
Step 16905 | loss: 3.046919 | lr:7.8370e-05 | norm 0.2938 | dt 338.24ms | 1550060.84 tokens/sec
Step 16906 | loss: 3.033729 | lr:7.8353e-05 | norm 0.2924 | dt 337.62ms | 1552872.86 tokens/sec
Step 16907 | loss: 3.028833 | lr:7.8337e-05 | norm 0.2717 | dt 338.54ms | 1548676.64 tokens/sec
Step 16908 | loss: 3.070460 | lr:7.8320e-05 | norm 0.2902 | dt 338.29ms | 1549807.39 tokens/sec
Step 16909 | loss: 3.076538 | lr:7.8303e-05 | norm 0.2757 | dt 1031.79ms | 508135.81 tokens/sec
Step 16910 | loss: 3.033644 | lr:7.8287e-05 | norm 0.2878 | dt 336.20ms | 1559440.55 tokens/sec
Step 16911 | loss: 3.094611 | lr:7.8270e-05 | norm 0.2920 | dt 337.90ms | 1551604.05 tokens/sec
Step 16912 | loss: 3.043991 | lr:7.8253e-05 | norm 0.2974 | dt 339.10ms | 1546102.59 tokens/sec
Step 16913 | loss: 3.005887 | lr:7.8236e-05 | norm 0.2716 | dt 338.29ms | 1549823.77 tokens/sec
Step 16914 | loss: 3.103318 | lr:7.8220e-05 | norm 0.2870 | dt 337.40ms | 1553893.35 tokens/sec
Step 16915 | loss: 3.115857 | lr:7.8203e-05 | norm 0.2903 | dt 337.76ms | 1552230.53 tokens/sec
Step 16916 | loss: 3.042328 | lr:7.8186e-05 | norm 0.3160 | dt 337.98ms | 1551240.67 tokens/sec
Step 16917 | loss: 3.073910 | lr:7.8170e-05 | norm 0.2752 | dt 337.92ms | 1551519.76 tokens/sec
Step 16918 | loss: 3.026987 | lr:7.8153e-05 | norm 0.2874 | dt 339.01ms | 1546532.09 tokens/sec
Step 16919 | loss: 2.970280 | lr:7.8136e-05 | norm 0.2720 | dt 337.92ms | 1551536.18 tokens/sec
Step 16920 | loss: 3.045212 | lr:7.8120e-05 | norm 0.2812 | dt 337.24ms | 1554639.27 tokens/sec
Step 16921 | loss: 3.004162 | lr:7.8103e-05 | norm 0.2812 | dt 338.60ms | 1548396.38 tokens/sec
Step 16922 | loss: 3.040803 | lr:7.8086e-05 | norm 0.2770 | dt 338.08ms | 1550798.70 tokens/sec
Step 16923 | loss: 3.057948 | lr:7.8070e-05 | norm 0.2765 | dt 337.64ms | 1552781.85 tokens/sec
Step 16924 | loss: 3.037064 | lr:7.8053e-05 | norm 0.3020 | dt 338.18ms | 1550323.11 tokens/sec
Step 16925 | loss: 3.092031 | lr:7.8037e-05 | norm 0.2936 | dt 338.65ms | 1548155.47 tokens/sec
Step 16926 | loss: 3.007636 | lr:7.8020e-05 | norm 0.2737 | dt 337.72ms | 1552456.27 tokens/sec
Step 16927 | loss: 3.016246 | lr:7.8003e-05 | norm 0.2854 | dt 338.51ms | 1548812.98 tokens/sec
Step 16928 | loss: 3.036152 | lr:7.7987e-05 | norm 0.2874 | dt 338.29ms | 1549840.16 tokens/sec
Step 16929 | loss: 3.152189 | lr:7.7970e-05 | norm 0.3007 | dt 338.06ms | 1550856.67 tokens/sec
Step 16930 | loss: 3.055199 | lr:7.7954e-05 | norm 0.3035 | dt 338.08ms | 1550784.49 tokens/sec
Step 16931 | loss: 3.141275 | lr:7.7937e-05 | norm 0.3079 | dt 337.92ms | 1551536.18 tokens/sec
Step 16932 | loss: 3.136890 | lr:7.7921e-05 | norm 0.3139 | dt 338.18ms | 1550338.41 tokens/sec
Step 16933 | loss: 3.036366 | lr:7.7904e-05 | norm 0.3316 | dt 338.85ms | 1547243.74 tokens/sec
Step 16934 | loss: 3.073580 | lr:7.7888e-05 | norm 0.2902 | dt 338.50ms | 1548875.16 tokens/sec
Step 16935 | loss: 3.069184 | lr:7.7871e-05 | norm 0.3229 | dt 337.44ms | 1553731.96 tokens/sec
Step 16936 | loss: 3.149382 | lr:7.7854e-05 | norm 0.3153 | dt 338.13ms | 1550567.98 tokens/sec
Step 16937 | loss: 3.082448 | lr:7.7838e-05 | norm 0.2920 | dt 337.84ms | 1551901.89 tokens/sec
Step 16938 | loss: 3.045817 | lr:7.7821e-05 | norm 0.3040 | dt 337.79ms | 1552123.16 tokens/sec
Step 16939 | loss: 3.041202 | lr:7.7805e-05 | norm 0.2861 | dt 337.87ms | 1551763.91 tokens/sec
Step 16940 | loss: 3.050279 | lr:7.7788e-05 | norm 0.2922 | dt 337.75ms | 1552302.84 tokens/sec
Step 16941 | loss: 3.040996 | lr:7.7772e-05 | norm 0.2931 | dt 338.71ms | 1547876.50 tokens/sec
Step 16942 | loss: 3.079101 | lr:7.7755e-05 | norm 0.2909 | dt 337.28ms | 1554462.34 tokens/sec
Step 16943 | loss: 3.071691 | lr:7.7739e-05 | norm 0.3084 | dt 337.57ms | 1553139.37 tokens/sec
Step 16944 | loss: 3.065390 | lr:7.7722e-05 | norm 0.2942 | dt 338.91ms | 1546980.33 tokens/sec
Step 16945 | loss: 3.074876 | lr:7.7706e-05 | norm 0.2967 | dt 338.21ms | 1550173.38 tokens/sec
Step 16946 | loss: 3.083594 | lr:7.7690e-05 | norm 0.3205 | dt 337.57ms | 1553127.31 tokens/sec
Step 16947 | loss: 3.069418 | lr:7.7673e-05 | norm 0.3149 | dt 337.90ms | 1551624.85 tokens/sec
Step 16948 | loss: 3.059791 | lr:7.7657e-05 | norm 0.2823 | dt 337.38ms | 1553982.30 tokens/sec
Step 16949 | loss: 3.123374 | lr:7.7640e-05 | norm 0.3044 | dt 338.45ms | 1549068.28 tokens/sec
Step 16950 | loss: 3.086931 | lr:7.7624e-05 | norm 0.2859 | dt 338.42ms | 1549204.70 tokens/sec
Step 16951 | loss: 3.011247 | lr:7.7607e-05 | norm 0.2835 | dt 336.97ms | 1555908.64 tokens/sec
Step 16952 | loss: 3.039088 | lr:7.7591e-05 | norm 0.2938 | dt 337.85ms | 1551830.71 tokens/sec
Step 16953 | loss: 3.024503 | lr:7.7575e-05 | norm 0.2774 | dt 338.05ms | 1550915.73 tokens/sec
Step 16954 | loss: 3.016351 | lr:7.7558e-05 | norm 0.2847 | dt 337.22ms | 1554733.80 tokens/sec
Step 16955 | loss: 3.028887 | lr:7.7542e-05 | norm 0.2900 | dt 338.60ms | 1548417.10 tokens/sec
Step 16956 | loss: 3.005897 | lr:7.7525e-05 | norm 0.2786 | dt 337.92ms | 1551496.77 tokens/sec
Step 16957 | loss: 3.013102 | lr:7.7509e-05 | norm 0.2630 | dt 337.22ms | 1554717.31 tokens/sec
Step 16958 | loss: 3.047202 | lr:7.7493e-05 | norm 0.2873 | dt 337.70ms | 1552519.84 tokens/sec
Step 16959 | loss: 3.027299 | lr:7.7476e-05 | norm 0.2766 | dt 337.97ms | 1551274.59 tokens/sec
Step 16960 | loss: 2.986089 | lr:7.7460e-05 | norm 0.2863 | dt 338.45ms | 1549096.66 tokens/sec
Step 16961 | loss: 3.025992 | lr:7.7444e-05 | norm 0.2649 | dt 338.23ms | 1550100.17 tokens/sec
Step 16962 | loss: 3.067692 | lr:7.7427e-05 | norm 0.2985 | dt 337.15ms | 1555058.13 tokens/sec
Step 16963 | loss: 3.020269 | lr:7.7411e-05 | norm 0.3191 | dt 337.22ms | 1554751.38 tokens/sec
Step 16964 | loss: 3.060604 | lr:7.7395e-05 | norm 0.2910 | dt 338.33ms | 1549652.30 tokens/sec
Step 16965 | loss: 3.094981 | lr:7.7378e-05 | norm 0.2749 | dt 337.50ms | 1553427.93 tokens/sec
Step 16966 | loss: 3.069050 | lr:7.7362e-05 | norm 0.3037 | dt 338.74ms | 1547753.39 tokens/sec
Step 16967 | loss: 3.078924 | lr:7.7346e-05 | norm 0.2914 | dt 338.39ms | 1549353.14 tokens/sec
Step 16968 | loss: 3.108456 | lr:7.7329e-05 | norm 0.3098 | dt 337.52ms | 1553345.63 tokens/sec
Step 16969 | loss: 3.054116 | lr:7.7313e-05 | norm 0.2855 | dt 337.88ms | 1551677.41 tokens/sec
Step 16970 | loss: 3.068443 | lr:7.7297e-05 | norm 0.3133 | dt 338.97ms | 1546686.55 tokens/sec
Step 16971 | loss: 3.070241 | lr:7.7281e-05 | norm 0.2958 | dt 337.48ms | 1553525.60 tokens/sec
Step 16972 | loss: 3.042212 | lr:7.7264e-05 | norm 0.3194 | dt 337.92ms | 1551509.91 tokens/sec
Step 16973 | loss: 3.015910 | lr:7.7248e-05 | norm 0.3209 | dt 338.17ms | 1550385.41 tokens/sec
Step 16974 | loss: 2.994552 | lr:7.7232e-05 | norm 0.3149 | dt 338.34ms | 1549584.60 tokens/sec
Step 16975 | loss: 3.034412 | lr:7.7216e-05 | norm 0.2897 | dt 338.50ms | 1548866.43 tokens/sec
Step 16976 | loss: 3.016777 | lr:7.7199e-05 | norm 0.3152 | dt 337.57ms | 1553128.41 tokens/sec
Step 16977 | loss: 3.057664 | lr:7.7183e-05 | norm 0.2883 | dt 337.75ms | 1552302.84 tokens/sec
Step 16978 | loss: 3.041161 | lr:7.7167e-05 | norm 0.2865 | dt 337.49ms | 1553504.75 tokens/sec
Step 16979 | loss: 3.003000 | lr:7.7151e-05 | norm 0.3182 | dt 337.91ms | 1551552.60 tokens/sec
Step 16980 | loss: 3.069055 | lr:7.7135e-05 | norm 0.2949 | dt 338.66ms | 1548126.04 tokens/sec
Step 16981 | loss: 3.081344 | lr:7.7118e-05 | norm 0.2717 | dt 337.83ms | 1551913.94 tokens/sec
Step 16982 | loss: 3.083788 | lr:7.7102e-05 | norm 0.2878 | dt 337.38ms | 1554008.65 tokens/sec
Step 16983 | loss: 3.087269 | lr:7.7086e-05 | norm 0.2860 | dt 337.67ms | 1552663.44 tokens/sec
Step 16984 | loss: 3.127646 | lr:7.7070e-05 | norm 0.2886 | dt 337.87ms | 1551757.34 tokens/sec
Step 16985 | loss: 3.083320 | lr:7.7054e-05 | norm 0.2813 | dt 337.32ms | 1554283.25 tokens/sec
Step 16986 | loss: 3.094429 | lr:7.7038e-05 | norm 0.2827 | dt 338.40ms | 1549310.57 tokens/sec
Step 16987 | loss: 3.014524 | lr:7.7021e-05 | norm 0.2884 | dt 338.52ms | 1548749.71 tokens/sec
Step 16988 | loss: 3.027538 | lr:7.7005e-05 | norm 0.2939 | dt 338.67ms | 1548067.19 tokens/sec
Step 16989 | loss: 3.044146 | lr:7.6989e-05 | norm 0.2695 | dt 337.47ms | 1553571.70 tokens/sec
Step 16990 | loss: 3.041408 | lr:7.6973e-05 | norm 0.2843 | dt 337.67ms | 1552656.86 tokens/sec
Step 16991 | loss: 3.019249 | lr:7.6957e-05 | norm 0.2728 | dt 338.19ms | 1550292.50 tokens/sec
Step 16992 | loss: 3.006392 | lr:7.6941e-05 | norm 0.2727 | dt 337.55ms | 1553206.29 tokens/sec
Step 16993 | loss: 3.001798 | lr:7.6925e-05 | norm 0.2808 | dt 338.23ms | 1550099.08 tokens/sec
Step 16994 | loss: 2.999631 | lr:7.6909e-05 | norm 0.2598 | dt 338.38ms | 1549398.99 tokens/sec
Step 16995 | loss: 3.098744 | lr:7.6892e-05 | norm 0.2672 | dt 338.14ms | 1550500.19 tokens/sec
Step 16996 | loss: 3.045513 | lr:7.6876e-05 | norm 0.2721 | dt 337.47ms | 1553568.40 tokens/sec
Step 16997 | loss: 3.016616 | lr:7.6860e-05 | norm 0.2885 | dt 337.17ms | 1554951.47 tokens/sec
Step 16998 | loss: 3.096012 | lr:7.6844e-05 | norm 0.2825 | dt 339.02ms | 1546474.45 tokens/sec
Step 16999 | loss: 3.089357 | lr:7.6828e-05 | norm 0.2838 | dt 338.61ms | 1548363.68 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 17000: 3.0913
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3056/10042=0.3043


ddp_rank 5: ####### Printing generated samples ####### 



ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but this one is pretty cool. It might not be a good job if I can't understand something, but I hope
rank 3 sample 0 >Hello, I'm a language model, so I'm gonna say to myself, is that what are you going to pick up right now? This is an extremely
rank 5 sample 1 >Hello, I'm a language model, an academic student in IIS (Intensive Advanced), a technology firm in Phoenix, Ariz.
You know,
rank 3 sample 1 >Hello, I'm a language model, so what does that mean? Is that what I'm looking for in my life?
I'm not a great teacher
rank 5 sample 2 >Hello, I'm a language model, and I just wanted to know if i could understand it.
But, that is for another day.
I know
rank 3 sample 2 >Hello, I'm a language model, so it might be your first thought to consider adding the language feature to the interface.
In C#, to start
rank 5 sample 3 >Hello, I'm a language model, right? We'll go for a minute and see why I'm so excited about your new language programming.
The answer


rank 3 sample 3 >Hello, I'm a language model, so in this course, I'm going to go back a little bit, and also to explain the evolution of a language




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I thought I could give you a rough idea.
For example: suppose that your user sends you an email that
rank 7 sample 1 >Hello, I'm a language model, too, I'm a translator too, but I use your language model.
For those of you who haven't learned
rank 7 sample 2 >Hello, I'm a language model, so I'll post about it right away.
Somehow I'd get a bit of a headache when I got my
rank 7 sample 3 >Hello, I'm a language model, and it's not really useful to the native (or even a beginner) person. It's about using the language so




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so I'd like to talk about building an algorithm. Here are some examples:
The idea here is to build a
rank 1 sample 1 >Hello, I'm a language model, a person who I think is more interested in learning than other people. I am happy, yes, but I'm not
rank 1 sample 2 >Hello, I'm a language model, but did you know that I'm going to be focusing on just a couple of things? What does the language do for
rank 1 sample 3 >Hello, I'm a language model, so I'm looking at that from a business perspective.”
And how do we implement the model?
�




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so my job here is to make sure I can be understood and understood. I also want to be able not to read
rank 2 sample 1 >Hello, I'm a language model, and I never heard of the word . Now, though, the same was true for the English word "louro
rank 2 sample 2 >Hello, I'm a language model, and I've never had the same interest in it myself. That's right! And I really, really want to help
rank 2 sample 3 >Hello, I'm a language model, but a story. It's a story of a life that can be told with no language boundaries but can be used by




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, this guy is so weird who's so crazy. You're a man who wants to be the first to learn a language
rank 6 sample 1 >Hello, I'm a language model, so I can't make it into a full-featured language. If you want to make a language for Linux,
rank 6 sample 2 >Hello, I'm a language model, but not so good. So here's some questions.
1. I like the English language. My father likes the
rank 6 sample 3 >Hello, I'm a language model, so I can try to understand all of this in a way that will help the reader. Some of it I need to




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I don't want to be a bad language model. So I need to define and test if we are going to
rank 4 sample 1 >Hello, I'm a language model, we'll be living in a universe where computers run things - and humans could have made some progress in their work too,


ddp_rank 0: ####### Printing generated samples ####### 

rank 4 sample 2 >Hello, I'm a language model, I wish I could say this is one of the most important concepts to understand. Thank you for taking over my posts,
rank 4 sample 3 >Hello, I'm a language model, so this gives all the power to our teacher in real-world scenarios. I always like the challenge, but, in


rank 0 sample 0 >Hello, I'm a language model, so I know that you don't even mind your first names, it's all about semantics. It doesn't matter where
rank 0 sample 1 >Hello, I'm a language model, and in the beginning of the course, I thought (but not very.) how do we use a command in a way
rank 0 sample 2 >Hello, I'm a language model, and I mean, can people possibly make a living in the web without a computer? I think so.
The second
rank 0 sample 3 >Hello, I'm a language model, and for the first time I've noticed how she's making errors in her code just in case she goes wrong! There


Step 17000 | loss: 3.057688 | lr:7.6812e-05 | norm 0.2966 | dt 18810.67ms | 27871.84 tokens/sec
Step 17001 | loss: 3.122031 | lr:7.6796e-05 | norm 0.2923 | dt 333.64ms | 1571412.22 tokens/sec
Step 17002 | loss: 3.044329 | lr:7.6780e-05 | norm 0.2813 | dt 334.77ms | 1566124.25 tokens/sec
Step 17003 | loss: 3.090153 | lr:7.6764e-05 | norm 0.2860 | dt 337.14ms | 1555082.33 tokens/sec
Step 17004 | loss: 3.072048 | lr:7.6748e-05 | norm 0.2768 | dt 335.44ms | 1563000.74 tokens/sec
Step 17005 | loss: 3.112922 | lr:7.6732e-05 | norm 0.2890 | dt 335.64ms | 1562047.03 tokens/sec
Step 17006 | loss: 3.070614 | lr:7.6716e-05 | norm 0.2830 | dt 335.89ms | 1560901.68 tokens/sec
Step 17007 | loss: 3.080290 | lr:7.6700e-05 | norm 0.2983 | dt 335.94ms | 1560676.79 tokens/sec
Step 17008 | loss: 3.088966 | lr:7.6684e-05 | norm 0.2790 | dt 336.39ms | 1558591.69 tokens/sec
Step 17009 | loss: 2.985393 | lr:7.6668e-05 | norm 0.3027 | dt 904.89ms | 579394.29 tokens/sec
Step 17010 | loss: 3.010994 | lr:7.6652e-05 | norm 0.3050 | dt 333.94ms | 1569988.52 tokens/sec
Step 17011 | loss: 3.045766 | lr:7.6636e-05 | norm 0.3150 | dt 336.25ms | 1559201.71 tokens/sec
Step 17012 | loss: 3.178405 | lr:7.6620e-05 | norm 0.3239 | dt 336.99ms | 1555774.35 tokens/sec
Step 17013 | loss: 3.051340 | lr:7.6604e-05 | norm 0.3037 | dt 337.99ms | 1551210.03 tokens/sec
Step 17014 | loss: 3.058589 | lr:7.6588e-05 | norm 0.3186 | dt 336.71ms | 1557076.45 tokens/sec
Step 17015 | loss: 3.059452 | lr:7.6572e-05 | norm 0.2959 | dt 336.20ms | 1559467.09 tokens/sec
Step 17016 | loss: 3.006835 | lr:7.6556e-05 | norm 0.3340 | dt 336.57ms | 1557751.49 tokens/sec
Step 17017 | loss: 3.065060 | lr:7.6540e-05 | norm 0.3082 | dt 337.15ms | 1555066.93 tokens/sec
Step 17018 | loss: 3.104900 | lr:7.6524e-05 | norm 0.2995 | dt 336.67ms | 1557277.13 tokens/sec
Step 17019 | loss: 3.045255 | lr:7.6508e-05 | norm 0.3042 | dt 337.20ms | 1554828.33 tokens/sec
Step 17020 | loss: 3.087744 | lr:7.6493e-05 | norm 0.2902 | dt 336.59ms | 1557629.01 tokens/sec
Step 17021 | loss: 3.042846 | lr:7.6477e-05 | norm 0.3060 | dt 336.88ms | 1556290.74 tokens/sec
Step 17022 | loss: 3.058058 | lr:7.6461e-05 | norm 0.2850 | dt 336.74ms | 1556952.98 tokens/sec
Step 17023 | loss: 3.036220 | lr:7.6445e-05 | norm 0.3693 | dt 336.11ms | 1559884.13 tokens/sec
Step 17024 | loss: 3.083109 | lr:7.6429e-05 | norm 0.3045 | dt 336.76ms | 1556872.51 tokens/sec
Step 17025 | loss: 3.044698 | lr:7.6413e-05 | norm 0.2916 | dt 337.48ms | 1553517.92 tokens/sec
Step 17026 | loss: 3.055421 | lr:7.6397e-05 | norm 0.3046 | dt 336.76ms | 1556847.16 tokens/sec
Step 17027 | loss: 3.014411 | lr:7.6381e-05 | norm 0.2854 | dt 336.56ms | 1557787.90 tokens/sec
Step 17028 | loss: 3.019753 | lr:7.6366e-05 | norm 0.3068 | dt 337.69ms | 1552589.99 tokens/sec
Step 17029 | loss: 3.000555 | lr:7.6350e-05 | norm 0.2922 | dt 336.77ms | 1556800.87 tokens/sec
Step 17030 | loss: 3.003935 | lr:7.6334e-05 | norm 0.2758 | dt 337.64ms | 1552797.20 tokens/sec
Step 17031 | loss: 2.985750 | lr:7.6318e-05 | norm 0.2784 | dt 336.85ms | 1556450.46 tokens/sec
Step 17032 | loss: 3.031888 | lr:7.6302e-05 | norm 0.2928 | dt 337.35ms | 1554137.15 tokens/sec
Step 17033 | loss: 3.027108 | lr:7.6287e-05 | norm 0.2743 | dt 336.76ms | 1556870.30 tokens/sec
Step 17034 | loss: 3.039387 | lr:7.6271e-05 | norm 0.2874 | dt 337.13ms | 1555167.01 tokens/sec
Step 17035 | loss: 3.103026 | lr:7.6255e-05 | norm 0.3177 | dt 340.41ms | 1540156.59 tokens/sec
Step 17036 | loss: 3.080523 | lr:7.6239e-05 | norm 0.2832 | dt 337.45ms | 1553686.95 tokens/sec
Step 17037 | loss: 3.005529 | lr:7.6223e-05 | norm 0.2906 | dt 337.44ms | 1553723.18 tokens/sec
Step 17038 | loss: 3.057053 | lr:7.6208e-05 | norm 0.3018 | dt 338.42ms | 1549230.89 tokens/sec
Step 17039 | loss: 3.109830 | lr:7.6192e-05 | norm 0.2715 | dt 337.98ms | 1551247.23 tokens/sec
Step 17040 | loss: 3.090639 | lr:7.6176e-05 | norm 0.3082 | dt 338.00ms | 1551130.15 tokens/sec
Step 17041 | loss: 3.019353 | lr:7.6160e-05 | norm 0.3075 | dt 338.77ms | 1547633.57 tokens/sec
Step 17042 | loss: 3.050895 | lr:7.6145e-05 | norm 0.2861 | dt 339.62ms | 1543739.71 tokens/sec
Step 17043 | loss: 3.200150 | lr:7.6129e-05 | norm 0.3135 | dt 337.56ms | 1553172.28 tokens/sec
Step 17044 | loss: 3.125787 | lr:7.6113e-05 | norm 0.2968 | dt 338.75ms | 1547702.19 tokens/sec
Step 17045 | loss: 3.055217 | lr:7.6097e-05 | norm 0.3161 | dt 339.08ms | 1546229.78 tokens/sec
Step 17046 | loss: 3.055423 | lr:7.6082e-05 | norm 0.2924 | dt 337.12ms | 1555179.11 tokens/sec
Step 17047 | loss: 3.063231 | lr:7.6066e-05 | norm 0.2670 | dt 337.78ms | 1552159.31 tokens/sec
Step 17048 | loss: 2.990055 | lr:7.6050e-05 | norm 0.2929 | dt 338.47ms | 1548995.18 tokens/sec
Step 17049 | loss: 3.046674 | lr:7.6035e-05 | norm 0.2706 | dt 337.58ms | 1553075.75 tokens/sec
Step 17050 | loss: 3.101128 | lr:7.6019e-05 | norm 0.2779 | dt 337.08ms | 1555391.40 tokens/sec
Step 17051 | loss: 3.041449 | lr:7.6003e-05 | norm 0.2941 | dt 338.13ms | 1550531.90 tokens/sec
Step 17052 | loss: 3.054906 | lr:7.5988e-05 | norm 0.2691 | dt 338.12ms | 1550604.06 tokens/sec
Step 17053 | loss: 3.063861 | lr:7.5972e-05 | norm 0.2965 | dt 337.82ms | 1551973.08 tokens/sec
Step 17054 | loss: 3.083771 | lr:7.5956e-05 | norm 0.2763 | dt 338.23ms | 1550103.45 tokens/sec
Step 17055 | loss: 3.083399 | lr:7.5941e-05 | norm 0.2839 | dt 338.00ms | 1551153.13 tokens/sec
Step 17056 | loss: 3.088171 | lr:7.5925e-05 | norm 0.2952 | dt 337.82ms | 1551959.94 tokens/sec
Step 17057 | loss: 3.061091 | lr:7.5909e-05 | norm 0.2755 | dt 337.77ms | 1552221.76 tokens/sec
Step 17058 | loss: 3.048664 | lr:7.5894e-05 | norm 0.2871 | dt 338.19ms | 1550257.53 tokens/sec
Step 17059 | loss: 3.013196 | lr:7.5878e-05 | norm 0.2813 | dt 337.61ms | 1552948.53 tokens/sec
Step 17060 | loss: 2.957244 | lr:7.5862e-05 | norm 0.2923 | dt 337.93ms | 1551470.50 tokens/sec
Step 17061 | loss: 2.977275 | lr:7.5847e-05 | norm 0.2732 | dt 338.25ms | 1550012.76 tokens/sec
Step 17062 | loss: 2.997193 | lr:7.5831e-05 | norm 0.2813 | dt 337.79ms | 1552115.49 tokens/sec
Step 17063 | loss: 3.071566 | lr:7.5816e-05 | norm 0.3083 | dt 337.27ms | 1554519.48 tokens/sec
Step 17064 | loss: 3.029770 | lr:7.5800e-05 | norm 0.2826 | dt 339.21ms | 1545597.28 tokens/sec
Step 17065 | loss: 3.075129 | lr:7.5785e-05 | norm 0.2823 | dt 338.13ms | 1550546.11 tokens/sec
Step 17066 | loss: 3.000607 | lr:7.5769e-05 | norm 0.2875 | dt 337.45ms | 1553657.31 tokens/sec
Step 17067 | loss: 3.045663 | lr:7.5753e-05 | norm 0.2792 | dt 338.01ms | 1551112.65 tokens/sec
Step 17068 | loss: 3.081089 | lr:7.5738e-05 | norm 0.2925 | dt 338.62ms | 1548320.07 tokens/sec
Step 17069 | loss: 3.159981 | lr:7.5722e-05 | norm 0.2917 | dt 337.97ms | 1551300.85 tokens/sec
Step 17070 | loss: 3.157178 | lr:7.5707e-05 | norm 0.3469 | dt 337.48ms | 1553528.89 tokens/sec
Step 17071 | loss: 3.029215 | lr:7.5691e-05 | norm 0.3114 | dt 337.79ms | 1552094.67 tokens/sec
Step 17072 | loss: 3.078948 | lr:7.5676e-05 | norm 0.2923 | dt 337.61ms | 1552959.49 tokens/sec
Step 17073 | loss: 3.084199 | lr:7.5660e-05 | norm 0.3141 | dt 338.90ms | 1547023.86 tokens/sec
Step 17074 | loss: 3.095483 | lr:7.5645e-05 | norm 0.3303 | dt 338.18ms | 1550341.69 tokens/sec
Step 17075 | loss: 3.090852 | lr:7.5629e-05 | norm 0.2930 | dt 337.76ms | 1552248.06 tokens/sec
Step 17076 | loss: 3.048280 | lr:7.5614e-05 | norm 0.2914 | dt 337.74ms | 1552331.34 tokens/sec
Step 17077 | loss: 3.065210 | lr:7.5598e-05 | norm 0.3029 | dt 337.61ms | 1552927.69 tokens/sec
Step 17078 | loss: 3.064248 | lr:7.5583e-05 | norm 0.2989 | dt 338.53ms | 1548713.72 tokens/sec
Step 17079 | loss: 3.028075 | lr:7.5567e-05 | norm 0.2855 | dt 337.78ms | 1552169.17 tokens/sec
Step 17080 | loss: 3.051059 | lr:7.5552e-05 | norm 0.2992 | dt 338.17ms | 1550372.29 tokens/sec
Step 17081 | loss: 3.086038 | lr:7.5536e-05 | norm 0.2805 | dt 337.57ms | 1553113.05 tokens/sec
Step 17082 | loss: 3.129134 | lr:7.5521e-05 | norm 0.3035 | dt 337.57ms | 1553111.95 tokens/sec
Step 17083 | loss: 3.064986 | lr:7.5506e-05 | norm 0.2969 | dt 337.92ms | 1551495.68 tokens/sec
Step 17084 | loss: 3.100410 | lr:7.5490e-05 | norm 0.2990 | dt 338.84ms | 1547318.86 tokens/sec
Step 17085 | loss: 3.125837 | lr:7.5475e-05 | norm 0.3135 | dt 337.69ms | 1552581.22 tokens/sec
Step 17086 | loss: 3.064125 | lr:7.5459e-05 | norm 0.2860 | dt 338.11ms | 1550655.45 tokens/sec
Step 17087 | loss: 3.037717 | lr:7.5444e-05 | norm 0.2875 | dt 338.27ms | 1549917.72 tokens/sec
Step 17088 | loss: 3.062638 | lr:7.5428e-05 | norm 0.3167 | dt 338.11ms | 1550629.20 tokens/sec
Step 17089 | loss: 3.066264 | lr:7.5413e-05 | norm 0.2691 | dt 337.60ms | 1553003.36 tokens/sec
Step 17090 | loss: 3.071405 | lr:7.5398e-05 | norm 0.2794 | dt 337.69ms | 1552594.38 tokens/sec
Step 17091 | loss: 3.069489 | lr:7.5382e-05 | norm 0.2771 | dt 337.73ms | 1552367.50 tokens/sec
Step 17092 | loss: 3.004706 | lr:7.5367e-05 | norm 0.2929 | dt 338.03ms | 1551014.18 tokens/sec
Step 17093 | loss: 3.075665 | lr:7.5352e-05 | norm 0.2947 | dt 337.87ms | 1551727.77 tokens/sec
Step 17094 | loss: 3.047078 | lr:7.5336e-05 | norm 0.2869 | dt 337.95ms | 1551370.90 tokens/sec
Step 17095 | loss: 3.036827 | lr:7.5321e-05 | norm 0.2913 | dt 340.19ms | 1541167.99 tokens/sec
Step 17096 | loss: 2.985740 | lr:7.5306e-05 | norm 0.2723 | dt 338.97ms | 1546730.07 tokens/sec
Step 17097 | loss: 3.099909 | lr:7.5290e-05 | norm 0.3030 | dt 339.59ms | 1543865.43 tokens/sec
Step 17098 | loss: 3.018927 | lr:7.5275e-05 | norm 0.2839 | dt 337.51ms | 1553419.15 tokens/sec
Step 17099 | loss: 3.044096 | lr:7.5260e-05 | norm 0.2774 | dt 936.63ms | 559761.55 tokens/sec
Step 17100 | loss: 3.015061 | lr:7.5244e-05 | norm 0.2815 | dt 336.61ms | 1557549.58 tokens/sec
Step 17101 | loss: 2.973670 | lr:7.5229e-05 | norm 0.2681 | dt 338.40ms | 1549316.03 tokens/sec
Step 17102 | loss: 2.985911 | lr:7.5214e-05 | norm 0.2995 | dt 338.61ms | 1548334.24 tokens/sec
Step 17103 | loss: 3.076999 | lr:7.5198e-05 | norm 0.3157 | dt 337.40ms | 1553905.43 tokens/sec
Step 17104 | loss: 3.056136 | lr:7.5183e-05 | norm 0.2922 | dt 336.83ms | 1556551.82 tokens/sec
Step 17105 | loss: 3.048944 | lr:7.5168e-05 | norm 0.2996 | dt 338.90ms | 1547020.60 tokens/sec
Step 17106 | loss: 3.091044 | lr:7.5153e-05 | norm 0.3037 | dt 338.55ms | 1548641.73 tokens/sec
Step 17107 | loss: 3.060736 | lr:7.5137e-05 | norm 0.2932 | dt 337.69ms | 1552568.07 tokens/sec
Step 17108 | loss: 3.073497 | lr:7.5122e-05 | norm 0.2948 | dt 337.00ms | 1555732.52 tokens/sec
Step 17109 | loss: 3.073610 | lr:7.5107e-05 | norm 0.3111 | dt 337.59ms | 1553048.33 tokens/sec
Step 17110 | loss: 3.077702 | lr:7.5092e-05 | norm 0.3165 | dt 337.43ms | 1553786.85 tokens/sec
Step 17111 | loss: 3.139297 | lr:7.5076e-05 | norm 0.3221 | dt 337.59ms | 1553009.94 tokens/sec
Step 17112 | loss: 3.108513 | lr:7.5061e-05 | norm 0.3547 | dt 337.90ms | 1551609.53 tokens/sec
Step 17113 | loss: 3.054691 | lr:7.5046e-05 | norm 0.3079 | dt 338.85ms | 1547248.09 tokens/sec
Step 17114 | loss: 3.031014 | lr:7.5031e-05 | norm 0.2887 | dt 337.81ms | 1552010.33 tokens/sec
Step 17115 | loss: 3.059752 | lr:7.5016e-05 | norm 0.3211 | dt 338.78ms | 1547581.29 tokens/sec
Step 17116 | loss: 3.030957 | lr:7.5000e-05 | norm 0.2912 | dt 338.34ms | 1549611.90 tokens/sec
Step 17117 | loss: 3.034363 | lr:7.4985e-05 | norm 0.3251 | dt 337.99ms | 1551191.42 tokens/sec
Step 17118 | loss: 3.043910 | lr:7.4970e-05 | norm 0.3254 | dt 338.55ms | 1548623.19 tokens/sec
Step 17119 | loss: 3.059607 | lr:7.4955e-05 | norm 0.2960 | dt 337.85ms | 1551843.85 tokens/sec
Step 17120 | loss: 3.063946 | lr:7.4940e-05 | norm 0.2953 | dt 338.36ms | 1549477.60 tokens/sec
Step 17121 | loss: 3.030636 | lr:7.4925e-05 | norm 0.3037 | dt 337.72ms | 1552434.35 tokens/sec
Step 17122 | loss: 3.042566 | lr:7.4909e-05 | norm 0.2722 | dt 338.08ms | 1550787.77 tokens/sec
Step 17123 | loss: 3.093064 | lr:7.4894e-05 | norm 0.2948 | dt 338.20ms | 1550246.60 tokens/sec
Step 17124 | loss: 3.041090 | lr:7.4879e-05 | norm 0.3059 | dt 338.47ms | 1548997.36 tokens/sec
Step 17125 | loss: 3.018034 | lr:7.4864e-05 | norm 0.2824 | dt 337.92ms | 1551492.39 tokens/sec
Step 17126 | loss: 3.043998 | lr:7.4849e-05 | norm 0.2958 | dt 338.39ms | 1549352.05 tokens/sec
Step 17127 | loss: 3.030875 | lr:7.4834e-05 | norm 0.2938 | dt 338.10ms | 1550683.88 tokens/sec
Step 17128 | loss: 3.032966 | lr:7.4819e-05 | norm 0.3223 | dt 337.44ms | 1553715.49 tokens/sec
Step 17129 | loss: 3.052229 | lr:7.4804e-05 | norm 0.2925 | dt 337.92ms | 1551508.81 tokens/sec
Step 17130 | loss: 3.062535 | lr:7.4788e-05 | norm 0.3016 | dt 337.58ms | 1553096.59 tokens/sec
Step 17131 | loss: 3.037977 | lr:7.4773e-05 | norm 0.2888 | dt 338.60ms | 1548409.47 tokens/sec
Step 17132 | loss: 3.051129 | lr:7.4758e-05 | norm 0.2855 | dt 337.98ms | 1551229.72 tokens/sec
Step 17133 | loss: 2.986294 | lr:7.4743e-05 | norm 0.2732 | dt 337.30ms | 1554361.25 tokens/sec
Step 17134 | loss: 3.019171 | lr:7.4728e-05 | norm 0.2854 | dt 337.44ms | 1553736.35 tokens/sec
Step 17135 | loss: 3.046375 | lr:7.4713e-05 | norm 0.2820 | dt 338.16ms | 1550393.06 tokens/sec
Step 17136 | loss: 3.028191 | lr:7.4698e-05 | norm 0.2944 | dt 338.32ms | 1549686.16 tokens/sec
Step 17137 | loss: 3.049898 | lr:7.4683e-05 | norm 0.2734 | dt 337.86ms | 1551772.67 tokens/sec
Step 17138 | loss: 3.122251 | lr:7.4668e-05 | norm 0.2918 | dt 338.66ms | 1548132.58 tokens/sec
Step 17139 | loss: 3.073312 | lr:7.4653e-05 | norm 0.2735 | dt 337.86ms | 1551796.76 tokens/sec
Step 17140 | loss: 3.121154 | lr:7.4638e-05 | norm 0.2877 | dt 338.19ms | 1550263.00 tokens/sec
Step 17141 | loss: 3.086229 | lr:7.4623e-05 | norm 0.2823 | dt 338.04ms | 1550973.71 tokens/sec
Step 17142 | loss: 3.059876 | lr:7.4608e-05 | norm 0.2800 | dt 337.86ms | 1551812.09 tokens/sec
Step 17143 | loss: 3.047773 | lr:7.4593e-05 | norm 0.2800 | dt 337.52ms | 1553332.46 tokens/sec
Step 17144 | loss: 3.072010 | lr:7.4578e-05 | norm 0.2844 | dt 337.94ms | 1551444.23 tokens/sec
Step 17145 | loss: 3.067637 | lr:7.4563e-05 | norm 0.2695 | dt 337.99ms | 1551206.74 tokens/sec
Step 17146 | loss: 3.064854 | lr:7.4548e-05 | norm 0.2917 | dt 337.06ms | 1555488.22 tokens/sec
Step 17147 | loss: 3.082075 | lr:7.4533e-05 | norm 0.2830 | dt 337.87ms | 1551733.25 tokens/sec
Step 17148 | loss: 3.041724 | lr:7.4518e-05 | norm 0.2886 | dt 338.07ms | 1550805.26 tokens/sec
Step 17149 | loss: 3.003743 | lr:7.4503e-05 | norm 0.3062 | dt 337.63ms | 1552831.19 tokens/sec
Step 17150 | loss: 3.038564 | lr:7.4488e-05 | norm 0.2888 | dt 337.70ms | 1552538.47 tokens/sec
Step 17151 | loss: 3.063484 | lr:7.4473e-05 | norm 0.3016 | dt 337.75ms | 1552294.08 tokens/sec
Step 17152 | loss: 3.027260 | lr:7.4458e-05 | norm 0.3031 | dt 338.30ms | 1549764.79 tokens/sec
Step 17153 | loss: 3.081265 | lr:7.4444e-05 | norm 0.2788 | dt 338.10ms | 1550675.13 tokens/sec
Step 17154 | loss: 3.049045 | lr:7.4429e-05 | norm 0.2969 | dt 338.62ms | 1548285.19 tokens/sec
Step 17155 | loss: 3.090220 | lr:7.4414e-05 | norm 0.2936 | dt 337.81ms | 1552001.56 tokens/sec
Step 17156 | loss: 3.061047 | lr:7.4399e-05 | norm 0.2889 | dt 338.39ms | 1549366.24 tokens/sec
Step 17157 | loss: 3.085724 | lr:7.4384e-05 | norm 0.2751 | dt 339.00ms | 1546566.89 tokens/sec
Step 17158 | loss: 3.147064 | lr:7.4369e-05 | norm 0.3012 | dt 338.62ms | 1548286.28 tokens/sec
Step 17159 | loss: 3.111020 | lr:7.4354e-05 | norm 0.2948 | dt 338.22ms | 1550141.69 tokens/sec
Step 17160 | loss: 3.117755 | lr:7.4339e-05 | norm 0.2912 | dt 338.30ms | 1549777.90 tokens/sec
Step 17161 | loss: 3.066400 | lr:7.4325e-05 | norm 0.2997 | dt 339.15ms | 1545910.20 tokens/sec
Step 17162 | loss: 3.101963 | lr:7.4310e-05 | norm 0.2894 | dt 339.03ms | 1546429.86 tokens/sec
Step 17163 | loss: 2.999161 | lr:7.4295e-05 | norm 0.2937 | dt 338.01ms | 1551115.93 tokens/sec
Step 17164 | loss: 3.038053 | lr:7.4280e-05 | norm 0.2934 | dt 338.28ms | 1549871.84 tokens/sec
Step 17165 | loss: 3.033882 | lr:7.4265e-05 | norm 0.2817 | dt 338.25ms | 1549984.36 tokens/sec
Step 17166 | loss: 3.008823 | lr:7.4250e-05 | norm 0.2803 | dt 338.26ms | 1549948.30 tokens/sec
Step 17167 | loss: 3.030466 | lr:7.4236e-05 | norm 0.2792 | dt 338.59ms | 1548460.71 tokens/sec
Step 17168 | loss: 3.048094 | lr:7.4221e-05 | norm 0.2815 | dt 338.52ms | 1548752.99 tokens/sec
Step 17169 | loss: 3.023591 | lr:7.4206e-05 | norm 0.2842 | dt 337.73ms | 1552405.85 tokens/sec
Step 17170 | loss: 2.958203 | lr:7.4191e-05 | norm 0.2678 | dt 339.05ms | 1546365.70 tokens/sec
Step 17171 | loss: 3.002792 | lr:7.4176e-05 | norm 0.2853 | dt 338.28ms | 1549866.37 tokens/sec
Step 17172 | loss: 3.044697 | lr:7.4162e-05 | norm 0.2806 | dt 337.83ms | 1551928.18 tokens/sec
Step 17173 | loss: 3.166933 | lr:7.4147e-05 | norm 0.3561 | dt 337.46ms | 1553618.89 tokens/sec
Step 17174 | loss: 3.090840 | lr:7.4132e-05 | norm 0.2893 | dt 337.97ms | 1551281.16 tokens/sec
Step 17175 | loss: 3.094113 | lr:7.4117e-05 | norm 0.3000 | dt 338.25ms | 1550021.50 tokens/sec
Step 17176 | loss: 3.011409 | lr:7.4103e-05 | norm 0.3176 | dt 337.88ms | 1551717.92 tokens/sec
Step 17177 | loss: 3.070159 | lr:7.4088e-05 | norm 0.2894 | dt 337.91ms | 1551540.56 tokens/sec
Step 17178 | loss: 2.985707 | lr:7.4073e-05 | norm 0.2987 | dt 339.29ms | 1545261.67 tokens/sec
Step 17179 | loss: 3.000792 | lr:7.4058e-05 | norm 0.3032 | dt 338.42ms | 1549223.25 tokens/sec
Step 17180 | loss: 3.050137 | lr:7.4044e-05 | norm 0.2945 | dt 338.52ms | 1548787.89 tokens/sec
Step 17181 | loss: 3.051999 | lr:7.4029e-05 | norm 0.2877 | dt 338.56ms | 1548569.76 tokens/sec
Step 17182 | loss: 3.102694 | lr:7.4014e-05 | norm 0.3143 | dt 337.73ms | 1552405.85 tokens/sec
Step 17183 | loss: 3.082212 | lr:7.4000e-05 | norm 0.2867 | dt 339.55ms | 1544069.23 tokens/sec
Step 17184 | loss: 3.057662 | lr:7.3985e-05 | norm 0.3036 | dt 338.42ms | 1549237.44 tokens/sec
Step 17185 | loss: 3.033860 | lr:7.3970e-05 | norm 0.2786 | dt 338.76ms | 1547678.23 tokens/sec
Step 17186 | loss: 3.032192 | lr:7.3956e-05 | norm 0.2940 | dt 339.12ms | 1546022.15 tokens/sec
Step 17187 | loss: 3.037094 | lr:7.3941e-05 | norm 0.2690 | dt 337.66ms | 1552695.23 tokens/sec
Step 17188 | loss: 3.058483 | lr:7.3926e-05 | norm 0.2978 | dt 338.57ms | 1548532.68 tokens/sec
Step 17189 | loss: 3.136800 | lr:7.3912e-05 | norm 0.2789 | dt 338.36ms | 1549493.98 tokens/sec
Step 17190 | loss: 3.079940 | lr:7.3897e-05 | norm 0.2647 | dt 338.38ms | 1549426.29 tokens/sec
Step 17191 | loss: 3.051713 | lr:7.3882e-05 | norm 0.2994 | dt 338.31ms | 1549723.29 tokens/sec
Step 17192 | loss: 3.082277 | lr:7.3868e-05 | norm 0.2875 | dt 339.44ms | 1544574.63 tokens/sec
Step 17193 | loss: 3.096458 | lr:7.3853e-05 | norm 0.2978 | dt 339.54ms | 1544098.51 tokens/sec
Step 17194 | loss: 3.055081 | lr:7.3839e-05 | norm 0.2942 | dt 340.04ms | 1541852.01 tokens/sec
Step 17195 | loss: 3.034919 | lr:7.3824e-05 | norm 0.2883 | dt 337.88ms | 1551706.97 tokens/sec
Step 17196 | loss: 3.046871 | lr:7.3809e-05 | norm 0.2698 | dt 337.97ms | 1551273.49 tokens/sec
Step 17197 | loss: 3.009562 | lr:7.3795e-05 | norm 0.3190 | dt 338.29ms | 1549799.74 tokens/sec
Step 17198 | loss: 3.025775 | lr:7.3780e-05 | norm 0.2736 | dt 1037.52ms | 505329.87 tokens/sec
Step 17199 | loss: 3.043321 | lr:7.3766e-05 | norm 0.3171 | dt 335.63ms | 1562122.49 tokens/sec
Step 17200 | loss: 2.996904 | lr:7.3751e-05 | norm 0.3066 | dt 337.38ms | 1554018.54 tokens/sec
Step 17201 | loss: 3.053317 | lr:7.3737e-05 | norm 0.2948 | dt 339.70ms | 1543370.25 tokens/sec
Step 17202 | loss: 3.041631 | lr:7.3722e-05 | norm 0.3191 | dt 336.84ms | 1556469.19 tokens/sec
Step 17203 | loss: 2.971892 | lr:7.3707e-05 | norm 0.3215 | dt 337.18ms | 1554930.58 tokens/sec
Step 17204 | loss: 2.989086 | lr:7.3693e-05 | norm 0.2894 | dt 337.63ms | 1552869.57 tokens/sec
Step 17205 | loss: 3.058694 | lr:7.3678e-05 | norm 0.2883 | dt 337.86ms | 1551781.43 tokens/sec
Step 17206 | loss: 3.011906 | lr:7.3664e-05 | norm 0.2856 | dt 338.37ms | 1549439.39 tokens/sec
Step 17207 | loss: 3.029496 | lr:7.3649e-05 | norm 0.2897 | dt 337.97ms | 1551289.91 tokens/sec
Step 17208 | loss: 3.081550 | lr:7.3635e-05 | norm 0.3044 | dt 337.31ms | 1554311.81 tokens/sec
Step 17209 | loss: 3.003017 | lr:7.3620e-05 | norm 0.3483 | dt 338.40ms | 1549292.02 tokens/sec
Step 17210 | loss: 3.018772 | lr:7.3606e-05 | norm 0.2886 | dt 337.74ms | 1552322.57 tokens/sec
Step 17211 | loss: 3.011767 | lr:7.3591e-05 | norm 0.3426 | dt 339.12ms | 1546027.58 tokens/sec
Step 17212 | loss: 3.106217 | lr:7.3577e-05 | norm 0.3166 | dt 338.55ms | 1548645.01 tokens/sec
Step 17213 | loss: 3.103013 | lr:7.3562e-05 | norm 0.3170 | dt 339.15ms | 1545902.60 tokens/sec
Step 17214 | loss: 3.044677 | lr:7.3548e-05 | norm 0.3080 | dt 339.03ms | 1546438.56 tokens/sec
Step 17215 | loss: 3.111029 | lr:7.3534e-05 | norm 0.3158 | dt 338.47ms | 1548986.45 tokens/sec
Step 17216 | loss: 3.054039 | lr:7.3519e-05 | norm 0.2770 | dt 339.74ms | 1543202.37 tokens/sec
Step 17217 | loss: 3.098614 | lr:7.3505e-05 | norm 0.2956 | dt 339.11ms | 1546090.63 tokens/sec
Step 17218 | loss: 3.102232 | lr:7.3490e-05 | norm 0.3096 | dt 338.70ms | 1547934.25 tokens/sec
Step 17219 | loss: 3.101933 | lr:7.3476e-05 | norm 0.2941 | dt 338.77ms | 1547640.11 tokens/sec
Step 17220 | loss: 3.046849 | lr:7.3461e-05 | norm 0.2865 | dt 339.15ms | 1545905.86 tokens/sec
Step 17221 | loss: 3.036282 | lr:7.3447e-05 | norm 0.2896 | dt 338.33ms | 1549617.36 tokens/sec
Step 17222 | loss: 3.045643 | lr:7.3433e-05 | norm 0.3000 | dt 338.55ms | 1548619.92 tokens/sec
Step 17223 | loss: 3.042934 | lr:7.3418e-05 | norm 0.3035 | dt 339.01ms | 1546509.25 tokens/sec
Step 17224 | loss: 3.131147 | lr:7.3404e-05 | norm 0.2980 | dt 339.39ms | 1544812.26 tokens/sec
Step 17225 | loss: 3.045076 | lr:7.3389e-05 | norm 0.3199 | dt 338.53ms | 1548739.90 tokens/sec
Step 17226 | loss: 3.050440 | lr:7.3375e-05 | norm 0.2938 | dt 338.40ms | 1549310.57 tokens/sec
Step 17227 | loss: 3.077704 | lr:7.3361e-05 | norm 0.2742 | dt 339.91ms | 1542448.99 tokens/sec
Step 17228 | loss: 3.123804 | lr:7.3346e-05 | norm 0.2892 | dt 338.84ms | 1547291.64 tokens/sec
Step 17229 | loss: 3.052886 | lr:7.3332e-05 | norm 0.3012 | dt 338.78ms | 1547561.69 tokens/sec
Step 17230 | loss: 3.049248 | lr:7.3318e-05 | norm 0.2806 | dt 339.05ms | 1546354.82 tokens/sec
Step 17231 | loss: 3.010426 | lr:7.3303e-05 | norm 0.2859 | dt 338.95ms | 1546794.26 tokens/sec
Step 17232 | loss: 3.045553 | lr:7.3289e-05 | norm 0.2781 | dt 337.84ms | 1551887.66 tokens/sec
Step 17233 | loss: 3.003284 | lr:7.3275e-05 | norm 0.2795 | dt 338.70ms | 1547950.59 tokens/sec
Step 17234 | loss: 3.004434 | lr:7.3260e-05 | norm 0.2973 | dt 338.76ms | 1547672.78 tokens/sec
Step 17235 | loss: 3.065975 | lr:7.3246e-05 | norm 0.2820 | dt 338.81ms | 1547449.52 tokens/sec
Step 17236 | loss: 3.048646 | lr:7.3232e-05 | norm 0.2836 | dt 338.76ms | 1547659.71 tokens/sec
Step 17237 | loss: 3.023761 | lr:7.3218e-05 | norm 0.2802 | dt 338.18ms | 1550330.76 tokens/sec
Step 17238 | loss: 3.049764 | lr:7.3203e-05 | norm 0.2599 | dt 338.08ms | 1550785.58 tokens/sec
Step 17239 | loss: 3.006100 | lr:7.3189e-05 | norm 0.2753 | dt 337.95ms | 1551388.41 tokens/sec
Step 17240 | loss: 3.040621 | lr:7.3175e-05 | norm 0.2788 | dt 338.41ms | 1549282.19 tokens/sec
Step 17241 | loss: 2.955988 | lr:7.3161e-05 | norm 0.2690 | dt 339.12ms | 1546027.58 tokens/sec
Step 17242 | loss: 3.073375 | lr:7.3146e-05 | norm 0.2880 | dt 338.62ms | 1548287.37 tokens/sec
Step 17243 | loss: 3.133801 | lr:7.3132e-05 | norm 0.2761 | dt 338.58ms | 1548509.78 tokens/sec
Step 17244 | loss: 3.068020 | lr:7.3118e-05 | norm 0.2760 | dt 338.44ms | 1549135.94 tokens/sec
Step 17245 | loss: 3.120070 | lr:7.3104e-05 | norm 0.2850 | dt 338.59ms | 1548449.81 tokens/sec
Step 17246 | loss: 3.105472 | lr:7.3089e-05 | norm 0.2779 | dt 339.11ms | 1546051.50 tokens/sec
Step 17247 | loss: 3.038330 | lr:7.3075e-05 | norm 0.3048 | dt 338.62ms | 1548289.55 tokens/sec
Step 17248 | loss: 3.085441 | lr:7.3061e-05 | norm 0.2806 | dt 337.92ms | 1551512.09 tokens/sec
Step 17249 | loss: 3.137760 | lr:7.3047e-05 | norm 0.3098 | dt 338.36ms | 1549490.70 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 17250: 3.0892
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3057/10042=0.3044


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm gonna say to the question, what do I call the base here? Is this a normal, a subset
rank 3 sample 1 >Hello, I'm a language model, so what I'm going to give you is some sort of a set of rules for how you can use the system's


ddp_rank 5: ####### Printing generated samples ####### 

rank 3 sample 2 >Hello, I'm a language model, so this will be only a matter of time. So I think I'll give you a general rundown on the structure.
rank 3 sample 3 >Hello, I'm a language model, so i'm a language model who really likes languages like Spanish. But, I hope, like, languages are easier to


rank 5 sample 0 >Hello, I'm a language model, but this isn't how I thought of it, you can only be good at language modeling. But I've actually taken


ddp_rank 2: ####### Printing generated samples ####### 

rank 5 sample 1 >Hello, I'm a language model, well, I'll help in my future post of this series. One thing is obvious - the idea is one thing,
rank 5 sample 2 >Hello, I'm a language model, and I was thinking right now, right now, what's the future for writing in binary?
The future is the
rank 2 sample 0 >Hello, I'm a language model, so here I just want to talk about how I can write an algorithm for the machine.
In this chapter, you
rank 5 sample 3 >Hello, I'm a language model, who is? Here's something that I'm gonna make myself familiar with:
(A) #define a function or


rank 2 sample 1 >Hello, I'm a language model, and I found the "word" part useful. Why? I'll need to write a lot more. I'll start
rank 2 sample 2 >Hello, I'm a language model, and I'll tell you what that has to do with how I write "Au" and some of the issues and
rank 2 sample 3 >Hello, I'm a language model, but not about the most advanced languages. Let's call it BASH. How did that function out? Let me do




ddp_rank 7: ####### Printing generated samples ####### 


ddp_rank 4: ####### Printing generated samples ####### 


rank 4 sample 0 >Hello, I'm a language model, and I know that languages are not like human beings. I know that, but I just had to say a little more

ddp_rank 1: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I just want to do a little word search, so I'm taking a close look at syntax, syntax, syntax

rank 7 sample 1 >Hello, I'm a language model, am we?
The basic building block of a language is its root. The verb is the root of the verb:
rank 7 sample 2 >Hello, I'm a language model, so I'll look at the "sorting" step.
Sorting is the first step in the process when creatingrank 1 sample 0 >Hello, I'm a language model, so I'll be trying to use [the number 1], it's going to be 1 in binary, but I'll
rank 4 sample 1 >Hello, I'm a language model, writing about the design of language design, using data. With this guide, I have just been to Google. Because Google

rank 4 sample 2 >Hello, I'm a language model, I read to read about things I need to know about using Java in my project. If you've visited my Facebook page
rank 1 sample 1 >Hello, I'm a language model, a programmer who creates the code, then reads it and reads it. I do that the way I want it to be
rank 4 sample 3 >Hello, I'm a language model, so you also needed to make a separate function called as method: a set of object values, you could use different typesrank 1 sample 2 >Hello, I'm a language model, but then it's not always easy to understand. How to help? Here's a quick example for you: if I

rank 7 sample 3 >Hello, I'm a language model, and you might be able to build code or applets that you could actually use, like a microcontroller or a touch




rank 1 sample 3 >Hello, I'm a language model, so I'm really going to work on coding. I already have a library...
Okay, let's start at the




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I know that you could write programs out of them, but let's go ahead and pick some language to write the
rank 0 sample 1 >Hello, I'm a language model, and what's the point of this?!
The same principles are at work. The language model, that's the same
rank 0 sample 2 >Hello, I'm a language model, and I will create a set of classes that implement the language that is the language, and that will be called the grammar
rank 0 sample 3 >Hello, I'm a language model, and how do I make a model that is effective for me?
If we build model objects that include the properties given




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, as my first language and then my second. I am going on a course with a lot of people who have a similar
rank 6 sample 1 >Hello, I'm a language model, so I want to take the grammar and rewrite it as a language. This is the way to go, if you wanted
rank 6 sample 2 >Hello, I'm a language model, but a machine is very good, you can make it work for me and others.
I'm thinking... How can
rank 6 sample 3 >Hello, I'm a language model, so I need to have some tools for creating and using the language.
First of all, I'll give you a


Step 17250 | loss: 3.058952 | lr:7.3033e-05 | norm 0.2853 | dt 12406.57ms | 42258.91 tokens/sec
Step 17251 | loss: 3.082781 | lr:7.3018e-05 | norm 0.2838 | dt 335.05ms | 1564814.78 tokens/sec
Step 17252 | loss: 3.051381 | lr:7.3004e-05 | norm 0.2879 | dt 336.55ms | 1557850.81 tokens/sec
Step 17253 | loss: 3.162365 | lr:7.2990e-05 | norm 0.3879 | dt 338.88ms | 1547117.46 tokens/sec
Step 17254 | loss: 3.104391 | lr:7.2976e-05 | norm 0.3204 | dt 336.09ms | 1559958.27 tokens/sec
Step 17255 | loss: 3.037138 | lr:7.2962e-05 | norm 0.3224 | dt 336.72ms | 1557031.25 tokens/sec
Step 17256 | loss: 3.033532 | lr:7.2948e-05 | norm 0.3202 | dt 336.90ms | 1556211.44 tokens/sec
Step 17257 | loss: 3.091635 | lr:7.2934e-05 | norm 0.3068 | dt 336.54ms | 1557866.26 tokens/sec
Step 17258 | loss: 3.039059 | lr:7.2919e-05 | norm 0.3179 | dt 336.59ms | 1557648.87 tokens/sec
Step 17259 | loss: 3.086212 | lr:7.2905e-05 | norm 0.3036 | dt 336.29ms | 1559030.37 tokens/sec
Step 17260 | loss: 3.120012 | lr:7.2891e-05 | norm 0.3064 | dt 336.66ms | 1557306.91 tokens/sec
Step 17261 | loss: 3.032968 | lr:7.2877e-05 | norm 0.2967 | dt 336.07ms | 1560035.74 tokens/sec
Step 17262 | loss: 3.155141 | lr:7.2863e-05 | norm 0.2920 | dt 336.61ms | 1557563.92 tokens/sec
Step 17263 | loss: 3.095535 | lr:7.2849e-05 | norm 0.3230 | dt 336.80ms | 1556654.29 tokens/sec
Step 17264 | loss: 3.037781 | lr:7.2835e-05 | norm 0.3025 | dt 336.15ms | 1559668.39 tokens/sec
Step 17265 | loss: 3.077991 | lr:7.2821e-05 | norm 0.3108 | dt 339.15ms | 1545867.82 tokens/sec
Step 17266 | loss: 2.990930 | lr:7.2807e-05 | norm 0.3019 | dt 338.01ms | 1551104.99 tokens/sec
Step 17267 | loss: 3.030160 | lr:7.2793e-05 | norm 0.2965 | dt 336.63ms | 1557452.50 tokens/sec
Step 17268 | loss: 3.039717 | lr:7.2779e-05 | norm 0.2799 | dt 337.03ms | 1555604.86 tokens/sec
Step 17269 | loss: 3.057624 | lr:7.2765e-05 | norm 0.3028 | dt 337.05ms | 1555498.13 tokens/sec
Step 17270 | loss: 2.993439 | lr:7.2750e-05 | norm 0.3048 | dt 337.94ms | 1551433.28 tokens/sec
Step 17271 | loss: 3.049482 | lr:7.2736e-05 | norm 0.2810 | dt 337.33ms | 1554228.32 tokens/sec
Step 17272 | loss: 3.039644 | lr:7.2722e-05 | norm 0.2904 | dt 336.88ms | 1556307.26 tokens/sec
Step 17273 | loss: 3.055711 | lr:7.2708e-05 | norm 0.2967 | dt 337.99ms | 1551171.73 tokens/sec
Step 17274 | loss: 3.062981 | lr:7.2694e-05 | norm 0.2848 | dt 337.25ms | 1554586.52 tokens/sec
Step 17275 | loss: 3.022699 | lr:7.2680e-05 | norm 0.3189 | dt 337.50ms | 1553456.46 tokens/sec
Step 17276 | loss: 3.044887 | lr:7.2666e-05 | norm 0.3011 | dt 338.07ms | 1550807.45 tokens/sec
Step 17277 | loss: 3.046896 | lr:7.2652e-05 | norm 0.2852 | dt 337.09ms | 1555351.80 tokens/sec
Step 17278 | loss: 3.060111 | lr:7.2638e-05 | norm 0.2947 | dt 337.06ms | 1555461.82 tokens/sec
Step 17279 | loss: 3.079932 | lr:7.2624e-05 | norm 0.3053 | dt 338.21ms | 1550199.61 tokens/sec
Step 17280 | loss: 3.089718 | lr:7.2611e-05 | norm 0.2949 | dt 337.83ms | 1551918.32 tokens/sec
Step 17281 | loss: 3.190906 | lr:7.2597e-05 | norm 0.3315 | dt 336.82ms | 1556604.71 tokens/sec
Step 17282 | loss: 3.053890 | lr:7.2583e-05 | norm 0.3131 | dt 337.68ms | 1552623.97 tokens/sec
Step 17283 | loss: 3.111888 | lr:7.2569e-05 | norm 0.3049 | dt 337.47ms | 1553561.82 tokens/sec
Step 17284 | loss: 3.061271 | lr:7.2555e-05 | norm 0.2826 | dt 338.69ms | 1547974.57 tokens/sec
Step 17285 | loss: 3.074807 | lr:7.2541e-05 | norm 0.3617 | dt 337.22ms | 1554746.99 tokens/sec
Step 17286 | loss: 3.102408 | lr:7.2527e-05 | norm 0.2975 | dt 337.66ms | 1552728.12 tokens/sec
Step 17287 | loss: 3.099091 | lr:7.2513e-05 | norm 0.2935 | dt 338.51ms | 1548800.98 tokens/sec
Step 17288 | loss: 3.054163 | lr:7.2499e-05 | norm 0.2945 | dt 337.53ms | 1553291.87 tokens/sec
Step 17289 | loss: 3.098277 | lr:7.2485e-05 | norm 0.3010 | dt 930.98ms | 563154.24 tokens/sec
Step 17290 | loss: 3.023101 | lr:7.2471e-05 | norm 0.2945 | dt 337.22ms | 1554725.00 tokens/sec
Step 17291 | loss: 3.075800 | lr:7.2457e-05 | norm 0.2893 | dt 338.92ms | 1546935.71 tokens/sec
Step 17292 | loss: 3.056562 | lr:7.2444e-05 | norm 0.2951 | dt 337.26ms | 1554541.46 tokens/sec
Step 17293 | loss: 3.095349 | lr:7.2430e-05 | norm 0.3074 | dt 338.30ms | 1549756.06 tokens/sec
Step 17294 | loss: 3.030321 | lr:7.2416e-05 | norm 0.2931 | dt 337.36ms | 1554087.73 tokens/sec
Step 17295 | loss: 3.088492 | lr:7.2402e-05 | norm 0.3534 | dt 337.97ms | 1551273.49 tokens/sec
Step 17296 | loss: 3.062446 | lr:7.2388e-05 | norm 0.2825 | dt 337.69ms | 1552589.99 tokens/sec
Step 17297 | loss: 3.089897 | lr:7.2374e-05 | norm 0.2955 | dt 337.64ms | 1552804.87 tokens/sec
Step 17298 | loss: 3.141661 | lr:7.2361e-05 | norm 0.3201 | dt 338.37ms | 1549467.77 tokens/sec
Step 17299 | loss: 3.090036 | lr:7.2347e-05 | norm 0.2716 | dt 337.57ms | 1553138.28 tokens/sec
Step 17300 | loss: 3.076847 | lr:7.2333e-05 | norm 0.3014 | dt 337.67ms | 1552665.63 tokens/sec
Step 17301 | loss: 3.017984 | lr:7.2319e-05 | norm 0.3057 | dt 339.19ms | 1545714.61 tokens/sec
Step 17302 | loss: 3.029536 | lr:7.2305e-05 | norm 0.2766 | dt 338.04ms | 1550945.27 tokens/sec
Step 17303 | loss: 3.066097 | lr:7.2292e-05 | norm 0.3056 | dt 338.20ms | 1550224.75 tokens/sec
Step 17304 | loss: 3.050568 | lr:7.2278e-05 | norm 0.2953 | dt 338.37ms | 1549441.57 tokens/sec
Step 17305 | loss: 3.007900 | lr:7.2264e-05 | norm 0.2706 | dt 338.26ms | 1549972.34 tokens/sec
Step 17306 | loss: 2.979445 | lr:7.2250e-05 | norm 0.2949 | dt 338.23ms | 1550108.91 tokens/sec
Step 17307 | loss: 3.003259 | lr:7.2236e-05 | norm 0.2887 | dt 337.93ms | 1551471.59 tokens/sec
Step 17308 | loss: 3.016333 | lr:7.2223e-05 | norm 0.2683 | dt 338.02ms | 1551039.34 tokens/sec
Step 17309 | loss: 2.940207 | lr:7.2209e-05 | norm 0.2835 | dt 337.88ms | 1551720.11 tokens/sec
Step 17310 | loss: 3.028539 | lr:7.2195e-05 | norm 0.2725 | dt 338.31ms | 1549740.77 tokens/sec
Step 17311 | loss: 3.063989 | lr:7.2182e-05 | norm 0.2711 | dt 337.52ms | 1553367.58 tokens/sec
Step 17312 | loss: 3.022056 | lr:7.2168e-05 | norm 0.2758 | dt 340.07ms | 1541712.56 tokens/sec
Step 17313 | loss: 3.181081 | lr:7.2154e-05 | norm 0.3381 | dt 338.79ms | 1547519.21 tokens/sec
Step 17314 | loss: 3.097751 | lr:7.2140e-05 | norm 0.3035 | dt 338.13ms | 1550553.76 tokens/sec
Step 17315 | loss: 3.052703 | lr:7.2127e-05 | norm 0.3049 | dt 338.69ms | 1547977.83 tokens/sec
Step 17316 | loss: 3.080359 | lr:7.2113e-05 | norm 0.2868 | dt 338.11ms | 1550651.07 tokens/sec
Step 17317 | loss: 3.031045 | lr:7.2099e-05 | norm 0.2921 | dt 338.59ms | 1548450.90 tokens/sec
Step 17318 | loss: 3.111088 | lr:7.2086e-05 | norm 0.2971 | dt 337.82ms | 1551984.04 tokens/sec
Step 17319 | loss: 3.072876 | lr:7.2072e-05 | norm 0.2718 | dt 338.93ms | 1546878.04 tokens/sec
Step 17320 | loss: 3.092836 | lr:7.2058e-05 | norm 0.2721 | dt 339.84ms | 1542760.65 tokens/sec
Step 17321 | loss: 3.104916 | lr:7.2045e-05 | norm 0.3085 | dt 337.62ms | 1552888.21 tokens/sec
Step 17322 | loss: 3.100217 | lr:7.2031e-05 | norm 0.2986 | dt 338.50ms | 1548856.62 tokens/sec
Step 17323 | loss: 3.067158 | lr:7.2017e-05 | norm 0.3786 | dt 338.72ms | 1547855.80 tokens/sec
Step 17324 | loss: 3.052441 | lr:7.2004e-05 | norm 0.2901 | dt 337.62ms | 1552886.02 tokens/sec
Step 17325 | loss: 3.081060 | lr:7.1990e-05 | norm 0.3292 | dt 339.03ms | 1546425.51 tokens/sec
Step 17326 | loss: 3.056098 | lr:7.1977e-05 | norm 0.3130 | dt 339.78ms | 1543012.87 tokens/sec
Step 17327 | loss: 3.064404 | lr:7.1963e-05 | norm 0.2894 | dt 339.68ms | 1543491.58 tokens/sec
Step 17328 | loss: 3.061018 | lr:7.1949e-05 | norm 0.3136 | dt 340.03ms | 1541883.36 tokens/sec
Step 17329 | loss: 3.010598 | lr:7.1936e-05 | norm 0.3034 | dt 338.91ms | 1546990.12 tokens/sec
Step 17330 | loss: 3.069349 | lr:7.1922e-05 | norm 0.3099 | dt 338.75ms | 1547702.19 tokens/sec
Step 17331 | loss: 3.085155 | lr:7.1909e-05 | norm 0.3156 | dt 339.03ms | 1546416.81 tokens/sec
Step 17332 | loss: 3.074220 | lr:7.1895e-05 | norm 0.2961 | dt 338.62ms | 1548303.72 tokens/sec
Step 17333 | loss: 3.045557 | lr:7.1881e-05 | norm 0.2926 | dt 338.26ms | 1549949.40 tokens/sec
Step 17334 | loss: 3.057431 | lr:7.1868e-05 | norm 0.3001 | dt 339.26ms | 1545387.64 tokens/sec
Step 17335 | loss: 3.051857 | lr:7.1854e-05 | norm 0.3160 | dt 338.80ms | 1547495.26 tokens/sec
Step 17336 | loss: 3.015130 | lr:7.1841e-05 | norm 0.2835 | dt 338.92ms | 1546917.21 tokens/sec
Step 17337 | loss: 3.029252 | lr:7.1827e-05 | norm 0.2978 | dt 338.96ms | 1546745.30 tokens/sec
Step 17338 | loss: 3.056952 | lr:7.1814e-05 | norm 0.3059 | dt 339.15ms | 1545883.04 tokens/sec
Step 17339 | loss: 3.063217 | lr:7.1800e-05 | norm 0.3117 | dt 338.91ms | 1546981.42 tokens/sec
Step 17340 | loss: 3.023119 | lr:7.1787e-05 | norm 0.2861 | dt 338.77ms | 1547601.99 tokens/sec
Step 17341 | loss: 2.992971 | lr:7.1773e-05 | norm 0.2676 | dt 339.03ms | 1546446.17 tokens/sec
Step 17342 | loss: 2.979775 | lr:7.1760e-05 | norm 0.2942 | dt 338.89ms | 1547095.69 tokens/sec
Step 17343 | loss: 3.011780 | lr:7.1746e-05 | norm 0.2823 | dt 338.67ms | 1548058.47 tokens/sec
Step 17344 | loss: 3.046560 | lr:7.1733e-05 | norm 0.2786 | dt 338.47ms | 1549006.09 tokens/sec
Step 17345 | loss: 3.023127 | lr:7.1719e-05 | norm 0.2734 | dt 338.38ms | 1549383.71 tokens/sec
Step 17346 | loss: 3.038141 | lr:7.1706e-05 | norm 0.2714 | dt 339.26ms | 1545405.02 tokens/sec
Step 17347 | loss: 3.042593 | lr:7.1692e-05 | norm 0.3021 | dt 338.35ms | 1549561.67 tokens/sec
Step 17348 | loss: 2.974123 | lr:7.1679e-05 | norm 0.2784 | dt 338.79ms | 1547524.66 tokens/sec
Step 17349 | loss: 3.065043 | lr:7.1666e-05 | norm 0.3119 | dt 338.84ms | 1547306.88 tokens/sec
Step 17350 | loss: 3.099662 | lr:7.1652e-05 | norm 0.2887 | dt 338.17ms | 1550365.74 tokens/sec
Step 17351 | loss: 3.077514 | lr:7.1639e-05 | norm 0.2884 | dt 338.45ms | 1549079.20 tokens/sec
Step 17352 | loss: 3.077038 | lr:7.1625e-05 | norm 0.2903 | dt 340.90ms | 1537965.64 tokens/sec
Step 17353 | loss: 3.027395 | lr:7.1612e-05 | norm 0.2948 | dt 339.10ms | 1546105.85 tokens/sec
Step 17354 | loss: 3.126096 | lr:7.1598e-05 | norm 0.2848 | dt 338.46ms | 1549050.82 tokens/sec
Step 17355 | loss: 3.050183 | lr:7.1585e-05 | norm 0.2836 | dt 338.35ms | 1549560.58 tokens/sec
Step 17356 | loss: 3.075986 | lr:7.1572e-05 | norm 0.3333 | dt 338.56ms | 1548577.39 tokens/sec
Step 17357 | loss: 3.042525 | lr:7.1558e-05 | norm 0.2861 | dt 337.78ms | 1552177.94 tokens/sec
Step 17358 | loss: 3.104495 | lr:7.1545e-05 | norm 0.3028 | dt 338.84ms | 1547282.93 tokens/sec
Step 17359 | loss: 3.073202 | lr:7.1532e-05 | norm 0.2901 | dt 338.73ms | 1547788.25 tokens/sec
Step 17360 | loss: 3.130469 | lr:7.1518e-05 | norm 0.2949 | dt 338.03ms | 1550998.87 tokens/sec
Step 17361 | loss: 3.049331 | lr:7.1505e-05 | norm 0.3156 | dt 338.29ms | 1549836.88 tokens/sec
Step 17362 | loss: 3.018053 | lr:7.1492e-05 | norm 0.2720 | dt 337.83ms | 1551924.89 tokens/sec
Step 17363 | loss: 3.053495 | lr:7.1478e-05 | norm 0.2763 | dt 337.93ms | 1551463.93 tokens/sec
Step 17364 | loss: 3.092338 | lr:7.1465e-05 | norm 0.2896 | dt 337.74ms | 1552353.25 tokens/sec
Step 17365 | loss: 3.040016 | lr:7.1452e-05 | norm 0.2846 | dt 337.85ms | 1551854.80 tokens/sec
Step 17366 | loss: 3.066680 | lr:7.1438e-05 | norm 0.3021 | dt 338.05ms | 1550934.33 tokens/sec
Step 17367 | loss: 3.047563 | lr:7.1425e-05 | norm 0.2957 | dt 338.20ms | 1550238.95 tokens/sec
Step 17368 | loss: 3.092616 | lr:7.1412e-05 | norm 0.2930 | dt 338.34ms | 1549603.16 tokens/sec
Step 17369 | loss: 3.066631 | lr:7.1398e-05 | norm 0.2940 | dt 338.27ms | 1549924.27 tokens/sec
Step 17370 | loss: 3.055899 | lr:7.1385e-05 | norm 0.2933 | dt 339.04ms | 1546385.27 tokens/sec
Step 17371 | loss: 3.082160 | lr:7.1372e-05 | norm 0.2956 | dt 338.39ms | 1549353.14 tokens/sec
Step 17372 | loss: 3.079327 | lr:7.1359e-05 | norm 0.3082 | dt 337.99ms | 1551194.71 tokens/sec
Step 17373 | loss: 2.969716 | lr:7.1345e-05 | norm 0.2889 | dt 338.20ms | 1550214.91 tokens/sec
Step 17374 | loss: 3.111626 | lr:7.1332e-05 | norm 0.2912 | dt 337.54ms | 1553280.90 tokens/sec
Step 17375 | loss: 3.036534 | lr:7.1319e-05 | norm 0.2988 | dt 338.13ms | 1550545.02 tokens/sec
Step 17376 | loss: 3.025023 | lr:7.1306e-05 | norm 0.2980 | dt 337.80ms | 1552074.96 tokens/sec
Step 17377 | loss: 3.023958 | lr:7.1292e-05 | norm 0.3008 | dt 337.74ms | 1552332.43 tokens/sec
Step 17378 | loss: 2.998384 | lr:7.1279e-05 | norm 0.2955 | dt 338.43ms | 1549187.24 tokens/sec
Step 17379 | loss: 3.029482 | lr:7.1266e-05 | norm 0.2734 | dt 338.95ms | 1546804.05 tokens/sec
Step 17380 | loss: 2.992910 | lr:7.1253e-05 | norm 0.2915 | dt 337.86ms | 1551775.95 tokens/sec
Step 17381 | loss: 3.019756 | lr:7.1240e-05 | norm 0.3109 | dt 338.95ms | 1546814.93 tokens/sec
Step 17382 | loss: 3.028481 | lr:7.1226e-05 | norm 0.2952 | dt 337.18ms | 1554907.49 tokens/sec
Step 17383 | loss: 3.020696 | lr:7.1213e-05 | norm 0.2905 | dt 337.34ms | 1554187.68 tokens/sec
Step 17384 | loss: 3.023319 | lr:7.1200e-05 | norm 0.2952 | dt 339.25ms | 1545432.17 tokens/sec
Step 17385 | loss: 3.089825 | lr:7.1187e-05 | norm 0.2998 | dt 337.91ms | 1551563.55 tokens/sec
Step 17386 | loss: 3.038485 | lr:7.1174e-05 | norm 0.3074 | dt 337.50ms | 1553444.39 tokens/sec
Step 17387 | loss: 3.126244 | lr:7.1160e-05 | norm 0.2903 | dt 903.52ms | 580270.65 tokens/sec
Step 17388 | loss: 3.068876 | lr:7.1147e-05 | norm 0.3080 | dt 335.21ms | 1564052.39 tokens/sec
Step 17389 | loss: 3.066346 | lr:7.1134e-05 | norm 0.2796 | dt 337.39ms | 1553973.51 tokens/sec
Step 17390 | loss: 3.123774 | lr:7.1121e-05 | norm 0.3017 | dt 337.89ms | 1551671.93 tokens/sec
Step 17391 | loss: 3.093763 | lr:7.1108e-05 | norm 0.2831 | dt 337.61ms | 1552926.59 tokens/sec
Step 17392 | loss: 3.127100 | lr:7.1095e-05 | norm 0.2841 | dt 338.61ms | 1548372.40 tokens/sec
Step 17393 | loss: 3.032506 | lr:7.1082e-05 | norm 0.2819 | dt 337.48ms | 1553546.45 tokens/sec
Step 17394 | loss: 3.143205 | lr:7.1069e-05 | norm 0.2886 | dt 337.53ms | 1553326.98 tokens/sec
Step 17395 | loss: 3.095185 | lr:7.1056e-05 | norm 0.2786 | dt 338.52ms | 1548749.71 tokens/sec
Step 17396 | loss: 3.045234 | lr:7.1042e-05 | norm 0.2884 | dt 338.40ms | 1549312.75 tokens/sec
Step 17397 | loss: 3.071344 | lr:7.1029e-05 | norm 0.2899 | dt 337.51ms | 1553395.01 tokens/sec
Step 17398 | loss: 3.026040 | lr:7.1016e-05 | norm 0.2897 | dt 337.80ms | 1552074.96 tokens/sec
Step 17399 | loss: 3.033383 | lr:7.1003e-05 | norm 0.2824 | dt 338.30ms | 1549789.91 tokens/sec
Step 17400 | loss: 3.092991 | lr:7.0990e-05 | norm 0.2732 | dt 337.64ms | 1552780.75 tokens/sec
Step 17401 | loss: 3.088247 | lr:7.0977e-05 | norm 0.2752 | dt 337.70ms | 1552540.67 tokens/sec
Step 17402 | loss: 3.051843 | lr:7.0964e-05 | norm 0.2713 | dt 338.87ms | 1547143.59 tokens/sec
Step 17403 | loss: 3.030182 | lr:7.0951e-05 | norm 0.2982 | dt 338.70ms | 1547958.22 tokens/sec
Step 17404 | loss: 3.092043 | lr:7.0938e-05 | norm 0.2837 | dt 338.75ms | 1547698.93 tokens/sec
Step 17405 | loss: 3.096885 | lr:7.0925e-05 | norm 0.2925 | dt 338.93ms | 1546870.42 tokens/sec
Step 17406 | loss: 3.093305 | lr:7.0912e-05 | norm 0.2683 | dt 339.83ms | 1542775.80 tokens/sec
Step 17407 | loss: 3.038229 | lr:7.0899e-05 | norm 0.2732 | dt 339.13ms | 1545964.54 tokens/sec
Step 17408 | loss: 3.073069 | lr:7.0886e-05 | norm 0.2991 | dt 338.95ms | 1546806.22 tokens/sec
Step 17409 | loss: 3.019144 | lr:7.0873e-05 | norm 0.2684 | dt 339.02ms | 1546478.80 tokens/sec
Step 17410 | loss: 2.993066 | lr:7.0860e-05 | norm 0.2749 | dt 338.27ms | 1549889.31 tokens/sec
Step 17411 | loss: 3.100323 | lr:7.0847e-05 | norm 0.3029 | dt 339.37ms | 1544870.86 tokens/sec
Step 17412 | loss: 2.984599 | lr:7.0834e-05 | norm 0.2716 | dt 339.53ms | 1544176.57 tokens/sec
Step 17413 | loss: 3.059510 | lr:7.0821e-05 | norm 0.2900 | dt 338.47ms | 1548975.54 tokens/sec
Step 17414 | loss: 3.034458 | lr:7.0808e-05 | norm 0.2857 | dt 339.44ms | 1544551.85 tokens/sec
Step 17415 | loss: 2.997651 | lr:7.0795e-05 | norm 0.3040 | dt 338.19ms | 1550289.23 tokens/sec
Step 17416 | loss: 3.021457 | lr:7.0782e-05 | norm 0.2888 | dt 338.97ms | 1546713.75 tokens/sec
Step 17417 | loss: 2.981030 | lr:7.0769e-05 | norm 0.3020 | dt 339.16ms | 1545837.39 tokens/sec
Step 17418 | loss: 3.039468 | lr:7.0757e-05 | norm 0.2823 | dt 338.30ms | 1549783.36 tokens/sec
Step 17419 | loss: 3.095502 | lr:7.0744e-05 | norm 0.3399 | dt 338.28ms | 1549860.91 tokens/sec
Step 17420 | loss: 3.116932 | lr:7.0731e-05 | norm 0.3349 | dt 338.92ms | 1546931.36 tokens/sec
Step 17421 | loss: 3.114505 | lr:7.0718e-05 | norm 0.3343 | dt 337.98ms | 1551258.17 tokens/sec
Step 17422 | loss: 3.092828 | lr:7.0705e-05 | norm 0.3137 | dt 338.57ms | 1548521.78 tokens/sec
Step 17423 | loss: 3.166216 | lr:7.0692e-05 | norm 0.3590 | dt 338.55ms | 1548615.56 tokens/sec
Step 17424 | loss: 3.067580 | lr:7.0679e-05 | norm 0.3127 | dt 338.35ms | 1549563.85 tokens/sec
Step 17425 | loss: 3.068496 | lr:7.0666e-05 | norm 0.3116 | dt 338.33ms | 1549653.40 tokens/sec
Step 17426 | loss: 3.090381 | lr:7.0654e-05 | norm 0.3212 | dt 338.54ms | 1548658.09 tokens/sec
Step 17427 | loss: 3.104545 | lr:7.0641e-05 | norm 0.2992 | dt 337.62ms | 1552909.05 tokens/sec
Step 17428 | loss: 3.091784 | lr:7.0628e-05 | norm 0.3122 | dt 337.88ms | 1551711.35 tokens/sec
Step 17429 | loss: 3.084409 | lr:7.0615e-05 | norm 0.3047 | dt 339.26ms | 1545375.70 tokens/sec
Step 17430 | loss: 3.093369 | lr:7.0602e-05 | norm 0.3143 | dt 339.33ms | 1545062.99 tokens/sec
Step 17431 | loss: 2.986932 | lr:7.0589e-05 | norm 0.3006 | dt 339.49ms | 1544343.58 tokens/sec
Step 17432 | loss: 3.029403 | lr:7.0577e-05 | norm 0.2871 | dt 338.22ms | 1550146.06 tokens/sec
Step 17433 | loss: 3.073710 | lr:7.0564e-05 | norm 0.3226 | dt 338.13ms | 1550540.64 tokens/sec
Step 17434 | loss: 3.115336 | lr:7.0551e-05 | norm 0.3246 | dt 338.29ms | 1549824.87 tokens/sec
Step 17435 | loss: 3.077439 | lr:7.0538e-05 | norm 0.2896 | dt 338.31ms | 1549725.47 tokens/sec
Step 17436 | loss: 3.104952 | lr:7.0525e-05 | norm 0.3144 | dt 338.56ms | 1548603.56 tokens/sec
Step 17437 | loss: 3.051916 | lr:7.0513e-05 | norm 0.2949 | dt 338.15ms | 1550458.65 tokens/sec
Step 17438 | loss: 3.154625 | lr:7.0500e-05 | norm 0.2939 | dt 339.23ms | 1545512.55 tokens/sec
Step 17439 | loss: 3.095443 | lr:7.0487e-05 | norm 0.2934 | dt 337.92ms | 1551523.04 tokens/sec
Step 17440 | loss: 3.051854 | lr:7.0474e-05 | norm 0.3042 | dt 338.35ms | 1549545.29 tokens/sec
Step 17441 | loss: 3.037131 | lr:7.0462e-05 | norm 0.3052 | dt 337.31ms | 1554309.62 tokens/sec
Step 17442 | loss: 3.077816 | lr:7.0449e-05 | norm 0.3101 | dt 338.73ms | 1547790.43 tokens/sec
Step 17443 | loss: 3.072694 | lr:7.0436e-05 | norm 0.2820 | dt 338.02ms | 1551052.47 tokens/sec
Step 17444 | loss: 3.090981 | lr:7.0423e-05 | norm 0.2719 | dt 338.39ms | 1549346.59 tokens/sec
Step 17445 | loss: 2.956392 | lr:7.0411e-05 | norm 0.2801 | dt 337.69ms | 1552586.70 tokens/sec
Step 17446 | loss: 2.993274 | lr:7.0398e-05 | norm 0.2797 | dt 338.23ms | 1550073.95 tokens/sec
Step 17447 | loss: 3.005102 | lr:7.0385e-05 | norm 0.2882 | dt 338.18ms | 1550326.39 tokens/sec
Step 17448 | loss: 2.997330 | lr:7.0373e-05 | norm 0.2741 | dt 338.32ms | 1549701.45 tokens/sec
Step 17449 | loss: 3.016766 | lr:7.0360e-05 | norm 0.2779 | dt 337.78ms | 1552156.02 tokens/sec
Step 17450 | loss: 3.032006 | lr:7.0347e-05 | norm 0.2740 | dt 337.77ms | 1552193.27 tokens/sec
Step 17451 | loss: 3.042293 | lr:7.0335e-05 | norm 0.3020 | dt 338.60ms | 1548414.92 tokens/sec
Step 17452 | loss: 3.039725 | lr:7.0322e-05 | norm 0.2940 | dt 338.31ms | 1549708.00 tokens/sec
Step 17453 | loss: 2.983767 | lr:7.0309e-05 | norm 0.2811 | dt 338.56ms | 1548570.85 tokens/sec
Step 17454 | loss: 3.012310 | lr:7.0297e-05 | norm 0.2949 | dt 342.15ms | 1532334.96 tokens/sec
Step 17455 | loss: 3.057543 | lr:7.0284e-05 | norm 0.2887 | dt 339.24ms | 1545463.67 tokens/sec
Step 17456 | loss: 3.066861 | lr:7.0271e-05 | norm 0.2964 | dt 338.57ms | 1548544.68 tokens/sec
Step 17457 | loss: 3.046713 | lr:7.0259e-05 | norm 0.2922 | dt 339.39ms | 1544812.26 tokens/sec
Step 17458 | loss: 3.074234 | lr:7.0246e-05 | norm 0.2811 | dt 338.64ms | 1548195.80 tokens/sec
Step 17459 | loss: 3.110486 | lr:7.0234e-05 | norm 0.2893 | dt 338.96ms | 1546757.26 tokens/sec
Step 17460 | loss: 3.035595 | lr:7.0221e-05 | norm 0.2970 | dt 338.26ms | 1549950.49 tokens/sec
Step 17461 | loss: 3.040839 | lr:7.0208e-05 | norm 0.2882 | dt 339.22ms | 1545560.34 tokens/sec
Step 17462 | loss: 3.032316 | lr:7.0196e-05 | norm 0.3127 | dt 338.95ms | 1546785.55 tokens/sec
Step 17463 | loss: 3.117891 | lr:7.0183e-05 | norm 0.2912 | dt 338.70ms | 1547928.80 tokens/sec
Step 17464 | loss: 3.109682 | lr:7.0171e-05 | norm 0.3071 | dt 338.24ms | 1550055.37 tokens/sec
Step 17465 | loss: 3.101183 | lr:7.0158e-05 | norm 0.2959 | dt 338.66ms | 1548116.24 tokens/sec
Step 17466 | loss: 3.079360 | lr:7.0146e-05 | norm 0.3118 | dt 338.42ms | 1549231.98 tokens/sec
Step 17467 | loss: 3.026252 | lr:7.0133e-05 | norm 0.2950 | dt 338.47ms | 1548989.72 tokens/sec
Step 17468 | loss: 3.034880 | lr:7.0120e-05 | norm 0.2791 | dt 337.35ms | 1554142.64 tokens/sec
Step 17469 | loss: 3.035029 | lr:7.0108e-05 | norm 0.2887 | dt 338.57ms | 1548537.04 tokens/sec
Step 17470 | loss: 3.065988 | lr:7.0095e-05 | norm 0.2860 | dt 338.42ms | 1549225.43 tokens/sec
Step 17471 | loss: 3.099289 | lr:7.0083e-05 | norm 0.2695 | dt 338.60ms | 1548408.38 tokens/sec
Step 17472 | loss: 3.127356 | lr:7.0070e-05 | norm 0.3461 | dt 338.01ms | 1551100.61 tokens/sec
Step 17473 | loss: 3.092085 | lr:7.0058e-05 | norm 0.2811 | dt 339.77ms | 1543055.10 tokens/sec
Step 17474 | loss: 3.188079 | lr:7.0045e-05 | norm 0.3536 | dt 338.29ms | 1549825.96 tokens/sec
Step 17475 | loss: 3.074244 | lr:7.0033e-05 | norm 0.3221 | dt 338.87ms | 1547145.76 tokens/sec
Step 17476 | loss: 3.045618 | lr:7.0020e-05 | norm 0.2891 | dt 339.31ms | 1545143.32 tokens/sec
Step 17477 | loss: 3.095509 | lr:7.0008e-05 | norm 0.2820 | dt 337.70ms | 1552537.38 tokens/sec
Step 17478 | loss: 3.059674 | lr:6.9996e-05 | norm 0.3025 | dt 338.01ms | 1551089.67 tokens/sec
Step 17479 | loss: 3.045331 | lr:6.9983e-05 | norm 0.2931 | dt 1000.96ms | 523787.48 tokens/sec
Step 17480 | loss: 3.049252 | lr:6.9971e-05 | norm 0.3009 | dt 337.30ms | 1554344.77 tokens/sec
Step 17481 | loss: 2.963504 | lr:6.9958e-05 | norm 0.3475 | dt 337.59ms | 1553041.75 tokens/sec
Step 17482 | loss: 3.014600 | lr:6.9946e-05 | norm 0.2913 | dt 338.71ms | 1547901.56 tokens/sec
Step 17483 | loss: 3.015198 | lr:6.9933e-05 | norm 0.2848 | dt 338.29ms | 1549806.30 tokens/sec
Step 17484 | loss: 3.117362 | lr:6.9921e-05 | norm 0.3097 | dt 337.63ms | 1552843.25 tokens/sec
Step 17485 | loss: 3.014138 | lr:6.9909e-05 | norm 0.2954 | dt 338.50ms | 1548856.62 tokens/sec
Step 17486 | loss: 3.010500 | lr:6.9896e-05 | norm 0.2705 | dt 338.61ms | 1548352.78 tokens/sec
Step 17487 | loss: 3.079250 | lr:6.9884e-05 | norm 0.2981 | dt 338.01ms | 1551091.86 tokens/sec
Step 17488 | loss: 3.027035 | lr:6.9871e-05 | norm 0.2806 | dt 338.48ms | 1548950.44 tokens/sec
Step 17489 | loss: 3.075417 | lr:6.9859e-05 | norm 0.2975 | dt 338.19ms | 1550285.95 tokens/sec
Step 17490 | loss: 3.093936 | lr:6.9847e-05 | norm 0.2975 | dt 337.43ms | 1553751.72 tokens/sec
Step 17491 | loss: 3.181987 | lr:6.9834e-05 | norm 0.3393 | dt 337.89ms | 1551655.51 tokens/sec
Step 17492 | loss: 3.083983 | lr:6.9822e-05 | norm 0.3122 | dt 337.71ms | 1552477.09 tokens/sec
Step 17493 | loss: 3.106767 | lr:6.9810e-05 | norm 0.3137 | dt 338.43ms | 1549189.42 tokens/sec
Step 17494 | loss: 3.058282 | lr:6.9797e-05 | norm 0.3000 | dt 338.08ms | 1550782.30 tokens/sec
Step 17495 | loss: 3.124976 | lr:6.9785e-05 | norm 0.2987 | dt 337.64ms | 1552820.22 tokens/sec
Step 17496 | loss: 3.032628 | lr:6.9773e-05 | norm 0.3077 | dt 338.24ms | 1550042.26 tokens/sec
Step 17497 | loss: 3.083169 | lr:6.9760e-05 | norm 0.3069 | dt 338.49ms | 1548894.80 tokens/sec
Step 17498 | loss: 3.195987 | lr:6.9748e-05 | norm 0.3659 | dt 337.60ms | 1553004.46 tokens/sec
Step 17499 | loss: 3.080401 | lr:6.9736e-05 | norm 0.3687 | dt 337.95ms | 1551388.41 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 17500: 3.0883
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3042/10042=0.3029


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but a lot of people don't think it is worth the time spent trying to learn a language.
The real issue
rank 5 sample 1 >Hello, I'm a language model, right? Yes, let me know how to model it in this article.<|endoftext|>It’s one of those things
rank 5 sample 2 >Hello, I'm a language model, and I like the same thing every day. You can tell me something about me. Here's a picture of a mouse
rank 5 sample 3 >Hello, I'm a language model, here is the latest version on my site:
And it's nice to see.<|endoftext|>A quick reminder: the American




ddp_rank 2: ####### Printing generated samples ####### 



ddp_rank 7: ####### Printing generated samples ####### 


rank 2 sample 0 >Hello, I'm a language model, so my name goes to Maths@home, and you have to see what your school's code looks on the screen

ddp_rank 6: ####### Printing generated samples ####### 

rank 2 sample 1 >Hello, I'm a language model, and I try to be as close as possible to these folks as possible. I'm not trying to take a completely scientific
rank 7 sample 0 >Hello, I'm a language model, so I thought I should get a basic working knowledge of how to write program in such a way. If I can do
rank 2 sample 2 >Hello, I'm a language model, and I've been using it ever since I first learned it. So to me, it was so different, and now
rank 6 sample 0 >Hello, I'm a language model, meaning my language model will also define its syntax. For this example, the model you're working with is called the standard
rank 2 sample 3 >Hello, I'm a language model, but if I'm a language model, maybe I wanna say hello.
Thank you very much for the post!
rank 7 sample 1 >Hello, I'm a language model, learning model is a process for building on knowledge. By means of these steps, this process of learning takes place, you



rank 6 sample 1 >Hello, I'm a language model, so I have to have some idea of whether it can be used in this way or not, but it can be used
rank 7 sample 2 >Hello, I'm a language model, so I'll look at the main concept and I'll write a tutorial as a language model.
A language model is
rank 6 sample 2 >Hello, I'm a language model, but how do you learn to use an algorithm and how do we go from simple to complex?
And then how can
rank 7 sample 3 >Hello, I'm a language model, and you just have to look at two questions for me. I've used the "list" tool on the other.


rank 6 sample 3 >Hello, I'm a language model, so I have a bunch of other languages. So I'm going to be giving a bunch of examples of different sentences you




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so how can I teach a language the best you can? - I'm a language model, not a programmer.

rank 1 sample 1 >Hello, I'm a language model, a programmer. To make a simple sentence is to be defined. Then, a couple statements that a programmer can use to
rank 1 sample 2 >Hello, I'm a language model, but people still talk about it. I'm not sure about which way you feel about it. If you want someone who
rank 1 sample 3 >Hello, I'm a language model, so I'm using the library here. Why not go down here and find information about HTML:
<html> <




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I'd like you to take up courses by learning other languages at high school. But because there is a lot of
rank 0 sample 1 >Hello, I'm a language model, and here's a quick reminder that to be honest for people with this, there are many more things a computer can do
rank 0 sample 2 >Hello, I'm a language model, and I started out making videos of what I do as a learner. I used a model of a computer to tell
rank 0 sample 3 >Hello, I'm a language model, and while I'm not a native speaker I won't be able to communicate. Because there's a new language to get




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not the one to actually be able to read data from strings? And you might be using the string as
rank 3 sample 1 >Hello, I'm a language model, so what are you doing? Do you want to create a language that's suitable for different languages?
As it says
rank 3 sample 2 >Hello, I'm a language model, so this one is different. For me, to put it in a nutshell, the first thing I do is take the
rank 3 sample 3 >Hello, I'm a language model, so in this course, you've already seen that you should use some standard input tags that are standardly called "string




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I'd like to thank you for listening to this.
I love languages, they open up a wide world to
rank 4 sample 1 >Hello, I'm a language model, can anyone explain why that would not work? (Yeah, I'm doing it on Windows 10, it's better,
rank 4 sample 2 >Hello, I'm a language model, I live in Tokyo. But I live in Beijing. I can tell you how Japan uses Tango more than a thousand
rank 4 sample 3 >Hello, I'm a language model, so it says this one. I had an error of 15%.
2) Use the default text-to-language


Step 17500 | loss: 3.081225 | lr:6.9723e-05 | norm 0.3290 | dt 18672.06ms | 28078.75 tokens/sec
Step 17501 | loss: 3.101131 | lr:6.9711e-05 | norm 0.3835 | dt 335.44ms | 1562964.08 tokens/sec
Step 17502 | loss: 3.045655 | lr:6.9699e-05 | norm 0.3006 | dt 335.37ms | 1563315.20 tokens/sec
Step 17503 | loss: 3.056793 | lr:6.9687e-05 | norm 0.2898 | dt 336.77ms | 1556826.21 tokens/sec
Step 17504 | loss: 3.047642 | lr:6.9674e-05 | norm 0.3364 | dt 336.03ms | 1560250.47 tokens/sec
Step 17505 | loss: 3.041759 | lr:6.9662e-05 | norm 0.2972 | dt 336.24ms | 1559260.31 tokens/sec
Step 17506 | loss: 3.086076 | lr:6.9650e-05 | norm 0.3129 | dt 337.13ms | 1555170.31 tokens/sec
Step 17507 | loss: 3.052717 | lr:6.9638e-05 | norm 0.2811 | dt 336.67ms | 1557282.65 tokens/sec
Step 17508 | loss: 3.044859 | lr:6.9625e-05 | norm 0.2989 | dt 336.72ms | 1557033.45 tokens/sec
Step 17509 | loss: 3.047876 | lr:6.9613e-05 | norm 0.3119 | dt 337.33ms | 1554248.10 tokens/sec
Step 17510 | loss: 3.113450 | lr:6.9601e-05 | norm 0.3022 | dt 336.13ms | 1559797.83 tokens/sec
Step 17511 | loss: 3.056845 | lr:6.9589e-05 | norm 0.3012 | dt 336.26ms | 1559188.45 tokens/sec
Step 17512 | loss: 3.080344 | lr:6.9576e-05 | norm 0.3436 | dt 336.35ms | 1558767.36 tokens/sec
Step 17513 | loss: 3.017705 | lr:6.9564e-05 | norm 0.3027 | dt 337.01ms | 1555689.60 tokens/sec
Step 17514 | loss: 3.019387 | lr:6.9552e-05 | norm 0.3020 | dt 336.77ms | 1556792.05 tokens/sec
Step 17515 | loss: 3.029297 | lr:6.9540e-05 | norm 0.3218 | dt 336.18ms | 1559523.49 tokens/sec
Step 17516 | loss: 2.980009 | lr:6.9528e-05 | norm 0.2686 | dt 336.92ms | 1556120.04 tokens/sec
Step 17517 | loss: 2.992914 | lr:6.9516e-05 | norm 0.3015 | dt 336.74ms | 1556968.41 tokens/sec
Step 17518 | loss: 3.044560 | lr:6.9503e-05 | norm 0.2874 | dt 336.48ms | 1558159.89 tokens/sec
Step 17519 | loss: 2.992845 | lr:6.9491e-05 | norm 0.2811 | dt 336.91ms | 1556188.32 tokens/sec
Step 17520 | loss: 3.017735 | lr:6.9479e-05 | norm 0.2731 | dt 336.68ms | 1557239.64 tokens/sec
Step 17521 | loss: 2.997780 | lr:6.9467e-05 | norm 0.2982 | dt 336.31ms | 1558939.74 tokens/sec
Step 17522 | loss: 3.070024 | lr:6.9455e-05 | norm 0.2852 | dt 337.48ms | 1553553.04 tokens/sec
Step 17523 | loss: 3.093225 | lr:6.9443e-05 | norm 0.2875 | dt 338.14ms | 1550523.15 tokens/sec
Step 17524 | loss: 3.091445 | lr:6.9431e-05 | norm 0.3127 | dt 336.54ms | 1557868.47 tokens/sec
Step 17525 | loss: 3.078370 | lr:6.9419e-05 | norm 0.3103 | dt 338.34ms | 1549605.35 tokens/sec
Step 17526 | loss: 3.075650 | lr:6.9406e-05 | norm 0.3234 | dt 337.40ms | 1553926.29 tokens/sec
Step 17527 | loss: 3.113185 | lr:6.9394e-05 | norm 0.2866 | dt 336.73ms | 1557012.50 tokens/sec
Step 17528 | loss: 3.074429 | lr:6.9382e-05 | norm 0.3155 | dt 337.64ms | 1552785.14 tokens/sec
Step 17529 | loss: 3.110461 | lr:6.9370e-05 | norm 0.3124 | dt 337.17ms | 1554971.26 tokens/sec
Step 17530 | loss: 3.086106 | lr:6.9358e-05 | norm 0.3171 | dt 337.76ms | 1552254.63 tokens/sec
Step 17531 | loss: 3.067265 | lr:6.9346e-05 | norm 0.2884 | dt 337.81ms | 1552022.38 tokens/sec
Step 17532 | loss: 3.060516 | lr:6.9334e-05 | norm 0.3072 | dt 338.06ms | 1550874.17 tokens/sec
Step 17533 | loss: 3.096879 | lr:6.9322e-05 | norm 0.3089 | dt 338.01ms | 1551124.68 tokens/sec
Step 17534 | loss: 3.186864 | lr:6.9310e-05 | norm 0.3012 | dt 338.10ms | 1550688.25 tokens/sec
Step 17535 | loss: 3.111967 | lr:6.9298e-05 | norm 0.2838 | dt 337.86ms | 1551803.33 tokens/sec
Step 17536 | loss: 3.039826 | lr:6.9286e-05 | norm 0.2986 | dt 337.69ms | 1552592.18 tokens/sec
Step 17537 | loss: 3.017688 | lr:6.9274e-05 | norm 0.2953 | dt 337.60ms | 1552990.20 tokens/sec
Step 17538 | loss: 3.043734 | lr:6.9262e-05 | norm 0.2767 | dt 337.52ms | 1553356.60 tokens/sec
Step 17539 | loss: 3.055034 | lr:6.9250e-05 | norm 0.3060 | dt 337.47ms | 1553573.89 tokens/sec
Step 17540 | loss: 3.089409 | lr:6.9238e-05 | norm 0.3043 | dt 338.46ms | 1549049.73 tokens/sec
Step 17541 | loss: 3.060691 | lr:6.9226e-05 | norm 0.9294 | dt 338.03ms | 1551008.71 tokens/sec
Step 17542 | loss: 3.077529 | lr:6.9214e-05 | norm 0.2948 | dt 338.11ms | 1550621.55 tokens/sec
Step 17543 | loss: 3.086633 | lr:6.9202e-05 | norm 0.3144 | dt 338.29ms | 1549818.31 tokens/sec
Step 17544 | loss: 3.055482 | lr:6.9190e-05 | norm 0.2958 | dt 338.76ms | 1547667.34 tokens/sec
Step 17545 | loss: 3.050510 | lr:6.9178e-05 | norm 0.2940 | dt 337.60ms | 1552973.75 tokens/sec
Step 17546 | loss: 3.077024 | lr:6.9166e-05 | norm 0.2924 | dt 337.74ms | 1552363.11 tokens/sec
Step 17547 | loss: 3.049121 | lr:6.9154e-05 | norm 0.2836 | dt 338.62ms | 1548289.55 tokens/sec
Step 17548 | loss: 3.019744 | lr:6.9142e-05 | norm 0.2944 | dt 337.55ms | 1553215.07 tokens/sec
Step 17549 | loss: 3.022436 | lr:6.9130e-05 | norm 0.2741 | dt 338.50ms | 1548859.89 tokens/sec
Step 17550 | loss: 3.028705 | lr:6.9118e-05 | norm 0.2795 | dt 337.88ms | 1551714.63 tokens/sec
Step 17551 | loss: 3.041090 | lr:6.9107e-05 | norm 0.2752 | dt 337.65ms | 1552768.69 tokens/sec
Step 17552 | loss: 3.045162 | lr:6.9095e-05 | norm 0.2996 | dt 337.98ms | 1551237.38 tokens/sec
Step 17553 | loss: 3.009106 | lr:6.9083e-05 | norm 0.2625 | dt 338.03ms | 1551012.00 tokens/sec
Step 17554 | loss: 2.954600 | lr:6.9071e-05 | norm 0.2789 | dt 337.87ms | 1551757.34 tokens/sec
Step 17555 | loss: 2.986773 | lr:6.9059e-05 | norm 0.2863 | dt 338.01ms | 1551117.02 tokens/sec
Step 17556 | loss: 3.033666 | lr:6.9047e-05 | norm 0.2706 | dt 338.01ms | 1551114.83 tokens/sec
Step 17557 | loss: 3.050468 | lr:6.9035e-05 | norm 0.2773 | dt 337.98ms | 1551254.89 tokens/sec
Step 17558 | loss: 2.976687 | lr:6.9023e-05 | norm 0.2840 | dt 338.15ms | 1550447.72 tokens/sec
Step 17559 | loss: 3.035591 | lr:6.9012e-05 | norm 0.2829 | dt 338.71ms | 1547899.38 tokens/sec
Step 17560 | loss: 3.030886 | lr:6.9000e-05 | norm 0.2640 | dt 338.82ms | 1547402.70 tokens/sec
Step 17561 | loss: 3.102922 | lr:6.8988e-05 | norm 0.3367 | dt 338.75ms | 1547702.19 tokens/sec
Step 17562 | loss: 3.148158 | lr:6.8976e-05 | norm 0.3042 | dt 339.08ms | 1546205.86 tokens/sec
Step 17563 | loss: 3.119099 | lr:6.8964e-05 | norm 0.2995 | dt 339.24ms | 1545481.05 tokens/sec
Step 17564 | loss: 3.114208 | lr:6.8953e-05 | norm 0.3290 | dt 338.27ms | 1549928.64 tokens/sec
Step 17565 | loss: 3.118498 | lr:6.8941e-05 | norm 0.2978 | dt 339.27ms | 1545353.98 tokens/sec
Step 17566 | loss: 3.232091 | lr:6.8929e-05 | norm 0.3654 | dt 337.96ms | 1551312.89 tokens/sec
Step 17567 | loss: 3.122930 | lr:6.8917e-05 | norm 0.3523 | dt 338.98ms | 1546648.48 tokens/sec
Step 17568 | loss: 3.115498 | lr:6.8905e-05 | norm 0.3323 | dt 337.93ms | 1551470.50 tokens/sec
Step 17569 | loss: 3.071710 | lr:6.8894e-05 | norm 0.3313 | dt 338.38ms | 1549409.91 tokens/sec
Step 17570 | loss: 2.999461 | lr:6.8882e-05 | norm 0.3246 | dt 337.32ms | 1554261.28 tokens/sec
Step 17571 | loss: 3.102104 | lr:6.8870e-05 | norm 0.3263 | dt 339.27ms | 1545332.26 tokens/sec
Step 17572 | loss: 3.020733 | lr:6.8858e-05 | norm 0.3161 | dt 339.34ms | 1545018.48 tokens/sec
Step 17573 | loss: 3.014938 | lr:6.8847e-05 | norm 0.2970 | dt 337.95ms | 1551394.97 tokens/sec
Step 17574 | loss: 3.116386 | lr:6.8835e-05 | norm 0.3087 | dt 338.03ms | 1551022.93 tokens/sec
Step 17575 | loss: 3.118991 | lr:6.8823e-05 | norm 0.2926 | dt 338.28ms | 1549851.08 tokens/sec
Step 17576 | loss: 3.104488 | lr:6.8812e-05 | norm 0.2854 | dt 1020.54ms | 513733.83 tokens/sec
Step 17577 | loss: 3.032017 | lr:6.8800e-05 | norm 0.2792 | dt 335.14ms | 1564391.76 tokens/sec
Step 17578 | loss: 3.100554 | lr:6.8788e-05 | norm 0.3186 | dt 337.73ms | 1552375.17 tokens/sec
Step 17579 | loss: 3.063684 | lr:6.8776e-05 | norm 0.3056 | dt 341.33ms | 1535999.75 tokens/sec
Step 17580 | loss: 3.057723 | lr:6.8765e-05 | norm 0.3162 | dt 337.89ms | 1551637.99 tokens/sec
Step 17581 | loss: 3.079923 | lr:6.8753e-05 | norm 0.3033 | dt 338.67ms | 1548102.07 tokens/sec
Step 17582 | loss: 3.113455 | lr:6.8741e-05 | norm 0.2977 | dt 337.38ms | 1554003.16 tokens/sec
Step 17583 | loss: 3.066210 | lr:6.8730e-05 | norm 0.2867 | dt 338.73ms | 1547816.58 tokens/sec
Step 17584 | loss: 3.127669 | lr:6.8718e-05 | norm 0.2862 | dt 338.37ms | 1549454.67 tokens/sec
Step 17585 | loss: 3.033504 | lr:6.8706e-05 | norm 0.2932 | dt 337.88ms | 1551698.21 tokens/sec
Step 17586 | loss: 3.020669 | lr:6.8695e-05 | norm 0.2792 | dt 338.40ms | 1549325.85 tokens/sec
Step 17587 | loss: 3.021110 | lr:6.8683e-05 | norm 0.2847 | dt 338.30ms | 1549751.69 tokens/sec
Step 17588 | loss: 3.007461 | lr:6.8672e-05 | norm 0.3003 | dt 338.20ms | 1550219.28 tokens/sec
Step 17589 | loss: 3.008727 | lr:6.8660e-05 | norm 0.2832 | dt 338.46ms | 1549048.64 tokens/sec
Step 17590 | loss: 3.045720 | lr:6.8648e-05 | norm 0.2883 | dt 338.36ms | 1549492.88 tokens/sec
Step 17591 | loss: 3.079907 | lr:6.8637e-05 | norm 0.3059 | dt 338.58ms | 1548490.15 tokens/sec
Step 17592 | loss: 3.027068 | lr:6.8625e-05 | norm 0.2765 | dt 339.67ms | 1543510.00 tokens/sec
Step 17593 | loss: 3.027721 | lr:6.8614e-05 | norm 0.2935 | dt 338.59ms | 1548441.09 tokens/sec
Step 17594 | loss: 3.048617 | lr:6.8602e-05 | norm 0.2979 | dt 338.74ms | 1547770.82 tokens/sec
Step 17595 | loss: 3.003848 | lr:6.8590e-05 | norm 0.2889 | dt 337.57ms | 1553128.41 tokens/sec
Step 17596 | loss: 3.096691 | lr:6.8579e-05 | norm 0.3120 | dt 339.00ms | 1546567.98 tokens/sec
Step 17597 | loss: 3.046828 | lr:6.8567e-05 | norm 0.2877 | dt 338.56ms | 1548570.85 tokens/sec
Step 17598 | loss: 3.104522 | lr:6.8556e-05 | norm 0.2953 | dt 338.52ms | 1548785.71 tokens/sec
Step 17599 | loss: 3.048182 | lr:6.8544e-05 | norm 0.3003 | dt 338.19ms | 1550297.97 tokens/sec
Step 17600 | loss: 3.067437 | lr:6.8533e-05 | norm 0.3050 | dt 338.53ms | 1548712.63 tokens/sec
Step 17601 | loss: 3.066687 | lr:6.8521e-05 | norm 0.2966 | dt 338.90ms | 1547031.48 tokens/sec
Step 17602 | loss: 3.092427 | lr:6.8510e-05 | norm 0.3004 | dt 339.25ms | 1545453.89 tokens/sec
Step 17603 | loss: 3.072703 | lr:6.8498e-05 | norm 0.3004 | dt 338.78ms | 1547558.42 tokens/sec
Step 17604 | loss: 3.120915 | lr:6.8487e-05 | norm 0.3014 | dt 338.78ms | 1547590.00 tokens/sec
Step 17605 | loss: 3.034945 | lr:6.8475e-05 | norm 0.2895 | dt 338.87ms | 1547158.83 tokens/sec
Step 17606 | loss: 3.062937 | lr:6.8464e-05 | norm 0.2935 | dt 339.35ms | 1544995.68 tokens/sec
Step 17607 | loss: 3.079975 | lr:6.8452e-05 | norm 0.2970 | dt 338.63ms | 1548264.47 tokens/sec
Step 17608 | loss: 3.075800 | lr:6.8441e-05 | norm 0.2783 | dt 339.21ms | 1545602.71 tokens/sec
Step 17609 | loss: 3.096991 | lr:6.8429e-05 | norm 0.2765 | dt 339.26ms | 1545398.50 tokens/sec
Step 17610 | loss: 3.070906 | lr:6.8418e-05 | norm 0.3131 | dt 338.58ms | 1548489.06 tokens/sec
Step 17611 | loss: 3.039172 | lr:6.8406e-05 | norm 0.2804 | dt 338.77ms | 1547612.88 tokens/sec
Step 17612 | loss: 3.040149 | lr:6.8395e-05 | norm 0.2721 | dt 338.21ms | 1550176.66 tokens/sec
Step 17613 | loss: 3.078156 | lr:6.8384e-05 | norm 0.2897 | dt 339.46ms | 1544498.69 tokens/sec
Step 17614 | loss: 3.153131 | lr:6.8372e-05 | norm 0.3173 | dt 339.21ms | 1545635.30 tokens/sec
Step 17615 | loss: 3.164751 | lr:6.8361e-05 | norm 0.3004 | dt 338.69ms | 1548001.81 tokens/sec
Step 17616 | loss: 3.032032 | lr:6.8349e-05 | norm 0.3058 | dt 337.87ms | 1551725.58 tokens/sec
Step 17617 | loss: 3.121175 | lr:6.8338e-05 | norm 0.2892 | dt 337.55ms | 1553228.23 tokens/sec
Step 17618 | loss: 3.025762 | lr:6.8327e-05 | norm 0.2826 | dt 338.72ms | 1547855.80 tokens/sec
Step 17619 | loss: 3.007368 | lr:6.8315e-05 | norm 0.2791 | dt 338.45ms | 1549068.28 tokens/sec
Step 17620 | loss: 3.036373 | lr:6.8304e-05 | norm 0.2823 | dt 337.77ms | 1552215.19 tokens/sec
Step 17621 | loss: 2.985621 | lr:6.8292e-05 | norm 0.2898 | dt 339.04ms | 1546398.32 tokens/sec
Step 17622 | loss: 3.046719 | lr:6.8281e-05 | norm 0.2769 | dt 337.59ms | 1553019.82 tokens/sec
Step 17623 | loss: 3.005968 | lr:6.8270e-05 | norm 0.2914 | dt 338.17ms | 1550376.67 tokens/sec
Step 17624 | loss: 3.010293 | lr:6.8258e-05 | norm 0.2711 | dt 338.11ms | 1550665.29 tokens/sec
Step 17625 | loss: 3.050676 | lr:6.8247e-05 | norm 0.2877 | dt 338.06ms | 1550891.67 tokens/sec
Step 17626 | loss: 3.020139 | lr:6.8236e-05 | norm 0.2787 | dt 337.45ms | 1553696.83 tokens/sec
Step 17627 | loss: 3.019651 | lr:6.8224e-05 | norm 0.2682 | dt 338.28ms | 1549858.73 tokens/sec
Step 17628 | loss: 3.047749 | lr:6.8213e-05 | norm 0.2761 | dt 337.93ms | 1551477.07 tokens/sec
Step 17629 | loss: 3.204560 | lr:6.8202e-05 | norm 0.3999 | dt 337.73ms | 1552398.18 tokens/sec
Step 17630 | loss: 3.063070 | lr:6.8190e-05 | norm 0.6520 | dt 338.84ms | 1547322.12 tokens/sec
Step 17631 | loss: 2.998124 | lr:6.8179e-05 | norm 0.2937 | dt 339.22ms | 1545564.69 tokens/sec
Step 17632 | loss: 3.105869 | lr:6.8168e-05 | norm 0.3037 | dt 338.77ms | 1547634.66 tokens/sec
Step 17633 | loss: 3.111149 | lr:6.8157e-05 | norm 0.3114 | dt 339.00ms | 1546567.98 tokens/sec
Step 17634 | loss: 3.163041 | lr:6.8145e-05 | norm 0.3290 | dt 339.16ms | 1545834.13 tokens/sec
Step 17635 | loss: 3.094931 | lr:6.8134e-05 | norm 0.3092 | dt 338.43ms | 1549170.86 tokens/sec
Step 17636 | loss: 3.046737 | lr:6.8123e-05 | norm 0.3197 | dt 339.51ms | 1544241.64 tokens/sec
Step 17637 | loss: 3.086362 | lr:6.8112e-05 | norm 0.3176 | dt 338.33ms | 1549633.74 tokens/sec
Step 17638 | loss: 3.050205 | lr:6.8100e-05 | norm 0.2944 | dt 337.72ms | 1552443.12 tokens/sec
Step 17639 | loss: 3.084994 | lr:6.8089e-05 | norm 0.3469 | dt 337.99ms | 1551191.42 tokens/sec
Step 17640 | loss: 3.099429 | lr:6.8078e-05 | norm 0.3018 | dt 337.43ms | 1553758.31 tokens/sec
Step 17641 | loss: 3.056996 | lr:6.8067e-05 | norm 0.3042 | dt 338.02ms | 1551034.97 tokens/sec
Step 17642 | loss: 3.126591 | lr:6.8055e-05 | norm 0.3637 | dt 338.38ms | 1549419.74 tokens/sec
Step 17643 | loss: 3.019389 | lr:6.8044e-05 | norm 0.3205 | dt 337.70ms | 1552520.94 tokens/sec
Step 17644 | loss: 3.087468 | lr:6.8033e-05 | norm 0.3205 | dt 338.77ms | 1547625.95 tokens/sec
Step 17645 | loss: 3.076404 | lr:6.8022e-05 | norm 0.3004 | dt 338.56ms | 1548591.57 tokens/sec
Step 17646 | loss: 3.041390 | lr:6.8011e-05 | norm 0.3058 | dt 338.32ms | 1549674.15 tokens/sec
Step 17647 | loss: 3.043200 | lr:6.8000e-05 | norm 0.3245 | dt 338.26ms | 1549966.88 tokens/sec
Step 17648 | loss: 3.072870 | lr:6.7988e-05 | norm 0.2884 | dt 338.67ms | 1548102.07 tokens/sec
Step 17649 | loss: 3.055266 | lr:6.7977e-05 | norm 0.2976 | dt 337.67ms | 1552670.02 tokens/sec
Step 17650 | loss: 3.036932 | lr:6.7966e-05 | norm 0.3177 | dt 337.97ms | 1551282.25 tokens/sec
Step 17651 | loss: 3.077751 | lr:6.7955e-05 | norm 0.2955 | dt 337.55ms | 1553213.97 tokens/sec
Step 17652 | loss: 3.061423 | lr:6.7944e-05 | norm 0.3004 | dt 338.45ms | 1549101.02 tokens/sec
Step 17653 | loss: 3.056226 | lr:6.7933e-05 | norm 0.2794 | dt 337.93ms | 1551489.11 tokens/sec
Step 17654 | loss: 3.042455 | lr:6.7922e-05 | norm 0.2687 | dt 337.54ms | 1553283.09 tokens/sec
Step 17655 | loss: 3.025454 | lr:6.7910e-05 | norm 0.2966 | dt 338.38ms | 1549405.54 tokens/sec
Step 17656 | loss: 2.986739 | lr:6.7899e-05 | norm 0.2824 | dt 338.22ms | 1550136.23 tokens/sec
Step 17657 | loss: 3.079498 | lr:6.7888e-05 | norm 0.2793 | dt 337.81ms | 1552001.56 tokens/sec
Step 17658 | loss: 3.065304 | lr:6.7877e-05 | norm 0.2861 | dt 338.33ms | 1549645.75 tokens/sec
Step 17659 | loss: 2.993509 | lr:6.7866e-05 | norm 0.2808 | dt 337.85ms | 1551843.85 tokens/sec
Step 17660 | loss: 3.008285 | lr:6.7855e-05 | norm 0.2749 | dt 338.20ms | 1550226.93 tokens/sec
Step 17661 | loss: 3.028227 | lr:6.7844e-05 | norm 0.2793 | dt 338.30ms | 1549793.19 tokens/sec
Step 17662 | loss: 3.002295 | lr:6.7833e-05 | norm 0.3180 | dt 338.21ms | 1550206.17 tokens/sec
Step 17663 | loss: 2.991275 | lr:6.7822e-05 | norm 0.2752 | dt 338.10ms | 1550678.41 tokens/sec
Step 17664 | loss: 3.011191 | lr:6.7811e-05 | norm 0.2644 | dt 339.08ms | 1546204.78 tokens/sec
Step 17665 | loss: 3.011013 | lr:6.7800e-05 | norm 0.2908 | dt 337.90ms | 1551593.10 tokens/sec
Step 17666 | loss: 3.053144 | lr:6.7789e-05 | norm 0.2883 | dt 338.16ms | 1550400.71 tokens/sec
Step 17667 | loss: 3.070493 | lr:6.7778e-05 | norm 0.2803 | dt 338.04ms | 1550951.83 tokens/sec
Step 17668 | loss: 3.078186 | lr:6.7767e-05 | norm 0.3002 | dt 338.70ms | 1547947.32 tokens/sec
Step 17669 | loss: 3.030014 | lr:6.7756e-05 | norm 0.3084 | dt 995.68ms | 526561.06 tokens/sec
Step 17670 | loss: 3.084327 | lr:6.7745e-05 | norm 0.2794 | dt 337.18ms | 1554901.99 tokens/sec
Step 17671 | loss: 3.023718 | lr:6.7734e-05 | norm 0.2899 | dt 337.47ms | 1553572.79 tokens/sec
Step 17672 | loss: 3.053677 | lr:6.7723e-05 | norm 0.3665 | dt 338.20ms | 1550245.51 tokens/sec
Step 17673 | loss: 3.124487 | lr:6.7712e-05 | norm 0.3088 | dt 337.03ms | 1555615.87 tokens/sec
Step 17674 | loss: 3.069251 | lr:6.7701e-05 | norm 0.3284 | dt 338.31ms | 1549708.00 tokens/sec
Step 17675 | loss: 3.046029 | lr:6.7690e-05 | norm 0.3358 | dt 337.71ms | 1552460.65 tokens/sec
Step 17676 | loss: 3.069156 | lr:6.7679e-05 | norm 0.2995 | dt 337.58ms | 1553058.20 tokens/sec
Step 17677 | loss: 3.074109 | lr:6.7668e-05 | norm 0.3252 | dt 338.40ms | 1549305.11 tokens/sec
Step 17678 | loss: 3.082883 | lr:6.7657e-05 | norm 0.3487 | dt 338.39ms | 1549381.53 tokens/sec
Step 17679 | loss: 3.096151 | lr:6.7646e-05 | norm 0.3234 | dt 338.03ms | 1550995.59 tokens/sec
Step 17680 | loss: 3.112798 | lr:6.7635e-05 | norm 0.3005 | dt 338.29ms | 1549830.33 tokens/sec
Step 17681 | loss: 3.140860 | lr:6.7624e-05 | norm 0.3265 | dt 338.62ms | 1548321.16 tokens/sec
Step 17682 | loss: 3.075843 | lr:6.7614e-05 | norm 0.3184 | dt 338.29ms | 1549797.56 tokens/sec
Step 17683 | loss: 3.104584 | lr:6.7603e-05 | norm 0.3129 | dt 338.28ms | 1549854.36 tokens/sec
Step 17684 | loss: 3.041238 | lr:6.7592e-05 | norm 0.3130 | dt 338.27ms | 1549918.81 tokens/sec
Step 17685 | loss: 3.031319 | lr:6.7581e-05 | norm 0.2750 | dt 340.25ms | 1540894.77 tokens/sec
Step 17686 | loss: 3.096524 | lr:6.7570e-05 | norm 0.3004 | dt 338.25ms | 1550001.84 tokens/sec
Step 17687 | loss: 3.102484 | lr:6.7559e-05 | norm 0.3044 | dt 338.26ms | 1549950.49 tokens/sec
Step 17688 | loss: 3.075901 | lr:6.7548e-05 | norm 0.2809 | dt 339.03ms | 1546417.89 tokens/sec
Step 17689 | loss: 3.005837 | lr:6.7537e-05 | norm 0.2874 | dt 338.38ms | 1549398.99 tokens/sec
Step 17690 | loss: 2.992437 | lr:6.7527e-05 | norm 0.2887 | dt 337.74ms | 1552335.72 tokens/sec
Step 17691 | loss: 3.037080 | lr:6.7516e-05 | norm 0.2887 | dt 338.24ms | 1550054.28 tokens/sec
Step 17692 | loss: 3.002415 | lr:6.7505e-05 | norm 0.2789 | dt 338.86ms | 1547223.05 tokens/sec
Step 17693 | loss: 3.046229 | lr:6.7494e-05 | norm 0.2966 | dt 337.55ms | 1553223.85 tokens/sec
Step 17694 | loss: 3.056049 | lr:6.7483e-05 | norm 0.2847 | dt 338.04ms | 1550975.90 tokens/sec
Step 17695 | loss: 2.972027 | lr:6.7473e-05 | norm 0.2907 | dt 339.67ms | 1543534.91 tokens/sec
Step 17696 | loss: 3.037141 | lr:6.7462e-05 | norm 0.2887 | dt 337.82ms | 1551963.23 tokens/sec
Step 17697 | loss: 3.030025 | lr:6.7451e-05 | norm 0.2817 | dt 338.95ms | 1546821.46 tokens/sec
Step 17698 | loss: 3.022761 | lr:6.7440e-05 | norm 0.2868 | dt 338.30ms | 1549781.18 tokens/sec
Step 17699 | loss: 3.038767 | lr:6.7429e-05 | norm 0.2683 | dt 337.45ms | 1553663.90 tokens/sec
Step 17700 | loss: 3.006398 | lr:6.7419e-05 | norm 0.3039 | dt 338.27ms | 1549910.07 tokens/sec
Step 17701 | loss: 3.061817 | lr:6.7408e-05 | norm 0.3101 | dt 338.72ms | 1547868.87 tokens/sec
Step 17702 | loss: 3.108737 | lr:6.7397e-05 | norm 0.3004 | dt 338.13ms | 1550550.48 tokens/sec
Step 17703 | loss: 3.086519 | lr:6.7386e-05 | norm 0.2920 | dt 338.36ms | 1549489.61 tokens/sec
Step 17704 | loss: 3.078858 | lr:6.7376e-05 | norm 0.2911 | dt 338.00ms | 1551126.87 tokens/sec
Step 17705 | loss: 3.141469 | lr:6.7365e-05 | norm 0.2923 | dt 339.13ms | 1545987.37 tokens/sec
Step 17706 | loss: 3.074209 | lr:6.7354e-05 | norm 0.3044 | dt 339.48ms | 1544398.90 tokens/sec
Step 17707 | loss: 3.095137 | lr:6.7344e-05 | norm 0.2904 | dt 338.21ms | 1550185.40 tokens/sec
Step 17708 | loss: 3.075102 | lr:6.7333e-05 | norm 0.3232 | dt 338.84ms | 1547309.06 tokens/sec
Step 17709 | loss: 3.056850 | lr:6.7322e-05 | norm 0.3035 | dt 338.09ms | 1550738.55 tokens/sec
Step 17710 | loss: 3.052810 | lr:6.7311e-05 | norm 0.2890 | dt 338.88ms | 1547133.79 tokens/sec
Step 17711 | loss: 3.031557 | lr:6.7301e-05 | norm 0.2919 | dt 339.47ms | 1544435.77 tokens/sec
Step 17712 | loss: 3.079751 | lr:6.7290e-05 | norm 0.2903 | dt 337.88ms | 1551691.64 tokens/sec
Step 17713 | loss: 3.094253 | lr:6.7279e-05 | norm 0.2969 | dt 338.68ms | 1548038.86 tokens/sec
Step 17714 | loss: 3.079980 | lr:6.7269e-05 | norm 0.2808 | dt 338.68ms | 1548036.68 tokens/sec
Step 17715 | loss: 3.038690 | lr:6.7258e-05 | norm 0.3008 | dt 338.49ms | 1548899.16 tokens/sec
Step 17716 | loss: 3.084492 | lr:6.7248e-05 | norm 0.2862 | dt 337.70ms | 1552529.70 tokens/sec
Step 17717 | loss: 3.095699 | lr:6.7237e-05 | norm 0.2834 | dt 337.49ms | 1553478.41 tokens/sec
Step 17718 | loss: 3.205770 | lr:6.7226e-05 | norm 0.3553 | dt 338.24ms | 1550035.71 tokens/sec
Step 17719 | loss: 3.035001 | lr:6.7216e-05 | norm 0.2966 | dt 337.61ms | 1552958.40 tokens/sec
Step 17720 | loss: 3.082136 | lr:6.7205e-05 | norm 0.2899 | dt 338.35ms | 1549530.01 tokens/sec
Step 17721 | loss: 3.077239 | lr:6.7194e-05 | norm 0.3099 | dt 339.43ms | 1544605.01 tokens/sec
Step 17722 | loss: 3.087322 | lr:6.7184e-05 | norm 0.2940 | dt 337.92ms | 1551531.80 tokens/sec
Step 17723 | loss: 2.976771 | lr:6.7173e-05 | norm 0.3019 | dt 338.62ms | 1548298.27 tokens/sec
Step 17724 | loss: 2.996637 | lr:6.7163e-05 | norm 0.2955 | dt 338.56ms | 1548565.40 tokens/sec
Step 17725 | loss: 3.022990 | lr:6.7152e-05 | norm 0.2916 | dt 343.58ms | 1525955.02 tokens/sec
Step 17726 | loss: 3.075868 | lr:6.7142e-05 | norm 0.3008 | dt 339.03ms | 1546443.99 tokens/sec
Step 17727 | loss: 2.993488 | lr:6.7131e-05 | norm 0.3256 | dt 339.09ms | 1546150.42 tokens/sec
Step 17728 | loss: 3.018795 | lr:6.7120e-05 | norm 0.2758 | dt 338.26ms | 1549977.80 tokens/sec
Step 17729 | loss: 3.004705 | lr:6.7110e-05 | norm 0.2743 | dt 338.78ms | 1547582.38 tokens/sec
Step 17730 | loss: 3.004795 | lr:6.7099e-05 | norm 0.2876 | dt 338.41ms | 1549284.37 tokens/sec
Step 17731 | loss: 2.995242 | lr:6.7089e-05 | norm 0.2670 | dt 338.73ms | 1547822.03 tokens/sec
Step 17732 | loss: 3.032934 | lr:6.7078e-05 | norm 0.2792 | dt 338.94ms | 1546847.57 tokens/sec
Step 17733 | loss: 3.020948 | lr:6.7068e-05 | norm 0.2834 | dt 338.38ms | 1549415.37 tokens/sec
Step 17734 | loss: 3.027667 | lr:6.7057e-05 | norm 0.2908 | dt 340.67ms | 1538984.94 tokens/sec
Step 17735 | loss: 3.092927 | lr:6.7047e-05 | norm 0.2860 | dt 338.48ms | 1548952.62 tokens/sec
Step 17736 | loss: 3.113944 | lr:6.7036e-05 | norm 0.3075 | dt 339.09ms | 1546176.51 tokens/sec
Step 17737 | loss: 3.085366 | lr:6.7026e-05 | norm 0.3022 | dt 339.34ms | 1545003.28 tokens/sec
Step 17738 | loss: 3.117767 | lr:6.7015e-05 | norm 0.2902 | dt 338.44ms | 1549145.76 tokens/sec
Step 17739 | loss: 3.055037 | lr:6.7005e-05 | norm 0.2698 | dt 338.01ms | 1551079.82 tokens/sec
Step 17740 | loss: 3.049093 | lr:6.6995e-05 | norm 0.2905 | dt 337.92ms | 1551504.43 tokens/sec
Step 17741 | loss: 3.041546 | lr:6.6984e-05 | norm 0.3005 | dt 338.07ms | 1550827.14 tokens/sec
Step 17742 | loss: 3.102200 | lr:6.6974e-05 | norm 0.2993 | dt 338.76ms | 1547660.80 tokens/sec
Step 17743 | loss: 3.089954 | lr:6.6963e-05 | norm 0.3179 | dt 338.28ms | 1549863.10 tokens/sec
Step 17744 | loss: 3.065017 | lr:6.6953e-05 | norm 0.2831 | dt 338.33ms | 1549657.77 tokens/sec
Step 17745 | loss: 3.109266 | lr:6.6942e-05 | norm 0.3355 | dt 339.02ms | 1546465.74 tokens/sec
Step 17746 | loss: 3.045359 | lr:6.6932e-05 | norm 0.3115 | dt 338.90ms | 1547040.19 tokens/sec
Step 17747 | loss: 3.025868 | lr:6.6922e-05 | norm 0.2972 | dt 338.25ms | 1549989.82 tokens/sec
Step 17748 | loss: 3.006045 | lr:6.6911e-05 | norm 0.3042 | dt 337.53ms | 1553305.03 tokens/sec
Step 17749 | loss: 3.089609 | lr:6.6901e-05 | norm 0.2997 | dt 338.02ms | 1551048.10 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 17750: 3.0860
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3053/10042=0.3040


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but that is not how I usually start to teach them to read. The first time I taught a class in Singapore,
rank 5 sample 1 >Hello, I'm a language model, my computer is just kind of dumb. It runs on my GPU, but there has never really been any software to do
rank 5 sample 2 >Hello, I'm a language model, so I just need to know the most basic of the things I use to help with what I'm going to do (
rank 5 sample 3 >Hello, I'm a language model, one who does very well communicating in a very professional style. I want to use Python as my learning language, but for




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I need to do that.
Dylan, when I say "meee" I'm using a different id


ddp_rank 3: ####### Printing generated samples ####### 

rank 7 sample 1 >Hello, I'm a language model, too, because I'm not just interested in the machine learning. I'm interested in how people learn in order to help
rank 7 sample 2 >Hello, I'm a language model, and I want to describe some different way we can handle it. Let's say you have a computer in a laptop that


ddp_rank 4: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not even trying to think about the grammar part.
Hello, I can read you text. But can
rank 7 sample 3 >Hello, I'm a language model, so it's a great skill for programmers. We're going to learn our language. And I would love to take this


rank 3 sample 1 >Hello, I'm a language model, and so what do I do here? I have a big problem. I have a problem with it, and one is
rank 4 sample 0 >Hello, I'm a language model, so I think I would want to do just one thing. I'm sure my program might say something like 'I want
rank 3 sample 2 >Hello, I'm a language model, and you have to really work with it. When I was writing this post, I was in primary school and then at
rank 4 sample 1 >Hello, I'm a language model, don't you know that? (If you like me, I'm still in 7th year of high school language teaching
rank 3 sample 3 >Hello, I'm a language model, and sometimes I want to tell me something about what a word means and why it's being used. With the help of


rank 4 sample 2 >Hello, I'm a language model, I wrote a guide that describes how to do a language model using the text editor Windows 8.x I'm in Windows

rank 4 sample 3 >Hello, I'm a language model, and it only requires me to know 3D models; let me explain this in a simple, way, so you understand



ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and in this case, we're able to communicate through one set of languages.
And the problem is, we're
rank 1 sample 1 >Hello, I'm a language model, a computer language developer, and a person who has learned Python. I'm just waiting on a long time for the next
rank 1 sample 2 >Hello, I'm a language model, but then it's not me. I'm a programming teacher - a person who has a very basic idea about computer science
rank 1 sample 3 >Hello, I'm a language model, and I'm interested to study it. At the moment I'm going to tell you these 3 questions:
- What





ddp_rank 2: ####### Printing generated samples ####### 


ddp_rank 6: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, and this one comes in handy for any programmer who wants to use it. I just added a bunch of examples to thisrank 6 sample 0 >Hello, I'm a language model, it has a lot of power, and it's also pretty good.
You can use a lot of the same rules

rank 6 sample 1 >Hello, I'm a language model, and I have a question for you. Would you mind if I ask.
It's not an exact translation, but
rank 2 sample 1 >Hello, I'm a language model, so I mean to me, I really really want to say something and try to say something that is true, true to
rank 6 sample 2 >Hello, I'm a language model, but if you want to be an IT developer I'll be happy to tell you how to do it. Let me tell
rank 2 sample 2 >Hello, I'm a language model, so I've got to be really aware of that.
Let's move on.
This example contains some type 1
rank 6 sample 3 >Hello, I'm a language model, and a linguistics researcher at Yale University, where I'm a member of one of two professors who, with James P


rank 2 sample 3 >Hello, I'm a language model, but my programming language doesn't work the way I did when i was a few years ago for "Programming" because




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I don't want to use languages from every language.
There are several ways in which a person can learn English
rank 0 sample 1 >Hello, I'm a language model, so we're going to teach them a little about us- a little bit, and I'm gonna try to say it
rank 0 sample 2 >Hello, I'm a language model, so I figured if I went to college, would I be in this field?
My guess is that I would prefer
rank 0 sample 3 >Hello, I'm a language model, so don't let me get you a bunch of tricks. I will tell you that in your lifetime, you'll understand


Step 17750 | loss: 3.074029 | lr:6.6890e-05 | norm 0.3086 | dt 12247.30ms | 42808.46 tokens/sec
Step 17751 | loss: 3.058883 | lr:6.6880e-05 | norm 0.3124 | dt 335.36ms | 1563381.89 tokens/sec
Step 17752 | loss: 3.115666 | lr:6.6870e-05 | norm 0.3093 | dt 338.24ms | 1550041.17 tokens/sec
Step 17753 | loss: 3.111141 | lr:6.6859e-05 | norm 0.2866 | dt 335.79ms | 1561376.02 tokens/sec
Step 17754 | loss: 3.117514 | lr:6.6849e-05 | norm 0.2987 | dt 336.91ms | 1556171.80 tokens/sec
Step 17755 | loss: 3.074680 | lr:6.6839e-05 | norm 0.2975 | dt 337.56ms | 1553164.61 tokens/sec
Step 17756 | loss: 3.057698 | lr:6.6828e-05 | norm 0.2771 | dt 335.85ms | 1561077.86 tokens/sec
Step 17757 | loss: 3.081711 | lr:6.6818e-05 | norm 0.2883 | dt 337.39ms | 1553940.57 tokens/sec
Step 17758 | loss: 3.022977 | lr:6.6808e-05 | norm 0.2830 | dt 337.35ms | 1554159.12 tokens/sec
Step 17759 | loss: 3.040266 | lr:6.6797e-05 | norm 0.3217 | dt 337.00ms | 1555750.13 tokens/sec
Step 17760 | loss: 2.989300 | lr:6.6787e-05 | norm 0.2896 | dt 336.36ms | 1558709.90 tokens/sec
Step 17761 | loss: 2.999045 | lr:6.6777e-05 | norm 0.2980 | dt 336.81ms | 1556648.78 tokens/sec
Step 17762 | loss: 2.988575 | lr:6.6767e-05 | norm 0.2972 | dt 337.35ms | 1554115.19 tokens/sec
Step 17763 | loss: 2.998544 | lr:6.6756e-05 | norm 0.2952 | dt 337.56ms | 1553151.44 tokens/sec
Step 17764 | loss: 2.960756 | lr:6.6746e-05 | norm 0.2891 | dt 338.21ms | 1550190.87 tokens/sec
Step 17765 | loss: 2.991365 | lr:6.6736e-05 | norm 0.2960 | dt 913.02ms | 574233.97 tokens/sec
Step 17766 | loss: 3.017142 | lr:6.6725e-05 | norm 0.2864 | dt 335.53ms | 1562558.71 tokens/sec
Step 17767 | loss: 3.066499 | lr:6.6715e-05 | norm 0.3060 | dt 336.88ms | 1556313.87 tokens/sec
Step 17768 | loss: 3.061228 | lr:6.6705e-05 | norm 0.2835 | dt 337.64ms | 1552805.97 tokens/sec
Step 17769 | loss: 3.030852 | lr:6.6695e-05 | norm 0.2993 | dt 336.69ms | 1557190.02 tokens/sec
Step 17770 | loss: 2.976498 | lr:6.6685e-05 | norm 0.3029 | dt 337.43ms | 1553746.23 tokens/sec
Step 17771 | loss: 3.071187 | lr:6.6674e-05 | norm 0.2939 | dt 337.87ms | 1551746.39 tokens/sec
Step 17772 | loss: 3.087814 | lr:6.6664e-05 | norm 0.2965 | dt 337.16ms | 1554995.45 tokens/sec
Step 17773 | loss: 3.020773 | lr:6.6654e-05 | norm 0.3270 | dt 337.79ms | 1552130.83 tokens/sec
Step 17774 | loss: 3.126866 | lr:6.6644e-05 | norm 0.3103 | dt 337.96ms | 1551339.16 tokens/sec
Step 17775 | loss: 3.093324 | lr:6.6634e-05 | norm 0.3131 | dt 337.97ms | 1551280.06 tokens/sec
Step 17776 | loss: 3.045518 | lr:6.6623e-05 | norm 0.2985 | dt 338.56ms | 1548602.47 tokens/sec
Step 17777 | loss: 3.059794 | lr:6.6613e-05 | norm 0.3082 | dt 338.05ms | 1550905.89 tokens/sec
Step 17778 | loss: 3.081839 | lr:6.6603e-05 | norm 0.3096 | dt 339.25ms | 1545452.81 tokens/sec
Step 17779 | loss: 3.074778 | lr:6.6593e-05 | norm 0.2898 | dt 337.30ms | 1554344.77 tokens/sec
Step 17780 | loss: 3.040253 | lr:6.6583e-05 | norm 0.3176 | dt 337.66ms | 1552690.85 tokens/sec
Step 17781 | loss: 3.073685 | lr:6.6573e-05 | norm 0.2892 | dt 338.07ms | 1550817.30 tokens/sec
Step 17782 | loss: 3.096464 | lr:6.6562e-05 | norm 0.3076 | dt 337.89ms | 1551635.80 tokens/sec
Step 17783 | loss: 3.105103 | lr:6.6552e-05 | norm 0.2986 | dt 338.87ms | 1547163.18 tokens/sec
Step 17784 | loss: 3.108496 | lr:6.6542e-05 | norm 0.2841 | dt 337.84ms | 1551890.94 tokens/sec
Step 17785 | loss: 3.054674 | lr:6.6532e-05 | norm 0.2848 | dt 337.70ms | 1552505.59 tokens/sec
Step 17786 | loss: 3.133611 | lr:6.6522e-05 | norm 0.2973 | dt 338.61ms | 1548354.96 tokens/sec
Step 17787 | loss: 3.069140 | lr:6.6512e-05 | norm 0.3023 | dt 338.05ms | 1550919.01 tokens/sec
Step 17788 | loss: 3.091236 | lr:6.6502e-05 | norm 0.2863 | dt 338.20ms | 1550237.86 tokens/sec
Step 17789 | loss: 3.042025 | lr:6.6492e-05 | norm 0.2693 | dt 338.24ms | 1550057.56 tokens/sec
Step 17790 | loss: 3.052224 | lr:6.6482e-05 | norm 0.2967 | dt 337.82ms | 1551956.66 tokens/sec
Step 17791 | loss: 3.143932 | lr:6.6472e-05 | norm 0.2962 | dt 337.60ms | 1552988.01 tokens/sec
Step 17792 | loss: 3.023374 | lr:6.6462e-05 | norm 0.2957 | dt 337.69ms | 1552556.01 tokens/sec
Step 17793 | loss: 3.081046 | lr:6.6452e-05 | norm 0.3082 | dt 337.97ms | 1551284.44 tokens/sec
Step 17794 | loss: 3.061813 | lr:6.6442e-05 | norm 0.3221 | dt 338.19ms | 1550272.83 tokens/sec
Step 17795 | loss: 2.979702 | lr:6.6432e-05 | norm 0.2896 | dt 337.47ms | 1553578.28 tokens/sec
Step 17796 | loss: 2.997575 | lr:6.6421e-05 | norm 0.2847 | dt 338.45ms | 1549071.56 tokens/sec
Step 17797 | loss: 3.014545 | lr:6.6411e-05 | norm 0.2824 | dt 338.87ms | 1547188.22 tokens/sec
Step 17798 | loss: 3.029272 | lr:6.6401e-05 | norm 0.2774 | dt 338.57ms | 1548520.69 tokens/sec
Step 17799 | loss: 2.983288 | lr:6.6391e-05 | norm 0.2715 | dt 338.79ms | 1547526.84 tokens/sec
Step 17800 | loss: 3.036138 | lr:6.6381e-05 | norm 0.2954 | dt 338.81ms | 1547424.47 tokens/sec
Step 17801 | loss: 2.996654 | lr:6.6371e-05 | norm 0.2787 | dt 338.43ms | 1549165.41 tokens/sec
Step 17802 | loss: 3.077722 | lr:6.6362e-05 | norm 0.2858 | dt 338.27ms | 1549906.79 tokens/sec
Step 17803 | loss: 3.000994 | lr:6.6352e-05 | norm 0.2814 | dt 339.16ms | 1545855.87 tokens/sec
Step 17804 | loss: 2.976321 | lr:6.6342e-05 | norm 0.2928 | dt 337.75ms | 1552301.75 tokens/sec
Step 17805 | loss: 3.001750 | lr:6.6332e-05 | norm 0.2773 | dt 337.54ms | 1553276.51 tokens/sec
Step 17806 | loss: 3.006033 | lr:6.6322e-05 | norm 0.2843 | dt 337.74ms | 1552347.77 tokens/sec
Step 17807 | loss: 3.051790 | lr:6.6312e-05 | norm 0.3215 | dt 338.82ms | 1547389.63 tokens/sec
Step 17808 | loss: 3.031487 | lr:6.6302e-05 | norm 0.2918 | dt 337.93ms | 1551448.61 tokens/sec
Step 17809 | loss: 3.083998 | lr:6.6292e-05 | norm 0.3011 | dt 338.07ms | 1550826.04 tokens/sec
Step 17810 | loss: 3.085587 | lr:6.6282e-05 | norm 0.2987 | dt 339.11ms | 1546080.85 tokens/sec
Step 17811 | loss: 3.120438 | lr:6.6272e-05 | norm 0.3077 | dt 337.91ms | 1551550.41 tokens/sec
Step 17812 | loss: 3.101963 | lr:6.6262e-05 | norm 0.3042 | dt 341.01ms | 1537452.73 tokens/sec
Step 17813 | loss: 3.031541 | lr:6.6252e-05 | norm 0.3008 | dt 338.09ms | 1550721.06 tokens/sec
Step 17814 | loss: 3.123399 | lr:6.6242e-05 | norm 0.3111 | dt 338.05ms | 1550921.20 tokens/sec
Step 17815 | loss: 3.059608 | lr:6.6233e-05 | norm 0.3047 | dt 338.64ms | 1548215.42 tokens/sec
Step 17816 | loss: 3.023841 | lr:6.6223e-05 | norm 0.2878 | dt 337.72ms | 1552449.69 tokens/sec
Step 17817 | loss: 3.101979 | lr:6.6213e-05 | norm 0.3118 | dt 338.80ms | 1547476.74 tokens/sec
Step 17818 | loss: 3.008959 | lr:6.6203e-05 | norm 0.3051 | dt 338.09ms | 1550748.40 tokens/sec
Step 17819 | loss: 3.129041 | lr:6.6193e-05 | norm 0.3019 | dt 338.48ms | 1548934.08 tokens/sec
Step 17820 | loss: 3.057631 | lr:6.6183e-05 | norm 0.2843 | dt 337.90ms | 1551609.53 tokens/sec
Step 17821 | loss: 3.056268 | lr:6.6173e-05 | norm 0.2925 | dt 337.89ms | 1551657.70 tokens/sec
Step 17822 | loss: 3.064159 | lr:6.6164e-05 | norm 0.2880 | dt 339.08ms | 1546222.17 tokens/sec
Step 17823 | loss: 3.052566 | lr:6.6154e-05 | norm 0.2903 | dt 337.13ms | 1555134.01 tokens/sec
Step 17824 | loss: 3.052678 | lr:6.6144e-05 | norm 0.2876 | dt 338.15ms | 1550451.00 tokens/sec
Step 17825 | loss: 3.089720 | lr:6.6134e-05 | norm 0.2790 | dt 338.27ms | 1549918.81 tokens/sec
Step 17826 | loss: 3.045902 | lr:6.6124e-05 | norm 0.2642 | dt 338.26ms | 1549933.01 tokens/sec
Step 17827 | loss: 2.989690 | lr:6.6115e-05 | norm 0.2886 | dt 337.02ms | 1555670.89 tokens/sec
Step 17828 | loss: 3.073533 | lr:6.6105e-05 | norm 0.3043 | dt 338.89ms | 1547058.69 tokens/sec
Step 17829 | loss: 2.996463 | lr:6.6095e-05 | norm 0.2718 | dt 338.50ms | 1548878.43 tokens/sec
Step 17830 | loss: 3.018429 | lr:6.6085e-05 | norm 0.2726 | dt 338.07ms | 1550835.89 tokens/sec
Step 17831 | loss: 3.019421 | lr:6.6076e-05 | norm 0.2816 | dt 337.80ms | 1552078.24 tokens/sec
Step 17832 | loss: 3.058250 | lr:6.6066e-05 | norm 0.3302 | dt 338.99ms | 1546627.81 tokens/sec
Step 17833 | loss: 2.958087 | lr:6.6056e-05 | norm 0.2874 | dt 338.34ms | 1549567.13 tokens/sec
Step 17834 | loss: 2.939904 | lr:6.6046e-05 | norm 0.2950 | dt 338.30ms | 1549754.96 tokens/sec
Step 17835 | loss: 2.985490 | lr:6.6037e-05 | norm 0.2963 | dt 339.29ms | 1545239.96 tokens/sec
Step 17836 | loss: 2.991064 | lr:6.6027e-05 | norm 0.2788 | dt 340.20ms | 1541128.03 tokens/sec
Step 17837 | loss: 3.001841 | lr:6.6017e-05 | norm 0.3113 | dt 339.33ms | 1545061.90 tokens/sec
Step 17838 | loss: 3.049227 | lr:6.6008e-05 | norm 0.2844 | dt 338.96ms | 1546760.53 tokens/sec
Step 17839 | loss: 2.956298 | lr:6.5998e-05 | norm 0.2966 | dt 338.45ms | 1549101.02 tokens/sec
Step 17840 | loss: 3.016525 | lr:6.5988e-05 | norm 0.2892 | dt 339.16ms | 1545841.74 tokens/sec
Step 17841 | loss: 3.070784 | lr:6.5979e-05 | norm 0.3183 | dt 338.50ms | 1548857.71 tokens/sec
Step 17842 | loss: 3.100354 | lr:6.5969e-05 | norm 0.3264 | dt 339.14ms | 1545950.42 tokens/sec
Step 17843 | loss: 3.078614 | lr:6.5959e-05 | norm 0.2763 | dt 339.08ms | 1546216.73 tokens/sec
Step 17844 | loss: 3.048799 | lr:6.5950e-05 | norm 0.3172 | dt 338.40ms | 1549308.39 tokens/sec
Step 17845 | loss: 2.999632 | lr:6.5940e-05 | norm 0.3579 | dt 338.70ms | 1547964.76 tokens/sec
Step 17846 | loss: 3.162525 | lr:6.5930e-05 | norm 0.3456 | dt 338.35ms | 1549533.28 tokens/sec
Step 17847 | loss: 3.048185 | lr:6.5921e-05 | norm 0.3151 | dt 338.49ms | 1548887.16 tokens/sec
Step 17848 | loss: 3.077218 | lr:6.5911e-05 | norm 0.3371 | dt 338.14ms | 1550494.73 tokens/sec
Step 17849 | loss: 3.034546 | lr:6.5901e-05 | norm 0.2987 | dt 338.22ms | 1550144.97 tokens/sec
Step 17850 | loss: 3.115504 | lr:6.5892e-05 | norm 0.2904 | dt 338.31ms | 1549735.30 tokens/sec
Step 17851 | loss: 3.050280 | lr:6.5882e-05 | norm 0.3199 | dt 338.76ms | 1547682.59 tokens/sec
Step 17852 | loss: 3.188612 | lr:6.5873e-05 | norm 0.2842 | dt 338.36ms | 1549479.78 tokens/sec
Step 17853 | loss: 3.056716 | lr:6.5863e-05 | norm 0.2887 | dt 338.01ms | 1551106.08 tokens/sec
Step 17854 | loss: 3.058257 | lr:6.5853e-05 | norm 0.2870 | dt 338.47ms | 1548976.63 tokens/sec
Step 17855 | loss: 3.069575 | lr:6.5844e-05 | norm 0.2772 | dt 337.92ms | 1551511.00 tokens/sec
Step 17856 | loss: 3.037426 | lr:6.5834e-05 | norm 0.2909 | dt 338.09ms | 1550739.65 tokens/sec
Step 17857 | loss: 3.081511 | lr:6.5825e-05 | norm 0.3094 | dt 338.29ms | 1549833.60 tokens/sec
Step 17858 | loss: 3.084702 | lr:6.5815e-05 | norm 0.3019 | dt 337.86ms | 1551789.09 tokens/sec
Step 17859 | loss: 3.042582 | lr:6.5806e-05 | norm 0.3193 | dt 1002.59ms | 522932.64 tokens/sec
Step 17860 | loss: 3.146163 | lr:6.5796e-05 | norm 0.3264 | dt 335.50ms | 1562714.17 tokens/sec
Step 17861 | loss: 3.059609 | lr:6.5787e-05 | norm 0.3018 | dt 337.76ms | 1552272.16 tokens/sec
Step 17862 | loss: 3.054840 | lr:6.5777e-05 | norm 0.2942 | dt 339.02ms | 1546500.55 tokens/sec
Step 17863 | loss: 3.077653 | lr:6.5768e-05 | norm 0.2987 | dt 337.40ms | 1553895.55 tokens/sec
Step 17864 | loss: 3.096202 | lr:6.5758e-05 | norm 0.2776 | dt 338.07ms | 1550826.04 tokens/sec
Step 17865 | loss: 3.041047 | lr:6.5749e-05 | norm 0.3120 | dt 337.32ms | 1554284.35 tokens/sec
Step 17866 | loss: 2.991685 | lr:6.5739e-05 | norm 0.2677 | dt 337.17ms | 1554972.36 tokens/sec
Step 17867 | loss: 3.028776 | lr:6.5730e-05 | norm 0.2722 | dt 337.86ms | 1551805.52 tokens/sec
Step 17868 | loss: 3.009355 | lr:6.5720e-05 | norm 0.2964 | dt 340.09ms | 1541612.05 tokens/sec
Step 17869 | loss: 3.021775 | lr:6.5711e-05 | norm 0.2885 | dt 337.75ms | 1552301.75 tokens/sec
Step 17870 | loss: 3.044997 | lr:6.5701e-05 | norm 0.2827 | dt 337.57ms | 1553133.89 tokens/sec
Step 17871 | loss: 2.994602 | lr:6.5692e-05 | norm 0.2960 | dt 337.75ms | 1552285.31 tokens/sec
Step 17872 | loss: 2.968361 | lr:6.5683e-05 | norm 0.2880 | dt 338.02ms | 1551064.51 tokens/sec
Step 17873 | loss: 2.947254 | lr:6.5673e-05 | norm 0.2857 | dt 338.21ms | 1550167.92 tokens/sec
Step 17874 | loss: 2.971803 | lr:6.5664e-05 | norm 0.2784 | dt 337.59ms | 1553009.94 tokens/sec
Step 17875 | loss: 2.987696 | lr:6.5654e-05 | norm 0.2849 | dt 338.24ms | 1550055.37 tokens/sec
Step 17876 | loss: 3.074202 | lr:6.5645e-05 | norm 0.3028 | dt 338.39ms | 1549367.33 tokens/sec
Step 17877 | loss: 3.084094 | lr:6.5635e-05 | norm 0.3059 | dt 337.81ms | 1552009.23 tokens/sec
Step 17878 | loss: 3.143313 | lr:6.5626e-05 | norm 0.2953 | dt 338.82ms | 1547407.05 tokens/sec
Step 17879 | loss: 3.138084 | lr:6.5617e-05 | norm 0.2872 | dt 339.00ms | 1546589.74 tokens/sec
Step 17880 | loss: 3.118107 | lr:6.5607e-05 | norm 0.2996 | dt 338.82ms | 1547379.83 tokens/sec
Step 17881 | loss: 3.127155 | lr:6.5598e-05 | norm 0.2887 | dt 338.67ms | 1548058.47 tokens/sec
Step 17882 | loss: 3.076378 | lr:6.5589e-05 | norm 0.3098 | dt 338.25ms | 1549989.82 tokens/sec
Step 17883 | loss: 3.082468 | lr:6.5579e-05 | norm 0.2812 | dt 338.65ms | 1548177.27 tokens/sec
Step 17884 | loss: 3.030563 | lr:6.5570e-05 | norm 0.2812 | dt 338.30ms | 1549761.52 tokens/sec
Step 17885 | loss: 3.096246 | lr:6.5561e-05 | norm 0.3061 | dt 338.33ms | 1549633.74 tokens/sec
Step 17886 | loss: 3.068753 | lr:6.5551e-05 | norm 0.2897 | dt 337.98ms | 1551258.17 tokens/sec
Step 17887 | loss: 3.059132 | lr:6.5542e-05 | norm 0.2931 | dt 339.05ms | 1546322.20 tokens/sec
Step 17888 | loss: 3.045969 | lr:6.5533e-05 | norm 0.2942 | dt 338.38ms | 1549420.83 tokens/sec
Step 17889 | loss: 3.068556 | lr:6.5523e-05 | norm 0.3014 | dt 338.55ms | 1548606.84 tokens/sec
Step 17890 | loss: 3.111423 | lr:6.5514e-05 | norm 0.2980 | dt 337.38ms | 1554014.14 tokens/sec
Step 17891 | loss: 3.064597 | lr:6.5505e-05 | norm 0.2967 | dt 338.62ms | 1548298.27 tokens/sec
Step 17892 | loss: 3.089297 | lr:6.5495e-05 | norm 0.3157 | dt 338.62ms | 1548308.08 tokens/sec
Step 17893 | loss: 3.048731 | lr:6.5486e-05 | norm 0.3050 | dt 337.49ms | 1553481.70 tokens/sec
Step 17894 | loss: 3.080671 | lr:6.5477e-05 | norm 0.3064 | dt 338.49ms | 1548901.35 tokens/sec
Step 17895 | loss: 3.055288 | lr:6.5468e-05 | norm 0.3215 | dt 338.53ms | 1548723.54 tokens/sec
Step 17896 | loss: 3.018542 | lr:6.5458e-05 | norm 0.2881 | dt 337.84ms | 1551883.27 tokens/sec
Step 17897 | loss: 3.048666 | lr:6.5449e-05 | norm 0.3251 | dt 339.24ms | 1545470.19 tokens/sec
Step 17898 | loss: 3.005300 | lr:6.5440e-05 | norm 0.3057 | dt 337.05ms | 1555523.43 tokens/sec
Step 17899 | loss: 3.032779 | lr:6.5431e-05 | norm 0.2984 | dt 337.61ms | 1552946.33 tokens/sec
Step 17900 | loss: 3.021157 | lr:6.5422e-05 | norm 0.2914 | dt 338.37ms | 1549433.93 tokens/sec
Step 17901 | loss: 3.079242 | lr:6.5412e-05 | norm 0.2933 | dt 338.14ms | 1550519.87 tokens/sec
Step 17902 | loss: 3.022687 | lr:6.5403e-05 | norm 0.3124 | dt 338.67ms | 1548092.26 tokens/sec
Step 17903 | loss: 3.013114 | lr:6.5394e-05 | norm 0.2927 | dt 337.49ms | 1553490.48 tokens/sec
Step 17904 | loss: 2.990461 | lr:6.5385e-05 | norm 0.2814 | dt 338.07ms | 1550849.01 tokens/sec
Step 17905 | loss: 2.984506 | lr:6.5376e-05 | norm 0.2971 | dt 338.42ms | 1549217.79 tokens/sec
Step 17906 | loss: 2.975616 | lr:6.5366e-05 | norm 0.3024 | dt 338.15ms | 1550458.65 tokens/sec
Step 17907 | loss: 2.983817 | lr:6.5357e-05 | norm 0.2961 | dt 338.16ms | 1550421.48 tokens/sec
Step 17908 | loss: 3.022960 | lr:6.5348e-05 | norm 0.2735 | dt 337.49ms | 1553504.75 tokens/sec
Step 17909 | loss: 3.024186 | lr:6.5339e-05 | norm 0.2680 | dt 338.12ms | 1550606.24 tokens/sec
Step 17910 | loss: 3.008292 | lr:6.5330e-05 | norm 0.3045 | dt 338.14ms | 1550489.26 tokens/sec
Step 17911 | loss: 3.081538 | lr:6.5321e-05 | norm 0.3153 | dt 337.87ms | 1551727.77 tokens/sec
Step 17912 | loss: 3.059529 | lr:6.5312e-05 | norm 0.2932 | dt 337.00ms | 1555739.13 tokens/sec
Step 17913 | loss: 3.114850 | lr:6.5302e-05 | norm 0.3254 | dt 338.04ms | 1550979.18 tokens/sec
Step 17914 | loss: 3.093728 | lr:6.5293e-05 | norm 0.3196 | dt 338.98ms | 1546672.41 tokens/sec
Step 17915 | loss: 3.109356 | lr:6.5284e-05 | norm 0.3094 | dt 337.74ms | 1552321.47 tokens/sec
Step 17916 | loss: 3.156892 | lr:6.5275e-05 | norm 0.3160 | dt 337.80ms | 1552056.33 tokens/sec
Step 17917 | loss: 3.053556 | lr:6.5266e-05 | norm 0.3176 | dt 338.52ms | 1548746.44 tokens/sec
Step 17918 | loss: 3.061140 | lr:6.5257e-05 | norm 0.2945 | dt 338.12ms | 1550592.03 tokens/sec
Step 17919 | loss: 3.126694 | lr:6.5248e-05 | norm 0.3039 | dt 338.56ms | 1548569.76 tokens/sec
Step 17920 | loss: 3.111789 | lr:6.5239e-05 | norm 0.3323 | dt 337.38ms | 1554011.95 tokens/sec
Step 17921 | loss: 3.062363 | lr:6.5230e-05 | norm 0.2953 | dt 337.57ms | 1553141.57 tokens/sec
Step 17922 | loss: 3.137173 | lr:6.5221e-05 | norm 0.3476 | dt 339.07ms | 1546265.66 tokens/sec
Step 17923 | loss: 3.026223 | lr:6.5212e-05 | norm 0.3036 | dt 338.42ms | 1549201.42 tokens/sec
Step 17924 | loss: 3.096124 | lr:6.5203e-05 | norm 0.3367 | dt 337.28ms | 1554437.07 tokens/sec
Step 17925 | loss: 3.049215 | lr:6.5194e-05 | norm 0.3033 | dt 337.46ms | 1553630.97 tokens/sec
Step 17926 | loss: 3.081873 | lr:6.5185e-05 | norm 0.2870 | dt 339.63ms | 1543687.69 tokens/sec
Step 17927 | loss: 3.064525 | lr:6.5176e-05 | norm 0.3020 | dt 338.47ms | 1549015.91 tokens/sec
Step 17928 | loss: 3.043237 | lr:6.5167e-05 | norm 0.3134 | dt 338.84ms | 1547295.99 tokens/sec
Step 17929 | loss: 3.054362 | lr:6.5158e-05 | norm 0.3252 | dt 338.49ms | 1548913.35 tokens/sec
Step 17930 | loss: 3.071758 | lr:6.5149e-05 | norm 0.3043 | dt 337.86ms | 1551786.90 tokens/sec
Step 17931 | loss: 3.083791 | lr:6.5140e-05 | norm 0.3380 | dt 339.17ms | 1545792.84 tokens/sec
Step 17932 | loss: 3.079316 | lr:6.5131e-05 | norm 0.2857 | dt 337.90ms | 1551611.72 tokens/sec
Step 17933 | loss: 3.056712 | lr:6.5122e-05 | norm 0.2848 | dt 337.45ms | 1553677.07 tokens/sec
Step 17934 | loss: 2.975184 | lr:6.5113e-05 | norm 0.3160 | dt 338.23ms | 1550071.76 tokens/sec
Step 17935 | loss: 3.000801 | lr:6.5104e-05 | norm 0.2835 | dt 337.53ms | 1553325.88 tokens/sec
Step 17936 | loss: 2.985342 | lr:6.5095e-05 | norm 0.2783 | dt 337.92ms | 1551528.51 tokens/sec
Step 17937 | loss: 3.007309 | lr:6.5086e-05 | norm 0.2980 | dt 338.30ms | 1549752.78 tokens/sec
Step 17938 | loss: 2.979038 | lr:6.5077e-05 | norm 0.2783 | dt 337.52ms | 1553364.28 tokens/sec
Step 17939 | loss: 2.983325 | lr:6.5068e-05 | norm 0.2974 | dt 337.89ms | 1551653.32 tokens/sec
Step 17940 | loss: 3.011289 | lr:6.5059e-05 | norm 0.2771 | dt 338.12ms | 1550609.52 tokens/sec
Step 17941 | loss: 3.036539 | lr:6.5050e-05 | norm 0.3072 | dt 338.32ms | 1549673.05 tokens/sec
Step 17942 | loss: 3.013679 | lr:6.5041e-05 | norm 0.2993 | dt 337.60ms | 1552981.43 tokens/sec
Step 17943 | loss: 3.040020 | lr:6.5033e-05 | norm 0.2842 | dt 337.39ms | 1553948.26 tokens/sec
Step 17944 | loss: 3.101600 | lr:6.5024e-05 | norm 0.3613 | dt 337.76ms | 1552267.78 tokens/sec
Step 17945 | loss: 2.965185 | lr:6.5015e-05 | norm 0.3133 | dt 337.69ms | 1552588.90 tokens/sec
Step 17946 | loss: 3.160733 | lr:6.5006e-05 | norm 0.3597 | dt 338.32ms | 1549688.34 tokens/sec
Step 17947 | loss: 3.077126 | lr:6.4997e-05 | norm 0.2829 | dt 337.68ms | 1552618.49 tokens/sec
Step 17948 | loss: 3.095453 | lr:6.4988e-05 | norm 0.3078 | dt 338.55ms | 1548625.38 tokens/sec
Step 17949 | loss: 3.136182 | lr:6.4979e-05 | norm 0.3193 | dt 337.44ms | 1553708.91 tokens/sec
Step 17950 | loss: 3.134416 | lr:6.4971e-05 | norm 0.3715 | dt 337.56ms | 1553182.16 tokens/sec
Step 17951 | loss: 3.108887 | lr:6.4962e-05 | norm 0.2903 | dt 338.27ms | 1549928.64 tokens/sec
Step 17952 | loss: 3.094288 | lr:6.4953e-05 | norm 0.2976 | dt 337.73ms | 1552401.47 tokens/sec
Step 17953 | loss: 3.048982 | lr:6.4944e-05 | norm 0.3031 | dt 337.73ms | 1552383.94 tokens/sec
Step 17954 | loss: 3.098140 | lr:6.4935e-05 | norm 0.2813 | dt 899.19ms | 583067.94 tokens/sec
Step 17955 | loss: 3.107814 | lr:6.4927e-05 | norm 0.2940 | dt 336.31ms | 1558964.06 tokens/sec
Step 17956 | loss: 3.117196 | lr:6.4918e-05 | norm 0.2841 | dt 337.41ms | 1553872.49 tokens/sec
Step 17957 | loss: 3.049084 | lr:6.4909e-05 | norm 0.2866 | dt 339.60ms | 1543850.26 tokens/sec
Step 17958 | loss: 3.159925 | lr:6.4900e-05 | norm 0.4854 | dt 337.18ms | 1554898.70 tokens/sec
Step 17959 | loss: 3.057917 | lr:6.4891e-05 | norm 0.2914 | dt 337.76ms | 1552255.73 tokens/sec
Step 17960 | loss: 3.047851 | lr:6.4883e-05 | norm 0.3579 | dt 337.67ms | 1552654.67 tokens/sec
Step 17961 | loss: 3.062432 | lr:6.4874e-05 | norm 0.3175 | dt 337.53ms | 1553285.28 tokens/sec
Step 17962 | loss: 3.054852 | lr:6.4865e-05 | norm 0.3153 | dt 337.61ms | 1552923.30 tokens/sec
Step 17963 | loss: 3.024720 | lr:6.4856e-05 | norm 0.3201 | dt 337.89ms | 1551650.03 tokens/sec
Step 17964 | loss: 3.059653 | lr:6.4848e-05 | norm 0.3030 | dt 338.09ms | 1550713.40 tokens/sec
Step 17965 | loss: 3.073113 | lr:6.4839e-05 | norm 0.3073 | dt 338.06ms | 1550887.29 tokens/sec
Step 17966 | loss: 3.069840 | lr:6.4830e-05 | norm 0.3072 | dt 337.87ms | 1551747.48 tokens/sec
Step 17967 | loss: 3.013459 | lr:6.4822e-05 | norm 0.3014 | dt 337.81ms | 1552024.57 tokens/sec
Step 17968 | loss: 3.059774 | lr:6.4813e-05 | norm 0.3657 | dt 338.53ms | 1548735.53 tokens/sec
Step 17969 | loss: 3.027459 | lr:6.4804e-05 | norm 0.3011 | dt 338.01ms | 1551106.08 tokens/sec
Step 17970 | loss: 3.032380 | lr:6.4796e-05 | norm 0.3263 | dt 338.45ms | 1549091.20 tokens/sec
Step 17971 | loss: 3.054290 | lr:6.4787e-05 | norm 0.2895 | dt 337.82ms | 1551967.61 tokens/sec
Step 17972 | loss: 3.018003 | lr:6.4778e-05 | norm 0.3067 | dt 338.15ms | 1550447.72 tokens/sec
Step 17973 | loss: 3.052188 | lr:6.4770e-05 | norm 0.3047 | dt 337.88ms | 1551698.21 tokens/sec
Step 17974 | loss: 3.024454 | lr:6.4761e-05 | norm 0.2854 | dt 337.70ms | 1552536.28 tokens/sec
Step 17975 | loss: 2.969732 | lr:6.4752e-05 | norm 0.3228 | dt 338.02ms | 1551078.73 tokens/sec
Step 17976 | loss: 3.057120 | lr:6.4744e-05 | norm 0.2835 | dt 338.27ms | 1549899.15 tokens/sec
Step 17977 | loss: 2.985741 | lr:6.4735e-05 | norm 0.2809 | dt 338.16ms | 1550417.11 tokens/sec
Step 17978 | loss: 3.008703 | lr:6.4727e-05 | norm 0.2857 | dt 337.56ms | 1553170.09 tokens/sec
Step 17979 | loss: 2.997246 | lr:6.4718e-05 | norm 0.2809 | dt 338.15ms | 1550446.63 tokens/sec
Step 17980 | loss: 2.970190 | lr:6.4709e-05 | norm 0.2731 | dt 338.03ms | 1551022.93 tokens/sec
Step 17981 | loss: 3.030693 | lr:6.4701e-05 | norm 0.2892 | dt 338.51ms | 1548817.34 tokens/sec
Step 17982 | loss: 3.043523 | lr:6.4692e-05 | norm 0.2992 | dt 338.18ms | 1550341.69 tokens/sec
Step 17983 | loss: 3.062140 | lr:6.4684e-05 | norm 0.2823 | dt 338.44ms | 1549113.02 tokens/sec
Step 17984 | loss: 3.069451 | lr:6.4675e-05 | norm 0.2938 | dt 337.85ms | 1551826.33 tokens/sec
Step 17985 | loss: 3.109563 | lr:6.4666e-05 | norm 0.3285 | dt 339.60ms | 1543843.76 tokens/sec
Step 17986 | loss: 3.101028 | lr:6.4658e-05 | norm 0.2840 | dt 338.06ms | 1550869.79 tokens/sec
Step 17987 | loss: 3.090201 | lr:6.4649e-05 | norm 0.2772 | dt 338.20ms | 1550243.32 tokens/sec
Step 17988 | loss: 3.100866 | lr:6.4641e-05 | norm 0.3120 | dt 338.75ms | 1547714.18 tokens/sec
Step 17989 | loss: 3.180510 | lr:6.4632e-05 | norm 0.3491 | dt 338.37ms | 1549465.59 tokens/sec
Step 17990 | loss: 3.047844 | lr:6.4624e-05 | norm 0.3113 | dt 337.91ms | 1551569.02 tokens/sec
Step 17991 | loss: 3.067806 | lr:6.4615e-05 | norm 0.3079 | dt 338.73ms | 1547816.58 tokens/sec
Step 17992 | loss: 3.079090 | lr:6.4607e-05 | norm 0.3038 | dt 337.73ms | 1552405.85 tokens/sec
Step 17993 | loss: 3.055853 | lr:6.4598e-05 | norm 0.3127 | dt 338.15ms | 1550452.09 tokens/sec
Step 17994 | loss: 3.056093 | lr:6.4590e-05 | norm 0.2814 | dt 337.71ms | 1552480.38 tokens/sec
Step 17995 | loss: 3.046035 | lr:6.4581e-05 | norm 0.3107 | dt 337.57ms | 1553109.76 tokens/sec
Step 17996 | loss: 3.043133 | lr:6.4573e-05 | norm 0.2987 | dt 339.26ms | 1545384.39 tokens/sec
Step 17997 | loss: 3.069740 | lr:6.4564e-05 | norm 0.3056 | dt 337.86ms | 1551780.33 tokens/sec
Step 17998 | loss: 3.113599 | lr:6.4556e-05 | norm 0.3020 | dt 337.15ms | 1555039.44 tokens/sec
Step 17999 | loss: 3.056402 | lr:6.4547e-05 | norm 0.2872 | dt 338.23ms | 1550095.80 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 18000: 3.0853
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3031/10042=0.3018


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm not trying to teach all the basics, but try to present vocabulary words for some grammar related vocabulary words using
rank 3 sample 1 >Hello, I'm a language model, so what are you doing? Do you want to see how you're doing?
I'm a language model teacher at
rank 3 sample 2 >Hello, I'm a language model, so this will be on my wall. How do I go out and do my best?
As you can remember,
rank 3 sample 3 >Hello, I'm a language model, so here's an example:
import c# # example # python c# g code to print 3 print("Hello




ddp_rank 2: ####### Printing generated samples ####### 



ddp_rank 7: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so my goal now is to teach myself to be a fluent speaker.
So for example, I'm just trying to


rank 2 sample 1 >Hello, I'm a language model, and I never wanted to learn a completely new language, but now I feel like I'm getting better! I'm tryingddp_rank 5: ####### Printing generated samples ####### 


rank 7 sample 0 >Hello, I'm a language model, so I'd like to take a look again at an example of a new language example.
Step #2: In
rank 2 sample 2 >Hello, I'm a language model, and I've written an article that covers more than just the basics of Japanese grammar and sentence structures that most people understand when
rank 7 sample 1 >Hello, I'm a language model, by trying to understand it, we'd say, look about a computer.
First of all, how to make that
rank 2 sample 3 >Hello, I'm a language model, but my family is mostly from the West Midlands. That's one of the reasons I decided to become an intern. Whenrank 5 sample 0 >Hello, I'm a language model, but the best way to learn is through online learning. I always ask the question "How do you learn?" This is



rank 7 sample 2 >Hello, I'm a language model, so I need to program this using one program. To do that, I need to create a program that takes one of
rank 7 sample 3 >Hello, I'm a language model, and you'll be using them if you make my site. I'll let you know about it so I can send allrank 5 sample 1 >Hello, I'm a language model, learning new languages and creating web-based learning models. So what this post has meant for you is: this post is



rank 5 sample 2 >Hello, I'm a language model, and I was thinking we could add these to our list of languages with ease, so there's a new language for each
rank 5 sample 3 >Hello, I'm a language model, language modeling enthusiast!
As I mentioned earlier, a programming language can be designed. The most challenging thing is that programmers




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so when you're done, you find a problem, look for a solution, and solve it right away.
I


ddp_rank 4: ####### Printing generated samples ####### 

rank 1 sample 1 >Hello, I'm a language model, I want to go out and make friends with people and other people. I think that one of the most important things that
rank 1 sample 2 >Hello, I'm a language model, but first you want to know how to write a program in such a language.
So I wrote an application to send
rank 4 sample 0 >Hello, I'm a language model, and I wanted to take that to a world where language is just a set of rules, without considering it in any particular
rank 1 sample 3 >Hello, I'm a language model, so I'm looking at ways to build dictionaries. These dictionaries might cover a rather technical part of the dictionary,


rank 4 sample 1 >Hello, I'm a language model, someone with a new programming language! I was recently selected to be a computer programmer as a graduate student of computer science,
rank 4 sample 2 >Hello, I'm a language model, a non-articulation language (not a language theory!) I'm not crazy! I can find a solution for
rank 4 sample 3 >Hello, I'm a language model, so it pays good for my job that if a developer should go there should go there. I'd love to find some




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, that you can go with an array. I'm really interested, for me the search is so much more than just trying
rank 6 sample 1 >Hello, I'm a language model, so I have to know the difference between using a text editor and a document editor, but the one that most people would
rank 6 sample 2 >Hello, I'm a language model, but if you want to write code like this I have a tutorial with the following code.
1. In Linux you
rank 6 sample 3 >Hello, I'm a language model, so here's a lesson that shows how to write a program that knows how to read text message. After learning how to




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I wanted to do a project like this - I could make anything here with a simple grid layout and have a bunch
rank 0 sample 1 >Hello, I'm a language model, and what do I mean by "proficient". Does I have enough trouble learning it all? I really can't,
rank 0 sample 2 >Hello, I'm a language model, and I had no idea. I wanted to go to the movies to do a movie. I wanted to do the things
rank 0 sample 3 >Hello, I'm a language model, and while I don't use it for writing myself, I do a lot of that on my computer. I can say


Step 18000 | loss: 3.044586 | lr:6.4539e-05 | norm 0.3023 | dt 12367.85ms | 42391.19 tokens/sec
Step 18001 | loss: 3.034889 | lr:6.4531e-05 | norm 0.2841 | dt 335.23ms | 1563972.30 tokens/sec
Step 18002 | loss: 3.051516 | lr:6.4522e-05 | norm 0.2928 | dt 336.28ms | 1559071.27 tokens/sec
Step 18003 | loss: 3.095151 | lr:6.4514e-05 | norm 0.3140 | dt 337.62ms | 1552879.44 tokens/sec
Step 18004 | loss: 3.033144 | lr:6.4505e-05 | norm 0.3011 | dt 336.21ms | 1559406.27 tokens/sec
Step 18005 | loss: 3.000986 | lr:6.4497e-05 | norm 0.3259 | dt 336.45ms | 1558306.74 tokens/sec
Step 18006 | loss: 2.965980 | lr:6.4489e-05 | norm 0.2693 | dt 336.32ms | 1558914.33 tokens/sec
Step 18007 | loss: 2.998796 | lr:6.4480e-05 | norm 0.3107 | dt 336.17ms | 1559589.86 tokens/sec
Step 18008 | loss: 2.994617 | lr:6.4472e-05 | norm 0.3078 | dt 337.58ms | 1553087.82 tokens/sec
Step 18009 | loss: 2.992860 | lr:6.4463e-05 | norm 0.2725 | dt 337.50ms | 1553431.22 tokens/sec
Step 18010 | loss: 3.075294 | lr:6.4455e-05 | norm 0.2826 | dt 336.55ms | 1557823.22 tokens/sec
Step 18011 | loss: 3.036480 | lr:6.4447e-05 | norm 0.2963 | dt 336.01ms | 1560323.54 tokens/sec
Step 18012 | loss: 3.028101 | lr:6.4438e-05 | norm 0.2929 | dt 336.85ms | 1556428.43 tokens/sec
Step 18013 | loss: 2.980496 | lr:6.4430e-05 | norm 0.2774 | dt 336.52ms | 1557961.18 tokens/sec
Step 18014 | loss: 3.040528 | lr:6.4422e-05 | norm 0.3088 | dt 336.41ms | 1558485.65 tokens/sec
Step 18015 | loss: 2.984645 | lr:6.4413e-05 | norm 0.2775 | dt 337.75ms | 1552289.70 tokens/sec
Step 18016 | loss: 2.988086 | lr:6.4405e-05 | norm 0.3357 | dt 337.48ms | 1553544.26 tokens/sec
Step 18017 | loss: 3.078752 | lr:6.4397e-05 | norm 0.2821 | dt 336.89ms | 1556266.51 tokens/sec
Step 18018 | loss: 3.108221 | lr:6.4388e-05 | norm 0.3193 | dt 337.37ms | 1554059.17 tokens/sec
Step 18019 | loss: 3.088048 | lr:6.4380e-05 | norm 0.3564 | dt 338.20ms | 1550234.58 tokens/sec
Step 18020 | loss: 3.057460 | lr:6.4372e-05 | norm 0.3268 | dt 337.45ms | 1553658.41 tokens/sec
Step 18021 | loss: 3.025862 | lr:6.4364e-05 | norm 0.3648 | dt 338.83ms | 1547366.76 tokens/sec
Step 18022 | loss: 3.039548 | lr:6.4355e-05 | norm 0.2951 | dt 338.34ms | 1549574.77 tokens/sec
Step 18023 | loss: 3.066525 | lr:6.4347e-05 | norm 0.3365 | dt 338.17ms | 1550382.13 tokens/sec
Step 18024 | loss: 3.038560 | lr:6.4339e-05 | norm 0.3323 | dt 338.76ms | 1547684.77 tokens/sec
Step 18025 | loss: 2.994346 | lr:6.4331e-05 | norm 0.3083 | dt 338.69ms | 1547973.48 tokens/sec
Step 18026 | loss: 3.081768 | lr:6.4322e-05 | norm 0.3299 | dt 339.23ms | 1545509.29 tokens/sec
Step 18027 | loss: 3.012556 | lr:6.4314e-05 | norm 0.2939 | dt 339.50ms | 1544277.42 tokens/sec
Step 18028 | loss: 3.076742 | lr:6.4306e-05 | norm 0.2968 | dt 340.13ms | 1541449.95 tokens/sec
Step 18029 | loss: 3.039428 | lr:6.4298e-05 | norm 0.3044 | dt 339.10ms | 1546101.50 tokens/sec
Step 18030 | loss: 3.043174 | lr:6.4289e-05 | norm 0.2951 | dt 338.95ms | 1546799.70 tokens/sec
Step 18031 | loss: 3.043683 | lr:6.4281e-05 | norm 0.3078 | dt 338.55ms | 1548615.56 tokens/sec
Step 18032 | loss: 3.016454 | lr:6.4273e-05 | norm 0.3111 | dt 339.26ms | 1545401.76 tokens/sec
Step 18033 | loss: 3.063129 | lr:6.4265e-05 | norm 0.2864 | dt 339.47ms | 1544437.94 tokens/sec
Step 18034 | loss: 3.055162 | lr:6.4257e-05 | norm 0.3253 | dt 339.70ms | 1543400.58 tokens/sec
Step 18035 | loss: 3.043004 | lr:6.4248e-05 | norm 0.3000 | dt 338.72ms | 1547834.01 tokens/sec
Step 18036 | loss: 3.081414 | lr:6.4240e-05 | norm 0.2871 | dt 339.06ms | 1546276.53 tokens/sec
Step 18037 | loss: 3.093875 | lr:6.4232e-05 | norm 0.3154 | dt 339.07ms | 1546273.27 tokens/sec
Step 18038 | loss: 3.062452 | lr:6.4224e-05 | norm 0.3177 | dt 338.74ms | 1547759.93 tokens/sec
Step 18039 | loss: 3.072067 | lr:6.4216e-05 | norm 0.3144 | dt 339.01ms | 1546513.60 tokens/sec
Step 18040 | loss: 3.041974 | lr:6.4208e-05 | norm 0.3056 | dt 339.24ms | 1545471.27 tokens/sec
Step 18041 | loss: 3.062092 | lr:6.4200e-05 | norm 0.3117 | dt 338.48ms | 1548932.98 tokens/sec
Step 18042 | loss: 3.060471 | lr:6.4192e-05 | norm 0.3300 | dt 337.68ms | 1552621.78 tokens/sec
Step 18043 | loss: 3.000530 | lr:6.4183e-05 | norm 0.3121 | dt 338.35ms | 1549546.38 tokens/sec
Step 18044 | loss: 3.049320 | lr:6.4175e-05 | norm 0.3154 | dt 338.92ms | 1546927.01 tokens/sec
Step 18045 | loss: 2.959606 | lr:6.4167e-05 | norm 0.2896 | dt 337.59ms | 1553027.49 tokens/sec
Step 18046 | loss: 2.997507 | lr:6.4159e-05 | norm 0.2939 | dt 338.47ms | 1548995.18 tokens/sec
Step 18047 | loss: 3.066179 | lr:6.4151e-05 | norm 0.2891 | dt 338.67ms | 1548083.54 tokens/sec
Step 18048 | loss: 2.959549 | lr:6.4143e-05 | norm 0.2865 | dt 338.42ms | 1549224.34 tokens/sec
Step 18049 | loss: 3.025197 | lr:6.4135e-05 | norm 0.3150 | dt 946.24ms | 554076.78 tokens/sec
Step 18050 | loss: 2.640002 | lr:6.4127e-05 | norm 0.2855 | dt 338.43ms | 1549169.77 tokens/sec
Step 18051 | loss: 2.951472 | lr:6.4119e-05 | norm 0.2946 | dt 338.04ms | 1550968.24 tokens/sec
Step 18052 | loss: 3.081803 | lr:6.4111e-05 | norm 0.2954 | dt 337.61ms | 1552943.04 tokens/sec
Step 18053 | loss: 3.018412 | lr:6.4103e-05 | norm 0.3202 | dt 337.78ms | 1552145.07 tokens/sec
Step 18054 | loss: 3.110510 | lr:6.4095e-05 | norm 0.2896 | dt 337.36ms | 1554097.61 tokens/sec
Step 18055 | loss: 3.003766 | lr:6.4087e-05 | norm 0.2945 | dt 338.55ms | 1548633.01 tokens/sec
Step 18056 | loss: 3.056220 | lr:6.4079e-05 | norm 0.2893 | dt 337.61ms | 1552930.98 tokens/sec
Step 18057 | loss: 3.054138 | lr:6.4071e-05 | norm 0.2989 | dt 337.25ms | 1554578.82 tokens/sec
Step 18058 | loss: 3.050806 | lr:6.4063e-05 | norm 0.2906 | dt 337.79ms | 1552124.25 tokens/sec
Step 18059 | loss: 2.984086 | lr:6.4055e-05 | norm 0.3566 | dt 337.69ms | 1552556.01 tokens/sec
Step 18060 | loss: 3.032590 | lr:6.4047e-05 | norm 0.3875 | dt 338.16ms | 1550408.37 tokens/sec
Step 18061 | loss: 3.059056 | lr:6.4039e-05 | norm 0.3198 | dt 337.50ms | 1553425.73 tokens/sec
Step 18062 | loss: 3.010003 | lr:6.4031e-05 | norm 0.3143 | dt 338.07ms | 1550824.95 tokens/sec
Step 18063 | loss: 3.082394 | lr:6.4023e-05 | norm 0.3055 | dt 338.90ms | 1547015.15 tokens/sec
Step 18064 | loss: 3.061754 | lr:6.4015e-05 | norm 0.3214 | dt 338.98ms | 1546643.04 tokens/sec
Step 18065 | loss: 3.037047 | lr:6.4007e-05 | norm 0.3017 | dt 338.68ms | 1548017.06 tokens/sec
Step 18066 | loss: 3.055615 | lr:6.3999e-05 | norm 0.2963 | dt 338.02ms | 1551048.10 tokens/sec
Step 18067 | loss: 3.001055 | lr:6.3991e-05 | norm 0.2961 | dt 337.67ms | 1552671.11 tokens/sec
Step 18068 | loss: 3.151917 | lr:6.3983e-05 | norm 0.3570 | dt 338.28ms | 1549886.04 tokens/sec
Step 18069 | loss: 2.984171 | lr:6.3975e-05 | norm 0.2889 | dt 338.32ms | 1549702.54 tokens/sec
Step 18070 | loss: 3.021952 | lr:6.3968e-05 | norm 0.2861 | dt 338.04ms | 1550948.55 tokens/sec
Step 18071 | loss: 3.088326 | lr:6.3960e-05 | norm 0.3284 | dt 338.06ms | 1550881.83 tokens/sec
Step 18072 | loss: 3.033458 | lr:6.3952e-05 | norm 0.2974 | dt 338.00ms | 1551149.85 tokens/sec
Step 18073 | loss: 3.051557 | lr:6.3944e-05 | norm 0.3390 | dt 337.50ms | 1553438.90 tokens/sec
Step 18074 | loss: 3.015665 | lr:6.3936e-05 | norm 0.3246 | dt 338.48ms | 1548963.53 tokens/sec
Step 18075 | loss: 3.056866 | lr:6.3928e-05 | norm 0.2972 | dt 338.21ms | 1550193.05 tokens/sec
Step 18076 | loss: 3.034080 | lr:6.3920e-05 | norm 0.3174 | dt 337.96ms | 1551326.02 tokens/sec
Step 18077 | loss: 3.004149 | lr:6.3912e-05 | norm 0.3037 | dt 338.64ms | 1548203.43 tokens/sec
Step 18078 | loss: 3.038744 | lr:6.3905e-05 | norm 0.2948 | dt 338.10ms | 1550674.04 tokens/sec
Step 18079 | loss: 2.951082 | lr:6.3897e-05 | norm 0.2938 | dt 338.25ms | 1550014.95 tokens/sec
Step 18080 | loss: 3.043292 | lr:6.3889e-05 | norm 0.3140 | dt 338.64ms | 1548212.15 tokens/sec
Step 18081 | loss: 3.001069 | lr:6.3881e-05 | norm 0.2792 | dt 337.04ms | 1555574.05 tokens/sec
Step 18082 | loss: 3.029945 | lr:6.3873e-05 | norm 0.2894 | dt 338.71ms | 1547877.59 tokens/sec
Step 18083 | loss: 3.056772 | lr:6.3866e-05 | norm 0.2970 | dt 338.50ms | 1548850.07 tokens/sec
Step 18084 | loss: 3.045296 | lr:6.3858e-05 | norm 0.3263 | dt 338.80ms | 1547481.10 tokens/sec
Step 18085 | loss: 3.011253 | lr:6.3850e-05 | norm 0.2839 | dt 337.79ms | 1552095.77 tokens/sec
Step 18086 | loss: 2.938760 | lr:6.3842e-05 | norm 0.2857 | dt 338.30ms | 1549776.81 tokens/sec
Step 18087 | loss: 3.041332 | lr:6.3834e-05 | norm 0.2936 | dt 338.00ms | 1551152.03 tokens/sec
Step 18088 | loss: 3.024158 | lr:6.3827e-05 | norm 0.3199 | dt 338.14ms | 1550483.79 tokens/sec
Step 18089 | loss: 3.079962 | lr:6.3819e-05 | norm 0.3112 | dt 337.10ms | 1555306.70 tokens/sec
Step 18090 | loss: 3.077233 | lr:6.3811e-05 | norm 0.3155 | dt 337.72ms | 1552451.88 tokens/sec
Step 18091 | loss: 3.059809 | lr:6.3804e-05 | norm 0.3053 | dt 338.81ms | 1547427.74 tokens/sec
Step 18092 | loss: 3.026173 | lr:6.3796e-05 | norm 0.3059 | dt 338.27ms | 1549931.92 tokens/sec
Step 18093 | loss: 3.057326 | lr:6.3788e-05 | norm 0.2900 | dt 337.75ms | 1552276.55 tokens/sec
Step 18094 | loss: 3.042371 | lr:6.3780e-05 | norm 0.3073 | dt 338.37ms | 1549468.86 tokens/sec
Step 18095 | loss: 3.054412 | lr:6.3773e-05 | norm 0.2983 | dt 338.26ms | 1549934.10 tokens/sec
Step 18096 | loss: 3.070377 | lr:6.3765e-05 | norm 0.3436 | dt 342.44ms | 1531056.84 tokens/sec
Step 18097 | loss: 3.049075 | lr:6.3757e-05 | norm 0.2841 | dt 338.35ms | 1549547.48 tokens/sec
Step 18098 | loss: 3.033891 | lr:6.3750e-05 | norm 0.2824 | dt 338.71ms | 1547903.74 tokens/sec
Step 18099 | loss: 3.058539 | lr:6.3742e-05 | norm 0.3237 | dt 339.13ms | 1546000.41 tokens/sec
Step 18100 | loss: 3.039091 | lr:6.3734e-05 | norm 0.3029 | dt 338.53ms | 1548723.54 tokens/sec
Step 18101 | loss: 3.004711 | lr:6.3727e-05 | norm 0.2919 | dt 338.59ms | 1548440.00 tokens/sec
Step 18102 | loss: 3.116709 | lr:6.3719e-05 | norm 0.2972 | dt 338.72ms | 1547857.98 tokens/sec
Step 18103 | loss: 3.061057 | lr:6.3711e-05 | norm 0.3014 | dt 339.12ms | 1546029.76 tokens/sec
Step 18104 | loss: 3.007149 | lr:6.3704e-05 | norm 0.2933 | dt 338.79ms | 1547552.97 tokens/sec
Step 18105 | loss: 3.055811 | lr:6.3696e-05 | norm 0.2850 | dt 339.34ms | 1545004.37 tokens/sec
Step 18106 | loss: 3.094418 | lr:6.3688e-05 | norm 0.3167 | dt 338.76ms | 1547685.85 tokens/sec
Step 18107 | loss: 3.099267 | lr:6.3681e-05 | norm 0.2800 | dt 338.36ms | 1549480.87 tokens/sec
Step 18108 | loss: 3.101442 | lr:6.3673e-05 | norm 0.2820 | dt 339.40ms | 1544758.00 tokens/sec
Step 18109 | loss: 3.061205 | lr:6.3666e-05 | norm 0.3039 | dt 338.52ms | 1548760.62 tokens/sec
Step 18110 | loss: 3.033945 | lr:6.3658e-05 | norm 0.2787 | dt 338.02ms | 1551078.73 tokens/sec
Step 18111 | loss: 2.980786 | lr:6.3650e-05 | norm 0.2915 | dt 339.07ms | 1546252.61 tokens/sec
Step 18112 | loss: 2.980425 | lr:6.3643e-05 | norm 0.2837 | dt 337.51ms | 1553418.05 tokens/sec
Step 18113 | loss: 2.972450 | lr:6.3635e-05 | norm 0.3080 | dt 337.85ms | 1551815.38 tokens/sec
Step 18114 | loss: 2.961181 | lr:6.3628e-05 | norm 0.2840 | dt 338.23ms | 1550110.00 tokens/sec
Step 18115 | loss: 3.053168 | lr:6.3620e-05 | norm 0.2839 | dt 337.30ms | 1554357.96 tokens/sec
Step 18116 | loss: 3.036222 | lr:6.3613e-05 | norm 0.2900 | dt 338.35ms | 1549545.29 tokens/sec
Step 18117 | loss: 2.989143 | lr:6.3605e-05 | norm 0.2717 | dt 337.43ms | 1553752.82 tokens/sec
Step 18118 | loss: 3.029099 | lr:6.3598e-05 | norm 0.2827 | dt 338.10ms | 1550674.04 tokens/sec
Step 18119 | loss: 2.965516 | lr:6.3590e-05 | norm 0.2748 | dt 338.44ms | 1549150.13 tokens/sec
Step 18120 | loss: 2.975590 | lr:6.3583e-05 | norm 0.2898 | dt 338.33ms | 1549642.48 tokens/sec
Step 18121 | loss: 2.967849 | lr:6.3575e-05 | norm 0.2809 | dt 338.08ms | 1550773.55 tokens/sec
Step 18122 | loss: 2.988276 | lr:6.3568e-05 | norm 0.2835 | dt 338.60ms | 1548404.02 tokens/sec
Step 18123 | loss: 3.014153 | lr:6.3560e-05 | norm 0.2907 | dt 336.94ms | 1556050.67 tokens/sec
Step 18124 | loss: 3.094201 | lr:6.3553e-05 | norm 0.3140 | dt 338.96ms | 1546748.56 tokens/sec
Step 18125 | loss: 3.006485 | lr:6.3545e-05 | norm 0.3284 | dt 337.76ms | 1552263.40 tokens/sec
Step 18126 | loss: 3.011519 | lr:6.3538e-05 | norm 0.3127 | dt 337.34ms | 1554178.89 tokens/sec
Step 18127 | loss: 3.073533 | lr:6.3530e-05 | norm 0.3319 | dt 337.85ms | 1551816.47 tokens/sec
Step 18128 | loss: 3.103861 | lr:6.3523e-05 | norm 0.2951 | dt 338.32ms | 1549683.97 tokens/sec
Step 18129 | loss: 3.042898 | lr:6.3515e-05 | norm 0.2989 | dt 337.61ms | 1552946.33 tokens/sec
Step 18130 | loss: 3.025671 | lr:6.3508e-05 | norm 0.3496 | dt 338.00ms | 1551152.03 tokens/sec
Step 18131 | loss: 3.012097 | lr:6.3501e-05 | norm 0.2940 | dt 343.18ms | 1527718.00 tokens/sec
Step 18132 | loss: 3.002068 | lr:6.3493e-05 | norm 0.3099 | dt 338.02ms | 1551056.85 tokens/sec
Step 18133 | loss: 3.088882 | lr:6.3486e-05 | norm 0.3311 | dt 338.24ms | 1550064.11 tokens/sec
Step 18134 | loss: 3.015373 | lr:6.3478e-05 | norm 0.2965 | dt 338.41ms | 1549289.83 tokens/sec
Step 18135 | loss: 3.041722 | lr:6.3471e-05 | norm 0.3086 | dt 338.85ms | 1547258.98 tokens/sec
Step 18136 | loss: 3.032505 | lr:6.3464e-05 | norm 0.3074 | dt 338.47ms | 1549009.36 tokens/sec
Step 18137 | loss: 3.021510 | lr:6.3456e-05 | norm 0.3101 | dt 339.63ms | 1543723.45 tokens/sec
Step 18138 | loss: 3.034845 | lr:6.3449e-05 | norm 0.2925 | dt 339.73ms | 1543232.69 tokens/sec
Step 18139 | loss: 3.070377 | lr:6.3442e-05 | norm 0.3069 | dt 338.81ms | 1547454.96 tokens/sec
Step 18140 | loss: 3.031499 | lr:6.3434e-05 | norm 0.3289 | dt 339.40ms | 1544742.81 tokens/sec
Step 18141 | loss: 3.116194 | lr:6.3427e-05 | norm 0.3202 | dt 339.55ms | 1544049.72 tokens/sec
Step 18142 | loss: 3.061095 | lr:6.3420e-05 | norm 0.2851 | dt 339.39ms | 1544778.62 tokens/sec
Step 18143 | loss: 3.071497 | lr:6.3412e-05 | norm 0.2972 | dt 898.13ms | 583755.95 tokens/sec
Step 18144 | loss: 3.073582 | lr:6.3405e-05 | norm 0.3116 | dt 335.05ms | 1564795.85 tokens/sec
Step 18145 | loss: 3.019803 | lr:6.3398e-05 | norm 0.3050 | dt 338.33ms | 1549616.27 tokens/sec
Step 18146 | loss: 3.065163 | lr:6.3390e-05 | norm 0.3234 | dt 339.37ms | 1544905.59 tokens/sec
Step 18147 | loss: 2.963290 | lr:6.3383e-05 | norm 0.2813 | dt 337.76ms | 1552234.91 tokens/sec
Step 18148 | loss: 3.009936 | lr:6.3376e-05 | norm 0.2995 | dt 337.35ms | 1554155.82 tokens/sec
Step 18149 | loss: 3.008894 | lr:6.3368e-05 | norm 0.2946 | dt 338.20ms | 1550210.54 tokens/sec
Step 18150 | loss: 3.032718 | lr:6.3361e-05 | norm 0.2926 | dt 338.83ms | 1547337.37 tokens/sec
Step 18151 | loss: 3.039743 | lr:6.3354e-05 | norm 0.2896 | dt 338.97ms | 1546701.78 tokens/sec
Step 18152 | loss: 3.047424 | lr:6.3347e-05 | norm 0.2860 | dt 338.99ms | 1546637.60 tokens/sec
Step 18153 | loss: 2.999107 | lr:6.3339e-05 | norm 0.2922 | dt 338.95ms | 1546793.17 tokens/sec
Step 18154 | loss: 2.984505 | lr:6.3332e-05 | norm 0.3080 | dt 338.31ms | 1549748.41 tokens/sec
Step 18155 | loss: 3.036623 | lr:6.3325e-05 | norm 0.2987 | dt 338.34ms | 1549603.16 tokens/sec
Step 18156 | loss: 2.968059 | lr:6.3318e-05 | norm 0.2784 | dt 339.52ms | 1544201.51 tokens/sec
Step 18157 | loss: 3.040414 | lr:6.3310e-05 | norm 0.2947 | dt 338.38ms | 1549401.18 tokens/sec
Step 18158 | loss: 2.949988 | lr:6.3303e-05 | norm 0.2867 | dt 339.36ms | 1544951.18 tokens/sec
Step 18159 | loss: 3.003847 | lr:6.3296e-05 | norm 0.3863 | dt 339.08ms | 1546223.26 tokens/sec
Step 18160 | loss: 3.042351 | lr:6.3289e-05 | norm 0.2936 | dt 338.49ms | 1548887.16 tokens/sec
Step 18161 | loss: 3.186566 | lr:6.3282e-05 | norm 0.4186 | dt 338.61ms | 1548370.22 tokens/sec
Step 18162 | loss: 3.068448 | lr:6.3274e-05 | norm 0.3168 | dt 338.70ms | 1547936.43 tokens/sec
Step 18163 | loss: 3.058596 | lr:6.3267e-05 | norm 0.3335 | dt 340.24ms | 1540948.76 tokens/sec
Step 18164 | loss: 2.970034 | lr:6.3260e-05 | norm 0.3731 | dt 338.34ms | 1549570.41 tokens/sec
Step 18165 | loss: 3.010885 | lr:6.3253e-05 | norm 0.3351 | dt 339.65ms | 1543622.68 tokens/sec
Step 18166 | loss: 3.031527 | lr:6.3246e-05 | norm 0.3263 | dt 338.85ms | 1547247.00 tokens/sec
Step 18167 | loss: 3.034172 | lr:6.3239e-05 | norm 0.3119 | dt 339.08ms | 1546191.73 tokens/sec
Step 18168 | loss: 3.024379 | lr:6.3232e-05 | norm 0.3765 | dt 339.04ms | 1546370.05 tokens/sec
Step 18169 | loss: 3.071246 | lr:6.3224e-05 | norm 0.3163 | dt 338.74ms | 1547753.39 tokens/sec
Step 18170 | loss: 3.083447 | lr:6.3217e-05 | norm 0.3174 | dt 338.81ms | 1547454.96 tokens/sec
Step 18171 | loss: 3.071486 | lr:6.3210e-05 | norm 0.3105 | dt 338.91ms | 1546985.77 tokens/sec
Step 18172 | loss: 3.151387 | lr:6.3203e-05 | norm 0.3199 | dt 338.13ms | 1550559.23 tokens/sec
Step 18173 | loss: 3.055519 | lr:6.3196e-05 | norm 0.3483 | dt 339.56ms | 1544006.35 tokens/sec
Step 18174 | loss: 3.060543 | lr:6.3189e-05 | norm 0.3249 | dt 338.07ms | 1550826.04 tokens/sec
Step 18175 | loss: 3.109468 | lr:6.3182e-05 | norm 0.3119 | dt 338.64ms | 1548238.31 tokens/sec
Step 18176 | loss: 3.053606 | lr:6.3175e-05 | norm 0.2777 | dt 339.24ms | 1545461.50 tokens/sec
Step 18177 | loss: 3.009143 | lr:6.3168e-05 | norm 0.3018 | dt 339.17ms | 1545786.32 tokens/sec
Step 18178 | loss: 3.038054 | lr:6.3161e-05 | norm 0.2959 | dt 338.33ms | 1549638.11 tokens/sec
Step 18179 | loss: 3.089994 | lr:6.3154e-05 | norm 0.3289 | dt 338.73ms | 1547811.13 tokens/sec
Step 18180 | loss: 3.027215 | lr:6.3147e-05 | norm 0.2834 | dt 339.54ms | 1544101.76 tokens/sec
Step 18181 | loss: 3.077982 | lr:6.3140e-05 | norm 0.2946 | dt 339.63ms | 1543713.70 tokens/sec
Step 18182 | loss: 3.040892 | lr:6.3133e-05 | norm 0.2869 | dt 339.26ms | 1545396.33 tokens/sec
Step 18183 | loss: 3.050340 | lr:6.3126e-05 | norm 0.2864 | dt 339.13ms | 1545972.15 tokens/sec
Step 18184 | loss: 3.002259 | lr:6.3119e-05 | norm 0.2868 | dt 338.69ms | 1548000.72 tokens/sec
Step 18185 | loss: 3.032376 | lr:6.3112e-05 | norm 0.2826 | dt 338.71ms | 1547874.32 tokens/sec
Step 18186 | loss: 2.974542 | lr:6.3105e-05 | norm 0.2948 | dt 339.14ms | 1545927.59 tokens/sec
Step 18187 | loss: 2.989033 | lr:6.3098e-05 | norm 0.2827 | dt 339.96ms | 1542226.15 tokens/sec
Step 18188 | loss: 3.028888 | lr:6.3091e-05 | norm 0.2765 | dt 339.09ms | 1546181.95 tokens/sec
Step 18189 | loss: 3.001803 | lr:6.3084e-05 | norm 0.2754 | dt 338.44ms | 1549133.76 tokens/sec
Step 18190 | loss: 3.064967 | lr:6.3077e-05 | norm 0.2781 | dt 338.67ms | 1548102.07 tokens/sec
Step 18191 | loss: 3.045945 | lr:6.3070e-05 | norm 0.2819 | dt 339.14ms | 1545925.42 tokens/sec
Step 18192 | loss: 3.000233 | lr:6.3063e-05 | norm 0.2889 | dt 338.79ms | 1547546.44 tokens/sec
Step 18193 | loss: 3.016032 | lr:6.3056e-05 | norm 0.2691 | dt 337.84ms | 1551873.42 tokens/sec
Step 18194 | loss: 3.002185 | lr:6.3049e-05 | norm 0.3113 | dt 338.59ms | 1548450.90 tokens/sec
Step 18195 | loss: 3.094884 | lr:6.3042e-05 | norm 0.3137 | dt 338.92ms | 1546953.12 tokens/sec
Step 18196 | loss: 3.027024 | lr:6.3035e-05 | norm 0.2818 | dt 337.67ms | 1552654.67 tokens/sec
Step 18197 | loss: 3.048632 | lr:6.3028e-05 | norm 0.2957 | dt 338.91ms | 1546960.74 tokens/sec
Step 18198 | loss: 3.057534 | lr:6.3021e-05 | norm 0.2983 | dt 338.52ms | 1548783.53 tokens/sec
Step 18199 | loss: 3.020401 | lr:6.3014e-05 | norm 0.2812 | dt 338.50ms | 1548847.89 tokens/sec
Step 18200 | loss: 3.033707 | lr:6.3007e-05 | norm 0.2974 | dt 338.82ms | 1547403.78 tokens/sec
Step 18201 | loss: 3.085829 | lr:6.3001e-05 | norm 0.2986 | dt 339.55ms | 1544082.24 tokens/sec
Step 18202 | loss: 3.038721 | lr:6.2994e-05 | norm 0.3104 | dt 338.73ms | 1547827.47 tokens/sec
Step 18203 | loss: 3.013530 | lr:6.2987e-05 | norm 0.3006 | dt 338.26ms | 1549970.15 tokens/sec
Step 18204 | loss: 3.096471 | lr:6.2980e-05 | norm 0.3099 | dt 338.15ms | 1550468.49 tokens/sec
Step 18205 | loss: 3.072309 | lr:6.2973e-05 | norm 0.3224 | dt 337.72ms | 1552452.98 tokens/sec
Step 18206 | loss: 2.954862 | lr:6.2966e-05 | norm 0.2934 | dt 338.72ms | 1547856.89 tokens/sec
Step 18207 | loss: 3.026417 | lr:6.2960e-05 | norm 0.3354 | dt 338.04ms | 1550955.11 tokens/sec
Step 18208 | loss: 3.048048 | lr:6.2953e-05 | norm 0.3077 | dt 339.42ms | 1544638.64 tokens/sec
Step 18209 | loss: 3.121719 | lr:6.2946e-05 | norm 0.3200 | dt 338.96ms | 1546759.44 tokens/sec
Step 18210 | loss: 3.065428 | lr:6.2939e-05 | norm 0.3022 | dt 339.73ms | 1543229.45 tokens/sec
Step 18211 | loss: 3.082060 | lr:6.2932e-05 | norm 0.2849 | dt 339.42ms | 1544654.92 tokens/sec
Step 18212 | loss: 3.052991 | lr:6.2926e-05 | norm 0.3227 | dt 339.09ms | 1546156.94 tokens/sec
Step 18213 | loss: 3.032575 | lr:6.2919e-05 | norm 0.2874 | dt 339.69ms | 1543433.08 tokens/sec
Step 18214 | loss: 3.014415 | lr:6.2912e-05 | norm 0.3123 | dt 338.11ms | 1550662.01 tokens/sec
Step 18215 | loss: 3.030833 | lr:6.2905e-05 | norm 0.3073 | dt 338.40ms | 1549334.59 tokens/sec
Step 18216 | loss: 3.080136 | lr:6.2898e-05 | norm 0.3041 | dt 338.39ms | 1549370.61 tokens/sec
Step 18217 | loss: 3.051853 | lr:6.2892e-05 | norm 0.3116 | dt 338.40ms | 1549318.21 tokens/sec
Step 18218 | loss: 3.040619 | lr:6.2885e-05 | norm 0.2920 | dt 337.62ms | 1552889.31 tokens/sec
Step 18219 | loss: 2.983869 | lr:6.2878e-05 | norm 0.2920 | dt 338.57ms | 1548535.95 tokens/sec
Step 18220 | loss: 3.051195 | lr:6.2872e-05 | norm 0.3038 | dt 339.11ms | 1546056.93 tokens/sec
Step 18221 | loss: 3.069596 | lr:6.2865e-05 | norm 0.2791 | dt 337.68ms | 1552602.05 tokens/sec
Step 18222 | loss: 3.003631 | lr:6.2858e-05 | norm 0.2859 | dt 338.25ms | 1549995.28 tokens/sec
Step 18223 | loss: 2.955061 | lr:6.2851e-05 | norm 0.2805 | dt 338.41ms | 1549254.90 tokens/sec
Step 18224 | loss: 3.018414 | lr:6.2845e-05 | norm 0.2783 | dt 338.54ms | 1548687.54 tokens/sec
Step 18225 | loss: 2.978622 | lr:6.2838e-05 | norm 0.2946 | dt 337.56ms | 1553189.84 tokens/sec
Step 18226 | loss: 2.986341 | lr:6.2831e-05 | norm 0.2654 | dt 337.88ms | 1551692.74 tokens/sec
Step 18227 | loss: 3.030724 | lr:6.2825e-05 | norm 0.2896 | dt 338.13ms | 1550553.76 tokens/sec
Step 18228 | loss: 3.005499 | lr:6.2818e-05 | norm 0.2810 | dt 337.74ms | 1552357.64 tokens/sec
Step 18229 | loss: 3.014481 | lr:6.2811e-05 | norm 0.2832 | dt 337.72ms | 1552412.43 tokens/sec
Step 18230 | loss: 3.045058 | lr:6.2805e-05 | norm 0.3184 | dt 337.80ms | 1552074.96 tokens/sec
Step 18231 | loss: 3.054451 | lr:6.2798e-05 | norm 0.2912 | dt 337.90ms | 1551624.85 tokens/sec
Step 18232 | loss: 3.020882 | lr:6.2791e-05 | norm 0.2958 | dt 338.13ms | 1550545.02 tokens/sec
Step 18233 | loss: 3.068935 | lr:6.2785e-05 | norm 0.3134 | dt 337.63ms | 1552857.51 tokens/sec
Step 18234 | loss: 3.118253 | lr:6.2778e-05 | norm 0.3565 | dt 337.85ms | 1551855.90 tokens/sec
Step 18235 | loss: 3.027688 | lr:6.2772e-05 | norm 0.2886 | dt 338.02ms | 1551053.57 tokens/sec
Step 18236 | loss: 3.066112 | lr:6.2765e-05 | norm 0.3191 | dt 337.30ms | 1554354.66 tokens/sec
Step 18237 | loss: 3.057109 | lr:6.2758e-05 | norm 0.3266 | dt 338.23ms | 1550082.69 tokens/sec
Step 18238 | loss: 3.055558 | lr:6.2752e-05 | norm 0.3228 | dt 337.71ms | 1552462.84 tokens/sec
Step 18239 | loss: 3.078266 | lr:6.2745e-05 | norm 0.4010 | dt 926.28ms | 566017.50 tokens/sec
Step 18240 | loss: 3.022797 | lr:6.2739e-05 | norm 0.3270 | dt 336.77ms | 1556797.56 tokens/sec
Step 18241 | loss: 3.100605 | lr:6.2732e-05 | norm 0.3415 | dt 339.19ms | 1545712.44 tokens/sec
Step 18242 | loss: 3.027468 | lr:6.2726e-05 | norm 0.3420 | dt 337.94ms | 1551440.95 tokens/sec
Step 18243 | loss: 3.058009 | lr:6.2719e-05 | norm 0.2987 | dt 337.16ms | 1555009.75 tokens/sec
Step 18244 | loss: 3.010431 | lr:6.2712e-05 | norm 0.3262 | dt 337.82ms | 1551978.56 tokens/sec
Step 18245 | loss: 3.112470 | lr:6.2706e-05 | norm 0.3670 | dt 338.37ms | 1549463.40 tokens/sec
Step 18246 | loss: 3.020648 | lr:6.2699e-05 | norm 0.3139 | dt 337.54ms | 1553241.40 tokens/sec
Step 18247 | loss: 3.091417 | lr:6.2693e-05 | norm 0.3312 | dt 337.67ms | 1552653.57 tokens/sec
Step 18248 | loss: 3.061287 | lr:6.2686e-05 | norm 0.3165 | dt 338.47ms | 1548976.63 tokens/sec
Step 18249 | loss: 3.008471 | lr:6.2680e-05 | norm 0.3051 | dt 338.64ms | 1548213.24 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 18250: 3.0839
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3076/10042=0.3063


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but my first language model is only about being a set. I just need to know how to read a set.<|endoftext|>
rank 5 sample 1 >Hello, I'm a language model, my wife doesn't make anything up and I thought I was the boss of a small company, but in doing this I
rank 5 sample 2 >Hello, I'm a language model, and I was thinking we could all create a way to tell a human story without a face.
The first thing we
rank 5 sample 3 >Hello, I'm a language model, because I used this on my own.
- A good way of doing this is using a web page that is going




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm gonna write something that could be useful in the future, specifically during my work with LQ. If my
rank 3 sample 1 >Hello, I'm a language model, so here is a quick code of how I want to express my ideas.
I'm going to write a couple code
rank 3 sample 2 >Hello, I'm a language model, so this one is, to some extent, one of my main goals as a language model. First of all my focus
rank 3 sample 3 >Hello, I'm a language model, so in this video I'm going to talk about some basics about language modeling. This is just a beginner video on the




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I wanted to create something that would include a native speaker. I found the [edit] project. I'm so
rank 7 sample 1 >Hello, I'm a language model, to illustrate this. I would also imagine that I get as much work done as there is.
Now, the only
rank 7 sample 2 >Hello, I'm a language model, so I'll get into it quickly, for example:
# create language model
# create language model
In a
rank 7 sample 3 >Hello, I'm a language model, and it's not my passion to change the teaching of languages, I like to make my students talk to me about language




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, so my goal when I think about languages and languages is that we can communicate in some way.
I hope you find
rank 2 sample 1 >Hello, I'm a language model, and I find this fascinating. I found them in a book, I had a lot of experience, I have a lot
rank 2 sample 2 >Hello, I'm a language model, and I've written a book that looks at some of the more famous ones. And I've even started doing some really
rank 2 sample 3 >Hello, I'm a language model, but how can I do it?
Here's where the power of language modeling comes into play. I like the analogy




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, so how can I tell how to improve it by doing grammar comparisons?
What is an example of a language model?
rank 1 sample 1 >Hello, I'm a language model, a programmer who spends time learning new languages and working with computer science models. If I went back to the basics, I
rank 1 sample 2 >Hello, I'm a language model, but after all, I'm not a language model. And while you're talking to me, if you want to hear
rank 1 sample 3 >Hello, I'm a language model, so I'm using the verb, verb (to be an I / o)(to look through).
I like this




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I can't get anything out of learning English. I have a huge, great body when I talk, but it


ddp_rank 0: ####### Printing generated samples ####### 

rank 4 sample 1 >Hello, I'm a language model, am not a statistician, this should be interesting! To learn more about statistics with Excel you can see this interactive chart
rank 0 sample 0 >Hello, I'm a language model, so I know that you might find these really awesome. I know my math, but what I like is when I try

rank 4 sample 2 >Hello, I'm a language model, I read things myself but am not quite sure how to do a test. I still like to see this as just another

ddp_rank 6: ####### Printing generated samples ####### 

rank 0 sample 1 >Hello, I'm a language model, and what do I mean by "pink". Why then do I tell you to "pink like the light"?
rank 4 sample 3 >Hello, I'm a language model, so it seemed weird when I first set this one up, except I couldn't write it down as it was supposed to


rank 0 sample 2 >Hello, I'm a language model, and I used this class myself. But I thought it was the perfect thing to do, because I felt like it can
rank 0 sample 3 >Hello, I'm a language model, and for the first time I'm starting with, what's the name of this model? Can I run this on one


rank 6 sample 0 >Hello, I'm a language model, who uses the "gadgets" and "dynamo" to build out the world. I've been making
rank 6 sample 1 >Hello, I'm a language model, so I can't go to any grammar blog. It's not just one blog, I have a bunch of one-
rank 6 sample 2 >Hello, I'm a language model, but a good one to have for an interview:
I've spent most of my career working in high-prec
rank 6 sample 3 >Hello, I'm a language model, so you might be interested in that if I know how to do it. But, you gotta know, there are basically


Step 18250 | loss: 3.094692 | lr:6.2673e-05 | norm 0.3453 | dt 12305.68ms | 42605.38 tokens/sec
Step 18251 | loss: 3.045043 | lr:6.2667e-05 | norm 0.2962 | dt 335.44ms | 1563005.19 tokens/sec
Step 18252 | loss: 3.070004 | lr:6.2660e-05 | norm 0.3047 | dt 336.78ms | 1556745.76 tokens/sec
Step 18253 | loss: 3.008627 | lr:6.2654e-05 | norm 0.3419 | dt 336.92ms | 1556118.94 tokens/sec
Step 18254 | loss: 3.006468 | lr:6.2648e-05 | norm 0.3119 | dt 336.35ms | 1558739.73 tokens/sec
Step 18255 | loss: 2.994968 | lr:6.2641e-05 | norm 0.3062 | dt 337.33ms | 1554244.80 tokens/sec
Step 18256 | loss: 3.024847 | lr:6.2635e-05 | norm 0.3208 | dt 337.66ms | 1552730.32 tokens/sec
Step 18257 | loss: 3.009497 | lr:6.2628e-05 | norm 0.3143 | dt 336.63ms | 1557480.08 tokens/sec
Step 18258 | loss: 3.022548 | lr:6.2622e-05 | norm 0.3228 | dt 337.67ms | 1552657.96 tokens/sec
Step 18259 | loss: 2.966616 | lr:6.2615e-05 | norm 0.3057 | dt 337.96ms | 1551353.38 tokens/sec
Step 18260 | loss: 3.162684 | lr:6.2609e-05 | norm 0.3315 | dt 337.07ms | 1555437.61 tokens/sec
Step 18261 | loss: 2.997236 | lr:6.2603e-05 | norm 0.3014 | dt 336.54ms | 1557858.53 tokens/sec
Step 18262 | loss: 2.991016 | lr:6.2596e-05 | norm 0.2757 | dt 338.38ms | 1549412.09 tokens/sec
Step 18263 | loss: 2.967191 | lr:6.2590e-05 | norm 0.2973 | dt 337.80ms | 1552046.47 tokens/sec
Step 18264 | loss: 3.003470 | lr:6.2583e-05 | norm 0.2975 | dt 338.32ms | 1549658.86 tokens/sec
Step 18265 | loss: 3.042803 | lr:6.2577e-05 | norm 0.2886 | dt 338.09ms | 1550757.14 tokens/sec
Step 18266 | loss: 3.074759 | lr:6.2571e-05 | norm 0.2985 | dt 338.16ms | 1550418.20 tokens/sec
Step 18267 | loss: 3.085137 | lr:6.2564e-05 | norm 0.2908 | dt 336.89ms | 1556249.99 tokens/sec
Step 18268 | loss: 3.072188 | lr:6.2558e-05 | norm 0.2937 | dt 337.36ms | 1554084.43 tokens/sec
Step 18269 | loss: 3.032525 | lr:6.2552e-05 | norm 0.2999 | dt 337.37ms | 1554042.70 tokens/sec
Step 18270 | loss: 3.067751 | lr:6.2545e-05 | norm 0.2912 | dt 338.25ms | 1550005.11 tokens/sec
Step 18271 | loss: 3.014127 | lr:6.2539e-05 | norm 0.3071 | dt 337.23ms | 1554687.63 tokens/sec
Step 18272 | loss: 3.118411 | lr:6.2533e-05 | norm 0.3414 | dt 337.19ms | 1554876.71 tokens/sec
Step 18273 | loss: 3.072998 | lr:6.2526e-05 | norm 0.2837 | dt 338.28ms | 1549886.04 tokens/sec
Step 18274 | loss: 3.133979 | lr:6.2520e-05 | norm 0.3038 | dt 337.43ms | 1553781.36 tokens/sec
Step 18275 | loss: 3.063777 | lr:6.2514e-05 | norm 0.3326 | dt 337.93ms | 1551489.11 tokens/sec
Step 18276 | loss: 3.074572 | lr:6.2507e-05 | norm 0.2928 | dt 337.44ms | 1553731.96 tokens/sec
Step 18277 | loss: 3.075562 | lr:6.2501e-05 | norm 0.3085 | dt 337.77ms | 1552202.04 tokens/sec
Step 18278 | loss: 3.049724 | lr:6.2495e-05 | norm 0.3009 | dt 337.76ms | 1552260.11 tokens/sec
Step 18279 | loss: 3.095978 | lr:6.2489e-05 | norm 0.2881 | dt 337.73ms | 1552400.37 tokens/sec
Step 18280 | loss: 3.084752 | lr:6.2482e-05 | norm 0.3380 | dt 338.43ms | 1549179.60 tokens/sec
Step 18281 | loss: 3.060490 | lr:6.2476e-05 | norm 0.3028 | dt 338.01ms | 1551099.52 tokens/sec
Step 18282 | loss: 3.026788 | lr:6.2470e-05 | norm 0.3342 | dt 337.04ms | 1555548.74 tokens/sec
Step 18283 | loss: 3.101627 | lr:6.2464e-05 | norm 0.2945 | dt 337.79ms | 1552113.30 tokens/sec
Step 18284 | loss: 3.082244 | lr:6.2457e-05 | norm 0.3106 | dt 339.01ms | 1546531.00 tokens/sec
Step 18285 | loss: 3.066303 | lr:6.2451e-05 | norm 0.2853 | dt 337.72ms | 1552456.27 tokens/sec
Step 18286 | loss: 3.033330 | lr:6.2445e-05 | norm 0.3004 | dt 337.96ms | 1551327.12 tokens/sec
Step 18287 | loss: 3.028111 | lr:6.2439e-05 | norm 0.2948 | dt 339.08ms | 1546188.47 tokens/sec
Step 18288 | loss: 3.095031 | lr:6.2433e-05 | norm 0.2859 | dt 338.73ms | 1547798.06 tokens/sec
Step 18289 | loss: 3.020592 | lr:6.2426e-05 | norm 0.2929 | dt 338.27ms | 1549891.50 tokens/sec
Step 18290 | loss: 2.983572 | lr:6.2420e-05 | norm 0.2816 | dt 337.40ms | 1553907.63 tokens/sec
Step 18291 | loss: 3.006866 | lr:6.2414e-05 | norm 0.2878 | dt 338.57ms | 1548537.04 tokens/sec
Step 18292 | loss: 2.982382 | lr:6.2408e-05 | norm 0.2940 | dt 338.13ms | 1550538.46 tokens/sec
Step 18293 | loss: 3.011003 | lr:6.2402e-05 | norm 0.3142 | dt 338.33ms | 1549640.29 tokens/sec
Step 18294 | loss: 3.022606 | lr:6.2396e-05 | norm 0.2813 | dt 337.31ms | 1554319.50 tokens/sec
Step 18295 | loss: 2.999800 | lr:6.2389e-05 | norm 0.3051 | dt 338.07ms | 1550808.55 tokens/sec
Step 18296 | loss: 2.989772 | lr:6.2383e-05 | norm 0.3057 | dt 338.59ms | 1548437.82 tokens/sec
Step 18297 | loss: 3.004223 | lr:6.2377e-05 | norm 0.2840 | dt 338.18ms | 1550308.90 tokens/sec
Step 18298 | loss: 2.996582 | lr:6.2371e-05 | norm 0.3121 | dt 337.49ms | 1553483.90 tokens/sec
Step 18299 | loss: 3.081294 | lr:6.2365e-05 | norm 0.3899 | dt 338.07ms | 1550826.04 tokens/sec
Step 18300 | loss: 3.001757 | lr:6.2359e-05 | norm 0.3181 | dt 339.01ms | 1546538.61 tokens/sec
Step 18301 | loss: 3.059841 | lr:6.2353e-05 | norm 0.3633 | dt 337.87ms | 1551728.87 tokens/sec
Step 18302 | loss: 3.097712 | lr:6.2347e-05 | norm 0.3272 | dt 337.94ms | 1551443.13 tokens/sec
Step 18303 | loss: 3.054065 | lr:6.2341e-05 | norm 0.3189 | dt 338.78ms | 1547570.40 tokens/sec
Step 18304 | loss: 3.018781 | lr:6.2335e-05 | norm 0.3295 | dt 338.22ms | 1550116.56 tokens/sec
Step 18305 | loss: 3.101779 | lr:6.2329e-05 | norm 0.3120 | dt 338.43ms | 1549166.50 tokens/sec
Step 18306 | loss: 3.139786 | lr:6.2322e-05 | norm 0.3534 | dt 339.09ms | 1546178.68 tokens/sec
Step 18307 | loss: 3.026270 | lr:6.2316e-05 | norm 0.2926 | dt 337.56ms | 1553164.61 tokens/sec
Step 18308 | loss: 3.091713 | lr:6.2310e-05 | norm 0.3163 | dt 338.90ms | 1547008.62 tokens/sec
Step 18309 | loss: 3.018929 | lr:6.2304e-05 | norm 0.3148 | dt 338.17ms | 1550349.34 tokens/sec
Step 18310 | loss: 3.128895 | lr:6.2298e-05 | norm 0.3202 | dt 337.85ms | 1551815.38 tokens/sec
Step 18311 | loss: 3.021033 | lr:6.2292e-05 | norm 0.2899 | dt 338.01ms | 1551121.40 tokens/sec
Step 18312 | loss: 3.060130 | lr:6.2286e-05 | norm 0.2956 | dt 338.54ms | 1548688.63 tokens/sec
Step 18313 | loss: 3.131606 | lr:6.2280e-05 | norm 0.2921 | dt 337.91ms | 1551575.59 tokens/sec
Step 18314 | loss: 3.094701 | lr:6.2274e-05 | norm 0.3136 | dt 337.28ms | 1554451.35 tokens/sec
Step 18315 | loss: 3.069047 | lr:6.2268e-05 | norm 0.2842 | dt 338.38ms | 1549406.63 tokens/sec
Step 18316 | loss: 3.089870 | lr:6.2262e-05 | norm 0.2993 | dt 338.47ms | 1549007.18 tokens/sec
Step 18317 | loss: 3.034055 | lr:6.2256e-05 | norm 0.3000 | dt 337.95ms | 1551381.84 tokens/sec
Step 18318 | loss: 3.100903 | lr:6.2250e-05 | norm 0.2867 | dt 338.11ms | 1550651.07 tokens/sec
Step 18319 | loss: 3.055954 | lr:6.2245e-05 | norm 0.3108 | dt 337.98ms | 1551230.82 tokens/sec
Step 18320 | loss: 3.037228 | lr:6.2239e-05 | norm 0.3038 | dt 338.59ms | 1548438.91 tokens/sec
Step 18321 | loss: 3.099632 | lr:6.2233e-05 | norm 0.2960 | dt 338.14ms | 1550500.19 tokens/sec
Step 18322 | loss: 3.077444 | lr:6.2227e-05 | norm 0.2982 | dt 339.33ms | 1545051.04 tokens/sec
Step 18323 | loss: 3.006667 | lr:6.2221e-05 | norm 0.3150 | dt 338.27ms | 1549912.25 tokens/sec
Step 18324 | loss: 3.039723 | lr:6.2215e-05 | norm 0.2986 | dt 338.66ms | 1548128.22 tokens/sec
Step 18325 | loss: 3.038256 | lr:6.2209e-05 | norm 0.3126 | dt 338.91ms | 1546997.74 tokens/sec
Step 18326 | loss: 2.958426 | lr:6.2203e-05 | norm 0.2904 | dt 338.16ms | 1550405.09 tokens/sec
Step 18327 | loss: 3.036600 | lr:6.2197e-05 | norm 0.3043 | dt 337.82ms | 1551982.94 tokens/sec
Step 18328 | loss: 2.937569 | lr:6.2191e-05 | norm 0.2820 | dt 338.56ms | 1548590.48 tokens/sec
Step 18329 | loss: 2.996509 | lr:6.2185e-05 | norm 0.2797 | dt 337.84ms | 1551883.27 tokens/sec
Step 18330 | loss: 3.021988 | lr:6.2180e-05 | norm 0.3039 | dt 338.25ms | 1549983.26 tokens/sec
Step 18331 | loss: 3.044074 | lr:6.2174e-05 | norm 0.2746 | dt 338.69ms | 1547990.91 tokens/sec
Step 18332 | loss: 2.995339 | lr:6.2168e-05 | norm 0.2717 | dt 1020.09ms | 513964.48 tokens/sec
Step 18333 | loss: 2.956291 | lr:6.2162e-05 | norm 0.2892 | dt 335.34ms | 1563456.36 tokens/sec
Step 18334 | loss: 3.031553 | lr:6.2156e-05 | norm 0.2875 | dt 338.22ms | 1550125.30 tokens/sec
Step 18335 | loss: 3.034755 | lr:6.2150e-05 | norm 0.2848 | dt 340.36ms | 1540397.18 tokens/sec
Step 18336 | loss: 3.048805 | lr:6.2145e-05 | norm 0.3019 | dt 336.45ms | 1558304.53 tokens/sec
Step 18337 | loss: 2.999965 | lr:6.2139e-05 | norm 0.3125 | dt 338.62ms | 1548285.19 tokens/sec
Step 18338 | loss: 3.009256 | lr:6.2133e-05 | norm 0.2934 | dt 337.89ms | 1551673.03 tokens/sec
Step 18339 | loss: 3.035601 | lr:6.2127e-05 | norm 0.2971 | dt 337.71ms | 1552476.00 tokens/sec
Step 18340 | loss: 3.045161 | lr:6.2121e-05 | norm 0.3184 | dt 338.90ms | 1547016.24 tokens/sec
Step 18341 | loss: 3.040964 | lr:6.2116e-05 | norm 0.3205 | dt 337.46ms | 1553615.60 tokens/sec
Step 18342 | loss: 3.076599 | lr:6.2110e-05 | norm 0.3183 | dt 337.66ms | 1552706.20 tokens/sec
Step 18343 | loss: 3.047404 | lr:6.2104e-05 | norm 0.3258 | dt 337.92ms | 1551529.61 tokens/sec
Step 18344 | loss: 3.058766 | lr:6.2098e-05 | norm 0.3002 | dt 339.31ms | 1545175.89 tokens/sec
Step 18345 | loss: 3.069837 | lr:6.2093e-05 | norm 0.2979 | dt 338.33ms | 1549652.30 tokens/sec
Step 18346 | loss: 3.085530 | lr:6.2087e-05 | norm 0.3194 | dt 338.14ms | 1550485.98 tokens/sec
Step 18347 | loss: 3.022343 | lr:6.2081e-05 | norm 0.3021 | dt 338.31ms | 1549709.09 tokens/sec
Step 18348 | loss: 3.062621 | lr:6.2075e-05 | norm 0.2758 | dt 338.44ms | 1549142.49 tokens/sec
Step 18349 | loss: 3.046272 | lr:6.2070e-05 | norm 0.3047 | dt 338.04ms | 1550962.77 tokens/sec
Step 18350 | loss: 3.015651 | lr:6.2064e-05 | norm 0.3207 | dt 338.29ms | 1549828.14 tokens/sec
Step 18351 | loss: 3.048768 | lr:6.2058e-05 | norm 0.2720 | dt 337.56ms | 1553167.90 tokens/sec
Step 18352 | loss: 3.058813 | lr:6.2053e-05 | norm 0.2901 | dt 337.68ms | 1552616.30 tokens/sec
Step 18353 | loss: 3.071622 | lr:6.2047e-05 | norm 0.3221 | dt 337.99ms | 1551188.14 tokens/sec
Step 18354 | loss: 3.056170 | lr:6.2041e-05 | norm 0.2902 | dt 338.60ms | 1548396.38 tokens/sec
Step 18355 | loss: 3.096070 | lr:6.2036e-05 | norm 0.3094 | dt 337.52ms | 1553348.92 tokens/sec
Step 18356 | loss: 3.049170 | lr:6.2030e-05 | norm 0.2989 | dt 338.48ms | 1548935.17 tokens/sec
Step 18357 | loss: 3.068761 | lr:6.2024e-05 | norm 0.2919 | dt 338.16ms | 1550402.90 tokens/sec
Step 18358 | loss: 3.020322 | lr:6.2019e-05 | norm 0.2934 | dt 337.99ms | 1551178.29 tokens/sec
Step 18359 | loss: 2.999551 | lr:6.2013e-05 | norm 0.2860 | dt 338.31ms | 1549747.32 tokens/sec
Step 18360 | loss: 3.048263 | lr:6.2007e-05 | norm 0.2835 | dt 338.16ms | 1550424.76 tokens/sec
Step 18361 | loss: 3.002201 | lr:6.2002e-05 | norm 0.3042 | dt 338.06ms | 1550888.39 tokens/sec
Step 18362 | loss: 3.003260 | lr:6.1996e-05 | norm 0.3098 | dt 338.49ms | 1548908.98 tokens/sec
Step 18363 | loss: 2.998101 | lr:6.1991e-05 | norm 0.2902 | dt 338.01ms | 1551099.52 tokens/sec
Step 18364 | loss: 3.015992 | lr:6.1985e-05 | norm 0.2882 | dt 339.30ms | 1545214.98 tokens/sec
Step 18365 | loss: 3.022374 | lr:6.1979e-05 | norm 0.2863 | dt 338.37ms | 1549430.65 tokens/sec
Step 18366 | loss: 2.977940 | lr:6.1974e-05 | norm 0.2878 | dt 339.15ms | 1545869.99 tokens/sec
Step 18367 | loss: 3.003738 | lr:6.1968e-05 | norm 0.2626 | dt 338.75ms | 1547714.18 tokens/sec
Step 18368 | loss: 2.963757 | lr:6.1963e-05 | norm 0.2873 | dt 338.57ms | 1548527.23 tokens/sec
Step 18369 | loss: 3.016042 | lr:6.1957e-05 | norm 0.2803 | dt 339.09ms | 1546169.99 tokens/sec
Step 18370 | loss: 3.072087 | lr:6.1952e-05 | norm 0.2993 | dt 338.88ms | 1547123.99 tokens/sec
Step 18371 | loss: 3.074616 | lr:6.1946e-05 | norm 0.2895 | dt 338.67ms | 1548080.27 tokens/sec
Step 18372 | loss: 3.065628 | lr:6.1940e-05 | norm 0.2856 | dt 338.79ms | 1547521.39 tokens/sec
Step 18373 | loss: 3.092106 | lr:6.1935e-05 | norm 0.3441 | dt 338.11ms | 1550656.54 tokens/sec
Step 18374 | loss: 3.026039 | lr:6.1929e-05 | norm 0.3117 | dt 338.05ms | 1550917.92 tokens/sec
Step 18375 | loss: 3.087801 | lr:6.1924e-05 | norm 0.3013 | dt 337.87ms | 1551766.10 tokens/sec
Step 18376 | loss: 3.109178 | lr:6.1918e-05 | norm 0.3331 | dt 337.96ms | 1551330.40 tokens/sec
Step 18377 | loss: 3.087734 | lr:6.1913e-05 | norm 0.3212 | dt 339.26ms | 1545409.36 tokens/sec
Step 18378 | loss: 3.030761 | lr:6.1907e-05 | norm 0.3019 | dt 337.84ms | 1551863.56 tokens/sec
Step 18379 | loss: 3.043149 | lr:6.1902e-05 | norm 0.2983 | dt 337.35ms | 1554119.58 tokens/sec
Step 18380 | loss: 3.087071 | lr:6.1896e-05 | norm 0.3170 | dt 338.67ms | 1548068.28 tokens/sec
Step 18381 | loss: 3.023331 | lr:6.1891e-05 | norm 0.3177 | dt 338.46ms | 1549037.73 tokens/sec
Step 18382 | loss: 3.078576 | lr:6.1886e-05 | norm 0.3116 | dt 339.95ms | 1542235.89 tokens/sec
Step 18383 | loss: 3.041570 | lr:6.1880e-05 | norm 0.3130 | dt 339.55ms | 1544072.49 tokens/sec
Step 18384 | loss: 3.106606 | lr:6.1875e-05 | norm 0.3221 | dt 338.98ms | 1546640.86 tokens/sec
Step 18385 | loss: 3.047940 | lr:6.1869e-05 | norm 0.3269 | dt 339.22ms | 1545565.77 tokens/sec
Step 18386 | loss: 3.044409 | lr:6.1864e-05 | norm 0.3045 | dt 338.14ms | 1550513.31 tokens/sec
Step 18387 | loss: 3.092293 | lr:6.1858e-05 | norm 0.2955 | dt 338.10ms | 1550709.03 tokens/sec
Step 18388 | loss: 3.024728 | lr:6.1853e-05 | norm 0.3264 | dt 338.47ms | 1549009.36 tokens/sec
Step 18389 | loss: 3.012136 | lr:6.1848e-05 | norm 0.2925 | dt 337.98ms | 1551262.55 tokens/sec
Step 18390 | loss: 3.098686 | lr:6.1842e-05 | norm 0.2978 | dt 337.37ms | 1554065.76 tokens/sec
Step 18391 | loss: 3.041564 | lr:6.1837e-05 | norm 0.2856 | dt 338.47ms | 1548987.54 tokens/sec
Step 18392 | loss: 3.061089 | lr:6.1831e-05 | norm 0.3374 | dt 339.08ms | 1546216.73 tokens/sec
Step 18393 | loss: 3.009879 | lr:6.1826e-05 | norm 0.2950 | dt 337.96ms | 1551339.16 tokens/sec
Step 18394 | loss: 3.029046 | lr:6.1821e-05 | norm 0.2910 | dt 338.88ms | 1547098.96 tokens/sec
Step 18395 | loss: 3.078426 | lr:6.1815e-05 | norm 0.3132 | dt 340.17ms | 1541268.45 tokens/sec
Step 18396 | loss: 3.034910 | lr:6.1810e-05 | norm 0.3079 | dt 338.47ms | 1548998.45 tokens/sec
Step 18397 | loss: 3.066222 | lr:6.1805e-05 | norm 0.3150 | dt 338.26ms | 1549965.78 tokens/sec
Step 18398 | loss: 2.984865 | lr:6.1799e-05 | norm 0.2939 | dt 338.59ms | 1548447.63 tokens/sec
Step 18399 | loss: 3.004228 | lr:6.1794e-05 | norm 0.2792 | dt 338.87ms | 1547179.51 tokens/sec
Step 18400 | loss: 2.948905 | lr:6.1789e-05 | norm 0.2891 | dt 338.92ms | 1546953.12 tokens/sec
Step 18401 | loss: 3.036231 | lr:6.1783e-05 | norm 0.2948 | dt 338.38ms | 1549415.37 tokens/sec
Step 18402 | loss: 2.983927 | lr:6.1778e-05 | norm 0.2823 | dt 338.54ms | 1548684.27 tokens/sec
Step 18403 | loss: 2.989033 | lr:6.1773e-05 | norm 0.2977 | dt 338.56ms | 1548598.11 tokens/sec
Step 18404 | loss: 2.969358 | lr:6.1768e-05 | norm 0.2875 | dt 338.53ms | 1548711.54 tokens/sec
Step 18405 | loss: 2.985462 | lr:6.1762e-05 | norm 0.2830 | dt 338.02ms | 1551063.41 tokens/sec
Step 18406 | loss: 3.072935 | lr:6.1757e-05 | norm 0.3019 | dt 338.99ms | 1546626.72 tokens/sec
Step 18407 | loss: 3.110363 | lr:6.1752e-05 | norm 0.5228 | dt 339.56ms | 1544021.53 tokens/sec
Step 18408 | loss: 3.027061 | lr:6.1746e-05 | norm 0.3124 | dt 338.51ms | 1548803.16 tokens/sec
Step 18409 | loss: 3.045259 | lr:6.1741e-05 | norm 0.3254 | dt 338.73ms | 1547789.34 tokens/sec
Step 18410 | loss: 3.108873 | lr:6.1736e-05 | norm 0.3409 | dt 339.22ms | 1545591.85 tokens/sec
Step 18411 | loss: 2.992684 | lr:6.1731e-05 | norm 0.3035 | dt 337.93ms | 1551459.55 tokens/sec
Step 18412 | loss: 2.965660 | lr:6.1726e-05 | norm 0.2863 | dt 339.19ms | 1545704.83 tokens/sec
Step 18413 | loss: 3.035435 | lr:6.1720e-05 | norm 0.2980 | dt 339.49ms | 1544353.34 tokens/sec
Step 18414 | loss: 3.027832 | lr:6.1715e-05 | norm 0.3006 | dt 338.57ms | 1548517.41 tokens/sec
Step 18415 | loss: 3.190045 | lr:6.1710e-05 | norm 0.4117 | dt 338.67ms | 1548082.45 tokens/sec
Step 18416 | loss: 3.051073 | lr:6.1705e-05 | norm 0.3346 | dt 338.67ms | 1548100.98 tokens/sec
Step 18417 | loss: 3.052872 | lr:6.1700e-05 | norm 0.3410 | dt 339.11ms | 1546056.93 tokens/sec
Step 18418 | loss: 3.040159 | lr:6.1694e-05 | norm 0.3180 | dt 338.53ms | 1548727.90 tokens/sec
Step 18419 | loss: 3.039893 | lr:6.1689e-05 | norm 0.3066 | dt 338.94ms | 1546862.80 tokens/sec
Step 18420 | loss: 3.048444 | lr:6.1684e-05 | norm 0.3096 | dt 338.86ms | 1547192.57 tokens/sec
Step 18421 | loss: 3.002765 | lr:6.1679e-05 | norm 0.2951 | dt 337.85ms | 1551849.32 tokens/sec
Step 18422 | loss: 3.065704 | lr:6.1674e-05 | norm 0.3273 | dt 337.94ms | 1551402.64 tokens/sec
Step 18423 | loss: 3.016734 | lr:6.1669e-05 | norm 0.3220 | dt 338.07ms | 1550821.67 tokens/sec
Step 18424 | loss: 3.076915 | lr:6.1664e-05 | norm 0.3007 | dt 337.53ms | 1553323.69 tokens/sec
Step 18425 | loss: 3.094838 | lr:6.1658e-05 | norm 0.3027 | dt 338.63ms | 1548275.37 tokens/sec
Step 18426 | loss: 3.018341 | lr:6.1653e-05 | norm 0.2951 | dt 338.10ms | 1550692.63 tokens/sec
Step 18427 | loss: 3.086694 | lr:6.1648e-05 | norm 0.2964 | dt 337.73ms | 1552397.09 tokens/sec
Step 18428 | loss: 3.078835 | lr:6.1643e-05 | norm 0.2855 | dt 337.75ms | 1552284.22 tokens/sec
Step 18429 | loss: 3.037188 | lr:6.1638e-05 | norm 0.3103 | dt 938.53ms | 558624.68 tokens/sec
Step 18430 | loss: 3.118360 | lr:6.1633e-05 | norm 0.3030 | dt 336.54ms | 1557887.23 tokens/sec
Step 18431 | loss: 3.000050 | lr:6.1628e-05 | norm 0.2964 | dt 338.07ms | 1550828.23 tokens/sec
Step 18432 | loss: 3.050719 | lr:6.1623e-05 | norm 0.2974 | dt 337.71ms | 1552467.23 tokens/sec
Step 18433 | loss: 3.037358 | lr:6.1618e-05 | norm 0.3013 | dt 337.48ms | 1553557.43 tokens/sec
Step 18434 | loss: 3.034512 | lr:6.1613e-05 | norm 0.2944 | dt 337.55ms | 1553212.87 tokens/sec
Step 18435 | loss: 3.014452 | lr:6.1608e-05 | norm 0.2851 | dt 337.47ms | 1553595.84 tokens/sec
Step 18436 | loss: 2.989759 | lr:6.1603e-05 | norm 0.2932 | dt 337.51ms | 1553397.20 tokens/sec
Step 18437 | loss: 3.005044 | lr:6.1598e-05 | norm 0.2924 | dt 337.85ms | 1551833.99 tokens/sec
Step 18438 | loss: 2.973407 | lr:6.1593e-05 | norm 0.2754 | dt 337.51ms | 1553411.47 tokens/sec
Step 18439 | loss: 3.024885 | lr:6.1588e-05 | norm 0.3024 | dt 337.78ms | 1552177.94 tokens/sec
Step 18440 | loss: 2.993622 | lr:6.1583e-05 | norm 0.2880 | dt 338.98ms | 1546658.27 tokens/sec
Step 18441 | loss: 3.033519 | lr:6.1578e-05 | norm 0.3238 | dt 337.56ms | 1553184.35 tokens/sec
Step 18442 | loss: 3.032409 | lr:6.1573e-05 | norm 0.3559 | dt 338.90ms | 1547029.30 tokens/sec
Step 18443 | loss: 3.053021 | lr:6.1568e-05 | norm 0.3025 | dt 338.00ms | 1551131.25 tokens/sec
Step 18444 | loss: 3.063827 | lr:6.1563e-05 | norm 0.3104 | dt 338.27ms | 1549922.09 tokens/sec
Step 18445 | loss: 3.008658 | lr:6.1558e-05 | norm 0.3040 | dt 338.86ms | 1547191.48 tokens/sec
Step 18446 | loss: 3.042756 | lr:6.1553e-05 | norm 0.3423 | dt 338.30ms | 1549783.36 tokens/sec
Step 18447 | loss: 2.977528 | lr:6.1548e-05 | norm 0.2926 | dt 338.99ms | 1546618.02 tokens/sec
Step 18448 | loss: 3.056290 | lr:6.1543e-05 | norm 0.3212 | dt 338.09ms | 1550750.58 tokens/sec
Step 18449 | loss: 3.013100 | lr:6.1538e-05 | norm 0.3321 | dt 338.82ms | 1547409.23 tokens/sec
Step 18450 | loss: 3.046872 | lr:6.1533e-05 | norm 0.3283 | dt 338.13ms | 1550573.44 tokens/sec
Step 18451 | loss: 3.119798 | lr:6.1528e-05 | norm 0.3252 | dt 339.28ms | 1545315.97 tokens/sec
Step 18452 | loss: 3.052212 | lr:6.1523e-05 | norm 0.3149 | dt 337.86ms | 1551806.62 tokens/sec
Step 18453 | loss: 3.087318 | lr:6.1518e-05 | norm 0.3159 | dt 340.03ms | 1541899.58 tokens/sec
Step 18454 | loss: 3.073812 | lr:6.1513e-05 | norm 0.3124 | dt 338.64ms | 1548214.33 tokens/sec
Step 18455 | loss: 3.059122 | lr:6.1509e-05 | norm 0.2963 | dt 338.52ms | 1548772.62 tokens/sec
Step 18456 | loss: 3.057426 | lr:6.1504e-05 | norm 0.3227 | dt 338.23ms | 1550070.67 tokens/sec
Step 18457 | loss: 3.064069 | lr:6.1499e-05 | norm 0.3358 | dt 337.94ms | 1551431.09 tokens/sec
Step 18458 | loss: 3.021747 | lr:6.1494e-05 | norm 0.3076 | dt 338.54ms | 1548665.73 tokens/sec
Step 18459 | loss: 3.056801 | lr:6.1489e-05 | norm 0.3093 | dt 339.23ms | 1545515.81 tokens/sec
Step 18460 | loss: 3.117531 | lr:6.1484e-05 | norm 0.3021 | dt 338.25ms | 1549992.00 tokens/sec
Step 18461 | loss: 3.024927 | lr:6.1479e-05 | norm 0.3185 | dt 339.03ms | 1546450.52 tokens/sec
Step 18462 | loss: 3.087723 | lr:6.1475e-05 | norm 0.3052 | dt 338.37ms | 1549432.84 tokens/sec
Step 18463 | loss: 3.071981 | lr:6.1470e-05 | norm 0.2890 | dt 338.54ms | 1548687.54 tokens/sec
Step 18464 | loss: 3.107787 | lr:6.1465e-05 | norm 0.3068 | dt 337.90ms | 1551620.47 tokens/sec
Step 18465 | loss: 3.043398 | lr:6.1460e-05 | norm 0.3088 | dt 337.60ms | 1552968.27 tokens/sec
Step 18466 | loss: 3.025998 | lr:6.1455e-05 | norm 0.2825 | dt 338.56ms | 1548576.30 tokens/sec
Step 18467 | loss: 3.008243 | lr:6.1451e-05 | norm 0.3123 | dt 337.96ms | 1551342.44 tokens/sec
Step 18468 | loss: 3.019278 | lr:6.1446e-05 | norm 0.3034 | dt 337.45ms | 1553699.03 tokens/sec
Step 18469 | loss: 3.015584 | lr:6.1441e-05 | norm 0.2940 | dt 338.82ms | 1547391.81 tokens/sec
Step 18470 | loss: 3.020067 | lr:6.1436e-05 | norm 0.2972 | dt 339.15ms | 1545897.16 tokens/sec
Step 18471 | loss: 3.034890 | lr:6.1432e-05 | norm 0.2905 | dt 338.04ms | 1550957.30 tokens/sec
Step 18472 | loss: 3.016187 | lr:6.1427e-05 | norm 0.2827 | dt 337.66ms | 1552714.97 tokens/sec
Step 18473 | loss: 2.995698 | lr:6.1422e-05 | norm 0.2875 | dt 339.10ms | 1546124.33 tokens/sec
Step 18474 | loss: 2.995067 | lr:6.1417e-05 | norm 0.2764 | dt 339.34ms | 1545032.59 tokens/sec
Step 18475 | loss: 2.953897 | lr:6.1413e-05 | norm 0.2811 | dt 337.98ms | 1551261.46 tokens/sec
Step 18476 | loss: 3.027497 | lr:6.1408e-05 | norm 0.2814 | dt 337.83ms | 1551942.42 tokens/sec
Step 18477 | loss: 2.994795 | lr:6.1403e-05 | norm 0.3104 | dt 338.20ms | 1550237.86 tokens/sec
Step 18478 | loss: 3.044667 | lr:6.1398e-05 | norm 0.3179 | dt 338.82ms | 1547371.12 tokens/sec
Step 18479 | loss: 3.086349 | lr:6.1394e-05 | norm 0.2865 | dt 338.34ms | 1549576.96 tokens/sec
Step 18480 | loss: 3.065897 | lr:6.1389e-05 | norm 0.3124 | dt 339.05ms | 1546325.46 tokens/sec
Step 18481 | loss: 3.051514 | lr:6.1384e-05 | norm 0.3056 | dt 339.03ms | 1546438.56 tokens/sec
Step 18482 | loss: 3.035947 | lr:6.1380e-05 | norm 0.2891 | dt 338.77ms | 1547634.66 tokens/sec
Step 18483 | loss: 2.996279 | lr:6.1375e-05 | norm 0.2918 | dt 339.72ms | 1543310.68 tokens/sec
Step 18484 | loss: 3.055087 | lr:6.1370e-05 | norm 0.2928 | dt 339.15ms | 1545869.99 tokens/sec
Step 18485 | loss: 3.136418 | lr:6.1366e-05 | norm 0.3473 | dt 339.80ms | 1542942.50 tokens/sec
Step 18486 | loss: 3.056700 | lr:6.1361e-05 | norm 0.3203 | dt 338.83ms | 1547346.08 tokens/sec
Step 18487 | loss: 3.080159 | lr:6.1356e-05 | norm 0.2947 | dt 339.49ms | 1544353.34 tokens/sec
Step 18488 | loss: 3.083087 | lr:6.1352e-05 | norm 0.2931 | dt 339.23ms | 1545506.03 tokens/sec
Step 18489 | loss: 3.102028 | lr:6.1347e-05 | norm 0.3042 | dt 338.88ms | 1547121.82 tokens/sec
Step 18490 | loss: 3.056867 | lr:6.1343e-05 | norm 0.3197 | dt 338.31ms | 1549722.20 tokens/sec
Step 18491 | loss: 3.064013 | lr:6.1338e-05 | norm 0.2983 | dt 339.37ms | 1544904.51 tokens/sec
Step 18492 | loss: 3.054858 | lr:6.1333e-05 | norm 0.2946 | dt 338.14ms | 1550517.68 tokens/sec
Step 18493 | loss: 3.075763 | lr:6.1329e-05 | norm 0.3083 | dt 338.94ms | 1546867.16 tokens/sec
Step 18494 | loss: 3.004873 | lr:6.1324e-05 | norm 0.3121 | dt 339.37ms | 1544904.51 tokens/sec
Step 18495 | loss: 3.079745 | lr:6.1320e-05 | norm 0.2784 | dt 338.91ms | 1546962.92 tokens/sec
Step 18496 | loss: 3.018307 | lr:6.1315e-05 | norm 0.2856 | dt 337.99ms | 1551176.11 tokens/sec
Step 18497 | loss: 3.038653 | lr:6.1311e-05 | norm 0.2811 | dt 338.44ms | 1549150.13 tokens/sec
Step 18498 | loss: 3.036611 | lr:6.1306e-05 | norm 0.2901 | dt 338.10ms | 1550710.12 tokens/sec
Step 18499 | loss: 3.041841 | lr:6.1302e-05 | norm 0.2952 | dt 337.80ms | 1552047.57 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 18500: 3.0823
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3073/10042=0.3060



ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 3: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but my first language isn't always what I'm looking for. My favorite way is to use a dialect of Korean that
rank 3 sample 0 >Hello, I'm a language model, and I'm not even trying to express anything, I'm making some good comparisons between it and real humans, so,
rank 5 sample 1 >Hello, I'm a language model, my wife is not that well, so I didn't have time to get it published in the newspaper, to be able
rank 3 sample 1 >Hello, I'm a language model, and you've been reading this whole book, you know, I've been reading it for a while. I love how
rank 5 sample 2 >Hello, I'm a language model, so I love to listen to your writing. If you want to use it in the business world, then I recommend learning
rank 3 sample 2 >Hello, I'm a language model, and it has been more than twenty years since I first came along.
What if I want to write a full paper
rank 5 sample 3 >Hello, I'm a language model, language designer. So, the question is: What if my language is not correct? And I had a question, that


rank 3 sample 3 >Hello, I'm a language model, and have been reading about how a language can be defined to behave in ways that conform to other languages". I think it




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I like to learn to code. Sometimes I find myself going through one of those hoops - so many languages, but
rank 7 sample 1 >Hello, I'm a language model, have a lot of fun, you all understand what I get?
I'm curious about your knowledge
I have the
rank 7 sample 2 >Hello, I'm a language model, and I want to explore the various things you can do with a language with a native speaker. I know, we all
rank 7 sample 3 >Hello, I'm a language model, so this is a nice article. First, for the sake of simplicity, I'll just say one thing: in many




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, and my first attempt at "language modelling".
I was just trying to get through the first part of building a "
rank 2 sample 1 >Hello, I'm a language model, so I couldn't explain this to all anyone. Not because of what was said, but because I felt it was more
rank 2 sample 2 >Hello, I'm a language model, so I'll show you a couple of really nice examples of that language type that can be found everywhere (like [the
rank 2 sample 3 >Hello, I'm a language model, but a high school English teacher. I really like her writing on the computer. But when talking with my class about this




ddp_rank 4: ####### Printing generated samples ####### 



ddp_rank 1: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I'd like to tell you a more in-depth story about some of the more useful lessons on using Python in
rank 4 sample 1 >Hello, I'm a language model, for me, knowing English and how to understand people is easier than most people imagine. There's no question, most of
rank 1 sample 0 >Hello, I'm a language model, and in the future, we'll run out of memory or get a lot of memory and lose a lot of memory.
rank 4 sample 2 >Hello, I'm a language model, I wanna find myself, when I have an input, I'll send it to the right speaker. A lot has been
rank 1 sample 1 >Hello, I'm a language model, a computer programmer and a computer scientist. I'm in grad school, and I get used to speaking in a very formal
rank 4 sample 3 >Hello, I'm a language model, and the good question is: what has happened, at the other hand, with the language of mathematics, which is a


rank 1 sample 2 >Hello, I'm a language model, but even more so than the English language. I'm trying to teach a basic language to children by having a teacher look
rank 1 sample 3 >Hello, I'm a language model, and I'm doing some translation work. Have you ever started? Any language software? Anyone? What language model can you




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I don't want to go around writing this article. So you already read this post, you know it's pretty
rank 0 sample 1 >Hello, I'm a language model, so how do I get my own writing style and set goals for it?
- What if my grammar was one of
rank 0 sample 2 >Hello, I'm a language model, so I didn't just tell you, "Go to the web" I mean, I'm going to tell you more
rank 0 sample 3 >Hello, I'm a language model, so a lot of people just say 'Hello...' and it does what they sound like. I love that it was




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, if you're trying to know what is a language in a certain way. In that case, I'm not going to
rank 6 sample 1 >Hello, I'm a language model, and I know that people who want to create a new language from scratch and create it with a set of languages. This
rank 6 sample 2 >Hello, I'm a language model, but if I were to say something more concrete that I would like to share my thoughts on it, I would make it
rank 6 sample 3 >Hello, I'm a language model, and so do you, I just wanted to share with you a tip on what you might still want to check out now


Step 18500 | loss: 3.048924 | lr:6.1297e-05 | norm 0.2873 | dt 12512.82ms | 41900.07 tokens/sec
Step 18501 | loss: 3.023844 | lr:6.1292e-05 | norm 0.2826 | dt 336.34ms | 1558786.14 tokens/sec
Step 18502 | loss: 2.986500 | lr:6.1288e-05 | norm 0.3008 | dt 337.06ms | 1555460.72 tokens/sec
Step 18503 | loss: 3.024475 | lr:6.1283e-05 | norm 0.3047 | dt 336.52ms | 1557952.35 tokens/sec
Step 18504 | loss: 3.026047 | lr:6.1279e-05 | norm 0.2927 | dt 336.63ms | 1557473.46 tokens/sec
Step 18505 | loss: 3.006441 | lr:6.1274e-05 | norm 0.2859 | dt 337.13ms | 1555169.21 tokens/sec
Step 18506 | loss: 3.068995 | lr:6.1270e-05 | norm 0.2999 | dt 336.30ms | 1558975.11 tokens/sec
Step 18507 | loss: 2.994847 | lr:6.1266e-05 | norm 0.2944 | dt 337.59ms | 1553025.30 tokens/sec
Step 18508 | loss: 2.958591 | lr:6.1261e-05 | norm 0.2811 | dt 336.88ms | 1556295.15 tokens/sec
Step 18509 | loss: 2.984264 | lr:6.1257e-05 | norm 0.2839 | dt 337.35ms | 1554131.66 tokens/sec
Step 18510 | loss: 2.965504 | lr:6.1252e-05 | norm 0.2900 | dt 337.44ms | 1553735.25 tokens/sec
Step 18511 | loss: 2.925025 | lr:6.1248e-05 | norm 0.2940 | dt 337.25ms | 1554595.31 tokens/sec
Step 18512 | loss: 2.973620 | lr:6.1243e-05 | norm 0.2867 | dt 336.96ms | 1555919.65 tokens/sec
Step 18513 | loss: 3.037723 | lr:6.1239e-05 | norm 0.3021 | dt 337.27ms | 1554523.88 tokens/sec
Step 18514 | loss: 3.076012 | lr:6.1234e-05 | norm 0.2977 | dt 338.89ms | 1547090.25 tokens/sec
Step 18515 | loss: 3.069156 | lr:6.1230e-05 | norm 0.3128 | dt 338.52ms | 1548747.53 tokens/sec
Step 18516 | loss: 3.063838 | lr:6.1226e-05 | norm 0.2977 | dt 337.58ms | 1553080.14 tokens/sec
Step 18517 | loss: 3.019248 | lr:6.1221e-05 | norm 0.3121 | dt 337.28ms | 1554437.07 tokens/sec
Step 18518 | loss: 3.035917 | lr:6.1217e-05 | norm 0.2965 | dt 339.91ms | 1542440.34 tokens/sec
Step 18519 | loss: 3.069314 | lr:6.1212e-05 | norm 0.3222 | dt 337.47ms | 1553566.21 tokens/sec
Step 18520 | loss: 3.066085 | lr:6.1208e-05 | norm 0.3043 | dt 338.81ms | 1547451.70 tokens/sec
Step 18521 | loss: 3.130192 | lr:6.1204e-05 | norm 0.3407 | dt 677.77ms | 773542.95 tokens/sec
Step 18522 | loss: 3.049931 | lr:6.1199e-05 | norm 0.3083 | dt 335.86ms | 1561029.10 tokens/sec
Step 18523 | loss: 3.105994 | lr:6.1195e-05 | norm 0.3179 | dt 338.14ms | 1550522.06 tokens/sec
Step 18524 | loss: 3.052505 | lr:6.1191e-05 | norm 0.3280 | dt 338.04ms | 1550980.27 tokens/sec
Step 18525 | loss: 3.083166 | lr:6.1186e-05 | norm 0.2955 | dt 337.72ms | 1552420.10 tokens/sec
Step 18526 | loss: 3.078799 | lr:6.1182e-05 | norm 0.3035 | dt 338.51ms | 1548823.89 tokens/sec
Step 18527 | loss: 3.082192 | lr:6.1178e-05 | norm 0.3127 | dt 337.36ms | 1554076.74 tokens/sec
Step 18528 | loss: 3.055521 | lr:6.1173e-05 | norm 0.3175 | dt 337.79ms | 1552126.44 tokens/sec
Step 18529 | loss: 3.035163 | lr:6.1169e-05 | norm 0.3017 | dt 338.37ms | 1549436.11 tokens/sec
Step 18530 | loss: 3.077398 | lr:6.1165e-05 | norm 0.2959 | dt 338.32ms | 1549682.88 tokens/sec
Step 18531 | loss: 3.073209 | lr:6.1161e-05 | norm 0.2964 | dt 337.99ms | 1551180.48 tokens/sec
Step 18532 | loss: 3.118087 | lr:6.1156e-05 | norm 0.3252 | dt 337.79ms | 1552091.39 tokens/sec
Step 18533 | loss: 3.064517 | lr:6.1152e-05 | norm 0.2929 | dt 340.10ms | 1541559.09 tokens/sec
Step 18534 | loss: 3.089545 | lr:6.1148e-05 | norm 0.3276 | dt 339.52ms | 1544214.53 tokens/sec
Step 18535 | loss: 3.016894 | lr:6.1144e-05 | norm 0.2911 | dt 337.83ms | 1551939.13 tokens/sec
Step 18536 | loss: 3.015734 | lr:6.1139e-05 | norm 0.2855 | dt 338.42ms | 1549201.42 tokens/sec
Step 18537 | loss: 2.989482 | lr:6.1135e-05 | norm 0.2955 | dt 338.29ms | 1549807.39 tokens/sec
Step 18538 | loss: 3.042622 | lr:6.1131e-05 | norm 0.2801 | dt 337.96ms | 1551349.01 tokens/sec
Step 18539 | loss: 3.008150 | lr:6.1127e-05 | norm 0.2839 | dt 337.39ms | 1553943.86 tokens/sec
Step 18540 | loss: 2.988752 | lr:6.1122e-05 | norm 0.2980 | dt 340.05ms | 1541816.33 tokens/sec
Step 18541 | loss: 3.010994 | lr:6.1118e-05 | norm 0.2867 | dt 337.92ms | 1551500.05 tokens/sec
Step 18542 | loss: 2.977978 | lr:6.1114e-05 | norm 0.3153 | dt 337.97ms | 1551275.68 tokens/sec
Step 18543 | loss: 3.007349 | lr:6.1110e-05 | norm 0.3315 | dt 338.53ms | 1548698.45 tokens/sec
Step 18544 | loss: 2.994035 | lr:6.1106e-05 | norm 0.2726 | dt 338.29ms | 1549828.14 tokens/sec
Step 18545 | loss: 2.968984 | lr:6.1101e-05 | norm 0.3023 | dt 338.29ms | 1549819.40 tokens/sec
Step 18546 | loss: 2.975303 | lr:6.1097e-05 | norm 0.3131 | dt 337.97ms | 1551280.06 tokens/sec
Step 18547 | loss: 3.015936 | lr:6.1093e-05 | norm 0.2788 | dt 338.75ms | 1547694.57 tokens/sec
Step 18548 | loss: 3.011421 | lr:6.1089e-05 | norm 0.3188 | dt 337.62ms | 1552888.21 tokens/sec
Step 18549 | loss: 3.054992 | lr:6.1085e-05 | norm 0.3232 | dt 338.16ms | 1550430.23 tokens/sec
Step 18550 | loss: 3.220179 | lr:6.1081e-05 | norm 0.3590 | dt 339.22ms | 1545584.24 tokens/sec
Step 18551 | loss: 3.078819 | lr:6.1077e-05 | norm 0.3302 | dt 339.33ms | 1545083.61 tokens/sec
Step 18552 | loss: 3.064063 | lr:6.1072e-05 | norm 0.3130 | dt 337.70ms | 1552519.84 tokens/sec
Step 18553 | loss: 3.009711 | lr:6.1068e-05 | norm 0.3216 | dt 338.04ms | 1550966.05 tokens/sec
Step 18554 | loss: 3.072759 | lr:6.1064e-05 | norm 0.3400 | dt 338.77ms | 1547617.23 tokens/sec
Step 18555 | loss: 3.025641 | lr:6.1060e-05 | norm 0.3158 | dt 337.97ms | 1551282.25 tokens/sec
Step 18556 | loss: 3.112116 | lr:6.1056e-05 | norm 0.3107 | dt 338.47ms | 1548976.63 tokens/sec
Step 18557 | loss: 3.062047 | lr:6.1052e-05 | norm 0.3482 | dt 339.34ms | 1545024.99 tokens/sec
Step 18558 | loss: 3.162760 | lr:6.1048e-05 | norm 0.3163 | dt 338.48ms | 1548949.35 tokens/sec
Step 18559 | loss: 3.031251 | lr:6.1044e-05 | norm 0.3129 | dt 338.71ms | 1547902.65 tokens/sec
Step 18560 | loss: 3.013830 | lr:6.1040e-05 | norm 0.3074 | dt 337.80ms | 1552059.62 tokens/sec
Step 18561 | loss: 3.064424 | lr:6.1036e-05 | norm 0.2911 | dt 337.93ms | 1551457.36 tokens/sec
Step 18562 | loss: 3.021520 | lr:6.1032e-05 | norm 0.3134 | dt 337.68ms | 1552622.88 tokens/sec
Step 18563 | loss: 3.038728 | lr:6.1028e-05 | norm 0.3245 | dt 337.78ms | 1552168.08 tokens/sec
Step 18564 | loss: 3.011970 | lr:6.1024e-05 | norm 0.3126 | dt 338.67ms | 1548088.99 tokens/sec
Step 18565 | loss: 3.023941 | lr:6.1020e-05 | norm 0.2921 | dt 338.87ms | 1547147.94 tokens/sec
Step 18566 | loss: 3.046905 | lr:6.1016e-05 | norm 0.3360 | dt 338.13ms | 1550572.35 tokens/sec
Step 18567 | loss: 3.095748 | lr:6.1012e-05 | norm 0.3024 | dt 337.77ms | 1552184.51 tokens/sec
Step 18568 | loss: 3.063367 | lr:6.1008e-05 | norm 0.2969 | dt 338.17ms | 1550365.74 tokens/sec
Step 18569 | loss: 3.054769 | lr:6.1004e-05 | norm 0.2994 | dt 338.70ms | 1547952.77 tokens/sec
Step 18570 | loss: 3.019313 | lr:6.1000e-05 | norm 0.2806 | dt 338.19ms | 1550282.67 tokens/sec
Step 18571 | loss: 2.945386 | lr:6.0996e-05 | norm 0.3097 | dt 340.39ms | 1540273.10 tokens/sec
Step 18572 | loss: 3.120183 | lr:6.0992e-05 | norm 0.3343 | dt 338.34ms | 1549579.14 tokens/sec
Step 18573 | loss: 3.014503 | lr:6.0988e-05 | norm 0.3204 | dt 337.74ms | 1552321.47 tokens/sec
Step 18574 | loss: 2.962700 | lr:6.0984e-05 | norm 0.3021 | dt 337.70ms | 1552532.99 tokens/sec
Step 18575 | loss: 3.004377 | lr:6.0980e-05 | norm 0.2915 | dt 337.73ms | 1552371.88 tokens/sec
Step 18576 | loss: 3.025259 | lr:6.0976e-05 | norm 0.3210 | dt 338.88ms | 1547133.79 tokens/sec
Step 18577 | loss: 3.046714 | lr:6.0972e-05 | norm 0.2800 | dt 338.68ms | 1548012.70 tokens/sec
Step 18578 | loss: 2.966872 | lr:6.0968e-05 | norm 0.2773 | dt 337.90ms | 1551601.86 tokens/sec
Step 18579 | loss: 3.006015 | lr:6.0964e-05 | norm 0.3280 | dt 338.84ms | 1547302.53 tokens/sec
Step 18580 | loss: 2.928377 | lr:6.0960e-05 | norm 0.2798 | dt 338.00ms | 1551156.41 tokens/sec
Step 18581 | loss: 3.010623 | lr:6.0956e-05 | norm 0.3540 | dt 338.26ms | 1549939.56 tokens/sec
Step 18582 | loss: 3.018024 | lr:6.0953e-05 | norm 0.3146 | dt 338.45ms | 1549104.29 tokens/sec
Step 18583 | loss: 3.071448 | lr:6.0949e-05 | norm 0.3099 | dt 338.63ms | 1548253.57 tokens/sec
Step 18584 | loss: 3.050959 | lr:6.0945e-05 | norm 0.3086 | dt 338.06ms | 1550884.01 tokens/sec
Step 18585 | loss: 2.952202 | lr:6.0941e-05 | norm 0.2997 | dt 337.18ms | 1554937.18 tokens/sec
Step 18586 | loss: 3.012222 | lr:6.0937e-05 | norm 0.3043 | dt 338.29ms | 1549798.65 tokens/sec
Step 18587 | loss: 3.000192 | lr:6.0933e-05 | norm 0.2957 | dt 339.08ms | 1546210.21 tokens/sec
Step 18588 | loss: 3.071860 | lr:6.0929e-05 | norm 0.2878 | dt 338.41ms | 1549247.26 tokens/sec
Step 18589 | loss: 3.009432 | lr:6.0926e-05 | norm 0.3139 | dt 338.31ms | 1549745.13 tokens/sec
Step 18590 | loss: 3.113940 | lr:6.0922e-05 | norm 0.3058 | dt 338.23ms | 1550100.17 tokens/sec
Step 18591 | loss: 3.101685 | lr:6.0918e-05 | norm 0.3171 | dt 338.16ms | 1550430.23 tokens/sec
Step 18592 | loss: 3.054754 | lr:6.0914e-05 | norm 0.2905 | dt 339.07ms | 1546256.96 tokens/sec
Step 18593 | loss: 3.025704 | lr:6.0910e-05 | norm 0.3284 | dt 339.22ms | 1545550.57 tokens/sec
Step 18594 | loss: 3.108422 | lr:6.0907e-05 | norm 0.2999 | dt 338.22ms | 1550126.40 tokens/sec
Step 18595 | loss: 3.106144 | lr:6.0903e-05 | norm 0.3043 | dt 338.66ms | 1548121.69 tokens/sec
Step 18596 | loss: 3.047208 | lr:6.0899e-05 | norm 0.2914 | dt 337.86ms | 1551785.81 tokens/sec
Step 18597 | loss: 3.083211 | lr:6.0895e-05 | norm 0.3155 | dt 338.84ms | 1547291.64 tokens/sec
Step 18598 | loss: 2.985473 | lr:6.0892e-05 | norm 0.2967 | dt 337.79ms | 1552092.48 tokens/sec
Step 18599 | loss: 3.037920 | lr:6.0888e-05 | norm 0.2910 | dt 338.42ms | 1549212.34 tokens/sec
Step 18600 | loss: 3.021383 | lr:6.0884e-05 | norm 0.3013 | dt 337.94ms | 1551442.04 tokens/sec
Step 18601 | loss: 3.082906 | lr:6.0880e-05 | norm 0.2898 | dt 338.45ms | 1549105.39 tokens/sec
Step 18602 | loss: 3.043001 | lr:6.0877e-05 | norm 0.3159 | dt 339.26ms | 1545371.35 tokens/sec
Step 18603 | loss: 3.038405 | lr:6.0873e-05 | norm 0.2782 | dt 338.20ms | 1550216.00 tokens/sec
Step 18604 | loss: 3.077333 | lr:6.0869e-05 | norm 0.3201 | dt 338.28ms | 1549877.30 tokens/sec
Step 18605 | loss: 3.023253 | lr:6.0865e-05 | norm 0.3173 | dt 338.54ms | 1548672.27 tokens/sec
Step 18606 | loss: 2.989894 | lr:6.0862e-05 | norm 0.2810 | dt 338.40ms | 1549331.31 tokens/sec
Step 18607 | loss: 3.020970 | lr:6.0858e-05 | norm 0.2867 | dt 338.90ms | 1547034.74 tokens/sec
Step 18608 | loss: 2.962406 | lr:6.0854e-05 | norm 0.2946 | dt 339.19ms | 1545727.65 tokens/sec
Step 18609 | loss: 2.941971 | lr:6.0851e-05 | norm 0.3031 | dt 337.74ms | 1552362.02 tokens/sec
Step 18610 | loss: 3.005847 | lr:6.0847e-05 | norm 0.3174 | dt 338.13ms | 1550538.46 tokens/sec
Step 18611 | loss: 2.990669 | lr:6.0843e-05 | norm 0.2813 | dt 337.92ms | 1551537.27 tokens/sec
Step 18612 | loss: 3.049080 | lr:6.0840e-05 | norm 0.2791 | dt 338.65ms | 1548183.81 tokens/sec
Step 18613 | loss: 2.954092 | lr:6.0836e-05 | norm 0.2910 | dt 338.61ms | 1548363.68 tokens/sec
Step 18614 | loss: 3.025030 | lr:6.0833e-05 | norm 0.2942 | dt 338.91ms | 1546980.33 tokens/sec
Step 18615 | loss: 2.998842 | lr:6.0829e-05 | norm 0.3011 | dt 337.91ms | 1551578.87 tokens/sec
Step 18616 | loss: 2.997042 | lr:6.0825e-05 | norm 0.2826 | dt 338.71ms | 1547886.31 tokens/sec
Step 18617 | loss: 3.112058 | lr:6.0822e-05 | norm 0.3237 | dt 337.71ms | 1552489.15 tokens/sec
Step 18618 | loss: 3.012184 | lr:6.0818e-05 | norm 0.3182 | dt 338.50ms | 1548834.80 tokens/sec
Step 18619 | loss: 3.079199 | lr:6.0814e-05 | norm 0.3012 | dt 745.30ms | 703458.51 tokens/sec
Step 18620 | loss: 3.054480 | lr:6.0811e-05 | norm 0.3121 | dt 338.54ms | 1548665.73 tokens/sec
Step 18621 | loss: 3.072996 | lr:6.0807e-05 | norm 0.3221 | dt 337.29ms | 1554401.90 tokens/sec
Step 18622 | loss: 3.060987 | lr:6.0804e-05 | norm 0.3425 | dt 339.82ms | 1542819.10 tokens/sec
Step 18623 | loss: 3.086091 | lr:6.0800e-05 | norm 0.3142 | dt 867.93ms | 604064.70 tokens/sec
Step 18624 | loss: 3.034730 | lr:6.0797e-05 | norm 0.3205 | dt 335.04ms | 1564867.12 tokens/sec
Step 18625 | loss: 3.080668 | lr:6.0793e-05 | norm 0.3099 | dt 337.15ms | 1555055.93 tokens/sec
Step 18626 | loss: 3.096891 | lr:6.0790e-05 | norm 0.3282 | dt 338.57ms | 1548527.23 tokens/sec
Step 18627 | loss: 3.027781 | lr:6.0786e-05 | norm 0.3611 | dt 337.78ms | 1552149.45 tokens/sec
Step 18628 | loss: 3.024321 | lr:6.0783e-05 | norm 0.2984 | dt 338.09ms | 1550731.99 tokens/sec
Step 18629 | loss: 3.041760 | lr:6.0779e-05 | norm 0.3135 | dt 339.15ms | 1545894.99 tokens/sec
Step 18630 | loss: 3.047801 | lr:6.0776e-05 | norm 0.3419 | dt 338.75ms | 1547728.34 tokens/sec
Step 18631 | loss: 3.092994 | lr:6.0772e-05 | norm 0.3052 | dt 337.88ms | 1551679.60 tokens/sec
Step 18632 | loss: 3.101112 | lr:6.0769e-05 | norm 0.3240 | dt 338.18ms | 1550320.92 tokens/sec
Step 18633 | loss: 3.088870 | lr:6.0765e-05 | norm 0.3205 | dt 338.14ms | 1550516.59 tokens/sec
Step 18634 | loss: 3.048645 | lr:6.0762e-05 | norm 0.2993 | dt 338.70ms | 1547958.22 tokens/sec
Step 18635 | loss: 3.109052 | lr:6.0758e-05 | norm 0.3832 | dt 337.44ms | 1553712.20 tokens/sec
Step 18636 | loss: 3.135171 | lr:6.0755e-05 | norm 0.3645 | dt 338.21ms | 1550170.10 tokens/sec
Step 18637 | loss: 3.087059 | lr:6.0751e-05 | norm 0.3557 | dt 339.07ms | 1546275.45 tokens/sec
Step 18638 | loss: 3.108437 | lr:6.0748e-05 | norm 0.3344 | dt 338.11ms | 1550627.02 tokens/sec
Step 18639 | loss: 3.129487 | lr:6.0744e-05 | norm 0.3092 | dt 338.60ms | 1548382.21 tokens/sec
Step 18640 | loss: 3.018685 | lr:6.0741e-05 | norm 0.3544 | dt 338.49ms | 1548902.44 tokens/sec
Step 18641 | loss: 3.002905 | lr:6.0737e-05 | norm 0.3190 | dt 338.40ms | 1549336.77 tokens/sec
Step 18642 | loss: 3.007615 | lr:6.0734e-05 | norm 0.3576 | dt 338.15ms | 1550465.21 tokens/sec
Step 18643 | loss: 2.981241 | lr:6.0731e-05 | norm 0.3403 | dt 338.60ms | 1548421.46 tokens/sec
Step 18644 | loss: 3.000627 | lr:6.0727e-05 | norm 0.3309 | dt 337.90ms | 1551609.53 tokens/sec
Step 18645 | loss: 3.038781 | lr:6.0724e-05 | norm 0.3076 | dt 338.25ms | 1549998.56 tokens/sec
Step 18646 | loss: 3.013910 | lr:6.0721e-05 | norm 0.2986 | dt 338.37ms | 1549462.31 tokens/sec
Step 18647 | loss: 2.987150 | lr:6.0717e-05 | norm 0.3039 | dt 339.17ms | 1545776.54 tokens/sec
Step 18648 | loss: 2.978907 | lr:6.0714e-05 | norm 0.3031 | dt 338.10ms | 1550669.66 tokens/sec
Step 18649 | loss: 2.970790 | lr:6.0710e-05 | norm 0.2959 | dt 337.89ms | 1551674.12 tokens/sec
Step 18650 | loss: 3.024335 | lr:6.0707e-05 | norm 0.3163 | dt 338.16ms | 1550436.79 tokens/sec
Step 18651 | loss: 3.041960 | lr:6.0704e-05 | norm 0.3142 | dt 339.07ms | 1546242.83 tokens/sec
Step 18652 | loss: 3.106792 | lr:6.0700e-05 | norm 0.3395 | dt 338.05ms | 1550904.79 tokens/sec
Step 18653 | loss: 3.115706 | lr:6.0697e-05 | norm 0.3476 | dt 337.87ms | 1551737.63 tokens/sec
Step 18654 | loss: 3.059174 | lr:6.0694e-05 | norm 0.2963 | dt 338.24ms | 1550032.43 tokens/sec
Step 18655 | loss: 3.059896 | lr:6.0690e-05 | norm 0.3547 | dt 338.78ms | 1547591.09 tokens/sec
Step 18656 | loss: 3.072782 | lr:6.0687e-05 | norm 0.3668 | dt 338.45ms | 1549082.47 tokens/sec
Step 18657 | loss: 3.127339 | lr:6.0684e-05 | norm 0.3509 | dt 338.17ms | 1550358.08 tokens/sec
Step 18658 | loss: 3.059311 | lr:6.0681e-05 | norm 0.3317 | dt 338.67ms | 1548092.26 tokens/sec
Step 18659 | loss: 3.042438 | lr:6.0677e-05 | norm 0.3213 | dt 337.97ms | 1551294.29 tokens/sec
Step 18660 | loss: 3.060967 | lr:6.0674e-05 | norm 0.3415 | dt 338.40ms | 1549313.85 tokens/sec
Step 18661 | loss: 3.002378 | lr:6.0671e-05 | norm 0.3396 | dt 338.56ms | 1548593.75 tokens/sec
Step 18662 | loss: 3.123290 | lr:6.0668e-05 | norm 0.3446 | dt 337.92ms | 1551515.38 tokens/sec
Step 18663 | loss: 3.048121 | lr:6.0664e-05 | norm 0.3063 | dt 337.86ms | 1551783.62 tokens/sec
Step 18664 | loss: 3.014186 | lr:6.0661e-05 | norm 0.3391 | dt 338.98ms | 1546674.58 tokens/sec
Step 18665 | loss: 2.985270 | lr:6.0658e-05 | norm 0.3389 | dt 337.55ms | 1553201.90 tokens/sec
Step 18666 | loss: 3.035938 | lr:6.0655e-05 | norm 0.2906 | dt 337.94ms | 1551422.34 tokens/sec
Step 18667 | loss: 2.992537 | lr:6.0651e-05 | norm 0.3500 | dt 337.91ms | 1551559.17 tokens/sec
Step 18668 | loss: 2.992202 | lr:6.0648e-05 | norm 0.3118 | dt 338.23ms | 1550078.32 tokens/sec
Step 18669 | loss: 3.041283 | lr:6.0645e-05 | norm 0.2869 | dt 338.36ms | 1549508.17 tokens/sec
Step 18670 | loss: 3.101596 | lr:6.0642e-05 | norm 0.3421 | dt 337.78ms | 1552160.41 tokens/sec
Step 18671 | loss: 3.099243 | lr:6.0639e-05 | norm 0.3421 | dt 338.44ms | 1549131.58 tokens/sec
Step 18672 | loss: 3.073989 | lr:6.0635e-05 | norm 0.2869 | dt 338.68ms | 1548042.13 tokens/sec
Step 18673 | loss: 3.059394 | lr:6.0632e-05 | norm 0.3278 | dt 338.03ms | 1550998.87 tokens/sec
Step 18674 | loss: 3.056644 | lr:6.0629e-05 | norm 0.3356 | dt 338.04ms | 1550945.27 tokens/sec
Step 18675 | loss: 3.120469 | lr:6.0626e-05 | norm 0.2872 | dt 337.89ms | 1551648.94 tokens/sec
Step 18676 | loss: 3.013602 | lr:6.0623e-05 | norm 0.3336 | dt 338.76ms | 1547664.07 tokens/sec
Step 18677 | loss: 2.973394 | lr:6.0620e-05 | norm 0.2962 | dt 338.24ms | 1550064.11 tokens/sec
Step 18678 | loss: 3.024312 | lr:6.0617e-05 | norm 0.3011 | dt 338.35ms | 1549535.47 tokens/sec
Step 18679 | loss: 2.994441 | lr:6.0613e-05 | norm 0.3130 | dt 338.03ms | 1551028.40 tokens/sec
Step 18680 | loss: 3.001500 | lr:6.0610e-05 | norm 0.2963 | dt 338.47ms | 1548978.81 tokens/sec
Step 18681 | loss: 2.992277 | lr:6.0607e-05 | norm 0.3012 | dt 339.00ms | 1546576.68 tokens/sec
Step 18682 | loss: 2.986606 | lr:6.0604e-05 | norm 0.3410 | dt 337.50ms | 1553429.03 tokens/sec
Step 18683 | loss: 3.019075 | lr:6.0601e-05 | norm 0.2813 | dt 337.92ms | 1551527.42 tokens/sec
Step 18684 | loss: 3.046615 | lr:6.0598e-05 | norm 0.2957 | dt 338.34ms | 1549585.69 tokens/sec
Step 18685 | loss: 3.002063 | lr:6.0595e-05 | norm 0.3183 | dt 337.27ms | 1554508.49 tokens/sec
Step 18686 | loss: 3.011497 | lr:6.0592e-05 | norm 0.2776 | dt 337.55ms | 1553217.26 tokens/sec
Step 18687 | loss: 2.994475 | lr:6.0589e-05 | norm 0.3281 | dt 338.40ms | 1549302.93 tokens/sec
Step 18688 | loss: 3.113212 | lr:6.0586e-05 | norm 0.3100 | dt 338.20ms | 1550234.58 tokens/sec
Step 18689 | loss: 3.131302 | lr:6.0583e-05 | norm 0.3049 | dt 337.82ms | 1551954.46 tokens/sec
Step 18690 | loss: 3.058288 | lr:6.0580e-05 | norm 0.3405 | dt 338.22ms | 1550130.77 tokens/sec
Step 18691 | loss: 3.142993 | lr:6.0577e-05 | norm 0.3005 | dt 337.34ms | 1554162.42 tokens/sec
Step 18692 | loss: 3.040829 | lr:6.0574e-05 | norm 0.3010 | dt 337.88ms | 1551719.01 tokens/sec
Step 18693 | loss: 3.087305 | lr:6.0571e-05 | norm 0.2847 | dt 338.42ms | 1549224.34 tokens/sec
Step 18694 | loss: 3.078778 | lr:6.0568e-05 | norm 0.2778 | dt 337.81ms | 1552039.90 tokens/sec
Step 18695 | loss: 3.063494 | lr:6.0565e-05 | norm 0.2931 | dt 337.33ms | 1554223.93 tokens/sec
Step 18696 | loss: 3.013578 | lr:6.0562e-05 | norm 0.3011 | dt 338.38ms | 1549411.00 tokens/sec
Step 18697 | loss: 3.117288 | lr:6.0559e-05 | norm 0.2899 | dt 337.70ms | 1552516.55 tokens/sec
Step 18698 | loss: 3.087626 | lr:6.0556e-05 | norm 0.2952 | dt 338.15ms | 1550480.51 tokens/sec
Step 18699 | loss: 3.059791 | lr:6.0553e-05 | norm 0.3038 | dt 338.44ms | 1549115.21 tokens/sec
Step 18700 | loss: 3.005827 | lr:6.0550e-05 | norm 0.3073 | dt 338.45ms | 1549094.47 tokens/sec
Step 18701 | loss: 3.039387 | lr:6.0547e-05 | norm 0.2932 | dt 337.51ms | 1553420.25 tokens/sec
Step 18702 | loss: 3.031239 | lr:6.0544e-05 | norm 0.2977 | dt 337.30ms | 1554352.46 tokens/sec
Step 18703 | loss: 3.025372 | lr:6.0541e-05 | norm 0.2948 | dt 337.83ms | 1551917.23 tokens/sec
Step 18704 | loss: 3.007246 | lr:6.0538e-05 | norm 0.2983 | dt 337.94ms | 1551442.04 tokens/sec
Step 18705 | loss: 3.037284 | lr:6.0535e-05 | norm 0.2893 | dt 338.28ms | 1549877.30 tokens/sec
Step 18706 | loss: 3.053563 | lr:6.0532e-05 | norm 0.3066 | dt 337.36ms | 1554089.92 tokens/sec
Step 18707 | loss: 3.004655 | lr:6.0529e-05 | norm 0.2971 | dt 338.87ms | 1547188.22 tokens/sec
Step 18708 | loss: 3.053327 | lr:6.0527e-05 | norm 0.2952 | dt 337.59ms | 1553037.36 tokens/sec
Step 18709 | loss: 3.017560 | lr:6.0524e-05 | norm 0.2855 | dt 338.13ms | 1550571.26 tokens/sec
Step 18710 | loss: 3.038735 | lr:6.0521e-05 | norm 0.3143 | dt 337.57ms | 1553126.21 tokens/sec
Step 18711 | loss: 3.044155 | lr:6.0518e-05 | norm 0.2934 | dt 338.03ms | 1551003.24 tokens/sec
Step 18712 | loss: 2.993558 | lr:6.0515e-05 | norm 0.3264 | dt 337.89ms | 1551634.71 tokens/sec
Step 18713 | loss: 2.951953 | lr:6.0512e-05 | norm 0.3094 | dt 337.86ms | 1551778.14 tokens/sec
Step 18714 | loss: 3.031333 | lr:6.0509e-05 | norm 0.3169 | dt 337.34ms | 1554169.01 tokens/sec
Step 18715 | loss: 2.968887 | lr:6.0507e-05 | norm 0.3067 | dt 338.25ms | 1549999.65 tokens/sec
Step 18716 | loss: 3.011687 | lr:6.0504e-05 | norm 0.2857 | dt 337.81ms | 1552025.66 tokens/sec
Step 18717 | loss: 3.051372 | lr:6.0501e-05 | norm 0.3056 | dt 337.48ms | 1553545.36 tokens/sec
Step 18718 | loss: 3.017641 | lr:6.0498e-05 | norm 0.2955 | dt 337.86ms | 1551794.57 tokens/sec
Step 18719 | loss: 3.040595 | lr:6.0495e-05 | norm 0.3022 | dt 338.56ms | 1548597.02 tokens/sec
Step 18720 | loss: 3.008912 | lr:6.0492e-05 | norm 0.2932 | dt 336.75ms | 1556892.35 tokens/sec
Step 18721 | loss: 2.974759 | lr:6.0490e-05 | norm 0.2946 | dt 895.98ms | 585156.77 tokens/sec
Step 18722 | loss: 3.045286 | lr:6.0487e-05 | norm 0.3081 | dt 336.87ms | 1556349.12 tokens/sec
Step 18723 | loss: 3.159045 | lr:6.0484e-05 | norm 0.2988 | dt 337.95ms | 1551378.56 tokens/sec
Step 18724 | loss: 3.132513 | lr:6.0481e-05 | norm 0.3038 | dt 337.28ms | 1554457.94 tokens/sec
Step 18725 | loss: 3.194722 | lr:6.0479e-05 | norm 0.3202 | dt 337.45ms | 1553699.03 tokens/sec
Step 18726 | loss: 3.160898 | lr:6.0476e-05 | norm 0.3253 | dt 337.47ms | 1553593.65 tokens/sec
Step 18727 | loss: 3.126517 | lr:6.0473e-05 | norm 0.3192 | dt 337.67ms | 1552660.15 tokens/sec
Step 18728 | loss: 3.222172 | lr:6.0470e-05 | norm 0.3319 | dt 337.89ms | 1551667.55 tokens/sec
Step 18729 | loss: 3.108603 | lr:6.0468e-05 | norm 0.3190 | dt 336.97ms | 1555881.12 tokens/sec
Step 18730 | loss: 3.146044 | lr:6.0465e-05 | norm 0.3071 | dt 337.68ms | 1552625.07 tokens/sec
Step 18731 | loss: 3.107996 | lr:6.0462e-05 | norm 0.2991 | dt 338.20ms | 1550221.47 tokens/sec
Step 18732 | loss: 3.073953 | lr:6.0460e-05 | norm 0.3092 | dt 337.77ms | 1552189.99 tokens/sec
Step 18733 | loss: 3.053912 | lr:6.0457e-05 | norm 0.2927 | dt 338.25ms | 1550023.69 tokens/sec
Step 18734 | loss: 3.058103 | lr:6.0454e-05 | norm 0.3032 | dt 338.16ms | 1550399.62 tokens/sec
Step 18735 | loss: 2.998800 | lr:6.0452e-05 | norm 0.3129 | dt 337.81ms | 1552028.95 tokens/sec
Step 18736 | loss: 3.075588 | lr:6.0449e-05 | norm 0.3173 | dt 338.49ms | 1548918.80 tokens/sec
Step 18737 | loss: 3.082344 | lr:6.0446e-05 | norm 0.4169 | dt 337.91ms | 1551555.88 tokens/sec
Step 18738 | loss: 3.014651 | lr:6.0444e-05 | norm 0.3097 | dt 338.18ms | 1550315.46 tokens/sec
Step 18739 | loss: 3.088951 | lr:6.0441e-05 | norm 0.3387 | dt 337.93ms | 1551475.97 tokens/sec
Step 18740 | loss: 3.032250 | lr:6.0438e-05 | norm 0.3504 | dt 338.28ms | 1549863.10 tokens/sec
Step 18741 | loss: 2.978045 | lr:6.0436e-05 | norm 0.3415 | dt 337.57ms | 1553111.95 tokens/sec
Step 18742 | loss: 3.013193 | lr:6.0433e-05 | norm 0.3279 | dt 337.65ms | 1552771.98 tokens/sec
Step 18743 | loss: 3.049669 | lr:6.0430e-05 | norm 0.3136 | dt 337.79ms | 1552103.44 tokens/sec
Step 18744 | loss: 3.170452 | lr:6.0428e-05 | norm 0.3131 | dt 337.80ms | 1552061.81 tokens/sec
Step 18745 | loss: 2.986320 | lr:6.0425e-05 | norm 0.3314 | dt 337.94ms | 1551434.38 tokens/sec
Step 18746 | loss: 3.097241 | lr:6.0423e-05 | norm 0.3288 | dt 337.53ms | 1553295.16 tokens/sec
Step 18747 | loss: 3.060349 | lr:6.0420e-05 | norm 0.2966 | dt 337.57ms | 1553129.50 tokens/sec
Step 18748 | loss: 3.011511 | lr:6.0417e-05 | norm 0.3052 | dt 338.45ms | 1549107.57 tokens/sec
Step 18749 | loss: 3.008980 | lr:6.0415e-05 | norm 0.3029 | dt 338.11ms | 1550635.76 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 18750: 3.0788
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3063/10042=0.3050


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, so I'm gonna try a lot of different ways. I love how useful words and numbers look. (And) all
rank 3 sample 1 >Hello, I'm a language model, so what are you going to tell me?
There is a very simple way to tell me this:
This sentence

rank 3 sample 2 >Hello, I'm a language model, so this tutorial is geared toward creating a working script to start building a language script.
In this tutorial, this script
rank 3 sample 3 >Hello, I'm a language model, so if you are an architect then you should know the rules for this stuff. This is something that some of my friends




ddp_rank 5: ####### Printing generated samples ####### 


ddp_rank 2: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but not an expert, so I've gone to different places to teach this to kids.
This is my first blog
rank 2 sample 0 >Hello, I'm a language model, so my question comes up because it's not just my first question. We have lots of questions and I am not a
rank 2 sample 1 >Hello, I'm a language model, and I believe you want to start developing at a human level with language.
I'm an experienced developer and I hoperank 5 sample 1 >Hello, I'm a language model, because, the most essential function to have is learning the basics of learning the langram language, you learn. I'm

rank 5 sample 2 >Hello, I'm a language model, and I just wanted to know all about it.
I'm working with the C API, and I'm working with
rank 2 sample 2 >Hello, I'm a language model, and I've been teaching languages in an advanced environment. I can't be the same thing. What's the difference between
rank 5 sample 3 >Hello, I'm a language model, my language model is Java programming language.
Can i learn Java programming language like to use it while i am learning in


rank 2 sample 3 >Hello, I'm a language model, but if I didn't, I'm already into coding. This is a project. If only people knew that I was




ddp_rank 7: ####### Printing generated samples ####### 



ddp_rank 6: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, so I think it will come in handy."
"There are a few ways I can say just like that. When


ddp_rank 1: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, where there are a whole lot more to it than simply a single syntax, right?
When I think of a bunch
rank 7 sample 1 >Hello, I'm a language model, someone might tell me I don't feel like I'm any more good at programming! I have a really hard time,
rank 6 sample 1 >Hello, I'm a language model, so I have to have some idea of grammar, grammar rules, and rules.
Let me say, that's it
rank 7 sample 2 >Hello, I'm a language model, so I'll share my favorite features in my next post.<|endoftext|>How We Use the Word "Sustainable" (orrank 1 sample 0 >Hello, I'm a language model, so I made a simple model of any text with an existing code. I need to add an index to each of the
rank 6 sample 2 >Hello, I'm a language model, but my teacher said something different, like how do I write in Chinese, if I say "I wish, wish"

rank 6 sample 3 >Hello, I'm a language model, so I have a list of questions to answer and I'm going to try my hand on here. Have an idea of


rank 7 sample 3 >Hello, I'm a language model, and you are going to try to imagine some really cool things. I was thinking that this is pretty cool, but really


rank 1 sample 1 >Hello, I'm a language model, a computer program. I'm an educator, a language engineer, and a designer, if you ask me. I'm
rank 1 sample 2 >Hello, I'm a language model, but thanks for having me.
I'm a language model: a model that works with language-related objects such as
rank 1 sample 3 >Hello, I'm a language model, so I'm working with different languages so am going to study there :)
It's something which I'm going to do




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, and I wanted to see how it works if you could do a little thing like that for other languages. For example:
rank 4 sample 1 >Hello, I'm a language model, am curious about more ways to understand things and not be confined to a simple, simplified system of rules (and with that
rank 4 sample 2 >Hello, I'm a language model, I could put stuff like this on the computer. I'm gonna write a program called Hello, so a program, I
rank 4 sample 3 >Hello, I'm a language model, so it sucks at that. I could do the magic while watching a song or video, or at the same time.




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, so I don't want to say what an application is, but in real life, the term, I am a programming
rank 0 sample 1 >Hello, I'm a language model, and like how I learned to program?
- As much as I'll get to the point of having to say that
rank 0 sample 2 >Hello, I'm a language model, and I had no idea this was an issue until I started looking up it. In fact, I always thought that my
rank 0 sample 3 >Hello, I'm a language model, and how do I make sure that the model output is the same as the model on the desktop. My model has 5


Step 18750 | loss: 3.055216 | lr:6.0412e-05 | norm 0.3067 | dt 13167.11ms | 39818.01 tokens/sec
Step 18751 | loss: 3.047056 | lr:6.0410e-05 | norm 0.3125 | dt 335.28ms | 1563749.87 tokens/sec
Step 18752 | loss: 3.006532 | lr:6.0407e-05 | norm 0.2974 | dt 337.62ms | 1552909.05 tokens/sec
Step 18753 | loss: 3.017423 | lr:6.0405e-05 | norm 0.3456 | dt 336.98ms | 1555858.01 tokens/sec
Step 18754 | loss: 3.037714 | lr:6.0402e-05 | norm 0.3190 | dt 336.23ms | 1559320.02 tokens/sec
Step 18755 | loss: 3.106972 | lr:6.0400e-05 | norm 0.3069 | dt 336.87ms | 1556354.63 tokens/sec
Step 18756 | loss: 3.084594 | lr:6.0397e-05 | norm 0.3181 | dt 337.09ms | 1555313.30 tokens/sec
Step 18757 | loss: 3.005574 | lr:6.0395e-05 | norm 0.3232 | dt 336.76ms | 1556857.08 tokens/sec
Step 18758 | loss: 3.032082 | lr:6.0392e-05 | norm 0.2839 | dt 336.97ms | 1555900.94 tokens/sec
Step 18759 | loss: 2.978881 | lr:6.0390e-05 | norm 0.3190 | dt 337.80ms | 1552089.20 tokens/sec
Step 18760 | loss: 2.967544 | lr:6.0387e-05 | norm 0.3430 | dt 336.35ms | 1558741.94 tokens/sec
Step 18761 | loss: 3.003622 | lr:6.0385e-05 | norm 0.2939 | dt 337.12ms | 1555189.00 tokens/sec
Step 18762 | loss: 3.031563 | lr:6.0382e-05 | norm 0.3136 | dt 337.10ms | 1555300.10 tokens/sec
Step 18763 | loss: 3.063218 | lr:6.0380e-05 | norm 0.3295 | dt 336.49ms | 1558124.56 tokens/sec
Step 18764 | loss: 3.055461 | lr:6.0377e-05 | norm 0.3110 | dt 337.29ms | 1554415.09 tokens/sec
Step 18765 | loss: 3.051322 | lr:6.0375e-05 | norm 0.2921 | dt 337.53ms | 1553288.58 tokens/sec
Step 18766 | loss: 2.991631 | lr:6.0373e-05 | norm 0.3041 | dt 336.91ms | 1556189.42 tokens/sec
Step 18767 | loss: 3.023915 | lr:6.0370e-05 | norm 0.3005 | dt 337.23ms | 1554708.52 tokens/sec
Step 18768 | loss: 3.001502 | lr:6.0368e-05 | norm 0.2716 | dt 337.29ms | 1554413.99 tokens/sec
Step 18769 | loss: 3.088701 | lr:6.0365e-05 | norm 0.3220 | dt 338.60ms | 1548398.57 tokens/sec
Step 18770 | loss: 3.159434 | lr:6.0363e-05 | norm 0.3241 | dt 337.44ms | 1553728.67 tokens/sec
Step 18771 | loss: 3.109023 | lr:6.0360e-05 | norm 0.3030 | dt 338.30ms | 1549775.72 tokens/sec
Step 18772 | loss: 3.088305 | lr:6.0358e-05 | norm 0.3134 | dt 338.80ms | 1547468.03 tokens/sec
Step 18773 | loss: 3.106839 | lr:6.0356e-05 | norm 0.3088 | dt 338.76ms | 1547652.09 tokens/sec
Step 18774 | loss: 3.096957 | lr:6.0353e-05 | norm 0.3065 | dt 338.85ms | 1547255.71 tokens/sec
Step 18775 | loss: 3.099308 | lr:6.0351e-05 | norm 0.3112 | dt 338.50ms | 1548879.53 tokens/sec
Step 18776 | loss: 3.049030 | lr:6.0349e-05 | norm 0.2780 | dt 338.49ms | 1548894.80 tokens/sec
Step 18777 | loss: 3.090664 | lr:6.0346e-05 | norm 0.3217 | dt 338.65ms | 1548167.46 tokens/sec
Step 18778 | loss: 3.092087 | lr:6.0344e-05 | norm 0.3055 | dt 338.18ms | 1550338.41 tokens/sec
Step 18779 | loss: 3.078259 | lr:6.0342e-05 | norm 0.2865 | dt 339.36ms | 1544921.87 tokens/sec
Step 18780 | loss: 3.049029 | lr:6.0339e-05 | norm 0.3088 | dt 339.32ms | 1545118.35 tokens/sec
Step 18781 | loss: 3.035046 | lr:6.0337e-05 | norm 0.3046 | dt 338.60ms | 1548399.66 tokens/sec
Step 18782 | loss: 3.057275 | lr:6.0335e-05 | norm 0.3030 | dt 337.79ms | 1552126.44 tokens/sec
Step 18783 | loss: 2.975812 | lr:6.0332e-05 | norm 0.3202 | dt 338.97ms | 1546705.04 tokens/sec
Step 18784 | loss: 3.049209 | lr:6.0330e-05 | norm 0.2809 | dt 337.86ms | 1551769.38 tokens/sec
Step 18785 | loss: 3.078256 | lr:6.0328e-05 | norm 0.2798 | dt 337.62ms | 1552912.34 tokens/sec
Step 18786 | loss: 3.049067 | lr:6.0326e-05 | norm 0.2769 | dt 337.75ms | 1552299.56 tokens/sec
Step 18787 | loss: 3.118702 | lr:6.0323e-05 | norm 0.2938 | dt 337.39ms | 1553929.59 tokens/sec
Step 18788 | loss: 3.044336 | lr:6.0321e-05 | norm 0.2859 | dt 337.50ms | 1553440.00 tokens/sec
Step 18789 | loss: 3.136304 | lr:6.0319e-05 | norm 0.3252 | dt 337.70ms | 1552530.80 tokens/sec
Step 18790 | loss: 3.099685 | lr:6.0317e-05 | norm 0.3137 | dt 338.22ms | 1550122.02 tokens/sec
Step 18791 | loss: 3.057580 | lr:6.0314e-05 | norm 0.3143 | dt 338.59ms | 1548457.44 tokens/sec
Step 18792 | loss: 3.067260 | lr:6.0312e-05 | norm 0.2956 | dt 337.81ms | 1552005.95 tokens/sec
Step 18793 | loss: 3.071788 | lr:6.0310e-05 | norm 0.3003 | dt 337.76ms | 1552271.07 tokens/sec
Step 18794 | loss: 3.023539 | lr:6.0308e-05 | norm 0.2934 | dt 338.24ms | 1550043.35 tokens/sec
Step 18795 | loss: 3.078292 | lr:6.0305e-05 | norm 0.2926 | dt 338.02ms | 1551071.07 tokens/sec
Step 18796 | loss: 3.074482 | lr:6.0303e-05 | norm 0.3058 | dt 338.04ms | 1550954.02 tokens/sec
Step 18797 | loss: 3.062547 | lr:6.0301e-05 | norm 0.2722 | dt 338.19ms | 1550254.25 tokens/sec
Step 18798 | loss: 3.045038 | lr:6.0299e-05 | norm 0.2722 | dt 338.80ms | 1547476.74 tokens/sec
Step 18799 | loss: 3.008564 | lr:6.0297e-05 | norm 0.2960 | dt 337.86ms | 1551798.95 tokens/sec
Step 18800 | loss: 3.034560 | lr:6.0295e-05 | norm 0.2699 | dt 337.14ms | 1555092.22 tokens/sec
Step 18801 | loss: 3.053940 | lr:6.0292e-05 | norm 0.2770 | dt 338.32ms | 1549693.80 tokens/sec
Step 18802 | loss: 3.050058 | lr:6.0290e-05 | norm 0.2901 | dt 338.26ms | 1549945.03 tokens/sec
Step 18803 | loss: 2.949412 | lr:6.0288e-05 | norm 0.2857 | dt 337.68ms | 1552613.01 tokens/sec
Step 18804 | loss: 3.049870 | lr:6.0286e-05 | norm 0.2947 | dt 337.56ms | 1553175.58 tokens/sec
Step 18805 | loss: 2.970319 | lr:6.0284e-05 | norm 0.2919 | dt 338.02ms | 1551041.53 tokens/sec
Step 18806 | loss: 2.996183 | lr:6.0282e-05 | norm 0.2798 | dt 338.29ms | 1549823.77 tokens/sec
Step 18807 | loss: 2.935275 | lr:6.0280e-05 | norm 0.2950 | dt 337.22ms | 1554729.40 tokens/sec
Step 18808 | loss: 2.958667 | lr:6.0278e-05 | norm 0.2878 | dt 337.87ms | 1551755.15 tokens/sec
Step 18809 | loss: 3.009012 | lr:6.0275e-05 | norm 0.2913 | dt 338.19ms | 1550264.09 tokens/sec
Step 18810 | loss: 3.004185 | lr:6.0273e-05 | norm 0.2892 | dt 337.39ms | 1553962.53 tokens/sec
Step 18811 | loss: 3.025778 | lr:6.0271e-05 | norm 0.3132 | dt 337.72ms | 1552436.54 tokens/sec
Step 18812 | loss: 3.014160 | lr:6.0269e-05 | norm 0.2873 | dt 901.50ms | 581574.94 tokens/sec
Step 18813 | loss: 3.022336 | lr:6.0267e-05 | norm 0.3151 | dt 336.02ms | 1560271.51 tokens/sec
Step 18814 | loss: 3.022916 | lr:6.0265e-05 | norm 0.3032 | dt 338.93ms | 1546912.86 tokens/sec
Step 18815 | loss: 3.037357 | lr:6.0263e-05 | norm 0.3303 | dt 339.43ms | 1544629.96 tokens/sec
Step 18816 | loss: 3.095901 | lr:6.0261e-05 | norm 0.3253 | dt 340.15ms | 1541325.71 tokens/sec
Step 18817 | loss: 3.155897 | lr:6.0259e-05 | norm 0.3334 | dt 338.94ms | 1546839.95 tokens/sec
Step 18818 | loss: 3.114987 | lr:6.0257e-05 | norm 0.3396 | dt 338.45ms | 1549072.65 tokens/sec
Step 18819 | loss: 3.085842 | lr:6.0255e-05 | norm 0.3272 | dt 338.60ms | 1548385.48 tokens/sec
Step 18820 | loss: 3.044970 | lr:6.0253e-05 | norm 0.3549 | dt 338.50ms | 1548879.53 tokens/sec
Step 18821 | loss: 3.101708 | lr:6.0251e-05 | norm 0.3565 | dt 339.28ms | 1545282.30 tokens/sec
Step 18822 | loss: 3.050989 | lr:6.0249e-05 | norm 0.3229 | dt 338.98ms | 1546643.04 tokens/sec
Step 18823 | loss: 3.147123 | lr:6.0247e-05 | norm 0.3383 | dt 338.58ms | 1548475.98 tokens/sec
Step 18824 | loss: 3.093194 | lr:6.0245e-05 | norm 0.3197 | dt 338.22ms | 1550135.14 tokens/sec
Step 18825 | loss: 3.099669 | lr:6.0243e-05 | norm 0.3291 | dt 338.62ms | 1548297.18 tokens/sec
Step 18826 | loss: 3.099123 | lr:6.0241e-05 | norm 0.3213 | dt 338.74ms | 1547769.73 tokens/sec
Step 18827 | loss: 3.028046 | lr:6.0239e-05 | norm 0.3185 | dt 339.94ms | 1542296.46 tokens/sec
Step 18828 | loss: 3.088709 | lr:6.0237e-05 | norm 0.3058 | dt 339.38ms | 1544823.11 tokens/sec
Step 18829 | loss: 3.089292 | lr:6.0235e-05 | norm 0.3116 | dt 339.01ms | 1546531.00 tokens/sec
Step 18830 | loss: 3.079298 | lr:6.0233e-05 | norm 0.2988 | dt 338.46ms | 1549030.09 tokens/sec
Step 18831 | loss: 3.051504 | lr:6.0232e-05 | norm 0.3093 | dt 337.71ms | 1552467.23 tokens/sec
Step 18832 | loss: 3.076535 | lr:6.0230e-05 | norm 0.3036 | dt 338.68ms | 1548025.78 tokens/sec
Step 18833 | loss: 3.070197 | lr:6.0228e-05 | norm 0.2866 | dt 338.59ms | 1548440.00 tokens/sec
Step 18834 | loss: 3.038580 | lr:6.0226e-05 | norm 0.2761 | dt 338.43ms | 1549190.51 tokens/sec
Step 18835 | loss: 3.009019 | lr:6.0224e-05 | norm 0.3111 | dt 337.58ms | 1553062.59 tokens/sec
Step 18836 | loss: 3.114085 | lr:6.0222e-05 | norm 0.3281 | dt 337.93ms | 1551461.74 tokens/sec
Step 18837 | loss: 3.025192 | lr:6.0220e-05 | norm 0.3185 | dt 338.77ms | 1547603.07 tokens/sec
Step 18838 | loss: 3.071779 | lr:6.0218e-05 | norm 0.2954 | dt 337.90ms | 1551602.96 tokens/sec
Step 18839 | loss: 3.069950 | lr:6.0216e-05 | norm 0.3300 | dt 338.72ms | 1547852.53 tokens/sec
Step 18840 | loss: 3.072288 | lr:6.0215e-05 | norm 0.3194 | dt 339.61ms | 1543793.90 tokens/sec
Step 18841 | loss: 3.056227 | lr:6.0213e-05 | norm 0.3205 | dt 337.48ms | 1553546.45 tokens/sec
Step 18842 | loss: 3.030216 | lr:6.0211e-05 | norm 0.3007 | dt 337.69ms | 1552568.07 tokens/sec
Step 18843 | loss: 3.056952 | lr:6.0209e-05 | norm 0.3048 | dt 339.60ms | 1543823.16 tokens/sec
Step 18844 | loss: 3.049669 | lr:6.0207e-05 | norm 0.2973 | dt 338.07ms | 1550815.11 tokens/sec
Step 18845 | loss: 3.058992 | lr:6.0205e-05 | norm 0.3172 | dt 338.30ms | 1549750.59 tokens/sec
Step 18846 | loss: 3.063561 | lr:6.0204e-05 | norm 0.2937 | dt 339.45ms | 1544511.71 tokens/sec
Step 18847 | loss: 3.058342 | lr:6.0202e-05 | norm 0.2873 | dt 337.80ms | 1552049.76 tokens/sec
Step 18848 | loss: 3.041728 | lr:6.0200e-05 | norm 0.3159 | dt 337.59ms | 1553008.85 tokens/sec
Step 18849 | loss: 3.015709 | lr:6.0198e-05 | norm 0.2823 | dt 337.70ms | 1552538.47 tokens/sec
Step 18850 | loss: 3.001688 | lr:6.0197e-05 | norm 0.2909 | dt 338.24ms | 1550060.84 tokens/sec
Step 18851 | loss: 2.961731 | lr:6.0195e-05 | norm 0.2883 | dt 338.22ms | 1550161.36 tokens/sec
Step 18852 | loss: 2.995739 | lr:6.0193e-05 | norm 0.2809 | dt 338.07ms | 1550805.26 tokens/sec
Step 18853 | loss: 2.925794 | lr:6.0191e-05 | norm 0.2875 | dt 338.29ms | 1549801.93 tokens/sec
Step 18854 | loss: 3.006417 | lr:6.0190e-05 | norm 0.3014 | dt 338.08ms | 1550791.05 tokens/sec
Step 18855 | loss: 2.997755 | lr:6.0188e-05 | norm 0.2898 | dt 339.32ms | 1545104.24 tokens/sec
Step 18856 | loss: 3.059877 | lr:6.0186e-05 | norm 0.2938 | dt 339.24ms | 1545472.36 tokens/sec
Step 18857 | loss: 3.086195 | lr:6.0184e-05 | norm 0.3151 | dt 338.17ms | 1550349.34 tokens/sec
Step 18858 | loss: 3.007871 | lr:6.0183e-05 | norm 0.3136 | dt 339.74ms | 1543208.87 tokens/sec
Step 18859 | loss: 3.000332 | lr:6.0181e-05 | norm 0.2922 | dt 338.20ms | 1550240.05 tokens/sec
Step 18860 | loss: 3.027256 | lr:6.0179e-05 | norm 0.2914 | dt 338.02ms | 1551052.47 tokens/sec
Step 18861 | loss: 3.063798 | lr:6.0178e-05 | norm 0.3026 | dt 338.60ms | 1548398.57 tokens/sec
Step 18862 | loss: 3.143437 | lr:6.0176e-05 | norm 0.3511 | dt 337.87ms | 1551749.67 tokens/sec
Step 18863 | loss: 3.126400 | lr:6.0174e-05 | norm 0.3100 | dt 338.11ms | 1550642.33 tokens/sec
Step 18864 | loss: 3.132064 | lr:6.0173e-05 | norm 0.3259 | dt 338.55ms | 1548615.56 tokens/sec
Step 18865 | loss: 3.091311 | lr:6.0171e-05 | norm 0.3134 | dt 337.95ms | 1551392.79 tokens/sec
Step 18866 | loss: 3.131916 | lr:6.0169e-05 | norm 0.3160 | dt 338.72ms | 1547831.83 tokens/sec
Step 18867 | loss: 3.049653 | lr:6.0168e-05 | norm 0.3087 | dt 337.67ms | 1552657.96 tokens/sec
Step 18868 | loss: 3.081632 | lr:6.0166e-05 | norm 0.2998 | dt 338.92ms | 1546928.09 tokens/sec
Step 18869 | loss: 3.149616 | lr:6.0165e-05 | norm 0.3275 | dt 338.48ms | 1548944.99 tokens/sec
Step 18870 | loss: 3.182084 | lr:6.0163e-05 | norm 0.3183 | dt 338.34ms | 1549570.41 tokens/sec
Step 18871 | loss: 3.140891 | lr:6.0161e-05 | norm 0.3249 | dt 338.34ms | 1549572.59 tokens/sec
Step 18872 | loss: 3.156425 | lr:6.0160e-05 | norm 0.3120 | dt 339.10ms | 1546094.98 tokens/sec
Step 18873 | loss: 3.038053 | lr:6.0158e-05 | norm 0.3291 | dt 338.00ms | 1551168.45 tokens/sec
Step 18874 | loss: 3.083457 | lr:6.0157e-05 | norm 0.3173 | dt 338.80ms | 1547494.17 tokens/sec
Step 18875 | loss: 3.072489 | lr:6.0155e-05 | norm 0.3138 | dt 338.01ms | 1551115.93 tokens/sec
Step 18876 | loss: 3.016406 | lr:6.0153e-05 | norm 0.3678 | dt 338.05ms | 1550899.33 tokens/sec
Step 18877 | loss: 3.098402 | lr:6.0152e-05 | norm 0.3203 | dt 338.15ms | 1550437.88 tokens/sec
Step 18878 | loss: 3.000767 | lr:6.0150e-05 | norm 0.3044 | dt 338.21ms | 1550166.83 tokens/sec
Step 18879 | loss: 3.117767 | lr:6.0149e-05 | norm 0.3269 | dt 338.15ms | 1550473.95 tokens/sec
Step 18880 | loss: 3.169276 | lr:6.0147e-05 | norm 0.3509 | dt 337.16ms | 1555026.24 tokens/sec
Step 18881 | loss: 3.064711 | lr:6.0146e-05 | norm 0.3120 | dt 338.03ms | 1550996.68 tokens/sec
Step 18882 | loss: 3.013515 | lr:6.0144e-05 | norm 0.3043 | dt 338.27ms | 1549931.92 tokens/sec
Step 18883 | loss: 3.095235 | lr:6.0143e-05 | norm 0.3258 | dt 337.69ms | 1552554.91 tokens/sec
Step 18884 | loss: 3.114148 | lr:6.0141e-05 | norm 0.2901 | dt 337.35ms | 1554134.96 tokens/sec
Step 18885 | loss: 3.052339 | lr:6.0140e-05 | norm 0.3108 | dt 338.31ms | 1549706.91 tokens/sec
Step 18886 | loss: 3.004242 | lr:6.0138e-05 | norm 0.3001 | dt 338.12ms | 1550612.80 tokens/sec
Step 18887 | loss: 3.053407 | lr:6.0137e-05 | norm 0.2880 | dt 338.88ms | 1547120.73 tokens/sec
Step 18888 | loss: 3.080386 | lr:6.0135e-05 | norm 0.2948 | dt 339.12ms | 1546010.19 tokens/sec
Step 18889 | loss: 3.039676 | lr:6.0134e-05 | norm 0.3063 | dt 338.36ms | 1549492.88 tokens/sec
Step 18890 | loss: 3.069662 | lr:6.0132e-05 | norm 0.3188 | dt 338.49ms | 1548884.98 tokens/sec
Step 18891 | loss: 3.019001 | lr:6.0131e-05 | norm 0.3223 | dt 338.41ms | 1549276.73 tokens/sec
Step 18892 | loss: 3.039357 | lr:6.0130e-05 | norm 0.3061 | dt 338.13ms | 1550536.27 tokens/sec
Step 18893 | loss: 3.024012 | lr:6.0128e-05 | norm 0.3180 | dt 338.41ms | 1549275.64 tokens/sec
Step 18894 | loss: 2.959953 | lr:6.0127e-05 | norm 0.3164 | dt 338.20ms | 1550210.54 tokens/sec
Step 18895 | loss: 3.012089 | lr:6.0125e-05 | norm 0.3055 | dt 339.21ms | 1545616.83 tokens/sec
Step 18896 | loss: 2.969469 | lr:6.0124e-05 | norm 0.2952 | dt 338.04ms | 1550971.52 tokens/sec
Step 18897 | loss: 2.991203 | lr:6.0122e-05 | norm 0.2919 | dt 339.82ms | 1542856.98 tokens/sec
Step 18898 | loss: 2.975345 | lr:6.0121e-05 | norm 0.2949 | dt 338.18ms | 1550338.41 tokens/sec
Step 18899 | loss: 3.062001 | lr:6.0120e-05 | norm 0.3041 | dt 338.11ms | 1550635.76 tokens/sec
Step 18900 | loss: 3.066844 | lr:6.0118e-05 | norm 0.3221 | dt 339.22ms | 1545564.69 tokens/sec
Step 18901 | loss: 3.000385 | lr:6.0117e-05 | norm 0.3003 | dt 338.42ms | 1549203.61 tokens/sec
Step 18902 | loss: 2.941380 | lr:6.0116e-05 | norm 0.2961 | dt 337.98ms | 1551252.70 tokens/sec
Step 18903 | loss: 3.019582 | lr:6.0114e-05 | norm 0.2949 | dt 339.04ms | 1546371.13 tokens/sec
Step 18904 | loss: 3.028964 | lr:6.0113e-05 | norm 0.3260 | dt 338.28ms | 1549851.08 tokens/sec
Step 18905 | loss: 3.005518 | lr:6.0112e-05 | norm 0.3055 | dt 338.37ms | 1549440.48 tokens/sec
Step 18906 | loss: 3.055903 | lr:6.0110e-05 | norm 0.3142 | dt 338.02ms | 1551042.63 tokens/sec
Step 18907 | loss: 3.062006 | lr:6.0109e-05 | norm 0.3419 | dt 338.73ms | 1547822.03 tokens/sec
Step 18908 | loss: 3.073983 | lr:6.0108e-05 | norm 0.3086 | dt 338.74ms | 1547780.63 tokens/sec
Step 18909 | loss: 3.114230 | lr:6.0106e-05 | norm 0.3155 | dt 337.75ms | 1552308.32 tokens/sec
Step 18910 | loss: 3.068034 | lr:6.0105e-05 | norm 0.3586 | dt 337.59ms | 1553028.59 tokens/sec
Step 18911 | loss: 3.085001 | lr:6.0104e-05 | norm 0.3168 | dt 1021.34ms | 513332.32 tokens/sec
Step 18912 | loss: 3.065928 | lr:6.0102e-05 | norm 0.3199 | dt 336.50ms | 1558046.18 tokens/sec
Step 18913 | loss: 3.146148 | lr:6.0101e-05 | norm 0.3421 | dt 337.85ms | 1551849.32 tokens/sec
Step 18914 | loss: 3.092247 | lr:6.0100e-05 | norm 0.3014 | dt 338.71ms | 1547918.99 tokens/sec
Step 18915 | loss: 3.100622 | lr:6.0099e-05 | norm 0.3353 | dt 337.88ms | 1551692.74 tokens/sec
Step 18916 | loss: 3.179813 | lr:6.0097e-05 | norm 0.3327 | dt 337.53ms | 1553295.16 tokens/sec
Step 18917 | loss: 3.039198 | lr:6.0096e-05 | norm 0.3104 | dt 338.22ms | 1550140.60 tokens/sec
Step 18918 | loss: 3.080535 | lr:6.0095e-05 | norm 0.3182 | dt 338.40ms | 1549305.11 tokens/sec
Step 18919 | loss: 3.079324 | lr:6.0094e-05 | norm 0.3161 | dt 337.83ms | 1551948.99 tokens/sec
Step 18920 | loss: 3.024472 | lr:6.0093e-05 | norm 0.3084 | dt 338.44ms | 1549140.31 tokens/sec
Step 18921 | loss: 3.067849 | lr:6.0091e-05 | norm 0.3159 | dt 337.68ms | 1552630.55 tokens/sec
Step 18922 | loss: 3.093319 | lr:6.0090e-05 | norm 0.2892 | dt 337.77ms | 1552220.67 tokens/sec
Step 18923 | loss: 3.062386 | lr:6.0089e-05 | norm 0.3024 | dt 337.55ms | 1553229.33 tokens/sec
Step 18924 | loss: 3.003876 | lr:6.0088e-05 | norm 0.3059 | dt 337.27ms | 1554499.70 tokens/sec
Step 18925 | loss: 3.034381 | lr:6.0087e-05 | norm 0.3050 | dt 337.92ms | 1551532.89 tokens/sec
Step 18926 | loss: 3.078031 | lr:6.0085e-05 | norm 0.3076 | dt 337.54ms | 1553241.40 tokens/sec
Step 18927 | loss: 3.090981 | lr:6.0084e-05 | norm 0.3001 | dt 337.60ms | 1553005.56 tokens/sec
Step 18928 | loss: 3.113267 | lr:6.0083e-05 | norm 0.3170 | dt 338.13ms | 1550561.42 tokens/sec
Step 18929 | loss: 3.042857 | lr:6.0082e-05 | norm 0.3124 | dt 337.71ms | 1552459.56 tokens/sec
Step 18930 | loss: 3.007421 | lr:6.0081e-05 | norm 0.3226 | dt 337.74ms | 1552342.29 tokens/sec
Step 18931 | loss: 3.109292 | lr:6.0080e-05 | norm 0.3221 | dt 338.21ms | 1550186.50 tokens/sec
Step 18932 | loss: 3.059215 | lr:6.0079e-05 | norm 0.3181 | dt 338.88ms | 1547098.96 tokens/sec
Step 18933 | loss: 3.046262 | lr:6.0077e-05 | norm 0.3192 | dt 337.58ms | 1553084.53 tokens/sec
Step 18934 | loss: 3.116038 | lr:6.0076e-05 | norm 0.3124 | dt 338.34ms | 1549567.13 tokens/sec
Step 18935 | loss: 3.002621 | lr:6.0075e-05 | norm 0.3208 | dt 338.49ms | 1548886.07 tokens/sec
Step 18936 | loss: 3.075243 | lr:6.0074e-05 | norm 0.3219 | dt 337.62ms | 1552886.02 tokens/sec
Step 18937 | loss: 2.989770 | lr:6.0073e-05 | norm 0.2861 | dt 337.75ms | 1552298.46 tokens/sec
Step 18938 | loss: 3.052421 | lr:6.0072e-05 | norm 0.3145 | dt 337.79ms | 1552106.72 tokens/sec
Step 18939 | loss: 3.009343 | lr:6.0071e-05 | norm 0.3083 | dt 338.06ms | 1550871.98 tokens/sec
Step 18940 | loss: 3.036249 | lr:6.0070e-05 | norm 0.3076 | dt 338.77ms | 1547603.07 tokens/sec
Step 18941 | loss: 3.037250 | lr:6.0069e-05 | norm 0.3233 | dt 337.46ms | 1553618.89 tokens/sec
Step 18942 | loss: 2.980727 | lr:6.0068e-05 | norm 0.3041 | dt 338.13ms | 1550541.74 tokens/sec
Step 18943 | loss: 3.043650 | lr:6.0067e-05 | norm 0.2883 | dt 338.69ms | 1547982.19 tokens/sec
Step 18944 | loss: 2.963234 | lr:6.0066e-05 | norm 0.3075 | dt 338.10ms | 1550672.94 tokens/sec
Step 18945 | loss: 2.993630 | lr:6.0065e-05 | norm 0.3094 | dt 338.04ms | 1550943.08 tokens/sec
Step 18946 | loss: 3.062484 | lr:6.0064e-05 | norm 0.3109 | dt 337.98ms | 1551234.10 tokens/sec
Step 18947 | loss: 3.010390 | lr:6.0063e-05 | norm 0.2934 | dt 337.71ms | 1552495.73 tokens/sec
Step 18948 | loss: 3.011899 | lr:6.0062e-05 | norm 0.3276 | dt 338.09ms | 1550730.90 tokens/sec
Step 18949 | loss: 3.011292 | lr:6.0061e-05 | norm 0.3069 | dt 337.85ms | 1551827.42 tokens/sec
Step 18950 | loss: 3.007638 | lr:6.0060e-05 | norm 0.3040 | dt 338.07ms | 1550830.42 tokens/sec
Step 18951 | loss: 2.987433 | lr:6.0059e-05 | norm 0.3122 | dt 337.54ms | 1553251.27 tokens/sec
Step 18952 | loss: 2.996271 | lr:6.0058e-05 | norm 0.2934 | dt 338.70ms | 1547962.58 tokens/sec
Step 18953 | loss: 3.074905 | lr:6.0057e-05 | norm 0.3452 | dt 338.75ms | 1547715.27 tokens/sec
Step 18954 | loss: 3.138261 | lr:6.0056e-05 | norm 0.3114 | dt 338.01ms | 1551091.86 tokens/sec
Step 18955 | loss: 3.087314 | lr:6.0055e-05 | norm 0.2965 | dt 338.26ms | 1549941.75 tokens/sec
Step 18956 | loss: 3.107428 | lr:6.0054e-05 | norm 0.3442 | dt 338.62ms | 1548322.25 tokens/sec
Step 18957 | loss: 3.108774 | lr:6.0053e-05 | norm 0.3183 | dt 338.42ms | 1549221.07 tokens/sec
Step 18958 | loss: 3.121826 | lr:6.0052e-05 | norm 0.3331 | dt 337.19ms | 1554861.32 tokens/sec
Step 18959 | loss: 3.041043 | lr:6.0051e-05 | norm 0.3124 | dt 337.43ms | 1553789.05 tokens/sec
Step 18960 | loss: 3.112711 | lr:6.0050e-05 | norm 0.3120 | dt 338.09ms | 1550730.90 tokens/sec
Step 18961 | loss: 3.083909 | lr:6.0050e-05 | norm 0.2979 | dt 338.75ms | 1547702.19 tokens/sec
Step 18962 | loss: 3.155316 | lr:6.0049e-05 | norm 0.3171 | dt 337.06ms | 1555451.91 tokens/sec
Step 18963 | loss: 3.058307 | lr:6.0048e-05 | norm 0.3245 | dt 337.45ms | 1553692.44 tokens/sec
Step 18964 | loss: 3.116130 | lr:6.0047e-05 | norm 0.3106 | dt 338.00ms | 1551137.81 tokens/sec
Step 18965 | loss: 3.062318 | lr:6.0046e-05 | norm 0.3042 | dt 338.74ms | 1547771.91 tokens/sec
Step 18966 | loss: 3.052688 | lr:6.0045e-05 | norm 0.2982 | dt 337.65ms | 1552758.82 tokens/sec
Step 18967 | loss: 3.029635 | lr:6.0044e-05 | norm 0.3061 | dt 337.13ms | 1555168.11 tokens/sec
Step 18968 | loss: 3.058393 | lr:6.0044e-05 | norm 0.2967 | dt 338.04ms | 1550944.17 tokens/sec
Step 18969 | loss: 3.043070 | lr:6.0043e-05 | norm 0.3151 | dt 338.28ms | 1549855.45 tokens/sec
Step 18970 | loss: 2.980596 | lr:6.0042e-05 | norm 0.2907 | dt 337.76ms | 1552229.43 tokens/sec
Step 18971 | loss: 3.002113 | lr:6.0041e-05 | norm 0.2959 | dt 338.22ms | 1550139.51 tokens/sec
Step 18972 | loss: 3.063529 | lr:6.0040e-05 | norm 0.2925 | dt 337.51ms | 1553401.59 tokens/sec
Step 18973 | loss: 3.125247 | lr:6.0040e-05 | norm 0.2977 | dt 337.47ms | 1553576.09 tokens/sec
Step 18974 | loss: 3.052192 | lr:6.0039e-05 | norm 0.2892 | dt 337.89ms | 1551663.17 tokens/sec
Step 18975 | loss: 3.132106 | lr:6.0038e-05 | norm 0.2897 | dt 337.71ms | 1552492.44 tokens/sec
Step 18976 | loss: 3.031626 | lr:6.0037e-05 | norm 0.2976 | dt 337.71ms | 1552485.86 tokens/sec
Step 18977 | loss: 3.044095 | lr:6.0036e-05 | norm 0.2754 | dt 337.26ms | 1554563.44 tokens/sec
Step 18978 | loss: 3.046127 | lr:6.0036e-05 | norm 0.2933 | dt 338.27ms | 1549888.22 tokens/sec
Step 18979 | loss: 3.044821 | lr:6.0035e-05 | norm 0.2827 | dt 337.84ms | 1551892.04 tokens/sec
Step 18980 | loss: 3.050088 | lr:6.0034e-05 | norm 0.2926 | dt 338.53ms | 1548728.99 tokens/sec
Step 18981 | loss: 3.056499 | lr:6.0033e-05 | norm 0.3399 | dt 338.16ms | 1550401.81 tokens/sec
Step 18982 | loss: 3.027314 | lr:6.0033e-05 | norm 0.3126 | dt 338.50ms | 1548878.43 tokens/sec
Step 18983 | loss: 3.019025 | lr:6.0032e-05 | norm 0.3248 | dt 337.74ms | 1552337.91 tokens/sec
Step 18984 | loss: 3.079490 | lr:6.0031e-05 | norm 0.2892 | dt 338.43ms | 1549164.32 tokens/sec
Step 18985 | loss: 3.032886 | lr:6.0031e-05 | norm 0.3168 | dt 337.97ms | 1551270.21 tokens/sec
Step 18986 | loss: 3.058410 | lr:6.0030e-05 | norm 0.2941 | dt 337.81ms | 1552030.04 tokens/sec
Step 18987 | loss: 3.054034 | lr:6.0029e-05 | norm 0.2969 | dt 339.63ms | 1543710.45 tokens/sec
Step 18988 | loss: 2.983663 | lr:6.0029e-05 | norm 0.2872 | dt 338.49ms | 1548891.53 tokens/sec
Step 18989 | loss: 2.982287 | lr:6.0028e-05 | norm 0.2841 | dt 339.21ms | 1545634.21 tokens/sec
Step 18990 | loss: 3.034356 | lr:6.0027e-05 | norm 0.3301 | dt 339.34ms | 1545026.08 tokens/sec
Step 18991 | loss: 2.980680 | lr:6.0027e-05 | norm 0.2905 | dt 338.18ms | 1550340.60 tokens/sec
Step 18992 | loss: 2.955107 | lr:6.0026e-05 | norm 0.3064 | dt 338.02ms | 1551049.19 tokens/sec
Step 18993 | loss: 3.002771 | lr:6.0025e-05 | norm 0.3104 | dt 339.82ms | 1542862.39 tokens/sec
Step 18994 | loss: 3.078662 | lr:6.0025e-05 | norm 0.3129 | dt 338.53ms | 1548708.27 tokens/sec
Step 18995 | loss: 3.010988 | lr:6.0024e-05 | norm 0.2956 | dt 338.41ms | 1549286.56 tokens/sec
Step 18996 | loss: 3.018805 | lr:6.0023e-05 | norm 0.2816 | dt 338.23ms | 1550079.41 tokens/sec
Step 18997 | loss: 2.998104 | lr:6.0023e-05 | norm 0.3182 | dt 338.79ms | 1547547.53 tokens/sec
Step 18998 | loss: 3.017870 | lr:6.0022e-05 | norm 0.3169 | dt 338.69ms | 1547970.21 tokens/sec
Step 18999 | loss: 3.047139 | lr:6.0022e-05 | norm 0.3136 | dt 339.15ms | 1545906.94 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 19000: 3.0763
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3074/10042=0.3061


ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not very happy about that. I think my next question came earlier this day when I decided to ask how
rank 3 sample 1 >Hello, I'm a language model, and this was a great step-by-step tutorial for me to get started.
Please check back for further details
rank 3 sample 2 >Hello, I'm a language model, and you've always imagined yourself there. As much as I was a little disappointed that I wasn't a good match,
rank 3 sample 3 >Hello, I'm a language model, and i'm just kind of an idiot, but you could be the one who decided i was born just because i'm




ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but my first language model is LANGUAGERS, and in fact I have a lot of these. There's
rank 5 sample 1 >Hello, I'm a language model, can anyone help me better if this is something he's working on?
Annotating is a useful but still not
rank 5 sample 2 >Hello, I'm a language model, so I would love to see how other people might be able to speak those words! 🙂
Please note that this project
rank 5 sample 3 >Hello, I'm a language model, language model is one of the most important tools to have to use. It provides structure for language with a set of parameters




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I like to learn new languages. Of course, I'm not too fluent by language; one has a lot to
rank 7 sample 1 >Hello, I'm a language model, trying to solve a problem by making mistakes. I understand most of the problems, but some of them aren't so difficult
rank 7 sample 2 >Hello, I'm a language model, and I want to develop a machine that is able to talk to its audience.
So, I'm going to be
rank 7 sample 3 >Hello, I'm a language model, so this is a very helpful topic you can write about. I hope you enjoyed reading!
1. What is V




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, learning and writing with Python.
It is very easy, I'm so used to it. I'm like, if
rank 2 sample 1 >Hello, I'm a language model, so I hate it.
I need these things written in the HTML that I'm going to be able to read on
rank 2 sample 2 >Hello, I'm a language model, so I need to be able to create patterns that I can associate with an image of. I will assume that we know
rank 2 sample 3 >Hello, I'm a language model, but if I'm a programmer, I wouldn't understand why anyone would want to read it: one thing, it's




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and this is my favourite language. Is there anything else I couldn't do this with this book:
This is a
rank 1 sample 1 >Hello, I'm a language model, a programmer and a programmer. I can work with anything as long as I write program-ing logic, and I can
rank 1 sample 2 >Hello, I'm a language model, but then it's not gonna work.
I'm gonna work in C++, but then that would make you use
rank 1 sample 3 >Hello, I'm a language model, and I'm really excited to see if another language model's actually make more significant contributions to future learning.
If you




ddp_rank 4: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I know that you might be wondering who's at the top of the list. It'll be an expert. I
rank 4 sample 1 >Hello, I'm a language model, having taught myself with this course is definitely something that is difficult to manage or manage on our part. In fact, it
rank 4 sample 2 >Hello, I'm a language model, I will first build on how to get to the point where there's a point to where there's exactly one language,
rank 4 sample 3 >Hello, I'm a language model, and it must mean the same thing now? What we got up with last year is that the people that are speaking the




ddp_rank 6: ####### Printing generated samples ####### 

rank 6 sample 0 >Hello, I'm a language model, in terms of the most difficult syntax.
I am going to tell the code:
If you want, you do
rank 6 sample 1 >Hello, I'm a language model, which means that I'm a programmer and in my book, I'm also a language model, but it has a lot
rank 6 sample 2 >Hello, I'm a language model, but not exactly. For the purposes of this discussion, I will give an update on the grammar of one of my favourite
rank 6 sample 3 >Hello, I'm a language model, and so are I. I think that a good language model is the one where the learner is on- and off




ddp_rank 0: ####### Printing generated samples ####### 

rank 0 sample 0 >Hello, I'm a language model, and I think I can say a lot when I get the word of the sentence. And with it's kind of interesting
rank 0 sample 1 >Hello, I'm a language model, so to add a new piece to this, let's run the
for_name. Then we start to read the
rank 0 sample 2 >Hello, I'm a language model, so I like to model, this looks like
Here's where we are going to find the
subset, for
rank 0 sample 3 >Hello, I'm a language model, so please find the best language to be a generalist.
And you're done. What do you think about writing


Step 19000 | loss: 3.068558 | lr:6.0021e-05 | norm 0.3104 | dt 18783.86ms | 27911.63 tokens/sec
Step 19001 | loss: 3.124326 | lr:6.0020e-05 | norm 0.3165 | dt 1014.42ms | 516835.95 tokens/sec
Step 19002 | loss: 3.106111 | lr:6.0020e-05 | norm 0.2981 | dt 335.32ms | 1563529.73 tokens/sec
Step 19003 | loss: 3.086495 | lr:6.0019e-05 | norm 0.2860 | dt 336.09ms | 1559973.76 tokens/sec
Step 19004 | loss: 3.104812 | lr:6.0019e-05 | norm 0.3343 | dt 337.66ms | 1552718.26 tokens/sec
Step 19005 | loss: 3.145634 | lr:6.0018e-05 | norm 0.3005 | dt 336.21ms | 1559397.42 tokens/sec
Step 19006 | loss: 3.053035 | lr:6.0018e-05 | norm 0.2930 | dt 335.88ms | 1560929.38 tokens/sec
Step 19007 | loss: 3.071708 | lr:6.0017e-05 | norm 0.3735 | dt 337.15ms | 1555080.13 tokens/sec
Step 19008 | loss: 3.121806 | lr:6.0017e-05 | norm 0.3483 | dt 341.69ms | 1534404.96 tokens/sec
Step 19009 | loss: 3.065246 | lr:6.0016e-05 | norm 0.3167 | dt 335.43ms | 1563044.07 tokens/sec
Step 19010 | loss: 3.035798 | lr:6.0016e-05 | norm 0.3090 | dt 337.10ms | 1555268.20 tokens/sec
Step 19011 | loss: 3.128513 | lr:6.0015e-05 | norm 0.3406 | dt 336.55ms | 1557811.08 tokens/sec
Step 19012 | loss: 2.973063 | lr:6.0015e-05 | norm 0.3176 | dt 335.77ms | 1561466.94 tokens/sec
Step 19013 | loss: 3.013621 | lr:6.0014e-05 | norm 0.3213 | dt 337.25ms | 1554618.39 tokens/sec
Step 19014 | loss: 3.091755 | lr:6.0014e-05 | norm 0.3228 | dt 336.74ms | 1556954.08 tokens/sec
Step 19015 | loss: 3.037552 | lr:6.0013e-05 | norm 0.3247 | dt 336.04ms | 1560184.05 tokens/sec
Step 19016 | loss: 3.048972 | lr:6.0013e-05 | norm 0.3167 | dt 336.62ms | 1557501.04 tokens/sec
Step 19017 | loss: 3.057752 | lr:6.0012e-05 | norm 0.3148 | dt 336.79ms | 1556741.35 tokens/sec
Step 19018 | loss: 3.079060 | lr:6.0012e-05 | norm 0.3029 | dt 336.56ms | 1557786.80 tokens/sec
Step 19019 | loss: 3.091704 | lr:6.0012e-05 | norm 0.3023 | dt 337.15ms | 1555069.13 tokens/sec
Step 19020 | loss: 3.018567 | lr:6.0011e-05 | norm 0.3222 | dt 337.52ms | 1553362.09 tokens/sec
Step 19021 | loss: 3.028541 | lr:6.0011e-05 | norm 0.3084 | dt 336.78ms | 1556787.64 tokens/sec
Step 19022 | loss: 3.114599 | lr:6.0010e-05 | norm 0.3182 | dt 337.66ms | 1552722.64 tokens/sec
Step 19023 | loss: 3.054271 | lr:6.0010e-05 | norm 0.2943 | dt 338.72ms | 1547843.81 tokens/sec
Step 19024 | loss: 3.081367 | lr:6.0009e-05 | norm 0.3217 | dt 336.50ms | 1558058.32 tokens/sec
Step 19025 | loss: 3.050966 | lr:6.0009e-05 | norm 0.2943 | dt 337.00ms | 1555739.13 tokens/sec
Step 19026 | loss: 3.026780 | lr:6.0009e-05 | norm 0.2940 | dt 337.05ms | 1555539.94 tokens/sec
Step 19027 | loss: 2.980828 | lr:6.0008e-05 | norm 0.3219 | dt 336.21ms | 1559414.01 tokens/sec
Step 19028 | loss: 3.018355 | lr:6.0008e-05 | norm 0.2889 | dt 336.71ms | 1557074.24 tokens/sec
Step 19029 | loss: 3.024793 | lr:6.0008e-05 | norm 0.3009 | dt 337.35ms | 1554119.58 tokens/sec
Step 19030 | loss: 3.059216 | lr:6.0007e-05 | norm 0.3015 | dt 338.57ms | 1548533.77 tokens/sec
Step 19031 | loss: 2.960344 | lr:6.0007e-05 | norm 0.2964 | dt 337.55ms | 1553208.49 tokens/sec
Step 19032 | loss: 3.051506 | lr:6.0007e-05 | norm 0.2977 | dt 337.08ms | 1555372.70 tokens/sec
Step 19033 | loss: 3.057088 | lr:6.0006e-05 | norm 0.3068 | dt 337.66ms | 1552712.77 tokens/sec
Step 19034 | loss: 2.967374 | lr:6.0006e-05 | norm 0.3041 | dt 337.72ms | 1552413.53 tokens/sec
Step 19035 | loss: 2.993704 | lr:6.0006e-05 | norm 0.2972 | dt 336.84ms | 1556466.99 tokens/sec
Step 19036 | loss: 2.992461 | lr:6.0005e-05 | norm 0.2741 | dt 336.89ms | 1556252.19 tokens/sec
Step 19037 | loss: 3.048051 | lr:6.0005e-05 | norm 0.3224 | dt 337.62ms | 1552892.60 tokens/sec
Step 19038 | loss: 3.032737 | lr:6.0005e-05 | norm 0.2926 | dt 337.35ms | 1554115.19 tokens/sec
Step 19039 | loss: 3.039609 | lr:6.0005e-05 | norm 0.2812 | dt 337.07ms | 1555439.81 tokens/sec
Step 19040 | loss: 3.009449 | lr:6.0004e-05 | norm 0.3189 | dt 337.71ms | 1552477.09 tokens/sec
Step 19041 | loss: 3.028307 | lr:6.0004e-05 | norm 0.2966 | dt 338.33ms | 1549640.29 tokens/sec
Step 19042 | loss: 3.005408 | lr:6.0004e-05 | norm 0.2857 | dt 337.46ms | 1553643.04 tokens/sec
Step 19043 | loss: 3.024103 | lr:6.0004e-05 | norm 0.3172 | dt 337.67ms | 1552679.88 tokens/sec
Step 19044 | loss: 3.010257 | lr:6.0003e-05 | norm 0.2974 | dt 337.58ms | 1553071.37 tokens/sec
Step 19045 | loss: 3.073132 | lr:6.0003e-05 | norm 0.3244 | dt 338.00ms | 1551147.66 tokens/sec
Step 19046 | loss: 3.074401 | lr:6.0003e-05 | norm 0.3479 | dt 338.05ms | 1550922.30 tokens/sec
Step 19047 | loss: 3.106128 | lr:6.0003e-05 | norm 0.3314 | dt 338.02ms | 1551059.04 tokens/sec
Step 19048 | loss: 3.118999 | lr:6.0002e-05 | norm 0.3471 | dt 338.26ms | 1549974.52 tokens/sec
Step 19049 | loss: 3.106361 | lr:6.0002e-05 | norm 0.3320 | dt 337.23ms | 1554697.52 tokens/sec
Step 19050 | loss: 3.089941 | lr:6.0002e-05 | norm 0.3202 | dt 337.82ms | 1551985.13 tokens/sec
Step 19051 | loss: 3.118211 | lr:6.0002e-05 | norm 0.3247 | dt 339.07ms | 1546235.22 tokens/sec
Step 19052 | loss: 3.151944 | lr:6.0002e-05 | norm 0.3245 | dt 337.98ms | 1551236.29 tokens/sec
Step 19053 | loss: 3.136093 | lr:6.0002e-05 | norm 0.3018 | dt 339.53ms | 1544149.47 tokens/sec
Step 19054 | loss: 3.113163 | lr:6.0001e-05 | norm 0.3238 | dt 339.52ms | 1544183.08 tokens/sec
Step 19055 | loss: 3.116204 | lr:6.0001e-05 | norm 0.3269 | dt 339.24ms | 1545499.51 tokens/sec
Step 19056 | loss: 3.104094 | lr:6.0001e-05 | norm 0.3136 | dt 338.40ms | 1549306.21 tokens/sec
Step 19057 | loss: 3.097383 | lr:6.0001e-05 | norm 0.3044 | dt 339.36ms | 1544913.19 tokens/sec
Step 19058 | loss: 3.080388 | lr:6.0001e-05 | norm 0.3259 | dt 341.68ms | 1534463.84 tokens/sec
Step 19059 | loss: 3.029080 | lr:6.0001e-05 | norm 0.3106 | dt 342.74ms | 1529679.76 tokens/sec
Step 19060 | loss: 3.088866 | lr:6.0001e-05 | norm 0.3113 | dt 338.17ms | 1550388.69 tokens/sec
Step 19061 | loss: 3.022632 | lr:6.0001e-05 | norm 0.3085 | dt 339.06ms | 1546315.68 tokens/sec
Step 19062 | loss: 3.070888 | lr:6.0000e-05 | norm 0.2898 | dt 339.19ms | 1545689.62 tokens/sec
Step 19063 | loss: 3.092277 | lr:6.0000e-05 | norm 0.3202 | dt 341.95ms | 1533233.48 tokens/sec
Step 19064 | loss: 3.105639 | lr:6.0000e-05 | norm 0.3419 | dt 339.43ms | 1544634.30 tokens/sec
Step 19065 | loss: 3.039515 | lr:6.0000e-05 | norm 0.2921 | dt 339.59ms | 1543869.77 tokens/sec
Step 19066 | loss: 3.018067 | lr:6.0000e-05 | norm 0.3092 | dt 339.13ms | 1545990.63 tokens/sec
Step 19067 | loss: 3.023188 | lr:6.0000e-05 | norm 0.3181 | dt 338.43ms | 1549189.42 tokens/sec
Step 19068 | loss: 3.121207 | lr:6.0000e-05 | norm 0.3270 | dt 339.32ms | 1545094.47 tokens/sec
Step 19069 | loss: 3.051602 | lr:6.0000e-05 | norm 0.2949 | dt 339.11ms | 1546051.50 tokens/sec
Step 19070 | loss: 3.055462 | lr:6.0000e-05 | norm 0.2887 | dt 339.19ms | 1545717.87 tokens/sec
Step 19071 | loss: 3.072696 | lr:6.0000e-05 | norm 0.3148 | dt 339.49ms | 1544354.43 tokens/sec
ddp_rank 1: Evaluating model on HellaSwag dataset
ddp_rank 2: Evaluating model on HellaSwag dataset
ddp_rank 6: Evaluating model on HellaSwag dataset
ddp_rank 3: Evaluating model on HellaSwag dataset
ddp_rank 7: Evaluating model on HellaSwag dataset
ddp_rank 5: Evaluating model on HellaSwag dataset
ddp_rank 4: Evaluating model on HellaSwag dataset
Validation loss at step 19072: 3.0748
ddp_rank 0: Evaluating model on HellaSwag dataset
HellaSwag Accuracy: 3067/10042=0.3054


ddp_rank 5: ####### Printing generated samples ####### 

rank 5 sample 0 >Hello, I'm a language model, but my first language model is French.<|endoftext|>A little over a quarter of the people in the world are obese. That
rank 5 sample 1 >Hello, I'm a language model, because language models are those languages we see when kids learn. And some of these have to be used because our language models
rank 5 sample 2 >Hello, I'm a language model, so I was thinking again about creating your own language. I can only guess what it can do, but I'm gonna
rank 5 sample 3 >Hello, I'm a language model, or one who knows how to do things, I just need to see how much fun and satisfaction will be created when reading




ddp_rank 2: ####### Printing generated samples ####### 

rank 2 sample 0 >Hello, I'm a language model, someone with an old laptop, I think it's great, and I do have some bad habits, but we're pretty
rank 2 sample 1 >Hello, I'm a language model, so I understand why some people make decisions rather than solve problems and have other people do the same thing. I'm just
rank 2 sample 2 >Hello, I'm a language model, so I need to be able to teach him English. I think he understands better in English. However, I want to
rank 2 sample 3 >Hello, I'm a language model, but a child of my age. I teach two people. A child of his age and this seems like it would be




ddp_rank 3: ####### Printing generated samples ####### 

rank 3 sample 0 >Hello, I'm a language model, and I'm not even an engineer... And, the most crucial part, actually, you just have to understand how things
rank 3 sample 1 >Hello, I'm a language model, and this was my first talk of it. In English, we have to explain what we're talking about. If someone
rank 3 sample 2 >Hello, I'm a language model, and you know, most of the people using Linux don't do anything. That's why I came to the post where
rank 3 sample 3 >Hello, I'm a language model, and your blog's code is different. I don't really care how it works, but in the "general" sense




ddp_rank 7: ####### Printing generated samples ####### 

rank 7 sample 0 >Hello, I'm a language model, and I was wondering why my teacher was unable to tell me how to approach it after it was decided. I'm sorry
rank 7 sample 1 >Hello, I'm a language model, we see this in the course called Introduction to Machine Learning Basics. But, I haven't had a better experience in it
rank 7 sample 2 >Hello, I'm a language model, and I've decided to take out my students' project in my classes - I've got a lot of projects I'm
rank 7 sample 3 >Hello, I'm a language model, so this is a simple and really efficient way to help you. I really like to explain it very simple.
Well




ddp_rank 1: ####### Printing generated samples ####### 

rank 1 sample 0 >Hello, I'm a language model, and here's a cool example of different typesetting in python:
import ld = python # ld.py
rank 1 sample 1 >Hello, I'm a language model, a person, and a person's computer. I'm talking about this software package. To tell someone what I'm talking
rank 1 sample 2 >Hello, I'm a language model, but today's video is about the basics of how to do word searches, but I'm a native English language, and


ddp_rank 0: ####### Printing generated samples ####### 

rank 1 sample 3 >Hello, I'm a language model, and I'm interested to discover the languages
that are able to read data through the human brains.
I was wondering


rank 0 sample 0 >Hello, I'm a language model, and I'd like you to take a short peek at how it worked. Let's go along a few more steps:
rank 0 sample 1 >Hello, I'm a language model, so we're going to just start speaking English and have these pictures that shows how we are going to sound like native speakers
rank 0 sample 2 >Hello, I'm a language model, so I figured it would be appropriate for my post.
Well this is a lot more complex than that. I do
rank 0 sample 3 >Hello, I'm a language model, so today I'm going to be giving a general introduction to the subject. So this is one of the first ones...





ddp_rank 4: ####### Printing generated samples ####### 


ddp_rank 6: ####### Printing generated samples ####### 

rank 4 sample 0 >Hello, I'm a language model, so I know that what I'm going to be doing is that I'd like to learn one of the different types ofrank 6 sample 0 >Hello, I'm a language model, who would you like to explain some ideas for a tutorial. If you want to make a list of all of the concepts

rank 6 sample 1 >Hello, I'm a language model, and I have a project to work on using the library. I'm just going to write the script and run it.
rank 4 sample 1 >Hello, I'm a language model, am not a biologist?
This is true for many programs, but is a true science, and that's quite the
rank 6 sample 2 >Hello, I'm a language model, but that's a long shot! We're trying to make it better for all of us.
First, if we
rank 4 sample 2 >Hello, I'm a language model, I believe you and the language model that's always been around the internet. You'd better call yourself just a programming model
rank 6 sample 3 >Hello, I'm a language model, and my blog is based on one of my favorite languages. I've been learning a few web programming languages (a core
rank 4 sample 3 >Hello, I'm a language model, and the following post is from my earlier version for another forum: http://blog.google.com/blog/





Step 19072 | loss: 3.039924 | lr:6.0000e-05 | norm 0.2895 | dt 15568.91ms | 33675.32 tokens/sec
Saving model to models
